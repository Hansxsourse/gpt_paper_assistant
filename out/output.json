{
    "2506.14686": {
        "authors": [
            "Xi Chen",
            "Hengshuang Zhao"
        ],
        "title": "FocalClick-XL: Towards Unified and High-quality Interactive Segmentation",
        "abstract": "arXiv:2506.14686v1 Announce Type: new  Abstract: Interactive segmentation enables users to extract binary masks of target objects through simple interactions such as clicks, scribbles, and boxes. However, existing methods often support only limited interaction forms and struggle to capture fine details. In this paper, we revisit the classical coarse-to-fine design of FocalClick and introduce significant extensions. Inspired by its multi-stage strategy, we propose a novel pipeline, FocalClick-XL, to address these challenges simultaneously. Following the emerging trend of large-scale pretraining, we decompose interactive segmentation into meta-tasks that capture different levels of information -- context, object, and detail -- assigning a dedicated subnet to each level.This decomposition allows each subnet to undergo scaled pretraining with independent data and supervision, maximizing its effectiveness. To enhance flexibility, we share context- and detail-level information across different interaction forms as common knowledge while introducing a prompting layer at the object level to encode specific interaction types. As a result, FocalClick-XL achieves state-of-the-art performance on click-based benchmarks and demonstrates remarkable adaptability to diverse interaction formats, including boxes, scribbles, and coarse masks. Beyond binary mask generation, it is also capable of predicting alpha mattes with fine-grained details, making it a versatile and powerful tool for interactive segmentation.",
        "arxiv_id": "2506.14686",
        "ARXIVID": "2506.14686",
        "COMMENT": "Matches criteria 3 closely as it predicts alpha mattes with fine-grained details, which is relevant to image matting.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.14322": {
        "authors": [
            "Avigail Cohen Rimon",
            "Mirela Ben-Chen",
            "Or Litany"
        ],
        "title": "FRIDU: Functional Map Refinement with Guided Image Diffusion",
        "abstract": "arXiv:2506.14322v1 Announce Type: new  Abstract: We propose a novel approach for refining a given correspondence map between two shapes. A correspondence map represented as a functional map, namely a change of basis matrix, can be additionally treated as a 2D image. With this perspective, we train an image diffusion model directly in the space of functional maps, enabling it to generate accurate maps conditioned on an inaccurate initial map. The training is done purely in the functional space, and thus is highly efficient. At inference time, we use the pointwise map corresponding to the current functional map as guidance during the diffusion process. The guidance can additionally encourage different functional map objectives, such as orthogonality and commutativity with the Laplace-Beltrami operator. We show that our approach is competitive with state-of-the-art methods of map refinement and that guided diffusion models provide a promising pathway to functional map processing.",
        "arxiv_id": "2506.14322",
        "ARXIVID": "2506.14322",
        "COMMENT": "Does not match any specific criteria closely. Focuses on map refinement using diffusion models, but not in a multi-task or unified framework.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.14399": {
        "authors": [
            "Tian Xia",
            "Fabio De Sousa Ribeiro",
            "Rajat R Rasal",
            "Avinash Kori",
            "Raghav Mehta",
            "Ben Glocker"
        ],
        "title": "Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models",
        "abstract": "arXiv:2506.14399v1 Announce Type: new  Abstract: Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.",
        "arxiv_id": "2506.14399",
        "ARXIVID": "2506.14399",
        "COMMENT": "Does not match any specific criteria closely. Focuses on counterfactual image generation with diffusion models, but not in a multi-task or unified framework.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}