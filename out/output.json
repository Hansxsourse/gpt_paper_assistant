{
    "2508.20020": {
        "authors": [
            "Yuhao Chen",
            "Shubin Chen",
            "Liang Lin",
            "Guangrun Wang"
        ],
        "title": "GS: Generative Segmentation via Label Diffusion",
        "abstract": "arXiv:2508.20020v1 Announce Type: new  Abstract: Language-driven image segmentation is a fundamental task in vision-language understanding, requiring models to segment regions of an image corresponding to natural language expressions. Traditional methods approach this as a discriminative problem, assigning each pixel to foreground or background based on semantic alignment. Recently, diffusion models have been introduced to this domain, but existing approaches remain image-centric: they either (i) use image diffusion models as visual feature extractors, (ii) synthesize segmentation data via image generation to train discriminative models, or (iii) perform diffusion inversion to extract attention cues from pre-trained image diffusion models-thereby treating segmentation as an auxiliary process. In this paper, we propose GS (Generative Segmentation), a novel framework that formulates segmentation itself as a generative task via label diffusion. Instead of generating images conditioned on label maps and text, GS reverses the generative process: it directly generates segmentation masks from noise, conditioned on both the input image and the accompanying language description. This paradigm makes label generation the primary modeling target, enabling end-to-end training with explicit control over spatial and semantic fidelity. To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic Narrative Grounding (PNG), a representative and challenging benchmark for multimodal segmentation that requires panoptic-level reasoning guided by narrative captions. Experimental results show that GS significantly outperforms existing discriminative and diffusion-based methods, setting a new state-of-the-art for language-driven segmentation.",
        "arxiv_id": "2508.20020",
        "ARXIVID": "2508.20020",
        "COMMENT": "Matches criterion 2 closely by proposing a diffusion model for generative segmentation.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2508.19320": {
        "authors": [
            "Ming Chen",
            "Liyuan Cui",
            "Wenyuan Zhang",
            "Haoxian Zhang",
            "Yan Zhou",
            "Xiaohan Li",
            "Xiaoqiang Liu",
            "Pengfei Wan"
        ],
        "title": "MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation",
        "abstract": "arXiv:2508.19320v1 Announce Type: new  Abstract: Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with high latency, heavy computational cost, and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64$\\times$ reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability.",
        "arxiv_id": "2508.19320",
        "ARXIVID": "2508.19320",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}