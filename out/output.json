{
    "2601.05138": {
        "authors": [
            "Sixiao Zheng",
            "Minghao Yin",
            "Wenbo Hu",
            "Xiaoyu Li",
            "Ying Shan",
            "Yanwei Fu"
        ],
        "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
        "abstract": "arXiv:2601.05138v1 Announce Type: new  Abstract: Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.",
        "arxiv_id": "2601.05138",
        "ARXIVID": "2601.05138",
        "COMMENT": "The paper does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.04339": {
        "authors": [
            "Jiahui Chen",
            "Philippe Hansen-Estruch",
            "Xiaochuang Han",
            "Yushi Hu",
            "Emily Dinan",
            "Amita Kamath",
            "Michal Drozdzal",
            "Reyhane Askari-Hemmat",
            "Luke Zettlemoyer",
            "Marjan Ghazvininejad"
        ],
        "title": "Unified Text-Image Generation with Weakness-Targeted Post-Training",
        "abstract": "arXiv:2601.04339v1 Announce Type: new  Abstract: Unified multimodal generation architectures that jointly produce text and images have recently emerged as a promising direction for text-to-image (T2I) synthesis. However, many existing systems rely on explicit modality switching, generating reasoning text before switching manually to image generation. This separate, sequential inference process limits cross-modal coupling and prohibits automatic multimodal generation. This work explores post-training to achieve fully unified text-image generation, where models autonomously transition from textual reasoning to visual synthesis within a single inference process. We examine the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training. We additionally explore different post-training data strategies, showing that a targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data. Using offline, reward-weighted post-training with fully self-generated synthetic data, our approach enables improvements in multimodal image generation across four diverse T2I benchmarks, demonstrating the effectiveness of reward-weighting both modalities and strategically designed post-training data.",
        "arxiv_id": "2601.04339",
        "ARXIVID": "2601.04339",
        "COMMENT": "The paper does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2601.04706": {
        "authors": [
            "Yanbing Zeng",
            "Jia Wang",
            "Hanghang Ma",
            "Junqiang Wu",
            "Jie Zhu",
            "Xiaoming Wei",
            "Jie Hu"
        ],
        "title": "Forge-and-Quench: Enhancing Image Generation for Higher Fidelity in Unified Multimodal Models",
        "abstract": "arXiv:2601.04706v1 Announce Type: new  Abstract: Integrating image generation and understanding into a single framework has become a pivotal goal in the multimodal domain. However, how understanding can effectively assist generation has not been fully explored. Unlike previous works that focus on leveraging reasoning abilities and world knowledge from understanding models, this paper introduces a novel perspective: leveraging understanding to enhance the fidelity and detail richness of generated images. To this end, we propose Forge-and-Quench, a new unified framework that puts this principle into practice. In the generation process of our framework, an MLLM first reasons over the entire conversational context, including text instructions, to produce an enhanced text instruction. This refined instruction is then mapped to a virtual visual representation, termed the Bridge Feature, via a novel Bridge Adapter. This feature acts as a crucial link, forging insights from the understanding model to quench and refine the generation process. It is subsequently injected into the T2I backbone as a visual guidance signal, alongside the enhanced text instruction that replaces the original input. To validate this paradigm, we conduct comprehensive studies on the design of the Bridge Feature and Bridge Adapter. Our framework demonstrates exceptional extensibility and flexibility, enabling efficient migration across different MLLM and T2I models with significant savings in training overhead, all without compromising the MLLM's inherent multimodal understanding capabilities. Experiments show that Forge-and-Quench significantly improves image fidelity and detail across multiple models, while also maintaining instruction-following accuracy and enhancing world knowledge application. Models and codes are available at https://github.com/YanbingZeng/Forge-and-Quench.",
        "arxiv_id": "2601.04706",
        "ARXIVID": "2601.04706",
        "COMMENT": "The paper does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}