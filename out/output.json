{
    "2502.19902": {
        "authors": [
            "Zaijing Li",
            "Yuquan Xie",
            "Rui Shao",
            "Gongwei Chen",
            "Dongmei Jiang",
            "Liqiang Nie"
        ],
        "title": "Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy",
        "abstract": "arXiv:2502.19902v1 Announce Type: new  Abstract: Building an agent that can mimic human behavior patterns to accomplish various open-world tasks is a long-term goal. To enable agents to effectively learn behavioral patterns across diverse tasks, a key challenge lies in modeling the intricate relationships among observations, actions, and language. To this end, we propose Optimus-2, a novel Minecraft agent that incorporates a Multimodal Large Language Model (MLLM) for high-level planning, alongside a Goal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP contains (1) an Action-guided Behavior Encoder that models causal relationships between observations and actions at each timestep, then dynamically interacts with the historical observation-action sequence, consolidating it into fixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with open-ended language instructions to predict actions auto-regressively. Moreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)} dataset, which contains 25,000 videos across 8 atomic tasks, providing about 30M goal-observation-action pairs. The automated construction method, along with the MGOA dataset, can contribute to the community's efforts to train Minecraft agents. Extensive experimental results demonstrate that Optimus-2 exhibits superior performance across atomic tasks, long-horizon tasks, and open-ended instruction tasks in Minecraft.",
        "arxiv_id": "2502.19902",
        "ARXIVID": "2502.19902",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (MGOA dataset) and a novel method (GOAP) for embodied AI in Minecraft.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2502.19500": {
        "authors": [
            "Konstantina Christakopoulou",
            "Iris Qu",
            "John Canny",
            "Andrew Goodridge",
            "Cj Adams",
            "Minmin Chen",
            "Maja Matari\\'c"
        ],
        "title": "Conversational Planning for Personal Plans",
        "abstract": "arXiv:2502.19500v1 Announce Type: new  Abstract: The language generation and reasoning capabilities of large language models (LLMs) have enabled conversational systems with impressive performance in a variety of tasks, from code generation, to composing essays, to passing STEM and legal exams, to a new paradigm for knowledge search. Besides those short-term use applications, LLMs are increasingly used to help with real-life goals or tasks that take a long time to complete, involving multiple sessions across days, weeks, months, or even years. Thus to enable conversational systems for long term interactions and tasks, we need language-based agents that can plan for long horizons. Traditionally, such capabilities were addressed by reinforcement learning agents with hierarchical planning capabilities. In this work, we explore a novel architecture where the LLM acts as the meta-controller deciding the agent's next macro-action, and tool use augmented LLM-based option policies execute the selected macro-action. We instantiate this framework for a specific set of macro-actions enabling adaptive planning for users' personal plans through conversation and follow-up questions collecting user feedback. We show how this paradigm can be applicable in scenarios ranging from tutoring for academic and non-academic tasks to conversational coaching for personal health plans.",
        "arxiv_id": "2502.19500",
        "ARXIVID": "2502.19500",
        "COMMENT": "Matches criterion 3 as it explores a novel architecture for long-horizon conversational planning in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.19610": {
        "authors": [
            "Matthew Toles",
            "Nikhil Balwani",
            "Rattandeep Singh",
            "Valentina Giulia Sartori Rodriguez",
            "Zhou Yu"
        ],
        "title": "Program Synthesis Dialog Agents for Interactive Decision-Making",
        "abstract": "arXiv:2502.19610v1 Announce Type: new  Abstract: Many real-world eligibility problems, ranging from medical diagnosis to tax planning, can be mapped to decision problems expressed in natural language, wherein a model must make a binary choice based on user features. Large-scale domains such as legal codes or frequently updated funding opportunities render human annotation (e.g., web forms or decision trees) impractical, highlighting the need for agents that can automatically assist in decision-making. Since relevant information is often only known to the user, it is crucial that these agents ask the right questions. As agents determine when to terminate a conversation, they face a trade-off between accuracy and the number of questions asked, a key metric for both user experience and cost. To evaluate this task, we propose BeNYfits, a new benchmark for determining user eligibility for multiple overlapping social benefits opportunities through interactive decision-making. Our experiments show that current language models struggle with frequent hallucinations, with GPT-4o scoring only 35.7 F1 using a ReAct-style chain-of-thought. To address this, we introduce ProADA, a novel approach that leverages program synthesis to assist in decision-making by mapping dialog planning to a code generation problem and using gaps in structured data to determine the best next action. Our agent, ProADA, improves the F1 score to 55.6 while maintaining nearly the same number of dialog turns.",
        "arxiv_id": "2502.19610",
        "ARXIVID": "2502.19610",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (BeNYfits) and a novel method (ProADA) for interactive decision-making in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.19546": {
        "authors": [
            "Anton Alyakin",
            "Jaden Stryker",
            "Daniel Alexander Alber",
            "Karl L. Sangwon",
            "Brandon Duderstadt",
            "Akshay Save",
            "David Kurland",
            "Spencer Frome",
            "Shrutika Singh",
            "Jeff Zhang",
            "Eunice Yang",
            "Ki Yun Park",
            "Cordelia Orillac",
            "Aly A. Valliani",
            "Sean Neifert",
            "Albert Liu",
            "Aneek Patel",
            "Christopher Livia",
            "Darryl Lau",
            "Ilya Laufer",
            "Peter A. Rozman",
            "Eveline Teresa Hidalgo",
            "Howard Riina",
            "Rui Feng",
            "Todd Hollon",
            "Yindalon Aphinyanaphongs",
            "John G. Golfinos",
            "Laura Snyder",
            "Eric Leuthardt",
            "Douglas Kondziolka",
            "Eric Karl Oermann"
        ],
        "title": "Repurposing the scientific literature with vision-language models",
        "abstract": "arXiv:2502.19546v1 Announce Type: new  Abstract: Research in AI for Science often focuses on using AI technologies to augment components of the scientific process, or in some cases, the entire scientific method; how about AI for scientific publications? Peer-reviewed journals are foundational repositories of specialized knowledge, written in discipline-specific language that differs from general Internet content used to train most large language models (LLMs) and vision-language models (VLMs). We hypothesized that by combining a family of scientific journals with generative AI models, we could invent novel tools for scientific communication, education, and clinical care. We converted 23,000 articles from Neurosurgery Publications into a multimodal database - NeuroPubs - of 134 million words and 78,000 image-caption pairs to develop six datasets for building AI models. We showed that the content of NeuroPubs uniquely represents neurosurgery-specific clinical contexts compared with broader datasets and PubMed. For publishing, we employed generalist VLMs to automatically generate graphical abstracts from articles. Editorial board members rated 70% of these as ready for publication without further edits. For education, we generated 89,587 test questions in the style of the ABNS written board exam, which trainee and faculty neurosurgeons found indistinguishable from genuine examples 54% of the time. We used these questions alongside a curriculum learning process to track knowledge acquisition while training our 34 billion-parameter VLM (CNS-Obsidian). In a blinded, randomized controlled trial, we demonstrated the non-inferiority of CNS-Obsidian to GPT-4o (p = 0.1154) as a diagnostic copilot for a neurosurgical service. Our findings lay a novel foundation for AI with Science and establish a framework to elevate scientific communication using state-of-the-art generative artificial intelligence while maintaining rigorous quality standards.",
        "arxiv_id": "2502.19546",
        "ARXIVID": "2502.19546",
        "COMMENT": "Matches criterion 2 as it focuses on vision-language models (VLMs) and their applications in scientific communication and education.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.20175": {
        "authors": [
            "Kaustubh Vyas",
            "Damien Graux",
            "S\\'ebastien Montella",
            "Pavlos Vougiouklis",
            "Ruofei Lai",
            "Keshuang Li",
            "Yang Ren",
            "Jeff Z. Pan"
        ],
        "title": "An Extensive Evaluation of PDDL Capabilities in off-the-shelf LLMs",
        "abstract": "arXiv:2502.20175v1 Announce Type: new  Abstract: In recent advancements, large language models (LLMs) have exhibited proficiency in code generation and chain-of-thought reasoning, laying the groundwork for tackling automatic formal planning tasks. This study evaluates the potential of LLMs to understand and generate Planning Domain Definition Language (PDDL), an essential representation in artificial intelligence planning. We conduct an extensive analysis across 20 distinct models spanning 7 major LLM families, both commercial and open-source. Our comprehensive evaluation sheds light on the zero-shot LLM capabilities of parsing, generating, and reasoning with PDDL. Our findings indicate that while some models demonstrate notable effectiveness in handling PDDL, others pose limitations in more complex scenarios requiring nuanced planning knowledge. These results highlight the promise and current limitations of LLMs in formal planning tasks, offering insights into their application and guiding future efforts in AI-driven planning paradigms.",
        "arxiv_id": "2502.20175",
        "ARXIVID": "2502.20175",
        "COMMENT": "Matches criterion 3 as it evaluates LLMs for formal planning tasks using PDDL, which is relevant to embodied AI benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.19918": {
        "authors": [
            "Yuan Sui",
            "Yufei He",
            "Tri Cao",
            "Simeng Han",
            "Bryan Hooi"
        ],
        "title": "Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning in Large Language Models",
        "abstract": "arXiv:2502.19918v1 Announce Type: new  Abstract: Large Language Models (LLMs) increasingly rely on prolonged reasoning chains to solve complex tasks. However, this trial-and-error approach often leads to high computational overhead and error propagation, where early mistakes can derail subsequent steps. To address these issues, we introduce Meta-Reasoner, a framework that dynamically optimizes inference-time reasoning by enabling LLMs to \"think about how to think.\" Drawing inspiration from human meta-cognition and dual-process theory, Meta-Reasoner operates as a strategic advisor, decoupling high-level guidance from step-by-step generation. It employs \"contextual multi-armed bandits\" to iteratively evaluate reasoning progress, and select optimal strategies (e.g., backtrack, clarify ambiguity, restart from scratch, or propose alternative approaches), and reallocates computational resources toward the most promising paths. Our evaluations on mathematical reasoning and puzzles highlight the potential of dynamic reasoning chains to overcome inherent challenges in the LLM reasoning process and also show promise in broader applications, offering a scalable and adaptable solution for reasoning-intensive tasks.",
        "arxiv_id": "2502.19918",
        "ARXIVID": "2502.19918",
        "COMMENT": "Does not match any specific criterion but is generally relevant to reasoning improvements in LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.20379": {
        "authors": [
            "Shalev Lifshitz",
            "Sheila A. McIlraith",
            "Yilun Du"
        ],
        "title": "Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers",
        "abstract": "arXiv:2502.20379v1 Announce Type: new  Abstract: By utilizing more computational resources at test-time, large language models (LLMs) can improve without additional training. One common strategy uses verifiers to evaluate candidate outputs. In this work, we propose a novel scaling dimension for test-time compute: scaling the number of verifiers. We introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that combines multiple verifiers to improve performance. We propose using Aspect Verifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of outputs, as one possible choice for the verifiers in a MAV system. AVs are a convenient building block for MAV since they can be easily combined without additional training. Moreover, we introduce BoN-MAV, a simple multi-agent verification algorithm that combines best-of-n sampling with multiple verifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency and reward model verification, and we demonstrate both weak-to-strong generalization, where combining weak verifiers improves even stronger LLMs, and self-improvement, where the same base model is used to both generate and verify outputs. Our results establish scaling the number of verifiers as a promising new dimension for improving language model performance at test-time.",
        "arxiv_id": "2502.20379",
        "ARXIVID": "2502.20379",
        "COMMENT": "Does not match any specific criterion but is generally relevant to your friend's interest in clever statistical tricks and LLM advancements.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.20309": {
        "authors": [
            "Franck Cappello",
            "Sandeep Madireddy",
            "Robert Underwood",
            "Neil Getty",
            "Nicholas Lee-Ping Chia",
            "Nesar Ramachandra",
            "Josh Nguyen",
            "Murat Keceli",
            "Tanwi Mallick",
            "Zilinghan Li",
            "Marieme Ngom",
            "Chenhui Zhang",
            "Angel Yanguas-Gil",
            "Evan Antoniuk",
            "Bhavya Kailkhura",
            "Minyang Tian",
            "Yufeng Du",
            "Yuan-Sen Ting",
            "Azton Wells",
            "Bogdan Nicolae",
            "Avinash Maurya",
            "M. Mustafa Rafique",
            "Eliu Huerta",
            "Bo Li",
            "Ian Foster",
            "Rick Stevens"
        ],
        "title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
        "abstract": "arXiv:2502.20309v1 Announce Type: new  Abstract: Recent advancements have positioned AI, and particularly Large Language Models (LLMs), as transformative tools for scientific research, capable of addressing complex tasks that require reasoning, problem-solving, and decision-making. Their exceptional capabilities suggest their potential as scientific research assistants but also highlight the need for holistic, rigorous, and domain-specific evaluation to assess effectiveness in real-world scientific applications. This paper describes a multifaceted methodology for Evaluating AI models as scientific Research Assistants (EAIRA) developed at Argonne National Laboratory. This methodology incorporates four primary classes of evaluations. 1) Multiple Choice Questions to assess factual recall; 2) Open Response to evaluate advanced reasoning and problem-solving skills; 3) Lab-Style Experiments involving detailed analysis of capabilities as research assistants in controlled environments; and 4) Field-Style Experiments to capture researcher-LLM interactions at scale in a wide range of scientific domains and applications. These complementary methods enable a comprehensive analysis of LLM strengths and weaknesses with respect to their scientific knowledge, reasoning abilities, and adaptability. Recognizing the rapid pace of LLM advancements, we designed the methodology to evolve and adapt so as to ensure its continued relevance and applicability. This paper describes the methodology state at the end of February 2025. Although developed within a subset of scientific domains, the methodology is designed to be generalizable to a wide range of scientific domains.",
        "arxiv_id": "2502.20309",
        "ARXIVID": "2502.20309",
        "COMMENT": "Does not match any specific criterion but is generally relevant to evaluating LLMs in scientific research applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.20284": {
        "authors": [
            "Shenghui Chen",
            "Yunhao Yang",
            "Kayla Boggess",
            "Seongkook Heo",
            "Lu Feng",
            "Ufuk Topcu"
        ],
        "title": "Evaluating Human Trust in LLM-Based Planners: A Preliminary Study",
        "abstract": "arXiv:2502.20284v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly used for planning tasks, offering unique capabilities not found in classical planners such as generating explanations and iterative refinement. However, trust--a critical factor in the adoption of planning systems--remains underexplored in the context of LLM-based planning tasks. This study bridges this gap by comparing human trust in LLM-based planners with classical planners through a user study in a Planning Domain Definition Language (PDDL) domain. Combining subjective measures, such as trust questionnaires, with objective metrics like evaluation accuracy, our findings reveal that correctness is the primary driver of trust and performance. Explanations provided by the LLM improved evaluation accuracy but had limited impact on trust, while plan refinement showed potential for increasing trust without significantly enhancing evaluation accuracy.",
        "arxiv_id": "2502.20284",
        "ARXIVID": "2502.20284",
        "COMMENT": "Does not match any specific criterion but is generally relevant to LLM-based planning and trust evaluation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.19596": {
        "authors": [
            "Nayoung Choi",
            "Grace Byun",
            "Andrew Chung",
            "Ellie S. Paek",
            "Shinsun Lee",
            "Jinho D. Choi"
        ],
        "title": "Trustworthy Answers, Messier Data: Bridging the Gap in Low-Resource Retrieval-Augmented Generation for Domain Expert Systems",
        "abstract": "arXiv:2502.19596v1 Announce Type: new  Abstract: RAG has become a key technique for enhancing LLMs by reducing hallucinations, especially in domain expert systems where LLMs may lack sufficient inherent knowledge. However, developing these systems in low-resource settings introduces several challenges: (1) handling heterogeneous data sources, (2) optimizing retrieval phase for trustworthy answers, and (3) evaluating generated answers across diverse aspects. To address these, we introduce a data generation pipeline that transforms raw multi-modal data into structured corpus and Q&A pairs, an advanced re-ranking phase improving retrieval precision, and a reference matching algorithm enhancing answer traceability. Applied to the automotive engineering domain, our system improves factual correctness (+1.94), informativeness (+1.16), and helpfulness (+1.67) over a non-RAG baseline, based on a 1-5 scale by an LLM judge. These results highlight the effectiveness of our approach across distinct aspects, with strong answer grounding and transparency.",
        "arxiv_id": "2502.19596",
        "ARXIVID": "2502.19596",
        "COMMENT": "Does not match any specific criterion but is generally relevant to multi-modal learning and retrieval-augmented generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}