{
    "2504.17828": {
        "authors": [
            "Bozheng Li",
            "Yongliang Wu",
            "Yi Lu",
            "Jiashuo Yu",
            "Licheng Tang",
            "Jiawang Cao",
            "Wenqing Zhu",
            "Yuyang Sun",
            "Jay Wu",
            "Wenbo Zhu"
        ],
        "title": "VEU-Bench: Towards Comprehensive Understanding of Video Editing",
        "abstract": "arXiv:2504.17828v1 Announce Type: new  Abstract: Widely shared videos on the internet are often edited. Recently, although Video Large Language Models (Vid-LLMs) have made great progress in general video understanding tasks, their capabilities in video editing understanding (VEU) tasks remain unexplored. To address this gap, in this paper, we introduce VEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark that categorizes video editing components across various dimensions, from intra-frame features like shot size to inter-shot attributes such as cut types and transitions. Unlike previous video editing understanding benchmarks that focus mainly on editing element classification, VEU-Bench encompasses 19 fine-grained tasks across three stages: recognition, reasoning, and judging. To enhance the annotation of VEU automatically, we built an annotation pipeline integrated with an ontology-based knowledge base. Through extensive experiments with 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs face significant challenges in VEU tasks, with some performing worse than random choice. To alleviate this issue, we develop Oscars, a VEU expert model fine-tuned on the curated VEU-Bench dataset. It outperforms existing open-source Vid-LLMs on VEU-Bench by over 28.3% in accuracy and achieves performance comparable to commercial models like GPT-4o. We also demonstrate that incorporating VEU data significantly enhances the performance of Vid-LLMs on general video understanding benchmarks, with an average improvement of 8.3% across nine reasoning tasks.",
        "arxiv_id": "2504.17828",
        "ARXIVID": "2504.17828",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (VEU-Bench) for video editing understanding, focusing on novel tasks and challenges.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2504.17821": {
        "authors": [
            "Xinyu Chen",
            "Yunxin Li",
            "Haoyuan Shi",
            "Baotian Hu",
            "Wenhan Luo",
            "Yaowei Wang",
            "Min Zhang"
        ],
        "title": "VideoVista-CulturalLingo: 360$^\\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension",
        "abstract": "arXiv:2504.17821v1 Announce Type: new  Abstract: Assessing the video comprehension capabilities of multimodal AI systems can effectively measure their understanding and reasoning abilities. Most video evaluation benchmarks are limited to a single language, typically English, and predominantly feature videos rooted in Western cultural contexts. In this paper, we present VideoVista-CulturalLingo, the first video evaluation benchmark designed to bridge cultural, linguistic, and domain divide in video comprehension. Our work differs from existing benchmarks in the following ways: 1) Cultural diversity, incorporating cultures from China, North America, and Europe; 2) Multi-linguistics, with questions presented in Chinese and English-two of the most widely spoken languages; and 3) Broad domain, featuring videos sourced from hundreds of human-created domains. VideoVista-CulturalLingo contains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent open-source or proprietary video large models. From the experiment results, we observe that: 1) Existing models perform worse on Chinese-centric questions than Western-centric ones, particularly those related to Chinese history; 2) Current open-source models still exhibit limitations in temporal understanding, especially in the Event Localization task, achieving a maximum score of only 45.2%; 3) Mainstream models demonstrate strong performance in general scientific questions, while open-source models demonstrate weak performance in mathematics.",
        "arxiv_id": "2504.17821",
        "ARXIVID": "2504.17821",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (VideoVista-CulturalLingo) for video comprehension with a focus on cultural and linguistic diversity, which is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.17991": {
        "authors": [
            "Zheng Qin",
            "Le Wang",
            "Yabing Wang",
            "Sanping Zhou",
            "Gang Hua",
            "Wei Tang"
        ],
        "title": "RSRNav: Reasoning Spatial Relationship for Image-Goal Navigation",
        "abstract": "arXiv:2504.17991v1 Announce Type: new  Abstract: Recent image-goal navigation (ImageNav) methods learn a perception-action policy by separately capturing semantic features of the goal and egocentric images, then passing them to a policy network. However, challenges remain: (1) Semantic features often fail to provide accurate directional information, leading to superfluous actions, and (2) performance drops significantly when viewpoint inconsistencies arise between training and application. To address these challenges, we propose RSRNav, a simple yet effective method that reasons spatial relationships between the goal and current observations as navigation guidance. Specifically, we model the spatial relationship by constructing correlations between the goal and current observations, which are then passed to the policy network for action prediction. These correlations are progressively refined using fine-grained cross-correlation and direction-aware correlation for more precise navigation. Extensive evaluation of RSRNav on three benchmark datasets demonstrates superior navigation performance, particularly in the \"user-matched goal\" setting, highlighting its potential for real-world applications.",
        "arxiv_id": "2504.17991",
        "ARXIVID": "2504.17991",
        "COMMENT": "Matches criterion 1 as it proposes a novel method for reasoning spatial relationships in image-goal navigation, improving spatial intelligence in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2504.18158": {
        "authors": [
            "Jiahao Zhang",
            "Bowen Wang",
            "Hong Liu",
            "Liangzhi Li",
            "Yuta Nakashima",
            "Hajime Nagahara"
        ],
        "title": "E-InMeMo: Enhanced Prompting for Visual In-Context Learning",
        "abstract": "arXiv:2504.18158v1 Announce Type: new  Abstract: Large-scale models trained on extensive datasets have become the standard due to their strong generalizability across diverse tasks. In-context learning (ICL), widely used in natural language processing, leverages these models by providing task-specific prompts without modifying their parameters. This paradigm is increasingly being adapted for computer vision, where models receive an input-output image pair, known as an in-context pair, alongside a query image to illustrate the desired output. However, the success of visual ICL largely hinges on the quality of these prompts. To address this, we propose Enhanced Instruct Me More (E-InMeMo), a novel approach that incorporates learnable perturbations into in-context pairs to optimize prompting. Through extensive experiments on standard vision tasks, E-InMeMo demonstrates superior performance over existing state-of-the-art methods. Notably, it improves mIoU scores by 7.99 for foreground segmentation and by 17.04 for single object detection when compared to the baseline without learnable prompts. These results highlight E-InMeMo as a lightweight yet effective strategy for enhancing visual ICL. Code is publicly available at: https://github.com/Jackieam/E-InMeMo",
        "arxiv_id": "2504.18158",
        "ARXIVID": "2504.18158",
        "COMMENT": "Matches criterion 2 as it proposes a novel enhancement for visual large language models (VLLMs) in in-context learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2504.17810": {
        "authors": [
            "Yuxin Yao",
            "Yan Zhang",
            "Zhening Huang",
            "Joan Lasenby"
        ],
        "title": "SmallGS: Gaussian Splatting-based Camera Pose Estimation for Small-Baseline Videos",
        "abstract": "arXiv:2504.17810v1 Announce Type: new  Abstract: Dynamic videos with small baseline motions are ubiquitous in daily life, especially on social media. However, these videos present a challenge to existing pose estimation frameworks due to ambiguous features, drift accumulation, and insufficient triangulation constraints. Gaussian splatting, which maintains an explicit representation for scenes, provides a reliable novel view rasterization when the viewpoint change is small. Inspired by this, we propose SmallGS, a camera pose estimation framework that is specifically designed for small-baseline videos. SmallGS optimizes sequential camera poses using Gaussian splatting, which reconstructs the scene from the first frame in each video segment to provide a stable reference for the rest. The temporal consistency of Gaussian splatting within limited viewpoint differences reduced the requirement of sufficient depth variations in traditional camera pose estimation. We further incorporate pretrained robust visual features, e.g. DINOv2, into Gaussian splatting, where high-dimensional feature map rendering enhances the robustness of camera pose estimation. By freezing the Gaussian splatting and optimizing camera viewpoints based on rasterized features, SmallGS effectively learns camera poses without requiring explicit feature correspondences or strong parallax motion. We verify the effectiveness of SmallGS in small-baseline videos in TUM-Dynamics sequences, which achieves impressive accuracy in camera pose estimation compared to MonST3R and DORID-SLAM for small-baseline videos in dynamic scenes. Our project page is at: https://yuxinyao620.github.io/SmallGS",
        "arxiv_id": "2504.17810",
        "ARXIVID": "2504.17810",
        "COMMENT": "Matches criterion 3 as it proposes a new method for camera pose estimation in small-baseline videos using Gaussian splatting, which is a novel angle compared to traditional methods.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.18039": {
        "authors": [
            "Zheng Zhang",
            "Nuoqian Xiao",
            "Qi Chai",
            "Deheng Ye",
            "Hao Wang"
        ],
        "title": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and Theory of Mind",
        "abstract": "arXiv:2504.18039v1 Announce Type: new  Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities in social deduction games (SDGs) like Werewolf, where strategic reasoning and social deception are essential. However, current approaches remain limited to textual information, ignoring crucial multimodal cues such as facial expressions and tone of voice that humans naturally use to communicate. Moreover, existing SDG agents primarily focus on inferring other players' identities without modeling how others perceive themselves or fellow players. To address these limitations, we use One Night Ultimate Werewolf (ONUW) as a testbed and present MultiMind, the first framework integrating multimodal information into SDG agents. MultiMind processes facial expressions and vocal tones alongside verbal content, while employing a Theory of Mind (ToM) model to represent each player's suspicion levels toward others. By combining this ToM model with Monte Carlo Tree Search (MCTS), our agent identifies communication strategies that minimize suspicion directed at itself. Through comprehensive evaluation in both agent-versus-agent simulations and studies with human players, we demonstrate MultiMind's superior performance in gameplay. Our work presents a significant advancement toward LLM agents capable of human-like social reasoning across multimodal domains.",
        "arxiv_id": "2504.18039",
        "ARXIVID": "2504.18039",
        "COMMENT": "Matches criterion 2 as it involves multimodal reasoning and theory of mind in LLM agents, which aligns with VLLM/MLLM advancements.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.18355": {
        "authors": [
            "Maximilian Xiling Li",
            "Korbinian Rudolf",
            "Nils Blank",
            "Rudolf Lioutikov"
        ],
        "title": "Interpretable Affordance Detection on 3D Point Clouds with Probabilistic Prototypes",
        "abstract": "arXiv:2504.18355v1 Announce Type: new  Abstract: Robotic agents need to understand how to interact with objects in their environment, both autonomously and during human-robot interactions. Affordance detection on 3D point clouds, which identifies object regions that allow specific interactions, has traditionally relied on deep learning models like PointNet++, DGCNN, or PointTransformerV3. However, these models operate as black boxes, offering no insight into their decision-making processes. Prototypical Learning methods, such as ProtoPNet, provide an interpretable alternative to black-box models by employing a \"this looks like that\" case-based reasoning approach. However, they have been primarily applied to image-based tasks. In this work, we apply prototypical learning to models for affordance detection on 3D point clouds. Experiments on the 3D-AffordanceNet benchmark dataset show that prototypical models achieve competitive performance with state-of-the-art black-box models and offer inherent interpretability. This makes prototypical models a promising candidate for human-robot interaction scenarios that require increased trust and safety.",
        "arxiv_id": "2504.18355",
        "ARXIVID": "2504.18355",
        "COMMENT": "Matches criterion 1 as it introduces a new interpretable method for affordance detection on 3D point clouds, which relates to spatial understanding in embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.18204": {
        "authors": [
            "Kun Li",
            "Jianhui Wang",
            "Yangfan He",
            "Xinyuan Song",
            "Ruoyu Wang",
            "Hongyang He",
            "Wenxin Zhang",
            "Jiaqi Chen",
            "Keqin Li",
            "Sida Li",
            "Miao Zhang",
            "Tianyu Shi",
            "Xueqian Wang"
        ],
        "title": "Optimizing Multi-Round Enhanced Training in Diffusion Models for Improved Preference Understanding",
        "abstract": "arXiv:2504.18204v1 Announce Type: new  Abstract: Generative AI has significantly changed industries by enabling text-driven image generation, yet challenges remain in achieving high-resolution outputs that align with fine-grained user preferences. Consequently, multi-round interactions are necessary to ensure the generated images meet expectations. Previous methods enhanced prompts via reward feedback but did not optimize over a multi-round dialogue dataset. In this work, we present a Visual Co-Adaptation (VCA) framework incorporating human-in-the-loop feedback, leveraging a well-trained reward model aligned with human preferences. Using a diverse multi-turn dialogue dataset, our framework applies multiple reward functions, such as diversity, consistency, and preference feedback, while fine-tuning the diffusion model through LoRA, thus optimizing image generation based on user input. We also construct multi-round dialogue datasets of prompts and image pairs aligned with user intent. Experiments demonstrate that our method outperforms state-of-the-art baselines, significantly improving image consistency and alignment with user intent. Our approach consistently surpasses competing models in user satisfaction, especially in multi-turn dialogue scenarios.",
        "arxiv_id": "2504.18204",
        "ARXIVID": "2504.18204",
        "COMMENT": "Matches criterion 2 as it discusses multi-round enhanced training in diffusion models for improved preference understanding, which relates to generative modeling in multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.18318": {
        "authors": [
            "Yunze Deng",
            "Haijun Xiong",
            "Bin Feng",
            "Xinggang Wang",
            "Wenyu Liu"
        ],
        "title": "STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D Gaussian Splatting",
        "abstract": "arXiv:2504.18318v1 Announce Type: new  Abstract: Text-to-4D generation is rapidly developing and widely applied in various scenarios. However, existing methods often fail to incorporate adequate spatio-temporal modeling and prompt alignment within a unified framework, resulting in temporal inconsistencies, geometric distortions, or low-quality 4D content that deviates from the provided texts. Therefore, we propose STP4D, a novel approach that aims to integrate comprehensive spatio-temporal-prompt consistency modeling for high-quality text-to-4D generation. Specifically, STP4D employs three carefully designed modules: Time-varying Prompt Embedding, Geometric Information Enhancement, and Temporal Extension Deformation, which collaborate to accomplish this goal. Furthermore, STP4D is among the first methods to exploit the Diffusion model to generate 4D Gaussians, combining the fine-grained modeling capabilities and the real-time rendering process of 4DGS with the rapid inference speed of the Diffusion model. Extensive experiments demonstrate that STP4D excels in generating high-fidelity 4D content with exceptional efficiency (approximately 4.6s per asset), surpassing existing methods in both quality and speed.",
        "arxiv_id": "2504.18318",
        "ARXIVID": "2504.18318",
        "COMMENT": "Matches criterion 4 as it focuses on text-to-4D generation using diffusion models, which is related to vision foundation models and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2504.18332": {
        "authors": [
            "Shuting Zhao",
            "Linxin Bai",
            "Liangjing Shao",
            "Ye Zhang",
            "Xinrong Chen"
        ],
        "title": "SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse Observations",
        "abstract": "arXiv:2504.18332v1 Announce Type: new  Abstract: The growing applications of AR/VR increase the demand for real-time full-body pose estimation from Head-Mounted Displays (HMDs). Although HMDs provide joint signals from the head and hands, reconstructing a full-body pose remains challenging due to the unconstrained lower body. Recent advancements often rely on conventional neural networks and generative models to improve performance in this task, such as Transformers and diffusion models. However, these approaches struggle to strike a balance between achieving precise pose reconstruction and maintaining fast inference speed. To overcome these challenges, a lightweight and efficient model, SSD-Poser, is designed for robust full-body motion estimation from sparse observations. SSD-Poser incorporates a well-designed hybrid encoder, State Space Attention Encoders, to adapt the state space duality to complex motion poses and enable real-time realistic pose reconstruction. Moreover, a Frequency-Aware Decoder is introduced to mitigate jitter caused by variable-frequency motion signals, remarkably enhancing the motion smoothness. Comprehensive experiments on the AMASS dataset demonstrate that SSD-Poser achieves exceptional accuracy and computational efficiency, showing outstanding inference efficiency compared to state-of-the-art methods.",
        "arxiv_id": "2504.18332",
        "ARXIVID": "2504.18332",
        "COMMENT": "Matches criterion 3 as it proposes a new method for full-body pose estimation from sparse observations, which is relevant to embodied AI methods.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2504.18068": {
        "authors": [
            "Zhuohao Yan",
            "Shaoquan Feng",
            "Xingxing Li",
            "Yuxuan Zhou",
            "Chunxi Xia",
            "Shengyu Li"
        ],
        "title": "S3MOT: Monocular 3D Object Tracking with Selective State Space Model",
        "abstract": "arXiv:2504.18068v1 Announce Type: new  Abstract: Accurate and reliable multi-object tracking (MOT) in 3D space is essential for advancing robotics and computer vision applications. However, it remains a significant challenge in monocular setups due to the difficulty of mining 3D spatiotemporal associations from 2D video streams. In this work, we present three innovative techniques to enhance the fusion and exploitation of heterogeneous cues for monocular 3D MOT: (1) we introduce the Hungarian State Space Model (HSSM), a novel data association mechanism that compresses contextual tracking cues across multiple paths, enabling efficient and comprehensive assignment decisions with linear complexity. HSSM features a global receptive field and dynamic weights, in contrast to traditional linear assignment algorithms that rely on hand-crafted association costs. (2) We propose Fully Convolutional One-stage Embedding (FCOE), which eliminates ROI pooling by directly using dense feature maps for contrastive learning, thus improving object re-identification accuracy under challenging conditions such as varying viewpoints and lighting. (3) We enhance 6-DoF pose estimation through VeloSSM, an encoder-decoder architecture that models temporal dependencies in velocity to capture motion dynamics, overcoming the limitations of frame-based 3D inference. Experiments on the KITTI public test benchmark demonstrate the effectiveness of our method, achieving a new state-of-the-art performance of 76.86~HOTA at 31~FPS. Our approach outperforms the previous best by significant margins of +2.63~HOTA and +3.62~AssA, showcasing its robustness and efficiency for monocular 3D MOT tasks. The code and models are available at https://github.com/bytepioneerX/s3mot.",
        "arxiv_id": "2504.18068",
        "ARXIVID": "2504.18068",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for monocular 3D object tracking, which is relevant to embodied AI methods.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2504.18447": {
        "authors": [
            "Ryo Yamaki",
            "Shintaro Shiba",
            "Guillermo Gallego",
            "Yoshimitsu Aoki"
        ],
        "title": "Iterative Event-based Motion Segmentation by Variational Contrast Maximization",
        "abstract": "arXiv:2504.18447v1 Announce Type: new  Abstract: Event cameras provide rich signals that are suitable for motion estimation since they respond to changes in the scene. As any visual changes in the scene produce event data, it is paramount to classify the data into different motions (i.e., motion segmentation), which is useful for various tasks such as object detection and visual servoing. We propose an iterative motion segmentation method, by classifying events into background (e.g., dominant motion hypothesis) and foreground (independent motion residuals), thus extending the Contrast Maximization framework. Experimental results demonstrate that the proposed method successfully classifies event clusters both for public and self-recorded datasets, producing sharp, motion-compensated edge-like images. The proposed method achieves state-of-the-art accuracy on moving object detection benchmarks with an improvement of over 30%, and demonstrates its possibility of applying to more complex and noisy real-world scenes. We hope this work broadens the sensitivity of Contrast Maximization with respect to both motion parameters and input events, thus contributing to theoretical advancements in event-based motion segmentation estimation. https://github.com/aoki-media-lab/event_based_segmentation_vcmax",
        "arxiv_id": "2504.18447",
        "ARXIVID": "2504.18447",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for motion segmentation using event cameras, which could be relevant for embodied AI benchmarks or methods.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2504.18317": {
        "authors": [
            "Zhengru Fang",
            "Zhenghao Liu",
            "Jingjing Wang",
            "Senkang Hu",
            "Yu Guo",
            "Yiqin Deng",
            "Yuguang Fang"
        ],
        "title": "Task-Oriented Communications for Visual Navigation with Edge-Aerial Collaboration in Low Altitude Economy",
        "abstract": "arXiv:2504.18317v1 Announce Type: new  Abstract: To support the Low Altitude Economy (LAE), precise unmanned aerial vehicles (UAVs) localization in urban areas where global positioning system (GPS) signals are unavailable. Vision-based methods offer a viable alternative but face severe bandwidth, memory and processing constraints on lightweight UAVs. Inspired by mammalian spatial cognition, we propose a task-oriented communication framework, where UAVs equipped with multi-camera systems extract compact multi-view features and offload localization tasks to edge servers. We introduce the Orthogonally-constrained Variational Information Bottleneck encoder (O-VIB), which incorporates automatic relevance determination (ARD) to prune non-informative features while enforcing orthogonality to minimize redundancy. This enables efficient and accurate localization with minimal transmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows that O-VIB achieves high-precision localization under stringent bandwidth budgets. Code and dataset will be made publicly available: github.com/fangzr/TOC-Edge-Aerial.",
        "arxiv_id": "2504.18317",
        "ARXIVID": "2504.18317",
        "COMMENT": "Matches criterion 1 as it proposes a task-oriented communication framework for UAVs, inspired by spatial cognition.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2504.17825": {
        "authors": [
            "Dehong Kong",
            "Fan Li",
            "Zhixin Wang",
            "Jiaqi Xu",
            "Renjing Pei",
            "Wenbo Li",
            "WenQi Ren"
        ],
        "title": "Dual Prompting Image Restoration with Diffusion Transformers",
        "abstract": "arXiv:2504.17825v1 Announce Type: new  Abstract: Recent state-of-the-art image restoration methods mostly adopt latent diffusion models with U-Net backbones, yet still facing challenges in achieving high-quality restoration due to their limited capabilities. Diffusion transformers (DiTs), like SD3, are emerging as a promising alternative because of their better quality with scalability. In this paper, we introduce DPIR (Dual Prompting Image Restoration), a novel image restoration method that effectivly extracts conditional information of low-quality images from multiple perspectives. Specifically, DPIR consits of two branches: a low-quality image conditioning branch and a dual prompting control branch. The first branch utilizes a lightweight module to incorporate image priors into the DiT with high efficiency. More importantly, we believe that in image restoration, textual description alone cannot fully capture its rich visual characteristics. Therefore, a dual prompting module is designed to provide DiT with additional visual cues, capturing both global context and local appearance. The extracted global-local visual prompts as extra conditional control, alongside textual prompts to form dual prompts, greatly enhance the quality of the restoration. Extensive experimental results demonstrate that DPIR delivers superior image restoration performance.",
        "arxiv_id": "2504.17825",
        "ARXIVID": "2504.17825",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models (Diffusion Transformers) and their application to image restoration, introducing a dual prompting mechanism.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2504.18215": {
        "authors": [
            "Nanjie Yao",
            "Gangjian Zhang",
            "Wenhao Shen",
            "Jian Shu",
            "Hao Wang"
        ],
        "title": "Unify3D: An Augmented Holistic End-to-end Monocular 3D Human Reconstruction via Anatomy Shaping and Twins Negotiating",
        "abstract": "arXiv:2504.18215v1 Announce Type: new  Abstract: Monocular 3D clothed human reconstruction aims to create a complete 3D avatar from a single image. To tackle the human geometry lacking in one RGB image, current methods typically resort to a preceding model for an explicit geometric representation. For the reconstruction itself, focus is on modeling both it and the input image. This routine is constrained by the preceding model, and overlooks the integrity of the reconstruction task. To address this, this paper introduces a novel paradigm that treats human reconstruction as a holistic process, utilizing an end-to-end network for direct prediction from 2D image to 3D avatar, eliminating any explicit intermediate geometry display. Based on this, we further propose a novel reconstruction framework consisting of two core components: the Anatomy Shaping Extraction module, which captures implicit shape features taking into account the specialty of human anatomy, and the Twins Negotiating Reconstruction U-Net, which enhances reconstruction through feature interaction between two U-Nets of different modalities. Moreover, we propose a Comic Data Augmentation strategy and construct 15k+ 3D human scans to bolster model performance in more complex case input. Extensive experiments on two test sets and many in-the-wild cases show the superiority of our method over SOTA methods. Our demos can be found in : https://e2e3dgsrecon.github.io/e2e3dgsrecon/.",
        "arxiv_id": "2504.18215",
        "ARXIVID": "2504.18215",
        "COMMENT": "Matches criterion 4 as it proposes a holistic end-to-end framework for monocular 3D human reconstruction, leveraging anatomy shaping and feature negotiation.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2504.18059": {
        "authors": [
            "Prachi Garg",
            "Joseph K J",
            "Vineeth N Balasubramanian",
            "Necati Cihan Camgoz",
            "Chengde Wan",
            "Kenrick Kin",
            "Weiguang Si",
            "Shugao Ma",
            "Fernando De La Torre"
        ],
        "title": "POET: Prompt Offset Tuning for Continual Human Action Adaptation",
        "abstract": "arXiv:2504.18059v1 Announce Type: new  Abstract: As extended reality (XR) is redefining how users interact with computing devices, research in human action recognition is gaining prominence. Typically, models deployed on immersive computing devices are static and limited to their default set of classes. The goal of our research is to provide users and developers with the capability to personalize their experience by adding new action classes to their device models continually. Importantly, a user should be able to add new classes in a low-shot and efficient manner, while this process should not require storing or replaying any of user's sensitive training data. We formalize this problem as privacy-aware few-shot continual action recognition. Towards this end, we propose POET: Prompt-Offset Tuning. While existing prompt tuning approaches have shown great promise for continual learning of image, text, and video modalities; they demand access to extensively pretrained transformers. Breaking away from this assumption, POET demonstrates the efficacy of prompt tuning a significantly lightweight backbone, pretrained exclusively on the base class data. We propose a novel spatio-temporal learnable prompt offset tuning approach, and are the first to apply such prompt tuning to Graph Neural Networks. We contribute two new benchmarks for our new problem setting in human action recognition: (i) NTU RGB+D dataset for activity recognition, and (ii) SHREC-2017 dataset for hand gesture recognition. We find that POET consistently outperforms comprehensive benchmarks. Source code at https://github.com/humansensinglab/POET-continual-action-recognition.",
        "arxiv_id": "2504.18059",
        "ARXIVID": "2504.18059",
        "COMMENT": "Matches criterion 3 as it introduces a novel continual learning method for human action recognition, focusing on privacy-aware and few-shot adaptation.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2504.18509": {
        "authors": [
            "Shivam Duggal",
            "Yushi Hu",
            "Oscar Michel",
            "Aniruddha Kembhavi",
            "William T. Freeman",
            "Noah A. Smith",
            "Ranjay Krishna",
            "Antonio Torralba",
            "Ali Farhadi",
            "Wei-Chiu Ma"
        ],
        "title": "Eval3D: Interpretable and Fine-grained Evaluation for 3D Generation",
        "abstract": "arXiv:2504.18509v1 Announce Type: new  Abstract: Despite the unprecedented progress in the field of 3D generation, current systems still often fail to produce high-quality 3D assets that are visually appealing and geometrically and semantically consistent across multiple viewpoints. To effectively assess the quality of the generated 3D data, there is a need for a reliable 3D evaluation tool. Unfortunately, existing 3D evaluation metrics often overlook the geometric quality of generated assets or merely rely on black-box multimodal large language models for coarse assessment. In this paper, we introduce Eval3D, a fine-grained, interpretable evaluation tool that can faithfully evaluate the quality of generated 3D assets based on various distinct yet complementary criteria. Our key observation is that many desired properties of 3D generation, such as semantic and geometric consistency, can be effectively captured by measuring the consistency among various foundation models and tools. We thus leverage a diverse set of models and tools as probes to evaluate the inconsistency of generated 3D assets across different aspects. Compared to prior work, Eval3D provides pixel-wise measurement, enables accurate 3D spatial feedback, and aligns more closely with human judgments. We comprehensively evaluate existing 3D generation models using Eval3D and highlight the limitations and challenges of current models.",
        "arxiv_id": "2504.18509",
        "ARXIVID": "2504.18509",
        "COMMENT": "Matches criterion 3 as it introduces a new evaluation tool for 3D generation, focusing on fine-grained and interpretable metrics.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2504.18203": {
        "authors": [
            "Raul David Dominguez Sanchez",
            "Xavier Diaz Ortiz",
            "Xingcheng Zhou",
            "Max Peter Ronecker",
            "Michael Karner",
            "Daniel Watzenig",
            "Alois Knoll"
        ],
        "title": "LiDAR-Guided Monocular 3D Object Detection for Long-Range Railway Monitoring",
        "abstract": "arXiv:2504.18203v1 Announce Type: new  Abstract: Railway systems, particularly in Germany, require high levels of automation to address legacy infrastructure challenges and increase train traffic safely. A key component of automation is robust long-range perception, essential for early hazard detection, such as obstacles at level crossings or pedestrians on tracks. Unlike automotive systems with braking distances of ~70 meters, trains require perception ranges exceeding 1 km. This paper presents an deep-learning-based approach for long-range 3D object detection tailored for autonomous trains. The method relies solely on monocular images, inspired by the Faraway-Frustum approach, and incorporates LiDAR data during training to improve depth estimation. The proposed pipeline consists of four key modules: (1) a modified YOLOv9 for 2.5D object detection, (2) a depth estimation network, and (3-4) dedicated short- and long-range 3D detection heads. Evaluations on the OSDaR23 dataset demonstrate the effectiveness of the approach in detecting objects up to 250 meters. Results highlight its potential for railway automation and outline areas for future improvement.",
        "arxiv_id": "2504.18203",
        "ARXIVID": "2504.18203",
        "COMMENT": "Matches criterion 3 as it introduces a new method for long-range 3D object detection tailored for railway monitoring, focusing on monocular images and LiDAR-guided training.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2504.17816": {
        "authors": [
            "Daneul Kim",
            "Jingxu Zhang",
            "Wonjoon Jin",
            "Sunghyun Cho",
            "Qi Dai",
            "Jaesik Park",
            "Chong Luo"
        ],
        "title": "Subject-driven Video Generation via Disentangled Identity and Motion",
        "abstract": "arXiv:2504.17816v1 Announce Type: new  Abstract: We propose to train a subject-driven customized video generation model through decoupling the subject-specific learning from temporal dynamics in zero-shot without additional tuning. A traditional method for video customization that is tuning-free often relies on large, annotated video datasets, which are computationally expensive and require extensive annotation. In contrast to the previous approach, we introduce the use of an image customization dataset directly on training video customization models, factorizing the video customization into two folds: (1) identity injection through image customization dataset and (2) temporal modeling preservation with a small set of unannotated videos through the image-to-video training method. Additionally, we employ random image token dropping with randomized image initialization during image-to-video fine-tuning to mitigate the copy-and-paste issue. To further enhance learning, we introduce stochastic switching during joint optimization of subject-specific and temporal features, mitigating catastrophic forgetting. Our method achieves strong subject consistency and scalability, outperforming existing video customization models in zero-shot settings, demonstrating the effectiveness of our framework.",
        "arxiv_id": "2504.17816",
        "ARXIVID": "2504.17816",
        "COMMENT": "Matches criterion 4 as it proposes a novel subject-driven video generation framework, which is related to generative modeling in vision.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2504.17815": {
        "authors": [
            "Mingxuan Cui",
            "Qing Guo",
            "Yuyi Wang",
            "Hongkai Yu",
            "Di Lin",
            "Qin Zou",
            "Ming-Ming Cheng",
            "Xi Li"
        ],
        "title": "Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene Conceptional Learning",
        "abstract": "arXiv:2504.17815v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful and efficient 3D representation for novel view synthesis. This paper extends 3DGS capabilities to inpainting, where masked objects in a scene are replaced with new contents that blend seamlessly with the surroundings. Unlike 2D image inpainting, 3D Gaussian inpainting (3DGI) is challenging in effectively leveraging complementary visual and semantic cues from multiple input views, as occluded areas in one view may be visible in others. To address this, we propose a method that measures the visibility uncertainties of 3D points across different input views and uses them to guide 3DGI in utilizing complementary visual cues. We also employ uncertainties to learn a semantic concept of scene without the masked object and use a diffusion model to fill masked objects in input images based on the learned concept. Finally, we build a novel 3DGI framework, VISTA, by integrating VISibility-uncerTainty-guided 3DGI with scene conceptuAl learning. VISTA generates high-quality 3DGS models capable of synthesizing artifact-free and naturally inpainted novel views. Furthermore, our approach extends to handling dynamic distractors arising from temporal object changes, enhancing its versatility in diverse scene reconstruction scenarios. We demonstrate the superior performance of our method over state-of-the-art techniques using two challenging datasets: the SPIn-NeRF dataset, featuring 10 diverse static 3D inpainting scenes, and an underwater 3D inpainting dataset derived from UTB180, including fast-moving fish as inpainting targets.",
        "arxiv_id": "2504.17815",
        "ARXIVID": "2504.17815",
        "COMMENT": "Matches criterion 4 as it extends 3D Gaussian Splatting for novel view synthesis and inpainting, which is related to vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2504.18419": {
        "authors": [
            "Carlo Sgaravatti",
            "Roberto Basla",
            "Riccardo Pieroni",
            "Matteo Corno",
            "Sergio M. Savaresi",
            "Luca Magri",
            "Giacomo Boracchi"
        ],
        "title": "A Multimodal Hybrid Late-Cascade Fusion Network for Enhanced 3D Object Detection",
        "abstract": "arXiv:2504.18419v1 Announce Type: new  Abstract: We present a new way to detect 3D objects from multimodal inputs, leveraging both LiDAR and RGB cameras in a hybrid late-cascade scheme, that combines an RGB detection network and a 3D LiDAR detector. We exploit late fusion principles to reduce LiDAR False Positives, matching LiDAR detections with RGB ones by projecting the LiDAR bounding boxes on the image. We rely on cascade fusion principles to recover LiDAR False Negatives leveraging epipolar constraints and frustums generated by RGB detections of separate views. Our solution can be plugged on top of any underlying single-modal detectors, enabling a flexible training process that can take advantage of pre-trained LiDAR and RGB detectors, or train the two branches separately. We evaluate our results on the KITTI object detection benchmark, showing significant performance improvements, especially for the detection of Pedestrians and Cyclists.",
        "arxiv_id": "2504.18419",
        "ARXIVID": "2504.18419",
        "COMMENT": "Matches criterion 3 as it introduces a novel multimodal fusion method for 3D object detection, focusing on LiDAR and RGB integration.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2504.18190": {
        "authors": [
            "Brun\\'o B. Englert",
            "Tommie Kerssies",
            "Gijs Dubbelman"
        ],
        "title": "What is the Added Value of UDA in the VFM Era?",
        "abstract": "arXiv:2504.18190v1 Announce Type: new  Abstract: Unsupervised Domain Adaptation (UDA) can improve a perception model's generalization to an unlabeled target domain starting from a labeled source domain. UDA using Vision Foundation Models (VFMs) with synthetic source data can achieve generalization performance comparable to fully-supervised learning with real target data. However, because VFMs have strong generalization from their pre-training, more straightforward, source-only fine-tuning can also perform well on the target. As data scenarios used in academic research are not necessarily representative for real-world applications, it is currently unclear (a) how UDA behaves with more representative and diverse data and (b) if source-only fine-tuning of VFMs can perform equally well in these scenarios. Our research aims to close these gaps and, similar to previous studies, we focus on semantic segmentation as a representative perception task. We assess UDA for synth-to-real and real-to-real use cases with different source and target data combinations. We also investigate the effect of using a small amount of labeled target data in UDA. We clarify that while these scenarios are more realistic, they are not necessarily more challenging. Our results show that, when using stronger synthetic source data, UDA's improvement over source-only fine-tuning of VFMs reduces from +8 mIoU to +2 mIoU, and when using more diverse real source data, UDA has no added value. However, UDA generalization is always higher in all synthetic data scenarios than source-only fine-tuning and, when including only 1/16 of Cityscapes labels, synthetic UDA obtains the same state-of-the-art segmentation quality of 85 mIoU as a fully-supervised model using all labels. Considering the mixed results, we discuss how UDA can best support robust autonomous driving at scale.",
        "arxiv_id": "2504.18190",
        "ARXIVID": "2504.18190",
        "COMMENT": "Matches criterion 4 as it discusses Vision Foundation Models (VFMs) and their application in domain adaptation for semantic segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.18165": {
        "authors": [
            "Michel Gokan Khan",
            "Renan Guarese",
            "Fabian Johnson",
            "Xi Vincent Wang",
            "Anders Bergman",
            "Benjamin Edvinsson",
            "Mario Romero",
            "J\\'er\\'emy Vachier",
            "Jan Kronqvist"
        ],
        "title": "PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models",
        "abstract": "arXiv:2504.18165v1 Announce Type: new  Abstract: We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning framework that combines camera and sensory data with 3D Gaussian Splatting and computer vision models for digital twinning, object tracking, and Key Performance Indicators (KPIs) extraction in industrial production lines. By utilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam offers a semi-automated approach to object tracking and spatial mapping, enabling digital twins that capture real-time KPIs such as availability, performance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts in the production line. We validate the effectiveness of PerfCam through a practical deployment within realistic test production lines in the pharmaceutical industry and contribute an openly published dataset to support further research and development in the field. The results demonstrate PerfCam's ability to deliver actionable insights through its precise digital twin capabilities, underscoring its value as an effective tool for developing usable digital twins in smart manufacturing environments and extracting operational analytics.",
        "arxiv_id": "2504.18165",
        "ARXIVID": "2504.18165",
        "COMMENT": "Matches criterion 4 as it discusses the use of 3D Gaussian Splatting and vision models for digital twinning in industrial applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.18256": {
        "authors": [
            "Elena Plekhanova",
            "Damien Robert",
            "Johannes Dollinger",
            "Emilia Arens",
            "Philipp Brun",
            "Jan Dirk Wegner",
            "Niklaus Zimmermann"
        ],
        "title": "SSL4Eco: A Global Seasonal Dataset for Geospatial Foundation Models in Ecology",
        "abstract": "arXiv:2504.18256v1 Announce Type: new  Abstract: With the exacerbation of the biodiversity and climate crises, macroecological pursuits such as global biodiversity mapping become more urgent. Remote sensing offers a wealth of Earth observation data for ecological studies, but the scarcity of labeled datasets remains a major challenge. Recently, self-supervised learning has enabled learning representations from unlabeled data, triggering the development of pretrained geospatial models with generalizable features. However, these models are often trained on datasets biased toward areas of high human activity, leaving entire ecological regions underrepresented. Additionally, while some datasets attempt to address seasonality through multi-date imagery, they typically follow calendar seasons rather than local phenological cycles. To better capture vegetation seasonality at a global scale, we propose a simple phenology-informed sampling strategy and introduce corresponding SSL4Eco, a multi-date Sentinel-2 dataset, on which we train an existing model with a season-contrastive objective. We compare representations learned from SSL4Eco against other datasets on diverse ecological downstream tasks and demonstrate that our straightforward sampling method consistently improves representation quality, highlighting the importance of dataset construction. The model pretrained on SSL4Eco reaches state of the art performance on 7 out of 8 downstream tasks spanning (multi-label) classification and regression. We release our code, data, and model weights to support macroecological and computer vision research at https://github.com/PlekhanovaElena/ssl4eco.",
        "arxiv_id": "2504.18256",
        "ARXIVID": "2504.18256",
        "COMMENT": "Matches criterion 4 as it discusses geospatial foundation models and their applications in ecology.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.17822": {
        "authors": [
            "Wenwen Li",
            "Chia-Yu Hsu",
            "Sizhe Wang",
            "Zhining Gu",
            "Yili Yang",
            "Brendan M. Rogers",
            "Anna Liljedahl"
        ],
        "title": "A multi-scale vision transformer-based multimodal GeoAI model for mapping Arctic permafrost thaw",
        "abstract": "arXiv:2504.17822v1 Announce Type: new  Abstract: Retrogressive Thaw Slumps (RTS) in Arctic regions are distinct permafrost landforms with significant environmental impacts. Mapping these RTS is crucial because their appearance serves as a clear indication of permafrost thaw. However, their small scale compared to other landform features, vague boundaries, and spatiotemporal variation pose significant challenges for accurate detection. In this paper, we employed a state-of-the-art deep learning model, the Cascade Mask R-CNN with a multi-scale vision transformer-based backbone, to delineate RTS features across the Arctic. Two new strategies were introduced to optimize multimodal learning and enhance the model's predictive performance: (1) a feature-level, residual cross-modality attention fusion strategy, which effectively integrates feature maps from multiple modalities to capture complementary information and improve the model's ability to understand complex patterns and relationships within the data; (2) pre-trained unimodal learning followed by multimodal fine-tuning to alleviate high computing demand while achieving strong model performance. Experimental results demonstrated that our approach outperformed existing models adopting data-level fusion, feature-level convolutional fusion, and various attention fusion strategies, providing valuable insights into the efficient utilization of multimodal data for RTS mapping. This research contributes to our understanding of permafrost landforms and their environmental implications.",
        "arxiv_id": "2504.17822",
        "ARXIVID": "2504.17822",
        "COMMENT": "Matches criterion 4 as it involves a vision transformer-based model applied to Arctic permafrost mapping.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2504.17990": {
        "authors": [
            "Yabing Wang",
            "Zhuotao Tian",
            "Qingpei Guo",
            "Zheng Qin",
            "Sanping Zhou",
            "Ming Yang",
            "Le Wang"
        ],
        "title": "From Mapping to Composing: A Two-Stage Framework for Zero-shot Composed Image Retrieval",
        "abstract": "arXiv:2504.17990v1 Announce Type: new  Abstract: Composed Image Retrieval (CIR) is a challenging multimodal task that retrieves a target image based on a reference image and accompanying modification text. Due to the high cost of annotating CIR triplet datasets, zero-shot (ZS) CIR has gained traction as a promising alternative. Existing studies mainly focus on projection-based methods, which map an image to a single pseudo-word token. However, these methods face three critical challenges: (1) insufficient pseudo-word token representation capacity, (2) discrepancies between training and inference phases, and (3) reliance on large-scale synthetic data. To address these issues, we propose a two-stage framework where the training is accomplished from mapping to composing. In the first stage, we enhance image-to-pseudo-word token learning by introducing a visual semantic injection module and a soft text alignment objective, enabling the token to capture richer and fine-grained image information. In the second stage, we optimize the text encoder using a small amount of synthetic triplet data, enabling it to effectively extract compositional semantics by combining pseudo-word tokens with modification text for accurate target image retrieval. The strong visual-to-pseudo mapping established in the first stage provides a solid foundation for the second stage, making our approach compatible with both high- and low-quality synthetic data, and capable of achieving significant performance gains with only a small amount of synthetic data. Extensive experiments were conducted on three public datasets, achieving superior performance compared to existing approaches.",
        "arxiv_id": "2504.17990",
        "ARXIVID": "2504.17990",
        "COMMENT": "Does not match any specific criteria but is related to multi-modal learning and retrieval tasks, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.18361": {
        "authors": [
            "Haozhen Yan",
            "Yan Hong",
            "Jiahui Zhan",
            "Yikun Ji",
            "Jun Lan",
            "Huijia Zhu",
            "Weiqiang Wang",
            "Jianfu Zhang"
        ],
        "title": "COCO-Inpaint: A Benchmark for Image Inpainting Detection and Manipulation Localization",
        "abstract": "arXiv:2504.18361v1 Announce Type: new  Abstract: Recent advancements in image manipulation have achieved unprecedented progress in generating photorealistic content, but also simultaneously eliminating barriers to arbitrary manipulation and editing, raising concerns about multimedia authenticity and cybersecurity. However, existing Image Manipulation Detection and Localization (IMDL) methodologies predominantly focus on splicing or copy-move forgeries, lacking dedicated benchmarks for inpainting-based manipulations. To bridge this gap, we present COCOInpaint, a comprehensive benchmark specifically designed for inpainting detection, with three key contributions: 1) High-quality inpainting samples generated by six state-of-the-art inpainting models, 2) Diverse generation scenarios enabled by four mask generation strategies with optional text guidance, and 3) Large-scale coverage with 258,266 inpainted images with rich semantic diversity. Our benchmark is constructed to emphasize intrinsic inconsistencies between inpainted and authentic regions, rather than superficial semantic artifacts such as object shapes. We establish a rigorous evaluation protocol using three standard metrics to assess existing IMDL approaches. The dataset will be made publicly available to facilitate future research in this area.",
        "arxiv_id": "2504.18361",
        "ARXIVID": "2504.18361",
        "COMMENT": "Does not match any specific criteria but introduces a benchmark for image inpainting detection, which is tangentially related to computer vision applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.18448": {
        "authors": [
            "Haotian Dong",
            "Xin Wang",
            "Di Lin",
            "Yipeng Wu",
            "Qin Chen",
            "Ruonan Liu",
            "Kairui Yang",
            "Ping Li",
            "Qing Guo"
        ],
        "title": "NoiseController: Towards Consistent Multi-view Video Generation via Noise Decomposition and Collaboration",
        "abstract": "arXiv:2504.18448v1 Announce Type: new  Abstract: High-quality video generation is crucial for many fields, including the film industry and autonomous driving. However, generating videos with spatiotemporal consistencies remains challenging. Current methods typically utilize attention mechanisms or modify noise to achieve consistent videos, neglecting global spatiotemporal information that could help ensure spatial and temporal consistency during video generation. In this paper, we propose the NoiseController, consisting of Multi-Level Noise Decomposition, Multi-Frame Noise Collaboration, and Joint Denoising, to enhance spatiotemporal consistencies in video generation. In multi-level noise decomposition, we first decompose initial noises into scene-level foreground/background noises, capturing distinct motion properties to model multi-view foreground/background variations. Furthermore, each scene-level noise is further decomposed into individual-level shared and residual components. The shared noise preserves consistency, while the residual component maintains diversity. In multi-frame noise collaboration, we introduce an inter-view spatiotemporal collaboration matrix and an intra-view impact collaboration matrix , which captures mutual cross-view effects and historical cross-frame impacts to enhance video quality. The joint denoising contains two parallel denoising U-Nets to remove each scene-level noise, mutually enhancing video generation. We evaluate our NoiseController on public datasets focusing on video generation and downstream tasks, demonstrating its state-of-the-art performance.",
        "arxiv_id": "2504.18448",
        "ARXIVID": "2504.18448",
        "COMMENT": "Does not match any specific criteria but discusses video generation with spatiotemporal consistency, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.18087": {
        "authors": [
            "Weipeng Tan",
            "Chuming Lin",
            "Chengming Xu",
            "FeiFan Xu",
            "Xiaobin Hu",
            "Xiaozhong Ji",
            "Junwei Zhu",
            "Chengjie Wang",
            "Yanwei Fu"
        ],
        "title": "Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation",
        "abstract": "arXiv:2504.18087v1 Announce Type: new  Abstract: Recent advances in Talking Head Generation (THG) have achieved impressive lip synchronization and visual quality through diffusion models; yet existing methods struggle to generate emotionally expressive portraits while preserving speaker identity. We identify three critical limitations in current emotional talking head generation: insufficient utilization of audio's inherent emotional cues, identity leakage in emotion representations, and isolated learning of emotion correlations. To address these challenges, we propose a novel framework dubbed as DICE-Talk, following the idea of disentangling identity with emotion, and then cooperating emotions with similar characteristics. First, we develop a disentangled emotion embedder that jointly models audio-visual emotional cues through cross-modal attention, representing emotions as identity-agnostic Gaussian distributions. Second, we introduce a correlation-enhanced emotion conditioning module with learnable Emotion Banks that explicitly capture inter-emotion relationships through vector quantization and attention-based feature aggregation. Third, we design an emotion discrimination objective that enforces affective consistency during the diffusion process through latent-space classification. Extensive experiments on MEAD and HDTF datasets demonstrate our method's superiority, outperforming state-of-the-art approaches in emotion accuracy while maintaining competitive lip-sync performance. Qualitative results and user studies further confirm our method's ability to generate identity-preserving portraits with rich, correlated emotional expressions that naturally adapt to unseen identities.",
        "arxiv_id": "2504.18087",
        "ARXIVID": "2504.18087",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling in multi-modal learning, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.18453": {
        "authors": [
            "Peiyuan Jing",
            "Kinhei Lee",
            "Zhenxuan Zhang",
            "Huichi Zhou",
            "Zhengqing Yuan",
            "Zhifan Gao",
            "Lei Zhu",
            "Giorgos Papanastasiou",
            "Yingying Fang",
            "Guang Yang"
        ],
        "title": "Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning for Verifiable Report Generation",
        "abstract": "arXiv:2504.18453v1 Announce Type: new  Abstract: Radiology report generation is critical for efficiency but current models lack the structured reasoning of experts, hindering clinical trust and explainability by failing to link visual findings to precise anatomical locations. This paper introduces BoxMed-RL, a groundbreaking unified training framework for generating spatially verifiable and explainable radiology reports. Built on a large vision-language model, BoxMed-RL revolutionizes report generation through two integrated phases: (1) In the Pretraining Phase, we refine the model via medical concept learning, using Chain-of-Thought supervision to internalize the radiologist-like workflow, followed by spatially verifiable reinforcement, which applies reinforcement learning to align medical findings with bounding boxes. (2) In the Downstream Adapter Phase, we freeze the pretrained weights and train a downstream adapter to ensure fluent and clinically credible reports. This framework precisely mimics radiologists' workflow, compelling the model to connect high-level medical concepts with definitive anatomical evidence. Extensive experiments on public datasets demonstrate that BoxMed-RL achieves an average 7% improvement in both METEOR and ROUGE-L metrics compared to state-of-the-art methods. An average 5% improvement in large language model-based metrics further underscores BoxMed-RL's robustness in generating high-quality radiology reports.",
        "arxiv_id": "2504.18453",
        "ARXIVID": "2504.18453",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to vision-language models and their applications in radiology.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.18201": {
        "authors": [
            "Yin Tang",
            "Jiankai Li",
            "Hongyu Yang",
            "Xuan Dong",
            "Lifeng Fan",
            "Weixin Li"
        ],
        "title": "Multi-Grained Compositional Visual Clue Learning for Image Intent Recognition",
        "abstract": "arXiv:2504.18201v1 Announce Type: new  Abstract: In an era where social media platforms abound, individuals frequently share images that offer insights into their intents and interests, impacting individual life quality and societal stability. Traditional computer vision tasks, such as object detection and semantic segmentation, focus on concrete visual representations, while intent recognition relies more on implicit visual clues. This poses challenges due to the wide variation and subjectivity of such clues, compounded by the problem of intra-class variety in conveying abstract concepts, e.g. \"enjoy life\". Existing methods seek to solve the problem by manually designing representative features or building prototypes for each class from global features. However, these methods still struggle to deal with the large visual diversity of each intent category. In this paper, we introduce a novel approach named Multi-grained Compositional visual Clue Learning (MCCL) to address these challenges for image intent recognition. Our method leverages the systematic compositionality of human cognition by breaking down intent recognition into visual clue composition and integrating multi-grained features. We adopt class-specific prototypes to alleviate data imbalance. We treat intent recognition as a multi-label classification problem, using a graph convolutional network to infuse prior knowledge through label embedding correlations. Demonstrated by a state-of-the-art performance on the Intentonomy and MDID datasets, our approach advances the accuracy of existing methods while also possessing good interpretability. Our work provides an attempt for future explorations in understanding complex and miscellaneous forms of human expression.",
        "arxiv_id": "2504.18201",
        "ARXIVID": "2504.18201",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.17902": {
        "authors": [
            "Girish A. Koushik",
            "Diptesh Kanojia",
            "Helen Treharne",
            "Aditya Joshi"
        ],
        "title": "CAMU: Context Augmentation for Meme Understanding",
        "abstract": "arXiv:2504.17902v1 Announce Type: new  Abstract: Social media memes are a challenging domain for hate detection because they intertwine visual and textual cues into culturally nuanced messages. We introduce a novel framework, CAMU, which leverages large vision-language models to generate more descriptive captions, a caption-scoring neural network to emphasise hate-relevant content, and parameter-efficient fine-tuning of CLIP's text encoder for an improved multimodal understanding of memes. Experiments on publicly available hateful meme datasets show that simple projection layer fine-tuning yields modest gains, whereas selectively tuning deeper text encoder layers significantly boosts performance on all evaluation metrics. Moreover, our approach attains high accuracy (0.807) and F1-score (0.806) on the Hateful Memes dataset, at par with the existing SoTA framework while being much more efficient, offering practical advantages in real-world scenarios that rely on fixed decision thresholds. CAMU also achieves the best F1-score of 0.673 on the MultiOFF dataset for offensive meme identification, demonstrating its generalisability. Additional analyses on benign confounders reveal that robust visual grounding and nuanced text representations are crucial for reliable hate and offence detection. We will publicly release CAMU along with the resultant models for further research.   Disclaimer: This paper includes references to potentially disturbing, hateful, or offensive content due to the nature of the task.",
        "arxiv_id": "2504.17902",
        "ARXIVID": "2504.17902",
        "COMMENT": "Does not match any specific criteria but is related to multi-modal learning and vision-language models, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.18249": {
        "authors": [
            "Qinyu Chen",
            "Chang Gao",
            "Min Liu",
            "Daniele Perrone",
            "Yan Ru Pei",
            "Zuowen Wang",
            "Zhuo Zou",
            "Shihang Tan",
            "Tao Han",
            "Guorui Lu",
            "Zhen Xu",
            "Junyuan Ding",
            "Ziteng Wang",
            "Zongwei Wu",
            "Han Han",
            "Yuliang Wu",
            "Jinze Chen",
            "Wei Zhai",
            "Yang Cao",
            "Zheng-jun Zha",
            "Nuwan Bandara",
            "Thivya Kandappu",
            "Archan Misra",
            "Xiaopeng Lin",
            "Hongxiang Huang",
            "Hongwei Ren",
            "Bojun Cheng",
            "Hoang M. Truong",
            "Vinh-Thuan Ly",
            "Huy G. Tran",
            "Thuan-Phat Nguyen",
            "Tram T. Doan"
        ],
        "title": "Event-Based Eye Tracking. 2025 Event-based Vision Workshop",
        "abstract": "arXiv:2504.18249v1 Announce Type: new  Abstract: This survey serves as a review for the 2025 Event-Based Eye Tracking Challenge organized as part of the 2025 CVPR event-based vision workshop. This challenge focuses on the task of predicting the pupil center by processing event camera recorded eye movement. We review and summarize the innovative methods from teams rank the top in the challenge to advance future event-based eye tracking research. In each method, accuracy, model size, and number of operations are reported. In this survey, we also discuss event-based eye tracking from the perspective of hardware design.",
        "arxiv_id": "2504.18249",
        "ARXIVID": "2504.18249",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to event-based vision and hardware design.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}