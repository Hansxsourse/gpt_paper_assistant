{
    "2505.22664": {
        "authors": [
            "Kaiyu Yue",
            "Vasu Singla",
            "Menglin Jia",
            "John Kirchenbauer",
            "Rifaa Qadri",
            "Zikui Cai",
            "Abhinav Bhatele",
            "Furong Huang",
            "Tom Goldstein"
        ],
        "title": "Zero-Shot Vision Encoder Grafting via LLM Surrogates",
        "abstract": "arXiv:2505.22664v1 Announce Type: new  Abstract: Vision language models (VLMs) typically pair a modestly sized vision encoder with a large language model (LLM), e.g., Llama-70B, making the decoder the primary computational burden during training. To reduce costs, a potential promising strategy is to first train the vision encoder using a small language model before transferring it to the large one. We construct small \"surrogate models\" that share the same embedding space and representation language as the large target LLM by directly inheriting its shallow layers. Vision encoders trained on the surrogate can then be directly transferred to the larger model, a process we call zero-shot grafting -- when plugged directly into the full-size target LLM, the grafted pair surpasses the encoder-surrogate pair and, on some benchmarks, even performs on par with full decoder training with the target LLM. Furthermore, our surrogate training approach reduces overall VLM training costs by ~45% when using Llama-70B as the decoder.",
        "arxiv_id": "2505.22664",
        "ARXIVID": "2505.22664",
        "COMMENT": "Matches criterion 2 as it focuses on vision-language models and proposes a cost-efficient training strategy.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2505.22657": {
        "authors": [
            "Wenbo Hu",
            "Yining Hong",
            "Yanjun Wang",
            "Leison Gao",
            "Zibu Wei",
            "Xingcheng Yao",
            "Nanyun Peng",
            "Yonatan Bitton",
            "Idan Szpektor",
            "Kai-Wei Chang"
        ],
        "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model",
        "abstract": "arXiv:2505.22657v1 Announce Type: new  Abstract: Humans excel at performing complex tasks by leveraging long-term memory across temporal and spatial experiences. In contrast, current Large Language Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D environments. We posit that part of this limitation is due to the lack of proper 3D spatial-temporal memory modeling in LLMs. To address this, we first introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000 trajectories and 2,892 embodied tasks, question-answering and captioning, designed to evaluate an agent's ability to reason over long-term memory in 3D environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management and fusion model for embodied spatial-temporal reasoning and actions in LLMs. Our model uses working memory tokens, which represents current observations, as queries to selectively attend to and fuse the most useful spatial and temporal features from episodic memory, which stores past observations and interactions. Our approach allows the agent to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments. Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art performance across various tasks, outperforming the strongest baselines by 16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied tasks.",
        "arxiv_id": "2505.22657",
        "ARXIVID": "2505.22657",
        "COMMENT": "Matches criterion 3 as it proposes a novel memory management model for embodied spatial-temporal reasoning in 3D environments.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2505.22143": {
        "authors": [
            "Fengyun Wang",
            "Sicheng Yu",
            "Jiawei Wu",
            "Jinhui Tang",
            "Hanwang Zhang",
            "Qianru Sun"
        ],
        "title": "3D Question Answering via only 2D Vision-Language Models",
        "abstract": "arXiv:2505.22143v1 Announce Type: new  Abstract: Large vision-language models (LVLMs) have significantly advanced numerous fields. In this work, we explore how to harness their potential to address 3D scene understanding tasks, using 3D question answering (3D-QA) as a representative example. Due to the limited training data in 3D, we do not train LVLMs but infer in a zero-shot manner. Specifically, we sample 2D views from a 3D point cloud and feed them into 2D models to answer a given question. When the 2D model is chosen, e.g., LLAVA-OV, the quality of sampled views matters the most. We propose cdViews, a novel approach to automatically selecting critical and diverse Views for 3D-QA. cdViews consists of two key components: viewSelector prioritizing critical views based on their potential to provide answer-specific information, and viewNMS enhancing diversity by removing redundant views based on spatial overlap. We evaluate cdViews on the widely-used ScanQA and SQA benchmarks, demonstrating that it achieves state-of-the-art performance in 3D-QA while relying solely on 2D models without fine-tuning. These findings support our belief that 2D LVLMs are currently the most effective alternative (of the resource-intensive 3D LVLMs) for addressing 3D tasks.",
        "arxiv_id": "2505.22143",
        "ARXIVID": "2505.22143",
        "COMMENT": "Matches criterion 2 as it explores the use of 2D vision-language models (LVLMs) for 3D question answering, showcasing a novel approach to leveraging LVLMs for 3D tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2505.22396": {
        "authors": [
            "Xudong Li",
            "Mengdan Zhang",
            "Peixian Chen",
            "Xiawu Zheng",
            "Yan Zhang",
            "Jingyuan Zheng",
            "Yunhang Shen",
            "Ke Li",
            "Chaoyou Fu",
            "Xing Sun",
            "Rongrong Ji"
        ],
        "title": "Zooming from Context to Cue: Hierarchical Preference Optimization for Multi-Image MLLMs",
        "abstract": "arXiv:2505.22396v1 Announce Type: new  Abstract: Multi-modal Large Language Models (MLLMs) excel at single-image tasks but struggle with multi-image understanding due to cross-modal misalignment, leading to hallucinations (context omission, conflation, and misinterpretation). Existing methods using Direct Preference Optimization (DPO) constrain optimization to a solitary image reference within the input sequence, neglecting holistic context modeling. We propose Context-to-Cue Direct Preference Optimization (CcDPO), a multi-level preference optimization framework that enhances per-image perception in multi-image settings by zooming into visual clues -- from sequential context to local details. It features: (i) Context-Level Optimization : Re-evaluates cognitive biases underlying MLLMs' multi-image context comprehension and integrates a spectrum of low-cost global sequence preferences for bias mitigation. (ii) Needle-Level Optimization : Directs attention to fine-grained visual details through region-targeted visual prompts and multimodal preference supervision. To support scalable optimization, we also construct MultiScope-42k, an automatically generated dataset with high-quality multi-level preference pairs. Experiments show that CcDPO significantly reduces hallucinations and yields consistent performance gains across general single- and multi-image tasks.",
        "arxiv_id": "2505.22396",
        "ARXIVID": "2505.22396",
        "COMMENT": "Matches criterion 2 as it proposes a framework for improving multi-image understanding in multimodal large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2505.22050": {
        "authors": [
            "Di Wu",
            "Jiaxin Fan",
            "Junzhe Zang",
            "Guanbo Wang",
            "Wei Yin",
            "Wenhao Li",
            "Bo Jin"
        ],
        "title": "Reinforced Reasoning for Embodied Planning",
        "abstract": "arXiv:2505.22050v1 Announce Type: new  Abstract: Embodied planning requires agents to make coherent multi-step decisions based on dynamic visual observations and natural language goals. While recent vision-language models (VLMs) excel at static perception tasks, they struggle with the temporal reasoning, spatial understanding, and commonsense grounding needed for planning in interactive environments. In this work, we introduce a reinforcement fine-tuning framework that brings R1-style reasoning enhancement into embodied planning. We first distill a high-quality dataset from a powerful closed-source model and perform supervised fine-tuning (SFT) to equip the model with structured decision-making priors. We then design a rule-based reward function tailored to multi-step action quality and optimize the policy via Generalized Reinforced Preference Optimization (GRPO). Our approach is evaluated on Embench, a recent benchmark for interactive embodied tasks, covering both in-domain and out-of-domain scenarios. Experimental results show that our method significantly outperforms models of similar or larger scale, including GPT-4o-mini and 70B+ open-source baselines, and exhibits strong generalization to unseen environments. This work highlights the potential of reinforcement-driven reasoning to advance long-horizon planning in embodied AI.",
        "arxiv_id": "2505.22050",
        "ARXIVID": "2505.22050",
        "COMMENT": "Matches criterion 3 as it introduces a reinforcement fine-tuning framework for embodied planning, focusing on reasoning enhancement in interactive environments.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2505.22651": {
        "authors": [
            "Yi Ding",
            "Ruqi Zhang"
        ],
        "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models",
        "abstract": "arXiv:2505.22651v1 Announce Type: new  Abstract: Reasoning Vision-Language Models (VLMs) have shown promising performance on complex multimodal tasks. However, they still face significant challenges: they are highly sensitive to reasoning errors, require large volumes of annotated data or accurate verifiers, and struggle to generalize beyond specific domains. To address these limitations, we explore self-correction as a strategy to enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning VLMs' self-correction abilities and identify key gaps. Based on our findings, we introduce Sherlock, a self-correction and self-improvement training framework. Sherlock introduces a trajectory-level self-correction objective, a preference data construction method based on visual perturbation, and a dynamic $\\beta$ for preference tuning. Once the model acquires self-correction capabilities using only 20k randomly sampled annotated data, it continues to self-improve without external supervision. Built on the Llama3.2-Vision-11B model, Sherlock achieves remarkable results across eight benchmarks, reaching an average accuracy of 64.1 with direct generation and 65.4 after self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and LlamaV-o1 (63.4) while using less than 20% of the annotated data.",
        "arxiv_id": "2505.22651",
        "ARXIVID": "2505.22651",
        "COMMENT": "Matches criterion 2 as it introduces a self-correcting framework for reasoning in vision-language models, which is relevant to improving VLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2505.21850": {
        "authors": [
            "Yanbei Jiang",
            "Yihao Ding",
            "Chao Lei",
            "Jiayang Ao",
            "Jey Han Lau",
            "Krista A. Ehinger"
        ],
        "title": "Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task",
        "abstract": "arXiv:2505.21850v1 Announce Type: new  Abstract: Current Multimodal Large Language Models (MLLMs) excel in general visual reasoning but remain underexplored in Abstract Visual Reasoning (AVR), which demands higher-order reasoning to identify abstract rules beyond simple perception. Existing AVR benchmarks focus on single-step reasoning, emphasizing the end result but neglecting the multi-stage nature of reasoning process. Past studies found MLLMs struggle with these benchmarks, but it doesn't explain how they fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR benchmark, based on RAVEN, designed to assess reasoning across varying levels of complexity. Additionally, existing metrics like accuracy only focus on the final outcomes while do not account for the correctness of intermediate steps. Therefore, we propose a novel metric, MSEval, which considers the correctness of intermediate steps in addition to the final outcomes. We conduct comprehensive experiments on MultiStAR using 17 representative close-source and open-source MLLMs. The results reveal that while existing MLLMs perform adequately on basic perception tasks, they continue to face challenges in more complex rule detection stages.",
        "arxiv_id": "2505.21850",
        "ARXIVID": "2505.21850",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (MultiStAR) for abstract visual reasoning in MLLMs, focusing on multi-stage reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2505.22039": {
        "authors": [
            "Shifang Zhao",
            "Yiheng Lin",
            "Lu Han",
            "Yao Zhao",
            "Yunchao Wei"
        ],
        "title": "OmniAD: Detect and Understand Industrial Anomaly via Multimodal Reasoning",
        "abstract": "arXiv:2505.22039v1 Announce Type: new  Abstract: While anomaly detection has made significant progress, generating detailed analyses that incorporate industrial knowledge remains a challenge. To address this gap, we introduce OmniAD, a novel framework that unifies anomaly detection and understanding for fine-grained analysis. OmniAD is a multimodal reasoner that combines visual and textual reasoning processes. The visual reasoning provides detailed inspection by leveraging Text-as-Mask Encoding to perform anomaly detection through text generation without manually selected thresholds. Following this, Visual Guided Textual Reasoning conducts comprehensive analysis by integrating visual perception. To enhance few-shot generalization, we employ an integrated training strategy that combines supervised fine-tuning (SFT) with reinforcement learning (GRPO), incorporating three sophisticated reward functions. Experimental results demonstrate that OmniAD achieves a performance of 79.1 on the MMAD benchmark, surpassing models such as Qwen2.5-VL-7B and GPT-4o. It also shows strong results across multiple anomaly detection benchmarks. These results highlight the importance of enhancing visual perception for effective reasoning in anomaly understanding. All codes and models will be publicly available.",
        "arxiv_id": "2505.22039",
        "ARXIVID": "2505.22039",
        "COMMENT": "Matches criterion 2 as it introduces a multimodal reasoning framework for anomaly detection and understanding, combining visual and textual reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.21956": {
        "authors": [
            "Mengdan Zhu",
            "Senhao Cheng",
            "Guangji Bai",
            "Yifei Zhang",
            "Liang Zhao"
        ],
        "title": "Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation",
        "abstract": "arXiv:2505.21956v1 Announce Type: new  Abstract: Text-to-image generation increasingly demands access to domain-specific, fine-grained, and rapidly evolving knowledge that pretrained models cannot fully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to address this by retrieving globally relevant images, but they fail when no single image contains all desired elements from a complex user query. We propose Cross-modal RAG, a novel framework that decomposes both queries and images into sub-dimensional components, enabling subquery-aware retrieval and generation. Our method introduces a hybrid retrieval strategy - combining a sub-dimensional sparse retriever with a dense retriever - to identify a Pareto-optimal set of images, each contributing complementary aspects of the query. During generation, a multimodal large language model is guided to selectively condition on relevant visual features aligned to specific subqueries, ensuring subquery-aware image synthesis. Extensive experiments on MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal RAG significantly outperforms existing baselines in both retrieval and generation quality, while maintaining high efficiency.",
        "arxiv_id": "2505.21956",
        "ARXIVID": "2505.21956",
        "COMMENT": "Matches criterion 2 as it proposes a novel framework for text-to-image generation using multimodal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.22129": {
        "authors": [
            "Jinhong Ni",
            "Chang-Bin Zhang",
            "Qiang Zhang",
            "Jing Zhang"
        ],
        "title": "What Makes for Text to 360-degree Panorama Generation with Stable Diffusion?",
        "abstract": "arXiv:2505.22129v1 Announce Type: new  Abstract: Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion, has stimulated research to adapt them to 360-degree panorama generation. Prior work has demonstrated the feasibility of using conventional low-rank adaptation techniques on pre-trained diffusion models to generate panoramic images. However, the substantial domain gap between perspective and panoramic images raises questions about the underlying mechanisms enabling this empirical success. We hypothesize and examine that the trainable counterparts exhibit distinct behaviors when fine-tuned on panoramic data, and such an adaptation conceals some intrinsic mechanism to leverage the prior knowledge within the pre-trained diffusion models. Our analysis reveals the following: 1) the query and key matrices in the attention modules are responsible for common information that can be shared between the panoramic and perspective domains, thus are less relevant to panorama generation; and 2) the value and output weight matrices specialize in adapting pre-trained knowledge to the panoramic domain, playing a more critical role during fine-tuning for panorama generation. We empirically verify these insights by introducing a simple framework called UniPano, with the objective of establishing an elegant baseline for future research. UniPano not only outperforms existing methods but also significantly reduces memory usage and training time compared to prior dual-branch approaches, making it scalable for end-to-end panorama generation with higher resolution. The code will be released.",
        "arxiv_id": "2505.22129",
        "ARXIVID": "2505.22129",
        "COMMENT": "Matches criterion 4 as it focuses on adapting vision foundation models (Stable Diffusion) for 360-degree panorama generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.21635": {
        "authors": [
            "Haoqian Liang",
            "Xiaohui Wang",
            "Zhichao Li",
            "Ya Yang",
            "Naiyan Wang"
        ],
        "title": "Object Concepts Emerge from Motion",
        "abstract": "arXiv:2505.21635v1 Announce Type: new  Abstract: Object concepts play a foundational role in human visual cognition, enabling perception, memory, and interaction in the physical world. Inspired by findings in developmental neuroscience - where infants are shown to acquire object understanding through observation of motion - we propose a biologically inspired framework for learning object-centric visual representations in an unsupervised manner. Our key insight is that motion boundary serves as a strong signal for object-level grouping, which can be used to derive pseudo instance supervision from raw videos. Concretely, we generate motion-based instance masks using off-the-shelf optical flow and clustering algorithms, and use them to train visual encoders via contrastive learning. Our framework is fully label-free and does not rely on camera calibration, making it scalable to large-scale unstructured video data. We evaluate our approach on three downstream tasks spanning both low-level (monocular depth estimation) and high-level (3D object detection and occupancy prediction) vision. Our models outperform previous supervised and self-supervised baselines and demonstrate strong generalization to unseen scenes. These results suggest that motion-induced object representations offer a compelling alternative to existing vision foundation models, capturing a crucial but overlooked level of abstraction: the visual instance. The corresponding code will be released upon paper acceptance.",
        "arxiv_id": "2505.21635",
        "ARXIVID": "2505.21635",
        "COMMENT": "Matches criterion 1 as it proposes a new method for spatial understanding using motion boundaries for object-centric visual representations.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.22087": {
        "authors": [
            "Ruxiao Chen",
            "Dezheng Han",
            "Wenjie Han",
            "Shuaishuai Guo"
        ],
        "title": "Cognitively-Inspired Emergent Communication via Knowledge Graphs for Assisting the Visually Impaired",
        "abstract": "arXiv:2505.22087v1 Announce Type: new  Abstract: Assistive systems for visually impaired individuals must deliver rapid, interpretable, and adaptive feedback to facilitate real-time navigation. Current approaches face a trade-off between latency and semantic richness: natural language-based systems provide detailed guidance but are too slow for dynamic scenarios, while emergent communication frameworks offer low-latency symbolic languages but lack semantic depth, limiting their utility in tactile modalities like vibration. To address these limitations, we introduce a novel framework, Cognitively-Inspired Emergent Communication via Knowledge Graphs (VAG-EC), which emulates human visual perception and cognitive mapping. Our method constructs knowledge graphs to represent objects and their relationships, incorporating attention mechanisms to prioritize task-relevant entities, thereby mirroring human selective attention. This structured approach enables the emergence of compact, interpretable, and context-sensitive symbolic languages. Extensive experiments across varying vocabulary sizes and message lengths demonstrate that VAG-EC outperforms traditional emergent communication methods in Topographic Similarity (TopSim) and Context Independence (CI). These findings underscore the potential of cognitively grounded emergent communication as a fast, adaptive, and human-aligned solution for real-time assistive technologies. Code is available at https://github.com/Anonymous-NLPcode/Anonymous_submission/tree/main.",
        "arxiv_id": "2505.22087",
        "ARXIVID": "2505.22087",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework for assistive technologies using emergent communication and knowledge graphs, which is a novel angle in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.21567": {
        "authors": [
            "Feng Jiang",
            "Zihao Zheng",
            "Xiuping Cui",
            "Maoliang Li",
            "JIayu Chen",
            "Xiang Chen"
        ],
        "title": "EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models",
        "abstract": "arXiv:2505.21567v1 Announce Type: new  Abstract: With the development of Embodied Artificial intelligence, the end-to-end control policy such as Vision-Language-Action (VLA) model has become the mainstream. Existing VLA models faces expensive computing/storage cost, which need to be optimized. Quantization is considered as the most effective method which can not only reduce the memory cost but also achieve computation acceleration. However, we find the token alignment of VLA models hinders the application of existing quantization methods. To address this, we proposed an optimized framework called EaqVLA, which apply encoding-aligned quantization to VLA models. Specifically, we propose an complete analysis method to find the misalignment in various granularity. Based on the analysis results, we propose a mixed precision quantization with the awareness of encoding alignment. Experiments shows that the porposed EaqVLA achieves better quantization performance (with the minimal quantization loss for end-to-end action control and xxx times acceleration) than existing quantization methods.",
        "arxiv_id": "2505.21567",
        "ARXIVID": "2505.21567",
        "COMMENT": "Matches criterion 1 as it proposes a quantization framework (EaqVLA) for Vision-Language-Action models, optimizing their spatial understanding and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.22596": {
        "authors": [
            "Jiaqi Huang",
            "Zunnan Xu",
            "Jun Zhou",
            "Ting Liu",
            "Yicheng Xiao",
            "Mingwen Ou",
            "Bowen Ji",
            "Xiu Li",
            "Kehong Yuan"
        ],
        "title": "SAM-R1: Leveraging SAM for Reward Feedback in Multimodal Segmentation via Reinforcement Learning",
        "abstract": "arXiv:2505.22596v1 Announce Type: new  Abstract: Leveraging multimodal large models for image segmentation has become a prominent research direction. However, existing approaches typically rely heavily on manually annotated datasets that include explicit reasoning processes, which are costly and time-consuming to produce. Recent advances suggest that reinforcement learning (RL) can endow large models with reasoning capabilities without requiring such reasoning-annotated data. In this paper, we propose SAM-R1, a novel framework that enables multimodal large models to perform fine-grained reasoning in image understanding tasks. Our approach is the first to incorporate fine-grained segmentation settings during the training of multimodal reasoning models. By integrating task-specific, fine-grained rewards with a tailored optimization objective, we further enhance the model's reasoning and segmentation alignment. We also leverage the Segment Anything Model (SAM) as a strong and flexible reward provider to guide the learning process. With only 3k training samples, SAM-R1 achieves strong performance across multiple benchmarks, demonstrating the effectiveness of reinforcement learning in equipping multimodal models with segmentation-oriented reasoning capabilities.",
        "arxiv_id": "2505.22596",
        "ARXIVID": "2505.22596",
        "COMMENT": "Matches criterion 2 as it leverages reinforcement learning to enhance multimodal large models for fine-grained segmentation tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.22098": {
        "authors": [
            "Junhuan Liu",
            "San Jiang",
            "Wei Ge",
            "Wei Huang",
            "Bingxuan Guo",
            "Qingquan Li"
        ],
        "title": "UAVPairs: A Challenging Benchmark for Match Pair Retrieval of Large-scale UAV Images",
        "abstract": "arXiv:2505.22098v1 Announce Type: new  Abstract: The primary contribution of this paper is a challenging benchmark dataset, UAVPairs, and a training pipeline designed for match pair retrieval of large-scale UAV images. First, the UAVPairs dataset, comprising 21,622 high-resolution images across 30 diverse scenes, is constructed; the 3D points and tracks generated by SfM-based 3D reconstruction are employed to define the geometric similarity of image pairs, ensuring genuinely matchable image pairs are used for training. Second, to solve the problem of expensive mining cost for global hard negative mining, a batched nontrivial sample mining strategy is proposed, leveraging the geometric similarity and multi-scene structure of the UAVPairs to generate training samples as to accelerate training. Third, recognizing the limitation of pair-based losses, the ranked list loss is designed to improve the discrimination of image retrieval models, which optimizes the global similarity structure constructed from the positive set and negative set. Finally, the effectiveness of the UAVPairs dataset and training pipeline is validated through comprehensive experiments on three distinct large-scale UAV datasets. The experiment results demonstrate that models trained with the UAVPairs dataset and the ranked list loss achieve significantly improved retrieval accuracy compared to models trained on existing datasets or with conventional losses. Furthermore, these improvements translate to enhanced view graph connectivity and higher quality of reconstructed 3D models. The models trained by the proposed approach perform more robustly compared with hand-crafted global features, particularly in challenging repetitively textured scenes and weakly textured scenes. For match pair retrieval of large-scale UAV images, the trained image retrieval models offer an effective solution. The dataset would be made publicly available at https://github.com/json87/UAVPairs.",
        "arxiv_id": "2505.22098",
        "ARXIVID": "2505.22098",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (UAVPairs) for match pair retrieval in UAV images and proposes novel training strategies.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.22457": {
        "authors": [
            "Haonan Wang",
            "Hongfu Liu",
            "Xiangyan Liu",
            "Chao Du",
            "Kenji Kawaguchi",
            "Ye Wang",
            "Tianyu Pang"
        ],
        "title": "Fostering Video Reasoning via Next-Event Prediction",
        "abstract": "arXiv:2505.22457v1 Announce Type: new  Abstract: Next-token prediction serves as the foundational learning task enabling reasoning in LLMs. But what should the learning task be when aiming to equip MLLMs with temporal reasoning capabilities over video inputs? Existing tasks such as video question answering often rely on annotations from humans or much stronger MLLMs, while video captioning tends to entangle temporal reasoning with spatial information. To address this gap, we propose next-event prediction (NEP), a learning task that harnesses future video segments as a rich, self-supervised signal to foster temporal reasoning. We segment each video into past and future frames: the MLLM takes the past frames as input and predicts a summary of events derived from the future frames, thereby encouraging the model to reason temporally in order to complete the task. To support this task, we curate V1-33K, a dataset comprising 33,000 automatically extracted video segments spanning diverse real-world scenarios. We further explore a range of video instruction-tuning strategies to study their effects on temporal reasoning. To evaluate progress, we introduce FutureBench to assess coherence in predicting unseen future events. Experiments validate that NEP offers a scalable and effective training paradigm for fostering temporal reasoning in MLLMs.",
        "arxiv_id": "2505.22457",
        "ARXIVID": "2505.22457",
        "COMMENT": "Matches criterion 2 as it proposes a new task (next-event prediction) for fostering temporal reasoning in multi-modal large language models (MLLMs).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.21962": {
        "authors": [
            "Mengjingcheng Mo",
            "Xinyang Tong",
            "Jiaxu Leng",
            "Mingpi Tan",
            "Jiankang Zheng",
            "Yiran Liu",
            "Haosheng Chen",
            "Ji Gan",
            "Weisheng Li",
            "Xinbo Gao"
        ],
        "title": "A2Seek: Towards Reasoning-Centric Benchmark for Aerial Anomaly Understanding",
        "abstract": "arXiv:2505.21962v1 Announce Type: new  Abstract: While unmanned aerial vehicles (UAVs) offer wide-area, high-altitude coverage for anomaly detection, they face challenges such as dynamic viewpoints, scale variations, and complex scenes. Existing datasets and methods, mainly designed for fixed ground-level views, struggle to adapt to these conditions, leading to significant performance drops in drone-view scenarios. To bridge this gap, we introduce A2Seek (Aerial Anomaly Seek), a large-scale, reasoning-centric benchmark dataset for aerial anomaly understanding. This dataset covers various scenarios and environmental conditions, providing high-resolution real-world aerial videos with detailed annotations, including anomaly categories, frame-level timestamps, region-level bounding boxes, and natural language explanations for causal reasoning. Building on this dataset, we propose A2Seek-R1, a novel reasoning framework that generalizes R1-style strategies to aerial anomaly understanding, enabling a deeper understanding of \"Where\" anomalies occur and \"Why\" they happen in aerial frames. To this end, A2Seek-R1 first employs a graph-of-thought (GoT)-guided supervised fine-tuning approach to activate the model's latent reasoning capabilities on A2Seek. Then, we introduce Aerial Group Relative Policy Optimization (A-GRPO) to design rule-based reward functions tailored to aerial scenarios. Furthermore, we propose a novel \"seeking\" mechanism that simulates UAV flight behavior by directing the model's attention to informative regions. Extensive experiments demonstrate that A2Seek-R1 achieves up to a 22.04% improvement in AP for prediction accuracy and a 13.9% gain in mIoU for anomaly localization, exhibiting strong generalization across complex environments and out-of-distribution scenarios. Our dataset and code will be released at https://hayneyday.github.io/A2Seek/.",
        "arxiv_id": "2505.21962",
        "ARXIVID": "2505.21962",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (A2Seek) for aerial anomaly understanding with a novel reasoning framework.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.21539": {
        "authors": [
            "Ziming Wang",
            "Nan Xue",
            "Rebecka J\\\"ornsten"
        ],
        "title": "Equivariant Flow Matching for Point Cloud Assembly",
        "abstract": "arXiv:2505.21539v1 Announce Type: new  Abstract: The goal of point cloud assembly is to reconstruct a complete 3D shape by aligning multiple point cloud pieces. This work presents a novel equivariant solver for assembly tasks based on flow matching models. We first theoretically show that the key to learning equivariant distributions via flow matching is to learn related vector fields. Based on this result, we propose an assembly model, called equivariant diffusion assembly (Eda), which learns related vector fields conditioned on the input pieces. We further construct an equivariant path for Eda, which guarantees high data efficiency of the training process. Our numerical results show that Eda is highly competitive on practical datasets, and it can even handle the challenging situation where the input pieces are non-overlapped.",
        "arxiv_id": "2505.21539",
        "ARXIVID": "2505.21539",
        "COMMENT": "Matches criterion 1 as it proposes a novel equivariant solver for spatial understanding in point cloud assembly.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.22200": {
        "authors": [
            "Darshana Saravanan",
            "Makarand Tapaswi",
            "Vineet Gandhi"
        ],
        "title": "Investigating Mechanisms for In-Context Vision Language Binding",
        "abstract": "arXiv:2505.22200v1 Announce Type: new  Abstract: To understand a prompt, Vision-Language models (VLMs) must perceive the image, comprehend the text, and build associations within and across both modalities. For instance, given an 'image of a red toy car', the model should associate this image to phrases like 'car', 'red toy', 'red object', etc. Feng and Steinhardt propose the Binding ID mechanism in LLMs, suggesting that the entity and its corresponding attribute tokens share a Binding ID in the model activations. We investigate this for image-text binding in VLMs using a synthetic dataset and task that requires models to associate 3D objects in an image with their descriptions in the text. Our experiments demonstrate that VLMs assign a distinct Binding ID to an object's image tokens and its textual references, enabling in-context association.",
        "arxiv_id": "2505.22200",
        "ARXIVID": "2505.22200",
        "COMMENT": "Matches criterion 2 as it investigates mechanisms for vision-language binding in vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2505.22523": {
        "authors": [
            "Junwen Chen",
            "Heyang Jiang",
            "Yanbin Wang",
            "Keming Wu",
            "Ji Li",
            "Chao Zhang",
            "Keiji Yanai",
            "Dong Chen",
            "Yuhui Yuan"
        ],
        "title": "PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image Generative Models",
        "abstract": "arXiv:2505.22523v1 Announce Type: new  Abstract: Generating high-quality, multi-layer transparent images from text prompts can unlock a new level of creative control, allowing users to edit each layer as effortlessly as editing text outputs from LLMs. However, the development of multi-layer generative models lags behind that of conventional text-to-image models due to the absence of a large, high-quality corpus of multi-layer transparent data. In this paper, we address this fundamental challenge by: (i) releasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro) dataset of 200K (20K) multilayer transparent images with accurate alpha mattes, (ii) introducing a trainingfree synthesis pipeline that generates such data on demand using off-the-shelf diffusion models, and (iii) delivering a strong, open-source multi-layer generation model, ART+, which matches the aesthetics of modern text-to-image generation models. The key technical contributions include: LayerFLUX, which excels at generating high-quality single transparent layers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple LayerFLUX outputs into complete images, guided by human-annotated semantic layout. To ensure higher quality, we apply a rigorous filtering stage to remove artifacts and semantic mismatches, followed by human selection. Fine-tuning the state-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which outperforms the original ART in 60% of head-to-head user study comparisons and even matches the visual quality of images generated by the FLUX.1-[dev] model. We anticipate that our work will establish a solid dataset foundation for the multi-layer transparent image generation task, enabling research and applications that require precise, editable, and visually compelling layered imagery.",
        "arxiv_id": "2505.22523",
        "ARXIVID": "2505.22523",
        "COMMENT": "Matches criterion 4 as it introduces a dataset and methods for multi-layer transparent image generation, which is related to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2505.21911": {
        "authors": [
            "Yiheng Lin",
            "Shifang Zhao",
            "Ting Liu",
            "Xiaochao Qu",
            "Luoqi Liu",
            "Yao Zhao",
            "Yunchao Wei"
        ],
        "title": "AlignGen: Boosting Personalized Image Generation with Cross-Modality Prior Alignment",
        "abstract": "arXiv:2505.21911v1 Announce Type: new  Abstract: Personalized image generation aims to integrate user-provided concepts into text-to-image models, enabling the generation of customized content based on a given prompt. Recent zero-shot approaches, particularly those leveraging diffusion transformers, incorporate reference image information through multi-modal attention mechanism. This integration allows the generated output to be influenced by both the textual prior from the prompt and the visual prior from the reference image. However, we observe that when the prompt and reference image are misaligned, the generated results exhibit a stronger bias toward the textual prior, leading to a significant loss of reference content. To address this issue, we propose AlignGen, a Cross-Modality Prior Alignment mechanism that enhances personalized image generation by: 1) introducing a learnable token to bridge the gap between the textual and visual priors, 2) incorporating a robust training strategy to ensure proper prior alignment, and 3) employing a selective cross-modal attention mask within the multi-modal attention mechanism to further align the priors. Experimental results demonstrate that AlignGen outperforms existing zero-shot methods and even surpasses popular test-time optimization approaches.",
        "arxiv_id": "2505.21911",
        "ARXIVID": "2505.21911",
        "COMMENT": "Matches criterion 2 as it proposes a novel mechanism for personalized image generation using cross-modality alignment, which is relevant to multi-modal large language models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2505.21754": {
        "authors": [
            "Martin B\\\"uchner",
            "Liza Dahiya",
            "Simon Dorer",
            "Vipul Ramtekkar",
            "Kenji Nishimiya",
            "Daniele Cattaneo",
            "Abhinav Valada"
        ],
        "title": "Visual Loop Closure Detection Through Deep Graph Consensus",
        "abstract": "arXiv:2505.21754v1 Announce Type: new  Abstract: Visual loop closure detection traditionally relies on place recognition methods to retrieve candidate loops that are validated using computationally expensive RANSAC-based geometric verification. As false positive loop closures significantly degrade downstream pose graph estimates, verifying a large number of candidates in online simultaneous localization and mapping scenarios is constrained by limited time and compute resources. While most deep loop closure detection approaches only operate on pairs of keyframes, we relax this constraint by considering neighborhoods of multiple keyframes when detecting loops. In this work, we introduce LoopGNN, a graph neural network architecture that estimates loop closure consensus by leveraging cliques of visually similar keyframes retrieved through place recognition. By propagating deep feature encodings among nodes of the clique, our method yields high-precision estimates while maintaining high recall. Extensive experimental evaluations on the TartanDrive 2.0 and NCLT datasets demonstrate that LoopGNN outperforms traditional baselines. Additionally, an ablation study across various keypoint extractors demonstrates that our method is robust, regardless of the type of deep feature encodings used, and exhibits higher computational efficiency compared to classical geometric verification baselines. We release our code, supplementary material, and keyframe data at https://loopgnn.cs.uni-freiburg.de.",
        "arxiv_id": "2505.21754",
        "ARXIVID": "2505.21754",
        "COMMENT": "Matches criterion 1 as it introduces a new graph neural network architecture for spatial understanding in loop closure detection.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2505.22569": {
        "authors": [
            "Dmitrii Sorokin",
            "Maksim Nakhodnov",
            "Andrey Kuznetsov",
            "Aibek Alanov"
        ],
        "title": "ImageReFL: Balancing Quality and Diversity in Human-Aligned Diffusion Models",
        "abstract": "arXiv:2505.22569v1 Announce Type: new  Abstract: Recent advances in diffusion models have led to impressive image generation capabilities, but aligning these models with human preferences remains challenging. Reward-based fine-tuning using models trained on human feedback improves alignment but often harms diversity, producing less varied outputs. In this work, we address this trade-off with two contributions. First, we introduce \\textit{combined generation}, a novel sampling strategy that applies a reward-tuned diffusion model only in the later stages of the generation process, while preserving the base model for earlier steps. This approach mitigates early-stage overfitting and helps retain global structure and diversity. Second, we propose \\textit{ImageReFL}, a fine-tuning method that improves image diversity with minimal loss in quality by training on real images and incorporating multiple regularizers, including diffusion and ReFL losses. Our approach outperforms conventional reward tuning methods on standard quality and diversity metrics. A user study further confirms that our method better balances human preference alignment and visual diversity. The source code can be found at https://github.com/ControlGenAI/ImageReFL .",
        "arxiv_id": "2505.22569",
        "ARXIVID": "2505.22569",
        "COMMENT": "Matches criterion 4 as it focuses on improving diffusion models for image generation, balancing quality and diversity.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.22065": {
        "authors": [
            "Mikko Impi\\\"o",
            "Philipp M. Rehsen",
            "Tiina Laamanen",
            "Arne J. Beermann",
            "Florian Leese",
            "Jenni Raitoharju"
        ],
        "title": "AquaMonitor: A multimodal multi-view image sequence dataset for real-life aquatic invertebrate biodiversity monitoring",
        "abstract": "arXiv:2505.22065v1 Announce Type: new  Abstract: This paper presents the AquaMonitor dataset, the first large computer vision dataset of aquatic invertebrates collected during routine environmental monitoring. While several large species identification datasets exist, they are rarely collected using standardized collection protocols, and none focus on aquatic invertebrates, which are particularly laborious to collect. For AquaMonitor, we imaged all specimens from two years of monitoring whenever imaging was possible given practical limitations. The dataset enables the evaluation of automated identification methods for real-life monitoring purposes using a realistically challenging and unbiased setup. The dataset has 2.7M images from 43,189 specimens, DNA sequences for 1358 specimens, and dry mass and size measurements for 1494 specimens, making it also one of the largest biological multi-view and multimodal datasets to date. We define three benchmark tasks and provide strong baselines for these: 1) Monitoring benchmark, reflecting real-life deployment challenges such as open-set recognition, distribution shift, and extreme class imbalance, 2) Classification benchmark, which follows a standard fine-grained visual categorization setup, and 3) Few-shot benchmark, which targets classes with only few training examples from very fine-grained categories. Advancements on the Monitoring benchmark can directly translate to improvement of aquatic biodiversity monitoring, which is an important component of regular legislative water quality assessment in many countries.",
        "arxiv_id": "2505.22065",
        "ARXIVID": "2505.22065",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for aquatic biodiversity monitoring with multimodal and multi-view data.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2505.22150": {
        "authors": [
            "Runze Xia",
            "Shuo Feng",
            "Renzhi Wang",
            "Congchi Yin",
            "Xuyun Wen",
            "Piji Li"
        ],
        "title": "Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging",
        "abstract": "arXiv:2505.22150v1 Announce Type: new  Abstract: Brain-to-Image reconstruction aims to recover visual stimuli perceived by humans from brain activity. However, the reconstructed visual stimuli often missing details and semantic inconsistencies, which may be attributed to insufficient semantic information. To address this issue, we propose an approach named Fine-grained Brain-to-Image reconstruction (FgB2I), which employs fine-grained text as bridge to improve image reconstruction. FgB2I comprises three key stages: detail enhancement, decoding fine-grained text descriptions, and text-bridged brain-to-image reconstruction. In the detail-enhancement stage, we leverage large vision-language models to generate fine-grained captions for visual stimuli and experimentally validate its importance. We propose three reward metrics (object accuracy, text-image semantic similarity, and image-image semantic similarity) to guide the language model in decoding fine-grained text descriptions from fMRI signals. The fine-grained text descriptions can be integrated into existing reconstruction methods to achieve fine-grained Brain-to-Image reconstruction.",
        "arxiv_id": "2505.22150",
        "ARXIVID": "2505.22150",
        "COMMENT": "Matches criterion 2 as it employs large vision-language models for fine-grained text generation and brain-to-image reconstruction.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.22461": {
        "authors": [
            "Qiucheng Yu",
            "Yuan Xie",
            "Xin Tan"
        ],
        "title": "SHTOcc: Effective 3D Occupancy Prediction with Sparse Head and Tail Voxels",
        "abstract": "arXiv:2505.22461v1 Announce Type: new  Abstract: 3D occupancy prediction has attracted much attention in the field of autonomous driving due to its powerful geometric perception and object recognition capabilities. However, existing methods have not explored the most essential distribution patterns of voxels, resulting in unsatisfactory results. This paper first explores the inter-class distribution and geometric distribution of voxels, thereby solving the long-tail problem caused by the inter-class distribution and the poor performance caused by the geometric distribution. Specifically, this paper proposes SHTOcc (Sparse Head-Tail Occupancy), which uses sparse head-tail voxel construction to accurately identify and balance key voxels in the head and tail classes, while using decoupled learning to reduce the model's bias towards the dominant (head) category and enhance the focus on the tail class. Experiments show that significant improvements have been made on multiple baselines: SHTOcc reduces GPU memory usage by 42.2%, increases inference speed by 58.6%, and improves accuracy by about 7%, verifying its effectiveness and efficiency. The code is available at https://github.com/ge95net/SHTOcc",
        "arxiv_id": "2505.22461",
        "ARXIVID": "2505.22461",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for 3D occupancy prediction with sparse head-tail voxel construction, addressing overlooked distribution patterns.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.22222": {
        "authors": [
            "Yunsoo Kim",
            "Jinge Wu",
            "Su-Hwan Kim",
            "Pardeep Vasudev",
            "Jiashu Shen",
            "Honghan Wu"
        ],
        "title": "Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation",
        "abstract": "arXiv:2505.22222v1 Announce Type: new  Abstract: Recent advancements in multimodal Large Language Models (LLMs) have significantly enhanced the automation of medical image analysis, particularly in generating radiology reports from chest X-rays (CXR). However, these models still suffer from hallucinations and clinically significant errors, limiting their reliability in real-world applications. In this study, we propose Look & Mark (L&M), a novel grounding fixation strategy that integrates radiologist eye fixations (Look) and bounding box annotations (Mark) into the LLM prompting framework. Unlike conventional fine-tuning, L&M leverages in-context learning to achieve substantial performance gains without retraining. When evaluated across multiple domain-specific and general-purpose models, L&M demonstrates significant gains, including a 1.2% improvement in overall metrics (A.AVG) for CXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for LLaVA-Med. General-purpose models also benefit from L&M combined with in-context learning, with LLaVA-OV achieving an 87.3% clinical average performance (C.AVG)-the highest among all models, even surpassing those explicitly trained for CXR report generation. Expert evaluations further confirm that L&M reduces clinically significant errors (by 0.43 average errors per report), such as false predictions and omissions, enhancing both accuracy and reliability. These findings highlight L&M's potential as a scalable and efficient solution for AI-assisted radiology, paving the way for improved diagnostic workflows in low-resource clinical settings.",
        "arxiv_id": "2505.22222",
        "ARXIVID": "2505.22222",
        "COMMENT": "Matches criterion 2 as it introduces a novel grounding fixation strategy for multimodal large language models in medical imaging.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.22522": {
        "authors": [
            "Yuan Zhang",
            "Feng Chen",
            "Yaolei Qi",
            "Guanyu Yang",
            "Huazhu Fu"
        ],
        "title": "PathFL: Multi-Alignment Federated Learning for Pathology Image Segmentation",
        "abstract": "arXiv:2505.22522v1 Announce Type: new  Abstract: Pathology image segmentation across multiple centers encounters significant challenges due to diverse sources of heterogeneity including imaging modalities, organs, and scanning equipment, whose variability brings representation bias and impedes the development of generalizable segmentation models. In this paper, we propose PathFL, a novel multi-alignment Federated Learning framework for pathology image segmentation that addresses these challenges through three-level alignment strategies of image, feature, and model aggregation. Firstly, at the image level, a collaborative style enhancement module aligns and diversifies local data by facilitating style information exchange across clients. Secondly, at the feature level, an adaptive feature alignment module ensures implicit alignment in the representation space by infusing local features with global insights, promoting consistency across heterogeneous client features learning. Finally, at the model aggregation level, a stratified similarity aggregation strategy hierarchically aligns and aggregates models on the server, using layer-specific similarity to account for client discrepancies and enhance global generalization. Comprehensive evaluations on four sets of heterogeneous pathology image datasets, encompassing cross-source, cross-modality, cross-organ, and cross-scanner variations, validate the effectiveness of our PathFL in achieving better performance and robustness against data heterogeneity.",
        "arxiv_id": "2505.22522",
        "ARXIVID": "2505.22522",
        "COMMENT": "Matches criterion 3 as it proposes a novel federated learning framework for pathology image segmentation, addressing heterogeneity in a unique way.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.21960": {
        "authors": [
            "Senmao Li",
            "Lei Wang",
            "Kai Wang",
            "Tao Liu",
            "Jiehang Xie",
            "Joost van de Weijer",
            "Fahad Shahbaz Khan",
            "Shiqi Yang",
            "Yaxing Wang",
            "Jian Yang"
        ],
        "title": "One-Way Ticket:Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Models",
        "abstract": "arXiv:2505.21960v1 Announce Type: new  Abstract: Text-to-Image (T2I) diffusion models have made remarkable advancements in generative modeling; however, they face a trade-off between inference speed and image quality, posing challenges for efficient deployment. Existing distilled T2I models can generate high-fidelity images with fewer sampling steps, but often struggle with diversity and quality, especially in one-step models. From our analysis, we observe redundant computations in the UNet encoders. Our findings suggest that, for T2I diffusion models, decoders are more adept at capturing richer and more explicit semantic information, while encoders can be effectively shared across decoders from diverse time steps. Based on these observations, we introduce the first Time-independent Unified Encoder TiUE for the student model UNet architecture, which is a loop-free image generation approach for distilling T2I diffusion models. Using a one-pass scheme, TiUE shares encoder features across multiple decoder time steps, enabling parallel sampling and significantly reducing inference time complexity. In addition, we incorporate a KL divergence term to regularize noise prediction, which enhances the perceptual realism and diversity of the generated images. Experimental results demonstrate that TiUE outperforms state-of-the-art methods, including LCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results while maintaining the computational efficiency.",
        "arxiv_id": "2505.21960",
        "ARXIVID": "2505.21960",
        "COMMENT": "Matches criterion 4 as it discusses improvements in text-to-image diffusion models, which are related to generative modeling and vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.21876": {
        "authors": [
            "Zun Wang",
            "Jaemin Cho",
            "Jialu Li",
            "Han Lin",
            "Jaehong Yoon",
            "Yue Zhang",
            "Mohit Bansal"
        ],
        "title": "EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance",
        "abstract": "arXiv:2505.21876v1 Announce Type: new  Abstract: Recent approaches on 3D camera control in video diffusion models (VDMs) often create anchor videos to guide diffusion models as a structured prior by rendering from estimated point clouds following annotated camera trajectories. However, errors inherent in point cloud estimation often lead to inaccurate anchor videos. Moreover, the requirement for extensive camera trajectory annotations further increases resource demands. To address these limitations, we introduce EPiC, an efficient and precise camera control learning framework that automatically constructs high-quality anchor videos without expensive camera trajectory annotations. Concretely, we create highly precise anchor videos for training by masking source videos based on first-frame visibility. This approach ensures high alignment, eliminates the need for camera trajectory annotations, and thus can be readily applied to any in-the-wild video to generate image-to-video (I2V) training pairs. Furthermore, we introduce Anchor-ControlNet, a lightweight conditioning module that integrates anchor video guidance in visible regions to pretrained VDMs, with less than 1% of backbone model parameters. By combining the proposed anchor video data and ControlNet module, EPiC achieves efficient training with substantially fewer parameters, training steps, and less data, without requiring modifications to the diffusion model backbone typically needed to mitigate rendering misalignments. Although being trained on masking-based anchor videos, our method generalizes robustly to anchor videos made with point clouds during inference, enabling precise 3D-informed camera control. EPiC achieves SOTA performance on RealEstate10K and MiraData for I2V camera control task, demonstrating precise and robust camera control ability both quantitatively and qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to video-to-video scenarios.",
        "arxiv_id": "2505.21876",
        "ARXIVID": "2505.21876",
        "COMMENT": "Matches criterion 3 as it introduces EPiC, a novel framework for efficient video camera control learning with precise anchor-video guidance.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2505.22597": {
        "authors": [
            "Ngoc La",
            "Ruaridh Mon-Williams",
            "Julie A. Shah"
        ],
        "title": "HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym",
        "abstract": "arXiv:2505.22597v1 Announce Type: new  Abstract: In recent years, reinforcement learning (RL) methods have been widely tested using tools like OpenAI Gym, though many tasks in these environments could also benefit from hierarchical planning. However, there is a lack of a tool that enables seamless integration of hierarchical planning with RL. Hierarchical Domain Definition Language (HDDL), used in classical planning, introduces a structured approach well-suited for model-based RL to address this gap. To bridge this integration, we introduce HDDLGym, a Python-based tool that automatically generates OpenAI Gym environments from HDDL domains and problems. HDDLGym serves as a link between RL and hierarchical planning, supporting multi-agent scenarios and enabling collaborative planning among agents. This paper provides an overview of HDDLGym's design and implementation, highlighting the challenges and design choices involved in integrating HDDL with the Gym interface, and applying RL policies to support hierarchical planning. We also provide detailed instructions and demonstrations for using the HDDLGym framework, including how to work with existing HDDL domains and problems from International Planning Competitions, exemplified by the Transport domain. Additionally, we offer guidance on creating new HDDL domains for multi-agent scenarios and demonstrate the practical use of HDDLGym in the Overcooked domain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a valuable tool for studying RL in hierarchical planning, particularly in multi-agent contexts.",
        "arxiv_id": "2505.22597",
        "ARXIVID": "2505.22597",
        "COMMENT": "Matches criterion 3 as it introduces HDDLGym, a new tool for studying multi-agent hierarchical problems and integrating hierarchical planning with RL.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2505.21547": {
        "authors": [
            "Weixing Wang",
            "Zifeng Ding",
            "Jindong Gu",
            "Rui Cao",
            "Christoph Meinel",
            "Gerard de Melo",
            "Haojin Yang"
        ],
        "title": "Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing",
        "abstract": "arXiv:2505.21547v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) with discrete image tokenizers unify multimodal representations by encoding visual inputs into a finite set of tokens. Despite their effectiveness, we find that these models still hallucinate non-existent objects. We hypothesize that this may be due to visual priors induced during training: When certain image tokens frequently co-occur in the same spatial regions and represent shared objects, they become strongly associated with the verbalizations of those objects. As a result, the model may hallucinate by evoking visually absent tokens that often co-occur with present ones. To test this assumption, we construct a co-occurrence graph of image tokens using a segmentation dataset and employ a Graph Neural Network (GNN) with contrastive learning followed by a clustering method to group tokens that frequently co-occur in similar visual contexts. We find that hallucinations predominantly correspond to clusters whose tokens dominate the input, and more specifically, that the visually absent tokens in those clusters show much higher correlation with hallucinated objects compared to tokens present in the image. Based on this observation, we propose a hallucination mitigation method that suppresses the influence of visually absent tokens by modifying latent image embeddings during generation. Experiments show our method reduces hallucinations while preserving expressivity. Code is available at https://github.com/weixingW/CGC-VTD/tree/main",
        "arxiv_id": "2505.21547",
        "ARXIVID": "2505.21547",
        "COMMENT": "Matches criterion 2 as it addresses hallucination in discrete tokenizer-based large vision-language models (LVLMs).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.22079": {
        "authors": [
            "Hanbin Ko",
            "Chang-Min Park"
        ],
        "title": "Bringing CLIP to the Clinic: Dynamic Soft Labels and Negation-Aware Learning for Medical Analysis",
        "abstract": "arXiv:2505.22079v1 Announce Type: new  Abstract: The development of large-scale image-text pair datasets has significantly advanced self-supervised learning in Vision-Language Processing (VLP). However, directly applying general-domain architectures such as CLIP to medical data presents challenges, particularly in handling negations and addressing the inherent data imbalance of medical datasets. To address these issues, we propose a novel approach that integrates clinically-enhanced dynamic soft labels and medical graphical alignment, thereby improving clinical comprehension and the applicability of contrastive loss in medical contexts. Furthermore, we introduce negation-based hard negatives to deepen the model's understanding of the complexities of clinical language. Our approach is easily integrated into the medical CLIP training pipeline and achieves state-of-the-art performance across multiple tasks, including zero-shot, fine-tuned classification, and report retrieval. To comprehensively evaluate our model's capacity for understanding clinical language, we introduce CXR-Align, a benchmark uniquely designed to evaluate the understanding of negation and clinical information within chest X-ray (CXR) datasets. Experimental results demonstrate that our proposed methods are straightforward to implement and generalize effectively across contrastive learning frameworks, enhancing medical VLP capabilities and advancing clinical language understanding in medical imaging.",
        "arxiv_id": "2505.22079",
        "ARXIVID": "2505.22079",
        "COMMENT": "Matches criterion 2 as it adapts CLIP for medical analysis with novel techniques for handling negations and data imbalance.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.22654": {
        "authors": [
            "Ce Zhang",
            "Kaixin Ma",
            "Tianqing Fang",
            "Wenhao Yu",
            "Hongming Zhang",
            "Zhisong Zhang",
            "Yaqi Xie",
            "Katia Sycara",
            "Haitao Mi",
            "Dong Yu"
        ],
        "title": "VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models",
        "abstract": "arXiv:2505.22654v1 Announce Type: new  Abstract: Recent Large Vision-Language Models (LVLMs) have advanced multi-modal understanding by incorporating finer-grained visual perception and encoding. However, such methods incur significant computational costs due to longer visual token sequences, posing challenges for real-time deployment. To mitigate this, prior studies have explored pruning unimportant visual tokens either at the output layer of the visual encoder or at the early layers of the language model. In this work, we revisit these design choices and reassess their effectiveness through comprehensive empirical studies of how visual tokens are processed throughout the visual encoding and language decoding stages. Guided by these insights, we propose VScan, a two-stage visual token reduction framework that addresses token redundancy by: (1) integrating complementary global and local scans with token merging during visual encoding, and (2) introducing pruning at intermediate layers of the language model. Extensive experimental results across four LVLMs validate the effectiveness of VScan in accelerating inference and demonstrate its superior performance over current state-of-the-arts on sixteen benchmarks. Notably, when applied to LLaVA-NeXT-7B, VScan achieves a 2.91$\\times$ speedup in prefilling and a 10$\\times$ reduction in FLOPs, while retaining 95.4% of the original performance.",
        "arxiv_id": "2505.22654",
        "ARXIVID": "2505.22654",
        "COMMENT": "Matches criterion 2 as it proposes VScan, a method for improving efficiency in large vision-language models (LVLMs).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.21795": {
        "authors": [
            "Claudia Cuttano",
            "Gabriele Trivigno",
            "Giuseppe Averta",
            "Carlo Masone"
        ],
        "title": "SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation",
        "abstract": "arXiv:2505.21795v1 Announce Type: new  Abstract: Few-shot segmentation aims to segment unseen object categories from just a handful of annotated examples. This requires mechanisms that can both identify semantically related objects across images and accurately produce segmentation masks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate mechanism, offers both strong segmentation capabilities and a built-in feature matching process. However, we show that its representations are entangled with task-specific cues optimized for object tracking, which impairs its use for tasks requiring higher level semantic understanding. Our key insight is that, despite its class-agnostic pretraining, SAM2 already encodes rich semantic structure in its features. We propose SANSA (Semantically AligNed Segment Anything 2), a framework that makes this latent structure explicit, and repurposes SAM2 for few-shot segmentation through minimal task-specific modifications. SANSA achieves state-of-the-art performance on few-shot segmentation benchmarks specifically designed to assess generalization, outperforms generalist methods in the popular in-context setting, supports various prompts flexible interaction via points, boxes, or scribbles, and remains significantly faster and more compact than prior approaches. Code is available at https://github.com/ClaudiaCuttano/SANSA.",
        "arxiv_id": "2505.21795",
        "ARXIVID": "2505.21795",
        "COMMENT": "Matches criterion 4 as it builds on the Segment Anything 2 (SAM2) model and enhances its semantic understanding for few-shot segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.21954": {
        "authors": [
            "Le Thien Phuc Nguyen",
            "Zhuoran Yu",
            "Khoa Quang Nhat Cao",
            "Yuwei Guo",
            "Tu Ho Manh Pham",
            "Tuan Tai Nguyen",
            "Toan Ngo Duc Vo",
            "Lucas Poon",
            "Soochahn Lee",
            "Yong Jae Lee"
        ],
        "title": "UniTalk: Towards Universal Active Speaker Detection in Real World Scenarios",
        "abstract": "arXiv:2505.21954v1 Announce Type: new  Abstract: We present UniTalk, a novel dataset specifically designed for the task of active speaker detection, emphasizing challenging scenarios to enhance model generalization. Unlike previously established benchmarks such as AVA, which predominantly features old movies and thus exhibits significant domain gaps, UniTalk focuses explicitly on diverse and difficult real-world conditions. These include underrepresented languages, noisy backgrounds, and crowded scenes - such as multiple visible speakers speaking concurrently or in overlapping turns. It contains over 44.5 hours of video with frame-level active speaker annotations across 48,693 speaking identities, and spans a broad range of video types that reflect real-world conditions. Through rigorous evaluation, we show that state-of-the-art models, while achieving nearly perfect scores on AVA, fail to reach saturation on UniTalk, suggesting that the ASD task remains far from solved under realistic conditions. Nevertheless, models trained on UniTalk demonstrate stronger generalization to modern \"in-the-wild\" datasets like Talkies and ASW, as well as to AVA. UniTalk thus establishes a new benchmark for active speaker detection, providing researchers with a valuable resource for developing and evaluating versatile and resilient models.   Dataset: https://huggingface.co/datasets/plnguyen2908/UniTalk-ASD   Code: https://github.com/plnguyen2908/UniTalk-ASD-code",
        "arxiv_id": "2505.21954",
        "ARXIVID": "2505.21954",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (UniTalk) for active speaker detection in real-world scenarios.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.22126": {
        "authors": [
            "Yifan Chang",
            "Yukang Feng",
            "Jianwen Sun",
            "Jiaxin Ai",
            "Chuanhao Li",
            "S. Kevin Zhou",
            "Kaipeng Zhang"
        ],
        "title": "SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model",
        "abstract": "arXiv:2505.22126v1 Announce Type: new  Abstract: Recent years have seen rapid advances in AI-driven image generation. Early diffusion models emphasized perceptual quality, while newer multimodal models like GPT-4o-image integrate high-level reasoning, improving semantic understanding and structural composition. Scientific illustration generation exemplifies this evolution: unlike general image synthesis, it demands accurate interpretation of technical content and transformation of abstract ideas into clear, standardized visuals. This task is significantly more knowledge-intensive and laborious, often requiring hours of manual work and specialized tools. Automating it in a controllable, intelligent manner would provide substantial practical value. Yet, no benchmark currently exists to evaluate AI on this front. To fill this gap, we introduce SridBench, the first benchmark for scientific figure generation. It comprises 1,120 instances curated from leading scientific papers across 13 natural and computer science disciplines, collected via human experts and MLLMs. Each sample is evaluated along six dimensions, including semantic fidelity and structural accuracy. Experimental results reveal that even top-tier models like GPT-4o-image lag behind human performance, with common issues in text/visual clarity and scientific correctness. These findings highlight the need for more advanced reasoning-driven visual generation capabilities.",
        "arxiv_id": "2505.22126",
        "ARXIVID": "2505.22126",
        "COMMENT": "Matches criterion 4 as it introduces a benchmark for scientific illustration generation, which is an application of vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2505.21668": {
        "authors": [
            "Yongchao Chen",
            "Yueying Liu",
            "Junwei Zhou",
            "Yilun Hao",
            "Jingquan Wang",
            "Yang Zhang",
            "Chuchu Fan"
        ],
        "title": "R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning",
        "abstract": "arXiv:2505.21668v1 Announce Type: new  Abstract: Despite advances in reasoning and planning of R1-like models, Large Language Models (LLMs) still struggle with tasks requiring precise computation, symbolic manipulation, optimization, and algorithmic reasoning, in which textual reasoning lacks the rigor of code execution. A key challenge is enabling LLMs to decide when to use textual reasoning versus code generation. While OpenAI trains models to invoke a Code Interpreter as needed, public research lacks guidance on aligning pre-trained LLMs to effectively leverage code and generalize across diverse tasks. We present R1-Code-Interpreter, an extension of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and reinforcement learning (RL) to autonomously generate multiple code queries during step-by-step reasoning. We curate 144 reasoning and planning tasks (107 for training, 37 for testing), each with over 200 diverse questions. We fine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies, investigating different answer formats, reasoning vs. non-reasoning models, cold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs. Unlike prior RL work on narrow domains, we find that Code Interpreter training is significantly harder due to high task diversity and expensive code execution, highlighting the critical role of the SFT stage. Our final model, R1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\\% to 64.1\\%, outperforming GPT-4o (text-only: 58.6\\%) and approaching GPT-4o with Code Interpreter (70.9\\%), with the emergent self-checking behavior via code generation. Datasets, Codes, and Models are available at https://github.com/yongchao98/R1-Code-Interpreter and https://huggingface.co/yongchao98.",
        "arxiv_id": "2505.21668",
        "ARXIVID": "2505.21668",
        "COMMENT": "Does not match any specific criterion but is related to reasoning and planning in LLMs, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2505.21742": {
        "authors": [
            "Briglia Maria Rosaria",
            "Mujtaba Hussain Mirza",
            "Giuseppe Lisanti",
            "Iacopo Masi"
        ],
        "title": "What is Adversarial Training for Diffusion Models?",
        "abstract": "arXiv:2505.21742v1 Announce Type: new  Abstract: We answer the question in the title, showing that adversarial training (AT) for diffusion models (DMs) fundamentally differs from classifiers: while AT in classifiers enforces output invariance, AT in DMs requires equivariance to keep the diffusion process aligned with the data distribution. AT is a way to enforce smoothness in the diffusion flow, improving robustness to outliers and corrupted data. Unlike prior art, our method makes no assumptions about the noise model and integrates seamlessly into diffusion training by adding random noise, similar to randomized smoothing, or adversarial noise, akin to AT. This enables intrinsic capabilities such as handling noisy data, dealing with extreme variability such as outliers, preventing memorization, and improving robustness. We rigorously evaluate our approach with proof-of-concept datasets with known distributions in low- and high-dimensional space, thereby taking a perfect measure of errors; we further evaluate on standard benchmarks such as CIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe noise, data corruption, and iterative adversarial attacks.",
        "arxiv_id": "2505.21742",
        "ARXIVID": "2505.21742",
        "COMMENT": "Does not match any specific criteria but explores adversarial training for diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.21533": {
        "authors": [
            "Thalles Silva",
            "Helio Pedrini",
            "Ad\\'in Ram\\'irez Rivera"
        ],
        "title": "Self-Organizing Visual Prototypes for Non-Parametric Representation Learning",
        "abstract": "arXiv:2505.21533v1 Announce Type: new  Abstract: We present Self-Organizing Visual Prototypes (SOP), a new training technique for unsupervised visual feature learning. Unlike existing prototypical self-supervised learning (SSL) methods that rely on a single prototype to encode all relevant features of a hidden cluster in the data, we propose the SOP strategy. In this strategy, a prototype is represented by many semantically similar representations, or support embeddings (SEs), each containing a complementary set of features that together better characterize their region in space and maximize training performance. We reaffirm the feasibility of non-parametric SSL by introducing novel non-parametric adaptations of two loss functions that implement the SOP strategy. Notably, we introduce the SOP Masked Image Modeling (SOP-MIM) task, where masked representations are reconstructed from the perspective of multiple non-parametric local SEs. We comprehensively evaluate the representations learned using the SOP strategy on a range of benchmarks, including retrieval, linear evaluation, fine-tuning, and object detection. Our pre-trained encoders achieve state-of-the-art performance on many retrieval benchmarks and demonstrate increasing performance gains with more complex encoders.",
        "arxiv_id": "2505.21533",
        "ARXIVID": "2505.21533",
        "COMMENT": "Does not match any specific criteria but is related to unsupervised visual feature learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.22167": {
        "authors": [
            "Weilun Feng",
            "Chuanguang Yang",
            "Haotong Qin",
            "Xiangqi Li",
            "Yu Wang",
            "Zhulin An",
            "Libo Huang",
            "Boyu Diao",
            "Zixiang Zhao",
            "Yongjun Xu",
            "Michele Magno"
        ],
        "title": "Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers",
        "abstract": "arXiv:2505.22167v1 Announce Type: new  Abstract: Diffusion transformers (DiT) have demonstrated exceptional performance in video generation. However, their large number of parameters and high computational complexity limit their deployment on edge devices. Quantization can reduce storage requirements and accelerate inference by lowering the bit-width of model parameters. Yet, existing quantization methods for image generation models do not generalize well to video generation tasks. We identify two primary challenges: the loss of information during quantization and the misalignment between optimization objectives and the unique requirements of video generation. To address these challenges, we present Q-VDiT, a quantization framework specifically designed for video DiT models. From the quantization perspective, we propose the Token-aware Quantization Estimator (TQE), which compensates for quantization errors in both the token and feature dimensions. From the optimization perspective, we introduce Temporal Maintenance Distillation (TMD), which preserves the spatiotemporal correlations between frames and enables the optimization of each frame with respect to the overall video context. Our W3A6 Q-VDiT achieves a scene consistency of 23.40, setting a new benchmark and outperforming current state-of-the-art quantization methods by 1.9$\\times$. Code will be available at https://github.com/cantbebetter2/Q-VDiT.",
        "arxiv_id": "2505.22167",
        "ARXIVID": "2505.22167",
        "COMMENT": "Does not match any specific criterion but is generally relevant to generative modeling and optimization for video generation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.21784": {
        "authors": [
            "Tharindu Kumarage",
            "Ninareh Mehrabi",
            "Anil Ramakrishna",
            "Xinyan Zhao",
            "Richard Zemel",
            "Kai-Wei Chang",
            "Aram Galstyan",
            "Rahul Gupta",
            "Charith Peris"
        ],
        "title": "Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation",
        "abstract": "arXiv:2505.21784v1 Announce Type: new  Abstract: Safety reasoning is a recent paradigm where LLMs reason over safety policies before generating responses, thereby mitigating limitations in existing safety measures such as over-refusal and jailbreak vulnerabilities. However, implementing this paradigm is challenging due to the resource-intensive process of creating high-quality policy-embedded chain-of-thought (CoT) datasets while ensuring reasoning remains accurate and free from hallucinations or policy conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation for Safety Reasoning, a novel data generation recipe that leverages multi-agent deliberation to iteratively expand reasoning on safety policies. A data refiner stage in AIDSAFE ensures high-quality outputs by eliminating repetitive, redundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong foundation for supervised fine-tuning (SFT)-based safety training. Additionally, to address the need of preference data in alignment stages, such as DPO training, we introduce a supplemental recipe that uses belief augmentation to create distinct selected and rejected CoT samples. Our evaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy adherence and reasoning quality. Consequently, we show that fine-tuning open-source LLMs on these CoTs can significantly improve safety generalization and jailbreak robustness while maintaining acceptable utility and over-refusal accuracy. AIDSAFE-generated CoT datasets can be found here: https://huggingface.co/datasets/AmazonScience/AIDSAFE",
        "arxiv_id": "2505.21784",
        "ARXIVID": "2505.21784",
        "COMMENT": "Does not match any specific criterion but is generally relevant to safety reasoning in large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.22099": {
        "authors": [
            "Wenwen Qiang",
            "Ziyin Gu",
            "Lingyu Si",
            "Jiangmeng Li",
            "Changwen Zheng",
            "Fuchun Sun",
            "Hui Xiong"
        ],
        "title": "On the Transferability and Discriminability of Repersentation Learning in Unsupervised Domain Adaptation",
        "abstract": "arXiv:2505.22099v1 Announce Type: new  Abstract: In this paper, we addressed the limitation of relying solely on distribution alignment and source-domain empirical risk minimization in Unsupervised Domain Adaptation (UDA). Our information-theoretic analysis showed that this standard adversarial-based framework neglects the discriminability of target-domain features, leading to suboptimal performance. To bridge this theoretical-practical gap, we defined \"good representation learning\" as guaranteeing both transferability and discriminability, and proved that an additional loss term targeting target-domain discriminability is necessary. Building on these insights, we proposed a novel adversarial-based UDA framework that explicitly integrates a domain alignment objective with a discriminability-enhancing constraint. Instantiated as Domain-Invariant Representation Learning with Global and Local Consistency (RLGLC), our method leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD) to address class imbalance and semantic dimension weighting, and employs a local consistency mechanism to preserve fine-grained target-domain discriminative information. Extensive experiments across multiple benchmark datasets demonstrate that RLGLC consistently surpasses state-of-the-art methods, confirming the value of our theoretical perspective and underscoring the necessity of enforcing both transferability and discriminability in adversarial-based UDA.",
        "arxiv_id": "2505.22099",
        "ARXIVID": "2505.22099",
        "COMMENT": "Does not match any specific criterion but is generally relevant to representation learning and domain adaptation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.22451": {
        "authors": [
            "Yuanhang Liu",
            "Yanxing Huang",
            "Yanqiao Wang",
            "Peng Li",
            "Yang Liu"
        ],
        "title": "AI Mathematician: Towards Fully Automated Frontier Mathematical Research",
        "abstract": "arXiv:2505.22451v1 Announce Type: new  Abstract: Large Reasoning Models (LRMs) have made significant progress in mathematical capabilities in recent times. However, these successes have been primarily confined to competition-level problems. In this work, we propose AI Mathematician (AIM) framework, which harnesses the reasoning strength of LRMs to support frontier mathematical research. We have identified two critical challenges of mathematical research compared to competition, {\\it the intrinsic complexity of research problems} and {\\it the requirement of procedural rigor}. To address these challenges, AIM incorporates two core strategies: an exploration mechanism to foster longer solution paths, and the pessimistic reasonable verification method to ensure reliability.   This early version of AIM already exhibits strong capability in tackling research-level tasks. We conducted extensive experiments across several real-world mathematical topics and obtained promising results. AIM is able to autonomously construct substantial portions of proofs and uncover non-trivial insights within each research area. These findings highlight the potential of LRMs in mathematical discovery and suggest that LRM-based agent systems could significantly accelerate mathematical research in the future.",
        "arxiv_id": "2505.22451",
        "ARXIVID": "2505.22451",
        "COMMENT": "Does not match any specific criteria but discusses reasoning in large models, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.21535": {
        "authors": [
            "Yuxin Ren",
            "Maxwell D Collins",
            "Miao Hu",
            "Huanrui Yang"
        ],
        "title": "Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement",
        "abstract": "arXiv:2505.21535v1 Announce Type: new  Abstract: While transformers excel across vision and language pretraining tasks, their reliance on attention mechanisms poses challenges for inference efficiency, especially on edge and embedded accelerators with limited parallelism and memory bandwidth. Hinted by the observed redundancy of attention at inference time, we hypothesize that though the model learns complicated token dependency through pretraining, the inference-time sequence-to-sequence mapping in each attention layer is actually ''simple'' enough to be represented with a much cheaper function. In this work, we explore FAR, a Function-preserving Attention Replacement framework that replaces all attention blocks in pretrained transformers with learnable sequence-to-sequence modules, exemplified by an LSTM. FAR optimize a multi-head LSTM architecture with a block-wise distillation objective and a global structural pruning framework to achieve a family of efficient LSTM-based models from pretrained transformers. We validate FAR on the DeiT vision transformer family and demonstrate that it matches the accuracy of the original models on ImageNet and multiple downstream tasks with reduced parameters and latency. Further analysis shows that FAR preserves the semantic token relationships and the token-to-token correlation learned in the transformer's attention module.",
        "arxiv_id": "2505.21535",
        "ARXIVID": "2505.21535",
        "COMMENT": "Does not match any specific criterion but is related to efficient transformer inference, which is tangentially relevant to your friend's interest in vision and language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.22195": {
        "authors": [
            "Guoan Xu",
            "Wenfeng Huang",
            "Wenjing Jia",
            "Jiamao Li",
            "Guangwei Gao",
            "Guo-Jun Qi"
        ],
        "title": "S2AFormer: Strip Self-Attention for Efficient Vision Transformer",
        "abstract": "arXiv:2505.22195v1 Announce Type: new  Abstract: Vision Transformer (ViT) has made significant advancements in computer vision, thanks to its token mixer's sophisticated ability to capture global dependencies between all tokens. However, the quadratic growth in computational demands as the number of tokens increases limits its practical efficiency. Although recent methods have combined the strengths of convolutions and self-attention to achieve better trade-offs, the expensive pairwise token affinity and complex matrix operations inherent in self-attention remain a bottleneck. To address this challenge, we propose S2AFormer, an efficient Vision Transformer architecture featuring novel Strip Self-Attention (SSA). We design simple yet effective Hybrid Perception Blocks (HPBs) to effectively integrate the local perception capabilities of CNNs with the global context modeling of Transformer's attention mechanisms. A key innovation of SSA lies in its reducing the spatial dimensions of $K$ and $V$ while compressing the channel dimensions of $Q$ and $K$. This design significantly reduces computational overhead while preserving accuracy, striking an optimal balance between efficiency and effectiveness. We evaluate the robustness and efficiency of S2AFormer through extensive experiments on multiple vision benchmarks, including ImageNet-1k for image classification, ADE20k for semantic segmentation, and COCO for object detection and instance segmentation. Results demonstrate that S2AFormer achieves significant accuracy gains with superior efficiency in both GPU and non-GPU environments, making it a strong candidate for efficient vision Transformers.",
        "arxiv_id": "2505.22195",
        "ARXIVID": "2505.22195",
        "COMMENT": "Does not match any specific criteria but focuses on efficient Vision Transformer architecture.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.21847": {
        "authors": [
            "Xuwei Xu",
            "Yang Li",
            "Yudong Chen",
            "Jiajun Liu",
            "Sen Wang"
        ],
        "title": "RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers",
        "abstract": "arXiv:2505.21847v1 Announce Type: new  Abstract: We reveal that feedforward network (FFN) layers, rather than attention layers, are the primary contributors to Vision Transformer (ViT) inference latency, with their impact signifying as model size increases. This finding highlights a critical opportunity for optimizing the efficiency of large-scale ViTs by focusing on FFN layers. In this work, we propose a novel channel idle mechanism that facilitates post-training structural reparameterization for efficient FFN layers during testing. Specifically, a set of feature channels remains idle and bypasses the nonlinear activation function in each FFN layer, thereby forming a linear pathway that enables structural reparameterization during inference. This mechanism results in a family of ReParameterizable Vision Transformers (RePaViTs), which achieve remarkable latency reductions with acceptable sacrifices (sometimes gains) in accuracy across various ViTs. The benefits of our method scale consistently with model sizes, demonstrating greater speed improvements and progressively narrowing accuracy gaps or even higher accuracies on larger models. In particular, RePa-ViT-Large and RePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1 accuracies under the same training strategy, respectively. RePaViT is the first to employ structural reparameterization on FFN layers to expedite ViTs to our best knowledge, and we believe that it represents an auspicious direction for efficient ViTs. Source code is available at https://github.com/Ackesnal/RePaViT.",
        "arxiv_id": "2505.21847",
        "ARXIVID": "2505.21847",
        "COMMENT": "Does not match any specific criteria but focuses on Vision Transformers and efficiency improvements.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.21935": {
        "authors": [
            "Kaiyu He",
            "Zhiyu Chen"
        ],
        "title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models",
        "abstract": "arXiv:2505.21935v1 Announce Type: new  Abstract: Since the advent of Large Language Models (LLMs), efforts have largely focused on improving their instruction-following and deductive reasoning abilities, leaving open the question of whether these models can truly discover new knowledge. In pursuit of artificial general intelligence (AGI), there is a growing need for models that not only execute commands or retrieve information but also learn, reason, and generate new knowledge by formulating novel hypotheses and theories that deepen our understanding of the world. Guided by Peirce's framework of abduction, deduction, and induction, this survey offers a structured lens to examine LLM-based hypothesis discovery. We synthesize existing work in hypothesis generation, application, and validation, identifying both key achievements and critical gaps. By unifying these threads, we illuminate how LLMs might evolve from mere ``information executors'' into engines of genuine innovation, potentially transforming research, science, and real-world problem solving.",
        "arxiv_id": "2505.21935",
        "ARXIVID": "2505.21935",
        "COMMENT": "Does not match any specific criterion but is generally relevant to hypothesis discovery and reasoning with large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.22490": {
        "authors": [
            "Ke Zhang",
            "Tianyu Ding",
            "Jiachen Jiang",
            "Tianyi Chen",
            "Ilya Zharkov",
            "Vishal M. Patel",
            "Luming Liang"
        ],
        "title": "ProCrop: Learning Aesthetic Image Cropping from Professional Compositions",
        "abstract": "arXiv:2505.22490v1 Announce Type: new  Abstract: Image cropping is crucial for enhancing the visual appeal and narrative impact of photographs, yet existing rule-based and data-driven approaches often lack diversity or require annotated training data. We introduce ProCrop, a retrieval-based method that leverages professional photography to guide cropping decisions. By fusing features from professional photographs with those of the query image, ProCrop learns from professional compositions, significantly boosting performance. Additionally, we present a large-scale dataset of 242K weakly-annotated images, generated by out-painting professional images and iteratively refining diverse crop proposals. This composition-aware dataset generation offers diverse high-quality crop proposals guided by aesthetic principles and becomes the largest publicly available dataset for image cropping. Extensive experiments show that ProCrop significantly outperforms existing methods in both supervised and weakly-supervised settings. Notably, when trained on the new dataset, our ProCrop surpasses previous weakly-supervised methods and even matches fully supervised approaches. Both the code and dataset will be made publicly available to advance research in image aesthetics and composition analysis.",
        "arxiv_id": "2505.22490",
        "ARXIVID": "2505.22490",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and aesthetics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.22342": {
        "authors": [
            "Shriram M S",
            "Xinyue Hao",
            "Shihao Hou",
            "Yang Lu",
            "Laura Sevilla-Lara",
            "Anurag Arnab",
            "Shreyank N Gowda"
        ],
        "title": "Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training",
        "abstract": "arXiv:2505.22342v1 Announce Type: new  Abstract: The success of the machine learning field has reliably depended on training on large datasets. While effective, this trend comes at an extraordinary cost. This is due to two deeply intertwined factors: the size of models and the size of datasets. While promising research efforts focus on reducing the size of models, the other half of the equation remains fairly mysterious. Indeed, it is surprising that the standard approach to training remains to iterate over and over, uniformly sampling the training dataset. In this paper we explore a series of alternative training paradigms that leverage insights from hard-data-mining and dropout, simple enough to implement and use that can become the new training standard. The proposed Progressive Data Dropout reduces the number of effective epochs to as little as 12.4% of the baseline. This savings actually do not come at any cost for accuracy. Surprisingly, the proposed method improves accuracy by up to 4.82%. Our approach requires no changes to model architecture or optimizer, and can be applied across standard training pipelines, thus posing an excellent opportunity for wide adoption. Code can be found here: https://github.com/bazyagami/LearningWithRevision",
        "arxiv_id": "2505.22342",
        "ARXIVID": "2505.22342",
        "COMMENT": "Does not match any specific criterion but is generally relevant to machine learning efficiency.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.21915": {
        "authors": [
            "Mir Sazzat Hossain",
            "Ovi Paul",
            "Md Akil Raihan Iftee",
            "Rakibul Hasan Rajib",
            "Abu Bakar Siddik Nayem",
            "Anis Sarker",
            "Arshad Momen",
            "Md. Ashraful Amin",
            "Amin Ahsan Ali",
            "AKM Mahbubur Rahman"
        ],
        "title": "BD Open LULC Map: High-resolution land use land cover mapping & benchmarking for urban development in Dhaka, Bangladesh",
        "abstract": "arXiv:2505.21915v1 Announce Type: new  Abstract: Land Use Land Cover (LULC) mapping using deep learning significantly enhances the reliability of LULC classification, aiding in understanding geography, socioeconomic conditions, poverty levels, and urban sprawl. However, the scarcity of annotated satellite data, especially in South/East Asian developing countries, poses a major challenge due to limited funding, diverse infrastructures, and dense populations. In this work, we introduce the BD Open LULC Map (BOLM), providing pixel-wise LULC annotations across eleven classes (e.g., Farmland, Water, Forest, Urban Structure, Rural Built-Up) for Dhaka metropolitan city and its surroundings using high-resolution Bing satellite imagery (2.22 m/pixel). BOLM spans 4,392 sq km (891 million pixels), with ground truth validated through a three-stage process involving GIS experts. We benchmark LULC segmentation using DeepLab V3+ across five major classes and compare performance on Bing and Sentinel-2A imagery. BOLM aims to support reliable deep models and domain adaptation tasks, addressing critical LULC dataset gaps in South/East Asia.",
        "arxiv_id": "2505.21915",
        "ARXIVID": "2505.21915",
        "COMMENT": "Does not match any specific criterion but is related to land use mapping and segmentation, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.21637": {
        "authors": [
            "Xiaole Tang",
            "Xiaoyi He",
            "Xiang Gu",
            "Jian Sun"
        ],
        "title": "BaryIR: Learning Multi-Source Unified Representation in Continuous Barycenter Space for Generalizable All-in-One Image Restoration",
        "abstract": "arXiv:2505.21637v1 Announce Type: new  Abstract: Despite remarkable advances made in all-in-one image restoration (AIR) for handling different types of degradations simultaneously, existing methods remain vulnerable to out-of-distribution degradations and images, limiting their real-world applicability. In this paper, we propose a multi-source representation learning framework BaryIR, which decomposes the latent space of multi-source degraded images into a continuous barycenter space for unified feature encoding and source-specific subspaces for specific semantic encoding. Specifically, we seek the multi-source unified representation by introducing a multi-source latent optimal transport barycenter problem, in which a continuous barycenter map is learned to transport the latent representations to the barycenter space. The transport cost is designed such that the representations from source-specific subspaces are contrasted with each other while maintaining orthogonality to those from the barycenter space. This enables BaryIR to learn compact representations with unified degradation-agnostic information from the barycenter space, as well as degradation-specific semantics from source-specific subspaces, capturing the inherent geometry of multi-source data manifold for generalizable AIR. Extensive experiments demonstrate that BaryIR achieves competitive performance compared to state-of-the-art all-in-one methods. Particularly, BaryIR exhibits superior generalization ability to real-world data and unseen degradations. The code will be publicly available at https://github.com/xl-tang3/BaryIR.",
        "arxiv_id": "2505.21637",
        "ARXIVID": "2505.21637",
        "COMMENT": "Does not match any specific criterion but is related to general computer vision and image restoration.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.21904": {
        "authors": [
            "Pardis Taghavi",
            "Tian Liu",
            "Renjie Li",
            "Reza Langari",
            "Zhengzhong Tu"
        ],
        "title": "CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation",
        "abstract": "arXiv:2505.21904v1 Announce Type: new  Abstract: Instance segmentation demands costly per-pixel annotations and large models. We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework that compresses pretrained vision foundation models (VFM) into compact experts using limited labeled and abundant unlabeled data. CAST unfolds in three stages: (1) domain adaptation of the VFM teacher(s) via self-training with contrastive pixel calibration, (2) distillation into a compact student via a unified multi-objective loss that couples standard supervision and pseudo-labels with our instance-aware pixel-wise contrastive term, and (3) fine-tuning on labeled data to remove residual pseudo-label bias. Central to CAST is an \\emph{instance-aware pixel-wise contrastive loss} that fuses mask and class scores to mine informative negatives and enforce clear inter-instance margins. By maintaining this contrastive signal across both adaptation and distillation, we align teacher and student embeddings and fully leverage unlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses its adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs. 15.2) and outperforms state-of-the-art semi-supervised approaches.",
        "arxiv_id": "2505.21904",
        "ARXIVID": "2505.21904",
        "COMMENT": "Does not match any specific criterion but is related to semi-supervised learning and instance segmentation, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.22636": {
        "authors": [
            "Jixin Zhao",
            "Shangchen Zhou",
            "Zhouxia Wang",
            "Peiqing Yang",
            "Chen Change Loy"
        ],
        "title": "ObjectClear: Complete Object Removal via Object-Effect Attention",
        "abstract": "arXiv:2505.22636v1 Announce Type: new  Abstract: Object removal requires eliminating not only the target object but also its effects, such as shadows and reflections. However, diffusion-based inpainting methods often produce artifacts, hallucinate content, alter background, and struggle to remove object effects accurately. To address this challenge, we introduce a new dataset for OBject-Effect Removal, named OBER, which provides paired images with and without object effects, along with precise masks for both objects and their associated visual artifacts. The dataset comprises high-quality captured and simulated data, covering diverse object categories and complex multi-object scenes. Building on OBER, we propose a novel framework, ObjectClear, which incorporates an object-effect attention mechanism to guide the model toward the foreground removal regions by learning attention masks, effectively decoupling foreground removal from background reconstruction. Furthermore, the predicted attention map enables an attention-guided fusion strategy during inference, greatly preserving background details. Extensive experiments demonstrate that ObjectClear outperforms existing methods, achieving improved object-effect removal quality and background fidelity, especially in complex scenarios.",
        "arxiv_id": "2505.22636",
        "ARXIVID": "2505.22636",
        "COMMENT": "Does not match any specific criterion but is related to computer vision and generative modeling, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.22368": {
        "authors": [
            "Enfang Cui",
            "Yujun Cheng",
            "Rui She",
            "Dan Liu",
            "Zhiyuan Liang",
            "Minxin Guo",
            "Tianzheng Li",
            "Qian Wei",
            "Wenjuan Xing",
            "Zhijie Zhong"
        ],
        "title": "AgentDNS: A Root Domain Naming System for LLM Agents",
        "abstract": "arXiv:2505.22368v1 Announce Type: new  Abstract: The rapid evolution of Large Language Model (LLM) agents has highlighted critical challenges in cross-vendor service discovery, interoperability, and communication. Existing protocols like model context protocol and agent-to-agent protocol have made significant strides in standardizing interoperability between agents and tools, as well as communication among multi-agents. However, there remains a lack of standardized protocols and solutions for service discovery across different agent and tool vendors. In this paper, we propose AgentDNS, a root domain naming and service discovery system designed to enable LLM agents to autonomously discover, resolve, and securely invoke third-party agent and tool services across organizational and technological boundaries. Inspired by the principles of the traditional DNS, AgentDNS introduces a structured mechanism for service registration, semantic service discovery, secure invocation, and unified billing. We detail the architecture, core functionalities, and use cases of AgentDNS, demonstrating its potential to streamline multi-agent collaboration in real-world scenarios. The source code will be published on https://github.com/agentdns.",
        "arxiv_id": "2505.22368",
        "ARXIVID": "2505.22368",
        "COMMENT": "Does not match any specific criterion but is generally relevant to multi-agent systems and service discovery.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.22581": {
        "authors": [
            "Kartik Kuckreja",
            "Parul Gupta",
            "Injy Hamed",
            "Thamar Solorio",
            "Muhammad Haris Khan",
            "Abhinav Dhall"
        ],
        "title": "Tell me Habibi, is it Real or Fake?",
        "abstract": "arXiv:2505.22581v1 Announce Type: new  Abstract: Deepfake generation methods are evolving fast, making fake media harder to detect and raising serious societal concerns. Most deepfake detection and dataset creation research focuses on monolingual content, often overlooking the challenges of multilingual and code-switched speech, where multiple languages are mixed within the same discourse. Code-switching, especially between Arabic and English, is common in the Arab world and is widely used in digital communication. This linguistic mixing poses extra challenges for deepfake detection, as it can confuse models trained mostly on monolingual data. To address this, we introduce \\textbf{ArEnAV}, the first large-scale Arabic-English audio-visual deepfake dataset featuring intra-utterance code-switching, dialectal variation, and monolingual Arabic content. It \\textbf{contains 387k videos and over 765 hours of real and fake videos}. Our dataset is generated using a novel pipeline integrating four Text-To-Speech and two lip-sync models, enabling comprehensive analysis of multilingual multimodal deepfake detection. We benchmark our dataset against existing monolingual and multilingual datasets, state-of-the-art deepfake detection models, and a human evaluation, highlighting its potential to advance deepfake research. The dataset can be accessed \\href{https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}.",
        "arxiv_id": "2505.22581",
        "ARXIVID": "2505.22581",
        "COMMENT": "Does not match any specific criteria but introduces a dataset for deepfake detection, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.22111": {
        "authors": [
            "Woonho Ko",
            "Jin Bok Park",
            "Il Yong Chun"
        ],
        "title": "Autoregression-free video prediction using diffusion model for mitigating error propagation",
        "abstract": "arXiv:2505.22111v1 Announce Type: new  Abstract: Existing long-term video prediction methods often rely on an autoregressive video prediction mechanism. However, this approach suffers from error propagation, particularly in distant future frames. To address this limitation, this paper proposes the first AutoRegression-Free (ARFree) video prediction framework using diffusion models. Different from an autoregressive video prediction mechanism, ARFree directly predicts any future frame tuples from the context frame tuple. The proposed ARFree consists of two key components: 1) a motion prediction module that predicts a future motion using motion feature extracted from the context frame tuple; 2) a training method that improves motion continuity and contextual consistency between adjacent future frame tuples. Our experiments with two benchmark datasets show that the proposed ARFree video prediction framework outperforms several state-of-the-art video prediction methods.",
        "arxiv_id": "2505.22111",
        "ARXIVID": "2505.22111",
        "COMMENT": "Does not match any specific criteria but discusses video prediction using diffusion models, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.22353": {
        "authors": [
            "Noora Al-Emadi",
            "Ingmar Weber",
            "Yin Yang",
            "Ferda Ofli"
        ],
        "title": "VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond",
        "abstract": "arXiv:2505.22353v1 Announce Type: new  Abstract: Detecting vehicles in satellite images is crucial for traffic management, urban planning, and disaster response. However, current models struggle with real-world diversity, particularly across different regions. This challenge is amplified by geographic bias in existing datasets, which often focus on specific areas and overlook regions like the Middle East. To address this gap, we present the Vehicles in the Middle East (VME) dataset, designed explicitly for vehicle detection in high-resolution satellite images from Middle Eastern countries. Sourced from Maxar, the VME dataset spans 54 cities across 12 countries, comprising over 4,000 image tiles and more than 100,000 vehicles, annotated using both manual and semi-automated methods. Additionally, we introduce the largest benchmark dataset for Car Detection in Satellite Imagery (CDSI), combining images from multiple sources to enhance global car detection. Our experiments demonstrate that models trained on existing datasets perform poorly on Middle Eastern images, while the VME dataset significantly improves detection accuracy in this region. Moreover, state-of-the-art models trained on CDSI achieve substantial improvements in global car detection.",
        "arxiv_id": "2505.22353",
        "ARXIVID": "2505.22353",
        "COMMENT": "Does not match any specific criteria but introduces a new dataset for vehicle detection in satellite imagery, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.22465": {
        "authors": [
            "Zobia Batool",
            "Huseyin Ozkan",
            "Erchan Aptoula"
        ],
        "title": "Single Domain Generalization for Alzheimer's Detection from 3D MRIs with Pseudo-Morphological Augmentations and Contrastive Learning",
        "abstract": "arXiv:2505.22465v1 Announce Type: new  Abstract: Although Alzheimer's disease detection via MRIs has advanced significantly thanks to contemporary deep learning models, challenges such as class imbalance, protocol variations, and limited dataset diversity often hinder their generalization capacity. To address this issue, this article focuses on the single domain generalization setting, where given the data of one domain, a model is designed and developed with maximal performance w.r.t. an unseen domain of distinct distribution. Since brain morphology is known to play a crucial role in Alzheimer's diagnosis, we propose the use of learnable pseudo-morphological modules aimed at producing shape-aware, anatomically meaningful class-specific augmentations in combination with a supervised contrastive learning module to extract robust class-specific representations. Experiments conducted across three datasets show improved performance and generalization capacity, especially under class imbalance and imaging protocol variations. The source code will be made available upon acceptance at https://github.com/zobia111/SDG-Alzheimer.",
        "arxiv_id": "2505.22465",
        "ARXIVID": "2505.22465",
        "COMMENT": "Does not match any specific criterion but is related to general computer vision and medical imaging applications.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.22434": {
        "authors": [
            "Zobia Batool",
            "Huseyin Ozkan",
            "Erchan Aptoula"
        ],
        "title": "Distance Transform Guided Mixup for Alzheimer's Detection",
        "abstract": "arXiv:2505.22434v1 Announce Type: new  Abstract: Alzheimer's detection efforts aim to develop accurate models for early disease diagnosis. Significant advances have been achieved with convolutional neural networks and vision transformer based approaches. However, medical datasets suffer heavily from class imbalance, variations in imaging protocols, and limited dataset diversity, which hinder model generalization. To overcome these challenges, this study focuses on single-domain generalization by extending the well-known mixup method. The key idea is to compute the distance transform of MRI scans, separate them spatially into multiple layers and then combine layers stemming from distinct samples to produce augmented images. The proposed approach generates diverse data while preserving the brain's structure. Experimental results show generalization performance improvement across both ADNI and AIBL datasets.",
        "arxiv_id": "2505.22434",
        "ARXIVID": "2505.22434",
        "COMMENT": "Does not match any specific criterion but is related to general computer vision and medical imaging applications.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.21522": {
        "authors": [
            "Shan Gao",
            "Zhiqiang Wu",
            "Yawen Niu",
            "Xiaotao Li",
            "Qingqing Xu"
        ],
        "title": "CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures",
        "abstract": "arXiv:2505.21522v1 Announce Type: new  Abstract: While deep neural network (DNN)-based video denoising has demonstrated significant performance, deploying state-of-the-art models on edge devices remains challenging due to stringent real-time and energy efficiency requirements. Computing-in-Memory (CIM) chips offer a promising solution by integrating computation within memory cells, enabling rapid matrix-vector multiplication (MVM). However, existing DNN models are often designed without considering CIM architectural constraints, thus limiting their acceleration potential during inference. To address this, we propose a hardware-algorithm co-design framework incorporating two innovations: (1) a CIM-Aware Architecture, CIM-NET, optimized for large receptive field operation and CIM's crossbar-based MVM acceleration; and (2) a pseudo-convolutional operator, CIM-CONV, used within CIM-NET to integrate slide-based processing with fully connected transformations for high-quality feature extraction and reconstruction. This framework significantly reduces the number of MVM operations, improving inference speed on CIM chips while maintaining competitive performance. Experimental results indicate that, compared to the conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM operations with a slight decrease in denoising performance. With a stride value of 8, CIM-NET reduces MVM operations to 1/77th of the original, while maintaining competitive PSNR (35.11 dB vs. 35.56 dB",
        "arxiv_id": "2505.21522",
        "ARXIVID": "2505.21522",
        "COMMENT": "Does not match any specific criterion but is related to video denoising and edge device optimization, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.22011": {
        "authors": [
            "Menghui Zhang",
            "Jing Zhang",
            "Lin Chen",
            "Li Zhuo"
        ],
        "title": "Prototype Embedding Optimization for Human-Object Interaction Detection in Livestreaming",
        "abstract": "arXiv:2505.22011v1 Announce Type: new  Abstract: Livestreaming often involves interactions between streamers and objects, which is critical for understanding and regulating web content. While human-object interaction (HOI) detection has made some progress in general-purpose video downstream tasks, when applied to recognize the interaction behaviors between a streamer and different objects in livestreaming, it tends to focuses too much on the objects and neglects their interactions with the streamer, which leads to object bias. To solve this issue, we propose a prototype embedding optimization for human-object interaction detection (PeO-HOI). First, the livestreaming is preprocessed using object detection and tracking techniques to extract features of the human-object (HO) pairs. Then, prototype embedding optimization is adopted to mitigate the effect of object bias on HOI. Finally, after modelling the spatio-temporal context between HO pairs, the HOI detection results are obtained by the prediction head. The experimental results show that the detection accuracy of the proposed PeO-HOI method has detection accuracies of 37.19%@full, 51.42%@non-rare, 26.20%@rare on the publicly available dataset VidHOI, 45.13%@full, 62.78%@non-rare and 30.37%@rare on the self-built dataset BJUT-HOI, which effectively improves the HOI detection performance in livestreaming.",
        "arxiv_id": "2505.22011",
        "ARXIVID": "2505.22011",
        "COMMENT": "Does not match any specific criteria but discusses human-object interaction detection, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.21513": {
        "authors": [
            "Nicolas Echevarrieta-Catalan",
            "Ana Ribas-Rodriguez",
            "Francisco Cedron",
            "Odelia Schwartz",
            "Vanessa Aguiar-Pulido"
        ],
        "title": "Enhancing Vision Transformer Explainability Using Artificial Astrocytes",
        "abstract": "arXiv:2505.21513v1 Announce Type: new  Abstract: Machine learning models achieve high precision, but their decision-making processes often lack explainability. Furthermore, as model complexity increases, explainability typically decreases. Existing efforts to improve explainability primarily involve developing new eXplainable artificial intelligence (XAI) techniques or incorporating explainability constraints during training. While these approaches yield specific improvements, their applicability remains limited. In this work, we propose the Vision Transformer with artificial Astrocytes (ViTA). This training-free approach is inspired by neuroscience and enhances the reasoning of a pretrained deep neural network to generate more human-aligned explanations. We evaluated our approach employing two well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a standard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the similarity between the heatmaps produced by the XAI techniques and a (human-aligned) ground truth. Our results consistently demonstrate that incorporating artificial astrocytes enhances the alignment of model explanations with human perception, leading to statistically significant improvements across all XAI techniques and metrics utilized.",
        "arxiv_id": "2505.21513",
        "ARXIVID": "2505.21513",
        "COMMENT": "Does not match any specific criteria but discusses explainability in Vision Transformers, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}