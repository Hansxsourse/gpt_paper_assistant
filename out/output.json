{
    "2504.01934": {
        "authors": [
            "Runhui Huang",
            "Chunwei Wang",
            "Junwei Yang",
            "Guansong Lu",
            "Yunlong Yuan",
            "Jianhua Han",
            "Lu Hou",
            "Wei Zhang",
            "Lanqing Hong",
            "Hengshuang Zhao",
            "Hang Xu"
        ],
        "title": "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement",
        "abstract": "arXiv:2504.01934v1 Announce Type: new  Abstract: We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: https://illume-unified-mllm.github.io/.",
        "arxiv_id": "2504.01934",
        "ARXIVID": "2504.01934",
        "COMMENT": "Matches criterion 2 and 4 as it introduces a unified MLLM with dual visual tokenization and diffusion refinement, addressing multimodal understanding, generation, and editing.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2504.01792": {
        "authors": [
            "Limeng Qiao",
            "Yiyang Gan",
            "Bairui Wang",
            "Jie Qin",
            "Shuang Xu",
            "Siqi Yang",
            "Lin Ma"
        ],
        "title": "UniViTAR: Unified Vision Transformer with Native Resolution",
        "abstract": "arXiv:2504.01792v1 Announce Type: new  Abstract: Conventional Vision Transformer simplifies visual modeling by standardizing input resolutions, often disregarding the variability of natural visual data and compromising spatial-contextual fidelity. While preliminary explorations have superficially investigated native resolution modeling, existing approaches still lack systematic analysis from a visual representation perspective. To bridge this gap, we introduce UniViTAR, a family of homogeneous vision foundation models tailored for unified visual modality and native resolution scenario in the era of multimodal. Our framework first conducts architectural upgrades to the vanilla paradigm by integrating multiple advanced components. Building upon these improvements, a progressive training paradigm is introduced, which strategically combines two core mechanisms: (1) resolution curriculum learning, transitioning from fixed-resolution pretraining to native resolution tuning, thereby leveraging ViT's inherent adaptability to variable-length sequences, and (2) visual modality adaptation via inter-batch image-video switching, which balances computational efficiency with enhanced temporal reasoning. In parallel, a hybrid training framework further synergizes sigmoid-based contrastive loss with feature distillation from a frozen teacher model, thereby accelerating early-stage convergence. Finally, trained exclusively on public datasets, externsive experiments across multiple model scales from 0.3B to 1B demonstrate its effectiveness.",
        "arxiv_id": "2504.01792",
        "ARXIVID": "2504.01792",
        "COMMENT": "Matches criterion 4 as it introduces a vision foundation model (UniViTAR) with architectural and training improvements for multimodal and native resolution scenarios.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2504.01328": {
        "authors": [
            "Min Shi",
            "Shihao Wang",
            "Chieh-Yun Chen",
            "Jitesh Jain",
            "Kai Wang",
            "Junjun Xiong",
            "Guilin Liu",
            "Zhiding Yu",
            "Humphrey Shi"
        ],
        "title": "Slow-Fast Architecture for Video Multi-Modal Large Language Models",
        "abstract": "arXiv:2504.01328v1 Announce Type: new  Abstract: Balancing temporal resolution and spatial detail under limited compute budget remains a key challenge for video-based multi-modal large language models (MLLMs). Existing methods typically compress video representations using predefined rules before feeding them into the LLM, resulting in irreversible information loss and often ignoring input instructions. To address this, we propose a novel slow-fast architecture that naturally circumvents this trade-off, enabling the use of more input frames while preserving spatial details. Inspired by how humans first skim a video before focusing on relevant parts, our slow-fast design employs a dual-token strategy: 1) \"fast\" visual tokens -- a compact set of compressed video features -- are fed into the LLM alongside text embeddings to provide a quick overview; 2) \"slow\" visual tokens -- uncompressed video features -- are cross-attended by text embeddings through specially designed hybrid decoder layers, enabling instruction-aware extraction of relevant visual details with linear complexity. We conduct systematic exploration to optimize both the overall architecture and key components. Experiments show that our model significantly outperforms self-attention-only baselines, extending the input capacity from 16 to 128 frames with just a 3% increase in computation, and achieving a 16% average performance improvement across five video understanding benchmarks. Our 7B model achieves state-of-the-art performance among models of similar size. Furthermore, our slow-fast architecture is a plug-and-play design that can be integrated into other video MLLMs to improve efficiency and scalability.",
        "arxiv_id": "2504.01328",
        "ARXIVID": "2504.01328",
        "COMMENT": "Matches criterion 2 as it introduces a novel slow-fast architecture for video-based multi-modal large language models.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2504.01805": {
        "authors": [
            "Kun Ouyang"
        ],
        "title": "Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning",
        "abstract": "arXiv:2504.01805v1 Announce Type: new  Abstract: Enhancing the spatial reasoning capabilities of Multi-modal Large Language Models (MLLMs) for video understanding is crucial yet challenging. We present Spatial-R1, a targeted approach involving two key contributions: the curation of SR, a new video spatial reasoning dataset from ScanNet with automatically generated QA pairs across seven task types, and the application of Task-Specific Group Relative Policy Optimization (GRPO) for fine-tuning. By training the Qwen2.5-VL-7B-Instruct model on SR using GRPO, Spatial-R1 significantly advances performance on the VSI-Bench benchmark, achieving a 7.4\\% gain over the baseline and outperforming strong contemporary models. This work validates the effectiveness of specialized data curation and optimization techniques for improving complex spatial reasoning in video MLLMs.",
        "arxiv_id": "2504.01805",
        "ARXIVID": "2504.01805",
        "COMMENT": "Matches criterion 1 and 2 as it focuses on improving spatial reasoning in MLLMs for video understanding, with a new dataset and optimization techniques.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2504.01383": {
        "authors": [
            "Chang-Bin Zhang",
            "Jinhong Ni",
            "Yujie Zhong",
            "Kai Han"
        ],
        "title": "v-CLR: View-Consistent Learning for Open-World Instance Segmentation",
        "abstract": "arXiv:2504.01383v1 Announce Type: new  Abstract: In this paper, we address the challenging problem of open-world instance segmentation. Existing works have shown that vanilla visual networks are biased toward learning appearance information, \\eg texture, to recognize objects. This implicit bias causes the model to fail in detecting novel objects with unseen textures in the open-world setting. To address this challenge, we propose a learning framework, called view-Consistent LeaRning (v-CLR), which aims to enforce the model to learn appearance-invariant representations for robust instance segmentation. In v-CLR, we first introduce additional views for each image, where the texture undergoes significant alterations while preserving the image's underlying structure. We then encourage the model to learn the appearance-invariant representation by enforcing the consistency between object features across different views, for which we obtain class-agnostic object proposals using off-the-shelf unsupervised models that possess strong object-awareness. These proposals enable cross-view object feature matching, greatly reducing the appearance dependency while enhancing the object-awareness. We thoroughly evaluate our method on public benchmarks under both cross-class and cross-dataset settings, achieving state-of-the-art performance. Project page: https://visual-ai.github.io/vclr",
        "arxiv_id": "2504.01383",
        "ARXIVID": "2504.01383",
        "COMMENT": "Matches criterion 1 as it proposes a new methodological improvement for spatial understanding in open-world instance segmentation by enforcing appearance-invariant representations.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.01472": {
        "authors": [
            "Yuejiao Su",
            "Yi Wang",
            "Qiongyang Hu",
            "Chuang Yang",
            "Lap-Pui Chau"
        ],
        "title": "ANNEXE: Unified Analyzing, Answering, and Pixel Grounding for Egocentric Interaction",
        "abstract": "arXiv:2504.01472v1 Announce Type: new  Abstract: Egocentric interaction perception is one of the essential branches in investigating human-environment interaction, which lays the basis for developing next-generation intelligent systems. However, existing egocentric interaction understanding methods cannot yield coherent textual and pixel-level responses simultaneously according to user queries, which lacks flexibility for varying downstream application requirements. To comprehend egocentric interactions exhaustively, this paper presents a novel task named Egocentric Interaction Reasoning and pixel Grounding (Ego-IRG). Taking an egocentric image with the query as input, Ego-IRG is the first task that aims to resolve the interactions through three crucial steps: analyzing, answering, and pixel grounding, which results in fluent textual and fine-grained pixel-level responses. Another challenge is that existing datasets cannot meet the conditions for the Ego-IRG task. To address this limitation, this paper creates the Ego-IRGBench dataset based on extensive manual efforts, which includes over 20k egocentric images with 1.6 million queries and corresponding multimodal responses about interactions. Moreover, we design a unified ANNEXE model to generate text- and pixel-level outputs utilizing multimodal large language models, which enables a comprehensive interpretation of egocentric interactions. The experiments on the Ego-IRGBench exhibit the effectiveness of our ANNEXE model compared with other works.",
        "arxiv_id": "2504.01472",
        "ARXIVID": "2504.01472",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark and method for egocentric interaction reasoning and pixel grounding, focusing on a novel angle in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.01890": {
        "authors": [
            "Shreyank N Gowda",
            "Boyan Gao",
            "Xiao Gu",
            "Xiaobo Jin"
        ],
        "title": "Is Temporal Prompting All We Need For Limited Labeled Action Recognition?",
        "abstract": "arXiv:2504.01890v1 Announce Type: new  Abstract: Video understanding has shown remarkable improvements in recent years, largely dependent on the availability of large scaled labeled datasets. Recent advancements in visual-language models, especially based on contrastive pretraining, have shown remarkable generalization in zero-shot tasks, helping to overcome this dependence on labeled datasets. Adaptations of such models for videos, typically involve modifying the architecture of vision-language models to cater to video data. However, this is not trivial, since such adaptations are mostly computationally intensive and struggle with temporal modeling. We present TP-CLIP, an adaptation of CLIP that leverages temporal visual prompting for temporal adaptation without modifying the core CLIP architecture. This preserves its generalization abilities. TP-CLIP efficiently integrates into the CLIP architecture, leveraging its pre-trained capabilities for video data. Extensive experiments across various datasets demonstrate its efficacy in zero-shot and few-shot learning, outperforming existing approaches with fewer parameters and computational efficiency. In particular, we use just 1/3 the GFLOPs and 1/28 the number of tuneable parameters in comparison to recent state-of-the-art and still outperform it by up to 15.8% depending on the task and dataset.",
        "arxiv_id": "2504.01890",
        "ARXIVID": "2504.01890",
        "COMMENT": "This paper matches criterion 2 as it adapts CLIP for video understanding using temporal visual prompting, achieving strong results in zero-shot and few-shot learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.01901": {
        "authors": [
            "Haochen Wang",
            "Yucheng Zhao",
            "Tiancai Wang",
            "Haoqiang Fan",
            "Xiangyu Zhang",
            "Zhaoxiang Zhang"
        ],
        "title": "Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness",
        "abstract": "arXiv:2504.01901v1 Announce Type: new  Abstract: The rapid development of Large Multimodal Models (LMMs) for 2D images and videos has spurred efforts to adapt these models for interpreting 3D scenes. However, the absence of large-scale 3D vision-language datasets has posed a significant obstacle. To address this issue, typical approaches focus on injecting 3D awareness into 2D LMMs by designing 3D input-level scene representations. This work provides a new perspective. We introduce reconstructive visual instruction tuning with 3D-awareness (Ross3D), which integrates 3D-aware visual supervision into the training procedure. Specifically, it incorporates cross-view and global-view reconstruction. The former requires reconstructing masked views by aggregating overlapping information from other views. The latter aims to aggregate information from all available views to recover Bird's-Eye-View images, contributing to a comprehensive overview of the entire scene. Empirically, Ross3D achieves state-of-the-art performance across various 3D scene understanding benchmarks. More importantly, our semi-supervised experiments demonstrate significant potential in leveraging large amounts of unlabeled 3D vision-only data.",
        "arxiv_id": "2504.01901",
        "ARXIVID": "2504.01901",
        "COMMENT": "This paper matches criterion 2 as it introduces a new method for integrating 3D-awareness into large multimodal models (LMMs) and achieves state-of-the-art results.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.01321": {
        "authors": [
            "Chunhui Zhang",
            "Li Liu",
            "Jialin Gao",
            "Xin Sun",
            "Hao Wen",
            "Xi Zhou",
            "Shiming Ge",
            "Yanfeng Wang"
        ],
        "title": "COST: Contrastive One-Stage Transformer for Vision-Language Small Object Tracking",
        "abstract": "arXiv:2504.01321v1 Announce Type: new  Abstract: Transformer has recently demonstrated great potential in improving vision-language (VL) tracking algorithms. However, most of the existing VL trackers rely on carefully designed mechanisms to perform the multi-stage multi-modal fusion. Additionally, direct multi-modal fusion without alignment ignores distribution discrepancy between modalities in feature space, potentially leading to suboptimal representations. In this work, we propose COST, a contrastive one-stage transformer fusion framework for VL tracking, aiming to learn semantically consistent and unified VL representations. Specifically, we introduce a contrastive alignment strategy that maximizes mutual information (MI) between a video and its corresponding language description. This enables effective cross-modal alignment, yielding semantically consistent features in the representation space. By leveraging a visual-linguistic transformer, we establish an efficient multi-modal fusion and reasoning mechanism, empirically demonstrating that a simple stack of transformer encoders effectively enables unified VL representations. Moreover, we contribute a newly collected VL tracking benchmark dataset for small object tracking, named VL-SOT500, with bounding boxes and language descriptions. Our dataset comprises two challenging subsets, VL-SOT230 and VL-SOT270, dedicated to evaluating generic and high-speed small object tracking, respectively. Small object tracking is notoriously challenging due to weak appearance and limited features, and this dataset is, to the best of our knowledge, the first to explore the usage of language cues to enhance visual representation for small object tracking. Extensive experiments demonstrate that COST achieves state-of-the-art performance on five existing VL tracking datasets, as well as on our proposed VL-SOT500 dataset. Source codes and dataset will be made publicly available.",
        "arxiv_id": "2504.01321",
        "ARXIVID": "2504.01321",
        "COMMENT": "Matches criterion 2 as it proposes a new vision-language transformer for small object tracking, and introduces a new benchmark dataset.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.01732": {
        "authors": [
            "Ulas Gunes",
            "Matias Turkulainen",
            "Xuqian Ren",
            "Arno Solin",
            "Juho Kannala",
            "Esa Rahtu"
        ],
        "title": "FIORD: A Fisheye Indoor-Outdoor Dataset with LIDAR Ground Truth for 3D Scene Reconstruction and Benchmarking",
        "abstract": "arXiv:2504.01732v1 Announce Type: new  Abstract: The development of large-scale 3D scene reconstruction and novel view synthesis methods mostly rely on datasets comprising perspective images with narrow fields of view (FoV). While effective for small-scale scenes, these datasets require large image sets and extensive structure-from-motion (SfM) processing, limiting scalability. To address this, we introduce a fisheye image dataset tailored for scene reconstruction tasks. Using dual 200-degree fisheye lenses, our dataset provides full 360-degree coverage of 5 indoor and 5 outdoor scenes. Each scene has sparse SfM point clouds and precise LIDAR-derived dense point clouds that can be used as geometric ground-truth, enabling robust benchmarking under challenging conditions such as occlusions and reflections. While the baseline experiments focus on vanilla Gaussian Splatting and NeRF based Nerfacto methods, the dataset supports diverse approaches for scene reconstruction, novel view synthesis, and image-based rendering.",
        "arxiv_id": "2504.01732",
        "ARXIVID": "2504.01732",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for 3D scene reconstruction with fisheye images and LIDAR ground truth.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.01324": {
        "authors": [
            "Ke Zhu",
            "Yu Wang",
            "Jiangjiang Liu",
            "Qunyi Xie",
            "Shanshan Liu",
            "Gang Zhang"
        ],
        "title": "On Data Synthesis and Post-training for Visual Abstract Reasoning",
        "abstract": "arXiv:2504.01324v1 Announce Type: new  Abstract: This paper is a pioneering work attempting to address abstract visual reasoning (AVR) problems for large vision-language models (VLMs). We make a common LLaVA-NeXT 7B model capable of perceiving and reasoning about specific AVR problems, surpassing both open-sourced (e.g., Qwen-2-VL-72B) and closed-sourced powerful VLMs (e.g., GPT-4o) with significant margin. This is a great breakthrough since almost all previous VLMs fail or show nearly random performance on representative AVR benchmarks. Our key success is our innovative data synthesis and post-training process, aiming to fully relieve the task difficulty and elicit the model to learn, step by step. Our 7B model is also shown to be behave well on AVR without sacrificing common multimodal comprehension abilities. We hope our paper could serve as an early effort in this area and would inspire further research in abstract visual reasoning.",
        "arxiv_id": "2504.01324",
        "ARXIVID": "2504.01324",
        "COMMENT": "Matches criterion 2 as it discusses a new vision-language model (VLM) with significant improvements in abstract visual reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.01023": {
        "authors": [
            "Chaofan Wu",
            "Jiaheng Li",
            "Jinghao Cao",
            "Ming Li",
            "Yongkang Feng",
            "Jiayu Wu Shuwen Xu",
            "Zihang Gao",
            "Sidan Du",
            "Yang Li"
        ],
        "title": "Omnidirectional Depth-Aided Occupancy Prediction based on Cylindrical Voxel for Autonomous Driving",
        "abstract": "arXiv:2504.01023v1 Announce Type: new  Abstract: Accurate 3D perception is essential for autonomous driving. Traditional methods often struggle with geometric ambiguity due to a lack of geometric prior. To address these challenges, we use omnidirectional depth estimation to introduce geometric prior. Based on the depth information, we propose a Sketch-Coloring framework OmniDepth-Occ. Additionally, our approach introduces a cylindrical voxel representation based on polar coordinate to better align with the radial nature of panoramic camera views. To address the lack of fisheye camera dataset in autonomous driving tasks, we also build a virtual scene dataset with six fisheye cameras, and the data volume has reached twice that of SemanticKITTI. Experimental results demonstrate that our Sketch-Coloring network significantly enhances 3D perception performance.",
        "arxiv_id": "2504.01023",
        "ARXIVID": "2504.01023",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset and a novel cylindrical voxel representation for autonomous driving, focusing on panoramic camera views.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2504.01960": {
        "authors": [
            "Niluthpol Chowdhury Mithun",
            "Tuan Pham",
            "Qiao Wang",
            "Ben Southall",
            "Kshitij Minhas",
            "Bogdan Matei",
            "Stephan Mandt",
            "Supun Samarasekera",
            "Rakesh Kumar"
        ],
        "title": "Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D Reconstruction and Novel View Synthesis",
        "abstract": "arXiv:2504.01960v1 Announce Type: new  Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have achieved impressive results in real-time 3D reconstruction and novel view synthesis. However, these methods struggle in large-scale, unconstrained environments where sparse and uneven input coverage, transient occlusions, appearance variability, and inconsistent camera settings lead to degraded quality. We propose GS-Diff, a novel 3DGS framework guided by a multi-view diffusion model to address these limitations. By generating pseudo-observations conditioned on multi-view inputs, our method transforms under-constrained 3D reconstruction problems into well-posed ones, enabling robust optimization even with sparse data. GS-Diff further integrates several enhancements, including appearance embedding, monocular depth priors, dynamic object modeling, anisotropy regularization, and advanced rasterization techniques, to tackle geometric and photometric challenges in real-world settings. Experiments on four benchmarks demonstrate that GS-Diff consistently outperforms state-of-the-art baselines by significant margins.",
        "arxiv_id": "2504.01960",
        "ARXIVID": "2504.01960",
        "COMMENT": "This paper aligns with criterion 4 as it proposes a novel 3D Gaussian Splatting framework guided by a multi-view diffusion model for large-scale 3D reconstruction and novel view synthesis.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2504.01666": {
        "authors": [
            "Sarah Alyami",
            "Hamzah Luqman"
        ],
        "title": "CLIP-SLA: Parameter-Efficient CLIP Adaptation for Continuous Sign Language Recognition",
        "abstract": "arXiv:2504.01666v1 Announce Type: new  Abstract: Continuous sign language recognition (CSLR) focuses on interpreting and transcribing sequences of sign language gestures in videos. In this work, we propose CLIP sign language adaptation (CLIP-SLA), a novel CSLR framework that leverages the powerful pre-trained visual encoder from the CLIP model to sign language tasks through parameter-efficient fine-tuning (PEFT). We introduce two variants, SLA-Adapter and SLA-LoRA, which integrate PEFT modules into the CLIP visual encoder, enabling fine-tuning with minimal trainable parameters. The effectiveness of the proposed frameworks is validated on four datasets: Phoenix2014, Phoenix2014-T, CSL-Daily, and Isharah-500, where both CLIP-SLA variants outperformed several SOTA models with fewer trainable parameters. Extensive ablation studies emphasize the effectiveness and flexibility of the proposed methods with different vision-language models for CSLR. These findings showcase the potential of adapting large-scale pre-trained models for scalable and efficient CSLR, which pave the way for future advancements in sign language understanding.",
        "arxiv_id": "2504.01666",
        "ARXIVID": "2504.01666",
        "COMMENT": "This paper adapts CLIP for continuous sign language recognition, which aligns with criterion 4 as it involves adapting vision foundation models for a specific application.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.01886": {
        "authors": [
            "Yanzhou Su",
            "Tianbin Li",
            "Jiyao Liu",
            "Chenglong Ma",
            "Junzhi Ning",
            "Cheng Tang",
            "Sibo Ju",
            "Jin Ye",
            "Pengcheng Chen",
            "Ming Hu",
            "Shixiang Tang",
            "Lihao Liu",
            "Bin Fu",
            "Wenqi Shao",
            "Xiaowei Hu",
            "Xiangwen Liao",
            "Yuanfeng Ji",
            "Junjun He"
        ],
        "title": "GMAI-VL-R1: Harnessing Reinforcement Learning for Multimodal Medical Reasoning",
        "abstract": "arXiv:2504.01886v1 Announce Type: new  Abstract: Recent advances in general medical AI have made significant strides, but existing models often lack the reasoning capabilities needed for complex medical decision-making. This paper presents GMAI-VL-R1, a multimodal medical reasoning model enhanced by reinforcement learning (RL) to improve its reasoning abilities. Through iterative training, GMAI-VL-R1 optimizes decision-making, significantly boosting diagnostic accuracy and clinical support. We also develop a reasoning data synthesis method, generating step-by-step reasoning data via rejection sampling, which further enhances the model's generalization. Experimental results show that after RL training, GMAI-VL-R1 excels in tasks such as medical image diagnosis and visual question answering. While the model demonstrates basic memorization with supervised fine-tuning, RL is crucial for true generalization. Our work establishes new evaluation benchmarks and paves the way for future advancements in medical reasoning models. Code, data, and model will be released at \\href{https://github.com/uni-medical/GMAI-VL-R1}{this link}.",
        "arxiv_id": "2504.01886",
        "ARXIVID": "2504.01886",
        "COMMENT": "This paper matches criterion 2 as it introduces a multimodal medical reasoning model enhanced by reinforcement learning, which is a novel application of vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.01641": {
        "authors": [
            "Zhixin Cheng",
            "Jiacheng Deng",
            "Xinjun Li",
            "Baoqun Yin",
            "Tianzhu Zhang"
        ],
        "title": "Bridge 2D-3D: Uncertainty-aware Hierarchical Registration Network with Domain Alignment",
        "abstract": "arXiv:2504.01641v1 Announce Type: new  Abstract: The method for image-to-point cloud registration typically determines the rigid transformation using a coarse-to-fine pipeline. However, directly and uniformly matching image patches with point cloud patches may lead to focusing on incorrect noise patches during matching while ignoring key ones. Moreover, due to the significant differences between image and point cloud modalities, it may be challenging to bridge the domain gap without specific improvements in design. To address the above issues, we innovatively propose the Uncertainty-aware Hierarchical Matching Module (UHMM) and the Adversarial Modal Alignment Module (AMAM). Within the UHMM, we model the uncertainty of critical information in image patches and facilitate multi-level fusion interactions between image and point cloud features. In the AMAM, we design an adversarial approach to reduce the domain gap between image and point cloud. Extensive experiments and ablation studies on RGB-D Scene V2 and 7-Scenes benchmarks demonstrate the superiority of our method, making it a state-of-the-art approach for image-to-point cloud registration tasks.",
        "arxiv_id": "2504.01641",
        "ARXIVID": "2504.01641",
        "COMMENT": "Matches criterion 1 as it introduces a novel hierarchical registration method for image-to-point cloud alignment, improving spatial understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.01644": {
        "authors": [
            "Kazuma Arii",
            "Satoshi Kurihara"
        ],
        "title": "Proposition of Affordance-Driven Environment Recognition Framework Using Symbol Networks in Large Language Models",
        "abstract": "arXiv:2504.01644v1 Announce Type: new  Abstract: In the quest to enable robots to coexist with humans, understanding dynamic situations and selecting appropriate actions based on common sense and affordances are essential. Conventional AI systems face challenges in applying affordance, as it represents implicit knowledge derived from common sense. However, large language models (LLMs) offer new opportunities due to their ability to process extensive human knowledge. This study proposes a method for automatic affordance acquisition by leveraging LLM outputs. The process involves generating text using LLMs, reconstructing the output into a symbol network using morphological and dependency analysis, and calculating affordances based on network distances. Experiments using ``apple'' as an example demonstrated the method's ability to extract context-dependent affordances with high explainability. The results suggest that the proposed symbol network, reconstructed from LLM outputs, enables robots to interpret affordances effectively, bridging the gap between symbolized data and human-like situational understanding.",
        "arxiv_id": "2504.01644",
        "ARXIVID": "2504.01644",
        "COMMENT": "Matches criterion 1 as it proposes a framework for spatial understanding using affordance-driven recognition in embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.01647": {
        "authors": [
            "Tobias Fischer",
            "Samuel Rota Bul\\`o",
            "Yung-Hsu Yang",
            "Nikhil Varma Keetha",
            "Lorenzo Porzi",
            "Norman M\\\"uller",
            "Katja Schwarz",
            "Jonathon Luiten",
            "Marc Pollefeys",
            "Peter Kontschieder"
        ],
        "title": "FlowR: Flowing from Sparse to Dense 3D Reconstructions",
        "abstract": "arXiv:2504.01647v1 Announce Type: new  Abstract: 3D Gaussian splatting enables high-quality novel view synthesis (NVS) at real-time frame rates. However, its quality drops sharply as we depart from the training views. Thus, dense captures are needed to match the high-quality expectations of some applications, e.g. Virtual Reality (VR). However, such dense captures are very laborious and expensive to obtain. Existing works have explored using 2D generative models to alleviate this requirement by distillation or generating additional training views. These methods are often conditioned only on a handful of reference input views and thus do not fully exploit the available 3D information, leading to inconsistent generation results and reconstruction artifacts. To tackle this problem, we propose a multi-view, flow matching model that learns a flow to connect novel view renderings from possibly sparse reconstructions to renderings that we expect from dense reconstructions. This enables augmenting scene captures with novel, generated views to improve reconstruction quality. Our model is trained on a novel dataset of 3.6M image pairs and can process up to 45 views at 540x960 resolution (91K tokens) on one H100 GPU in a single forward pass. Our pipeline consistently improves NVS in sparse- and dense-view scenarios, leading to higher-quality reconstructions than prior works across multiple, widely-used NVS benchmarks.",
        "arxiv_id": "2504.01647",
        "ARXIVID": "2504.01647",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for improving 3D reconstructions and addresses limitations in sparse-to-dense view synthesis.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.01956": {
        "authors": [
            "Hanyang Wang",
            "Fangfu Liu",
            "Jiawei Chi",
            "Yueqi Duan"
        ],
        "title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step",
        "abstract": "arXiv:2504.01956v1 Announce Type: new  Abstract: Recovering 3D scenes from sparse views is a challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degradation by minimal overlap across input views with insufficient visual information. Fortunately, recent video generative models show promise in addressing this challenge as they are capable of generating video clips with plausible 3D structures. Powered by large pretrained video diffusion models, some pioneering research start to explore the potential of video generative prior and create 3D scenes from sparse views. Despite impressive improvements, they are limited by slow inference time and the lack of 3D constraint, leading to inefficiencies and reconstruction artifacts that do not align with real-world geometry structure. In this paper, we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step, aiming to build an efficient and effective tool to bridge the gap from video to 3D. Specifically, we design a 3D-aware leap flow distillation strategy to leap over time-consuming redundant information and train a dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate that our VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, highlighting its potential as an efficient tool for future video to 3D applications. Project Page: https://hanyang-21.github.io/VideoScene",
        "arxiv_id": "2504.01956",
        "ARXIVID": "2504.01956",
        "COMMENT": "Matches criterion 4 as it focuses on video diffusion models and their application to 3D scene generation, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2504.01503": {
        "authors": [
            "Ziteng Cui",
            "Xuangeng Chu",
            "Tatsuya Harada"
        ],
        "title": "Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting Conditions with View-Adaptive Curve Adjustment",
        "abstract": "arXiv:2504.01503v1 Announce Type: new  Abstract: Capturing high-quality photographs under diverse real-world lighting conditions is challenging, as both natural lighting (e.g., low-light) and camera exposure settings (e.g., exposure time) significantly impact image quality. This challenge becomes more pronounced in multi-view scenarios, where variations in lighting and image signal processor (ISP) settings across viewpoints introduce photometric inconsistencies. Such lighting degradations and view-dependent variations pose substantial challenges to novel view synthesis (NVS) frameworks based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). To address this, we introduce Luminance-GS, a novel approach to achieving high-quality novel view synthesis results under diverse challenging lighting conditions using 3DGS. By adopting per-view color matrix mapping and view-adaptive curve adjustments, Luminance-GS achieves state-of-the-art (SOTA) results across various lighting conditions -- including low-light, overexposure, and varying exposure -- while not altering the original 3DGS explicit representation. Compared to previous NeRF- and 3DGS-based baselines, Luminance-GS provides real-time rendering speed with improved reconstruction quality.",
        "arxiv_id": "2504.01503",
        "ARXIVID": "2504.01503",
        "COMMENT": "This paper matches criterion 4 as it adapts 3D Gaussian Splatting for challenging lighting conditions, improving novel view synthesis under diverse scenarios.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2504.01512": {
        "authors": [
            "Yiyang Shen",
            "Kun Zhou",
            "He Wang",
            "Yin Yang",
            "Tianjia Shao"
        ],
        "title": "High-fidelity 3D Object Generation from Single Image with RGBN-Volume Gaussian Reconstruction Model",
        "abstract": "arXiv:2504.01512v1 Announce Type: new  Abstract: Recently single-view 3D generation via Gaussian splatting has emerged and developed quickly. They learn 3D Gaussians from 2D RGB images generated from pre-trained multi-view diffusion (MVD) models, and have shown a promising avenue for 3D generation through a single image. Despite the current progress, these methods still suffer from the inconsistency jointly caused by the geometric ambiguity in the 2D images, and the lack of structure of 3D Gaussians, leading to distorted and blurry 3D object generation. In this paper, we propose to fix these issues by GS-RGBN, a new RGBN-volume Gaussian Reconstruction Model designed to generate high-fidelity 3D objects from single-view images. Our key insight is a structured 3D representation can simultaneously mitigate the afore-mentioned two issues. To this end, we propose a novel hybrid Voxel-Gaussian representation, where a 3D voxel representation contains explicit 3D geometric information, eliminating the geometric ambiguity from 2D images. It also structures Gaussians during learning so that the optimization tends to find better local optima. Our 3D voxel representation is obtained by a fusion module that aligns RGB features and surface normal features, both of which can be estimated from 2D images. Extensive experiments demonstrate the superiority of our methods over prior works in terms of high-quality reconstruction results, robust generalization, and good efficiency.",
        "arxiv_id": "2504.01512",
        "ARXIVID": "2504.01512",
        "COMMENT": "Matches criterion 4 as it proposes a novel method for high-fidelity 3D object generation from single images, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.01382": {
        "authors": [
            "Tianci Xue",
            "Weijian Qi",
            "Tianneng Shi",
            "Chan Hee Song",
            "Boyu Gou",
            "Dawn Song",
            "Huan Sun",
            "Yu Su"
        ],
        "title": "An Illusion of Progress? Assessing the Current State of Web Agents",
        "abstract": "arXiv:2504.01382v1 Announce Type: new  Abstract: As digitalization and cloud technologies evolve, the web is becoming increasingly important in the modern society. Autonomous web agents based on large language models (LLMs) hold a great potential in work automation. It is therefore important to accurately measure and monitor the progression of their capabilities. In this work, we conduct a comprehensive and rigorous assessment of the current state of web agents. Our results depict a very different picture of the competency of current agents, suggesting over-optimism in previously reported results. This gap can be attributed to shortcomings in existing benchmarks. We introduce Online-Mind2Web, an online evaluation benchmark consisting of 300 diverse and realistic tasks spanning 136 websites. It enables us to evaluate web agents under a setting that approximates how real users use these agents. To facilitate more scalable evaluation and development, we also develop a novel LLM-as-a-Judge automatic evaluation method and show that it can achieve around 85% agreement with human judgment, substantially higher than existing methods. Finally, we present the first comprehensive comparative analysis of current web agents, highlighting both their strengths and limitations to inspire future research.",
        "arxiv_id": "2504.01382",
        "ARXIVID": "2504.01382",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (Online-Mind2Web) for evaluating web agents, which is relevant to embodied AI benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.01941": {
        "authors": [
            "Yingyan Li",
            "Yuqi Wang",
            "Yang Liu",
            "Jiawei He",
            "Lue Fan",
            "Zhaoxiang Zhang"
        ],
        "title": "End-to-End Driving with Online Trajectory Evaluation via BEV World Model",
        "abstract": "arXiv:2504.01941v1 Announce Type: new  Abstract: End-to-end autonomous driving has achieved remarkable progress by integrating perception, prediction, and planning into a fully differentiable framework. Yet, to fully realize its potential, an effective online trajectory evaluation is indispensable to ensure safety. By forecasting the future outcomes of a given trajectory, trajectory evaluation becomes much more effective. This goal can be achieved by employing a world model to capture environmental dynamics and predict future states. Therefore, we propose an end-to-end driving framework WoTE, which leverages a BEV World model to predict future BEV states for Trajectory Evaluation. The proposed BEV world model is latency-efficient compared to image-level world models and can be seamlessly supervised using off-the-shelf BEV-space traffic simulators. We validate our framework on both the NAVSIM benchmark and the closed-loop Bench2Drive benchmark based on the CARLA simulator, achieving state-of-the-art performance. Code is released at https://github.com/liyingyanUCAS/WoTE.",
        "arxiv_id": "2504.01941",
        "ARXIVID": "2504.01941",
        "COMMENT": "Matches criterion 3 as it proposes a new end-to-end driving framework with a novel BEV world model for trajectory evaluation, which is relevant to embodied AI benchmarks and methods.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.01386": {
        "authors": [
            "Junjie Wu",
            "Jiangtao Xie",
            "Zhaolin Zhang",
            "Qilong Wang",
            "Qinghua Hu",
            "Peihua Li",
            "Sen Xu"
        ],
        "title": "DALIP: Distribution Alignment-based Language-Image Pre-Training for Domain-Specific Data",
        "abstract": "arXiv:2504.01386v1 Announce Type: new  Abstract: Recently, Contrastive Language-Image Pre-training (CLIP) has shown promising performance in domain-specific data (e.g., biology), and has attracted increasing research attention. Existing works generally focus on collecting extensive domain-specific data and directly tuning the original CLIP models. Intuitively, such a paradigm takes no full consideration of the characteristics lying in domain-specific data (e.g., fine-grained nature of biological data) and so limits model capability, while mostly losing the original ability of CLIP in the general domain. In this paper, we propose a Distribution Alignment-based Language-Image Pre-Training (DALIP) method for biological data. Specifically, DALIP optimizes CLIP models by matching the similarity between feature distribution of image-text pairs instead of the original [cls] token, which can capture rich yet effective information inherent in image-text pairs as powerful representations, and so better cope with fine-grained nature of biological data. Particularly, our DALIP efficiently approximates feature distribution via its first- and second-order statistics, while presenting a Multi-head Brownian Distance Covariance (MBDC) module to acquire second-order statistics of token features efficiently. Furthermore, we collect a new dataset for plant domain (e.g., specific data in biological domain) comprising 10M plant data with 3M general-domain data (namely PlantMix-13M) according to data mixing laws. Extensive experiments show that DALIP clearly outperforms existing CLIP counterparts in biological domain, while well generalizing to remote sensing and medical imaging domains. Besides, our PlantMix-13M dataset further boosts performance of DALIP in plant domain, while preserving model ability in general domain.",
        "arxiv_id": "2504.01386",
        "ARXIVID": "2504.01386",
        "COMMENT": "Matches criterion 2 as it proposes a new method for domain-specific language-image pre-training, which is relevant to visual large language models (VLLMs).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.01466": {
        "authors": [
            "Kaiwei Zhang",
            "Dandan Zhu",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ],
        "title": "Mesh Mamba: A Unified State Space Model for Saliency Prediction in Non-Textured and Textured Meshes",
        "abstract": "arXiv:2504.01466v1 Announce Type: new  Abstract: Mesh saliency enhances the adaptability of 3D vision by identifying and emphasizing regions that naturally attract visual attention. To investigate the interaction between geometric structure and texture in shaping visual attention, we establish a comprehensive mesh saliency dataset, which is the first to systematically capture the differences in saliency distribution under both textured and non-textured visual conditions. Furthermore, we introduce mesh Mamba, a unified saliency prediction model based on a state space model (SSM), designed to adapt across various mesh types. Mesh Mamba effectively analyzes the geometric structure of the mesh while seamlessly incorporating texture features into the topological framework, ensuring coherence throughout appearance-enhanced modeling. More importantly, by subgraph embedding and a bidirectional SSM, the model enables global context modeling for both local geometry and texture, preserving the topological structure and improving the understanding of visual details and structural complexity. Through extensive theoretical and empirical validation, our model not only improves performance across various mesh types but also demonstrates high scalability and versatility, particularly through cross validations of various visual features.",
        "arxiv_id": "2504.01466",
        "ARXIVID": "2504.01466",
        "COMMENT": "Matches criterion 4 as it focuses on mesh saliency prediction and introduces a unified state space model, which is related to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.01955": {
        "authors": [
            "Oliver Hahn",
            "Christoph Reich",
            "Nikita Araslanov",
            "Daniel Cremers",
            "Christian Rupprecht",
            "Stefan Roth"
        ],
        "title": "Scene-Centric Unsupervised Panoptic Segmentation",
        "abstract": "arXiv:2504.01955v1 Announce Type: new  Abstract: Unsupervised panoptic segmentation aims to partition an image into semantically meaningful regions and distinct object instances without training on manually annotated data. In contrast to prior work on unsupervised panoptic scene understanding, we eliminate the need for object-centric training data, enabling the unsupervised understanding of complex scenes. To that end, we present the first unsupervised panoptic method that directly trains on scene-centric imagery. In particular, we propose an approach to obtain high-resolution panoptic pseudo labels on complex scene-centric data, combining visual representations, depth, and motion cues. Utilizing both pseudo-label training and a panoptic self-training strategy yields a novel approach that accurately predicts panoptic segmentation of complex scenes without requiring any human annotations. Our approach significantly improves panoptic quality, e.g., surpassing the recent state of the art in unsupervised panoptic segmentation on Cityscapes by 9.4% points in PQ.",
        "arxiv_id": "2504.01955",
        "ARXIVID": "2504.01955",
        "COMMENT": "Matches criterion 4 as it focuses on unsupervised panoptic segmentation, which is related to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.01024": {
        "authors": [
            "Yufei He",
            "Xucong Zhang",
            "Arno H. A. Stienen"
        ],
        "title": "Gaze-Guided 3D Hand Motion Prediction for Detecting Intent in Egocentric Grasping Tasks",
        "abstract": "arXiv:2504.01024v1 Announce Type: new  Abstract: Human intention detection with hand motion prediction is critical to drive the upper-extremity assistive robots in neurorehabilitation applications. However, the traditional methods relying on physiological signal measurement are restrictive and often lack environmental context. We propose a novel approach that predicts future sequences of both hand poses and joint positions. This method integrates gaze information, historical hand motion sequences, and environmental object data, adapting dynamically to the assistive needs of the patient without prior knowledge of the intended object for grasping. Specifically, we use a vector-quantized variational autoencoder for robust hand pose encoding with an autoregressive generative transformer for effective hand motion sequence prediction. We demonstrate the usability of these novel techniques in a pilot study with healthy subjects. To train and evaluate the proposed method, we collect a dataset consisting of various types of grasp actions on different objects from multiple subjects. Through extensive experiments, we demonstrate that the proposed method can successfully predict sequential hand movement. Especially, the gaze information shows significant enhancements in prediction capabilities, particularly with fewer input frames, highlighting the potential of the proposed method for real-world applications.",
        "arxiv_id": "2504.01024",
        "ARXIVID": "2504.01024",
        "COMMENT": "Matches criterion 3 as it focuses on embodied AI with a novel method for hand motion prediction using gaze and environmental context.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.01476": {
        "authors": [
            "Junlong Ren",
            "Hao Wang"
        ],
        "title": "Enhanced Cross-modal 3D Retrieval via Tri-modal Reconstruction",
        "abstract": "arXiv:2504.01476v1 Announce Type: new  Abstract: Cross-modal 3D retrieval is a critical yet challenging task, aiming to achieve bi-directional retrieval between 3D and text modalities. Current methods predominantly rely on a certain 3D representation (e.g., point cloud), with few exploiting the 2D-3D consistency and complementary relationships, which constrains their performance. To bridge this gap, we propose to adopt multi-view images and point clouds to jointly represent 3D shapes, facilitating tri-modal alignment (i.e., image, point, text) for enhanced cross-modal 3D retrieval. Notably, we introduce tri-modal reconstruction to improve the generalization ability of encoders. Given point features, we reconstruct image features under the guidance of text features, and vice versa. With well-aligned point cloud and multi-view image features, we aggregate them as multimodal embeddings through fine-grained 2D-3D fusion to enhance geometric and semantic understanding. Recognizing the significant noise in current datasets where many 3D shapes and texts share similar semantics, we employ hard negative contrastive training to emphasize harder negatives with greater significance, leading to robust discriminative embeddings. Extensive experiments on the Text2Shape dataset demonstrate that our method significantly outperforms previous state-of-the-art methods in both shape-to-text and text-to-shape retrieval tasks by a substantial margin.",
        "arxiv_id": "2504.01476",
        "ARXIVID": "2504.01476",
        "COMMENT": "Matches criterion 2 as it focuses on cross-modal 3D retrieval and tri-modal alignment, which involves multi-modal learning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.01538": {
        "authors": [
            "You-Le Fang",
            "Dong-Shan Jian",
            "Xiang Li",
            "Yan-Qing Ma"
        ],
        "title": "AI-Newton: A Concept-Driven Physical Law Discovery System without Prior Physical Knowledge",
        "abstract": "arXiv:2504.01538v1 Announce Type: new  Abstract: Current limitations in human scientific discovery necessitate a new research paradigm. While advances in artificial intelligence (AI) offer a highly promising solution, enabling AI to emulate human-like scientific discovery remains an open challenge. To address this, we propose AI-Newton, a concept-driven discovery system capable of autonomously deriving physical laws from raw data -- without supervision or prior physical knowledge. The system integrates a knowledge base and knowledge representation centered on physical concepts, along with an autonomous discovery workflow. As a proof of concept, we apply AI-Newton to a large set of Newtonian mechanics problems. Given experimental data with noise, the system successfully rediscovers fundamental laws, including Newton's second law, energy conservation and law of gravitation, using autonomously defined concepts. This achievement marks a significant step toward AI-driven autonomous scientific discovery.",
        "arxiv_id": "2504.01538",
        "ARXIVID": "2504.01538",
        "COMMENT": "Does not match any specific criterion but is an interesting AI-driven system for physical law discovery, which may appeal to general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 8
    },
    "2504.01308": {
        "authors": [
            "Jiawei Wang",
            "Yushen Zuo",
            "Yuanjun Chai",
            "Zhendong Liu",
            "Yichen Fu",
            "Yichun Feng",
            "Kin-man Lam"
        ],
        "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks",
        "abstract": "arXiv:2504.01308v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.",
        "arxiv_id": "2504.01308",
        "ARXIVID": "2504.01308",
        "COMMENT": "Matches criterion 2 as it focuses on improving the robustness of vision-language models (VLMs) against adversarial attacks.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2504.01873": {
        "authors": [
            "Zheng-Peng Duan",
            "Jiawei Zhang",
            "Siyu Liu",
            "Zheng Lin",
            "Chun-Le Guo",
            "Dongqing Zou",
            "Jimmy Ren",
            "Chongyi Li"
        ],
        "title": "A Diffusion-Based Framework for Occluded Object Movement",
        "abstract": "arXiv:2504.01873v1 Announce Type: new  Abstract: Seamlessly moving objects within a scene is a common requirement for image editing, but it is still a challenge for existing editing methods. Especially for real-world images, the occlusion situation further increases the difficulty. The main difficulty is that the occluded portion needs to be completed before movement can proceed. To leverage the real-world knowledge embedded in the pre-trained diffusion models, we propose a Diffusion-based framework specifically designed for Occluded Object Movement, named DiffOOM. The proposed DiffOOM consists of two parallel branches that perform object de-occlusion and movement simultaneously. The de-occlusion branch utilizes a background color-fill strategy and a continuously updated object mask to focus the diffusion process on completing the obscured portion of the target object. Concurrently, the movement branch employs latent optimization to place the completed object in the target location and adopts local text-conditioned guidance to integrate the object into new surroundings appropriately. Extensive evaluations demonstrate the superior performance of our method, which is further validated by a comprehensive user study.",
        "arxiv_id": "2504.01873",
        "ARXIVID": "2504.01873",
        "COMMENT": "Does not directly match any specific criterion but is relevant to your friend's general interest in generative modeling and image editing.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.01515": {
        "authors": [
            "Zixuan Wang",
            "Duo Peng",
            "Feng Chen",
            "Yuwei Yang",
            "Yinjie Lei"
        ],
        "title": "Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis",
        "abstract": "arXiv:2504.01515v1 Announce Type: new  Abstract: Conditional image synthesis is a crucial task with broad applications, such as artistic creation and virtual reality. However, current generative methods are often task-oriented with a narrow scope, handling a restricted condition with constrained applicability. In this paper, we propose a novel approach that treats conditional image synthesis as the modular combination of diverse fundamental condition units. Specifically, we divide conditions into three primary units: text, layout, and drag. To enable effective control over these conditions, we design a dedicated alignment module for each. For the text condition, we introduce a Dense Concept Alignment (DCA) module, which achieves dense visual-text alignment by drawing on diverse textual concepts. For the layout condition, we propose a Dense Geometry Alignment (DGA) module to enforce comprehensive geometric constraints that preserve the spatial configuration. For the drag condition, we introduce a Dense Motion Alignment (DMA) module to apply multi-level motion regularization, ensuring that each pixel follows its desired trajectory without visual artifacts. By flexibly inserting and combining these alignment modules, our framework enhances the model's adaptability to diverse conditional generation tasks and greatly expands its application range. Extensive experiments demonstrate the superior performance of our framework across a variety of conditions, including textual description, segmentation mask (bounding box), drag manipulation, and their combinations. Code is available at https://github.com/ZixuanWang0525/DADG.",
        "arxiv_id": "2504.01515",
        "ARXIVID": "2504.01515",
        "COMMENT": "Does not directly match any specific criterion but is relevant to your friend's general interest in generative modeling and multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.01603": {
        "authors": [
            "Yizhe Tang",
            "Zhimin Sun",
            "Yuzhen Du",
            "Ran Yi",
            "Guangben Lu",
            "Teng Hu",
            "Luying Li",
            "Lizhuang Ma",
            "Fangyuan Zou"
        ],
        "title": "A$^\\text{T}$A: Adaptive Transformation Agent for Text-Guided Subject-Position Variable Background Inpainting",
        "abstract": "arXiv:2504.01603v1 Announce Type: new  Abstract: Image inpainting aims to fill the missing region of an image. Recently, there has been a surge of interest in foreground-conditioned background inpainting, a sub-task that fills the background of an image while the foreground subject and associated text prompt are provided. Existing background inpainting methods typically strictly preserve the subject's original position from the source image, resulting in inconsistencies between the subject and the generated background. To address this challenge, we propose a new task, the \"Text-Guided Subject-Position Variable Background Inpainting\", which aims to dynamically adjust the subject position to achieve a harmonious relationship between the subject and the inpainted background, and propose the Adaptive Transformation Agent (A$^\\text{T}$A) for this task. Firstly, we design a PosAgent Block that adaptively predicts an appropriate displacement based on given features to achieve variable subject-position. Secondly, we design the Reverse Displacement Transform (RDT) module, which arranges multiple PosAgent blocks in a reverse structure, to transform hierarchical feature maps from deep to shallow based on semantic information. Thirdly, we equip A$^\\text{T}$A with a Position Switch Embedding to control whether the subject's position in the generated image is adaptively predicted or fixed. Extensive comparative experiments validate the effectiveness of our A$^\\text{T}$A approach, which not only demonstrates superior inpainting capabilities in subject-position variable inpainting, but also ensures good performance on subject-position fixed inpainting.",
        "arxiv_id": "2504.01603",
        "ARXIVID": "2504.01603",
        "COMMENT": "This paper introduces a novel method for text-guided image inpainting, which does not directly match any of the criteria but may be tangentially relevant to generative modeling in vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.01689": {
        "authors": [
            "Noam Elata",
            "Hyungjin Chung",
            "Jong Chul Ye",
            "Tomer Michaeli",
            "Michael Elad"
        ],
        "title": "InvFussion: Bridging Supervised and Zero-shot Diffusion for Inverse Problems",
        "abstract": "arXiv:2504.01689v1 Announce Type: new  Abstract: Diffusion Models have demonstrated remarkable capabilities in handling inverse problems, offering high-quality posterior-sampling-based solutions. Despite significant advances, a fundamental trade-off persists, regarding the way the conditioned synthesis is employed: Training-based methods achieve high quality results, while zero-shot approaches trade this with flexibility. This work introduces a framework that combines the best of both worlds -- the strong performance of supervised approaches and the flexibility of zero-shot methods. This is achieved through a novel architectural design that seamlessly integrates the degradation operator directly into the denoiser. In each block, our proposed architecture applies the degradation operator on the network activations and conditions the output using the attention mechanism, enabling adaptation to diverse degradation scenarios while maintaining high performance. Our work demonstrates the versatility of the proposed architecture, operating as a general MMSE estimator, a posterior sampler, or a Neural Posterior Principal Component estimator. This flexibility enables a wide range of downstream tasks, highlighting the broad applicability of our framework. The proposed modification of the denoiser network offers a versatile, accurate, and computationally efficient solution, demonstrating the advantages of dedicated network architectures for complex inverse problems. Experimental results on the FFHQ and ImageNet datasets demonstrate state-of-the-art posterior-sampling performance, surpassing both training-based and zero-shot alternatives.",
        "arxiv_id": "2504.01689",
        "ARXIVID": "2504.01689",
        "COMMENT": "This paper does not match any specific criteria but introduces a novel architectural design for diffusion models in inverse problems, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.01348": {
        "authors": [
            "Yuji Nozawa",
            "Yu-Chieh Lin",
            "Kazumoto Nakamura",
            "Youyang Ng"
        ],
        "title": "Prompt-Guided Attention Head Selection for Focus-Oriented Image Retrieval",
        "abstract": "arXiv:2504.01348v1 Announce Type: new  Abstract: The goal of this paper is to enhance pretrained Vision Transformer (ViT) models for focus-oriented image retrieval with visual prompting. In real-world image retrieval scenarios, both query and database images often exhibit complexity, with multiple objects and intricate backgrounds. Users often want to retrieve images with specific object, which we define as the Focus-Oriented Image Retrieval (FOIR) task. While a standard image encoder can be employed to extract image features for similarity matching, it may not perform optimally in the multi-object-based FOIR task. This is because each image is represented by a single global feature vector. To overcome this, a prompt-based image retrieval solution is required. We propose an approach called Prompt-guided attention Head Selection (PHS) to leverage the head-wise potential of the multi-head attention mechanism in ViT in a promptable manner. PHS selects specific attention heads by matching their attention maps with user's visual prompts, such as a point, box, or segmentation. This empowers the model to focus on specific object of interest while preserving the surrounding visual context. Notably, PHS does not necessitate model re-training and avoids any image alteration. Experimental results show that PHS substantially improves performance on multiple datasets, offering a practical and training-free solution to enhance model performance in the FOIR task.",
        "arxiv_id": "2504.01348",
        "ARXIVID": "2504.01348",
        "COMMENT": "This paper proposes a novel method for focus-oriented image retrieval using Vision Transformers, which does not directly match any of the criteria but is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.01739": {
        "authors": [
            "Lukas Boehm",
            "Jonas Leo Mueller",
            "Christoffer Loeffler",
            "Leo Schwinn",
            "Bjoern Eskofier",
            "Dario Zanca"
        ],
        "title": "Understanding Cross-Model Perceptual Invariances Through Ensemble Metamers",
        "abstract": "arXiv:2504.01739v1 Announce Type: new  Abstract: Understanding the perceptual invariances of artificial neural networks is essential for improving explainability and aligning models with human vision. Metamers - stimuli that are physically distinct yet produce identical neural activations - serve as a valuable tool for investigating these invariances. We introduce a novel approach to metamer generation by leveraging ensembles of artificial neural networks, capturing shared representational subspaces across diverse architectures, including convolutional neural networks and vision transformers. To characterize the properties of the generated metamers, we employ a suite of image-based metrics that assess factors such as semantic fidelity and naturalness. Our findings show that convolutional neural networks generate more recognizable and human-like metamers, while vision transformers produce realistic but less transferable metamers, highlighting the impact of architectural biases on representational invariances.",
        "arxiv_id": "2504.01739",
        "ARXIVID": "2504.01739",
        "COMMENT": "Does not directly match any specific criterion but is relevant to your friend's general interest in understanding vision models and their invariances.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.01326": {
        "authors": [
            "Jin Lian",
            "Zhongyu Wan",
            "Ming Gao",
            "JunFeng Chen"
        ],
        "title": "CFMD: Dynamic Cross-layer Feature Fusion for Salient Object Detection",
        "abstract": "arXiv:2504.01326v1 Announce Type: new  Abstract: Cross-layer feature pyramid networks (CFPNs) have achieved notable progress in multi-scale feature fusion and boundary detail preservation for salient object detection. However, traditional CFPNs still suffer from two core limitations: (1) a computational bottleneck caused by complex feature weighting operations, and (2) degraded boundary accuracy due to feature blurring in the upsampling process. To address these challenges, we propose CFMD, a novel cross-layer feature pyramid network that introduces two key innovations. First, we design a context-aware feature aggregation module (CFLMA), which incorporates the state-of-the-art Mamba architecture to construct a dynamic weight distribution mechanism. This module adaptively adjusts feature importance based on image context, significantly improving both representation efficiency and generalization. Second, we introduce an adaptive dynamic upsampling unit (CFLMD) that preserves spatial details during resolution recovery. By adjusting the upsampling range dynamically and initializing with a bilinear strategy, the module effectively reduces feature overlap and maintains fine-grained boundary structures. Extensive experiments on three standard benchmarks using three mainstream backbone networks demonstrate that CFMD achieves substantial improvements in pixel-level accuracy and boundary segmentation quality, especially in complex scenes. The results validate the effectiveness of CFMD in jointly enhancing computational efficiency and segmentation performance, highlighting its strong potential in salient object detection tasks.",
        "arxiv_id": "2504.01326",
        "ARXIVID": "2504.01326",
        "COMMENT": "This paper focuses on salient object detection with cross-layer feature fusion, which does not directly match any of the criteria but may be tangentially relevant to computer vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.01298": {
        "authors": [
            "Shiyong Liu",
            "Zhihao Li",
            "Xiao Tang",
            "Jianzhuang Liu"
        ],
        "title": "Direction-Aware Hybrid Representation Learning for 3D Hand Pose and Shape Estimation",
        "abstract": "arXiv:2504.01298v1 Announce Type: new  Abstract: Most model-based 3D hand pose and shape estimation methods directly regress the parametric model parameters from an image to obtain 3D joints under weak supervision. However, these methods involve solving a complex optimization problem with many local minima, making training difficult. To address this challenge, we propose learning direction-aware hybrid features (DaHyF) that fuse implicit image features and explicit 2D joint coordinate features. This fusion is enhanced by the pixel direction information in the camera coordinate system to estimate pose, shape, and camera viewpoint. Our method directly predicts 3D hand poses with DaHyF representation and reduces jittering during motion capture using prediction confidence based on contrastive learning. We evaluate our method on the FreiHAND dataset and show that it outperforms existing state-of-the-art methods by more than 33% in accuracy. DaHyF also achieves the top ranking on both the HO3Dv2 and HO3Dv3 leaderboards for the metric of Mean Joint Error (after scale and translation alignment). Compared to the second-best results, the largest improvement observed is 10%. We also demonstrate its effectiveness in real-time motion capture scenarios with hand position variability, occlusion, and motion blur.",
        "arxiv_id": "2504.01298",
        "ARXIVID": "2504.01298",
        "COMMENT": "Does not match any specific criterion but is related to 3D hand pose and shape estimation, which is tangentially relevant to spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.01668": {
        "authors": [
            "Junjie Chen",
            "Yuecong Xu",
            "Haosheng Li",
            "Kemi Ding"
        ],
        "title": "Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation",
        "abstract": "arXiv:2504.01668v1 Announce Type: new  Abstract: 3D point cloud semantic segmentation (PCSS) is a cornerstone for environmental perception in robotic systems and autonomous driving, enabling precise scene understanding through point-wise classification. While unsupervised domain adaptation (UDA) mitigates label scarcity in PCSS, existing methods critically overlook the inherent vulnerability to real-world perturbations (e.g., snow, fog, rain) and adversarial distortions. This work first identifies two intrinsic limitations that undermine current PCSS-UDA robustness: (a) unsupervised features overlap from unaligned boundaries in shared-class regions and (b) feature structure erosion caused by domain-invariant learning that suppresses target-specific patterns. To address the proposed problems, we propose a tripartite framework consisting of: 1) a robustness evaluation model quantifying resilience against adversarial attack/corruption types through robustness metrics; 2) an invertible attention alignment module (IAAM) enabling bidirectional domain mapping while preserving discriminative structure via attention-guided overlap suppression; and 3) a contrastive memory bank with quality-aware contrastive learning that progressively refines pseudo-labels with feature quality for more discriminative representations. Extensive experiments on SynLiDAR-to-SemanticPOSS adaptation demonstrate a maximum mIoU improvement of 14.3\\% under adversarial attack.",
        "arxiv_id": "2504.01668",
        "ARXIVID": "2504.01668",
        "COMMENT": "Does not match any specific criterion but is related to 3D semantic segmentation and domain adaptation, which is tangentially relevant to spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.01648": {
        "authors": [
            "Haosheng Li",
            "Yuecong Xu",
            "Junjie Chen",
            "Kemi Ding"
        ],
        "title": "ProtoGuard-guided PROPEL: Class-Aware Prototype Enhancement and Progressive Labeling for Incremental 3D Point Cloud Segmentation",
        "abstract": "arXiv:2504.01648v1 Announce Type: new  Abstract: 3D point cloud semantic segmentation technology has been widely used. However, in real-world scenarios, the environment is evolving. Thus, offline-trained segmentation models may lead to catastrophic forgetting of previously seen classes. Class-incremental learning (CIL) is designed to address the problem of catastrophic forgetting. While point clouds are common, we observe high similarity and unclear boundaries between different classes. Meanwhile, they are known to be imbalanced in class distribution. These lead to issues including misclassification between similar classes and the long-tail problem, which have not been adequately addressed in previous CIL methods. We thus propose ProtoGuard and PROPEL (Progressive Refinement Of PsEudo-Labels). In the base-class training phase, ProtoGuard maintains geometric and semantic prototypes for each class, which are combined into prototype features using an attention mechanism. In the novel-class training phase, PROPEL inherits the base feature extractor and classifier, guiding pseudo-label propagation and updates based on density distribution and semantic similarity. Extensive experiments show that our approach achieves remarkable results on both the S3DIS and ScanNet datasets, improving the mIoU of 3D point cloud segmentation by a maximum of 20.39% under the 5-step CIL scenario on S3DIS.",
        "arxiv_id": "2504.01648",
        "ARXIVID": "2504.01648",
        "COMMENT": "Does not match any specific criterion but is related to 3D point cloud segmentation, which is tangentially relevant to spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.01771": {
        "authors": [
            "Theodoros Aivalis",
            "Iraklis A. Klampanos",
            "Antonis Troumpoukis",
            "Joemon M. Jose"
        ],
        "title": "Enhancing Interpretability in Generative AI Through Search-Based Data Influence Analysis",
        "abstract": "arXiv:2504.01771v1 Announce Type: new  Abstract: Generative AI models offer powerful capabilities but often lack transparency, making it difficult to interpret their output. This is critical in cases involving artistic or copyrighted content. This work introduces a search-inspired approach to improve the interpretability of these models by analysing the influence of training data on their outputs. Our method provides observational interpretability by focusing on a model's output rather than on its internal state. We consider both raw data and latent-space embeddings when searching for the influence of data items in generated content. We evaluate our method by retraining models locally and by demonstrating the method's ability to uncover influential subsets in the training data. This work lays the groundwork for future extensions, including user-based evaluations with domain experts, which is expected to improve observational interpretability further.",
        "arxiv_id": "2504.01771",
        "ARXIVID": "2504.01771",
        "COMMENT": "Does not match any specific criteria but is related to interpretability in generative AI, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.01724": {
        "authors": [
            "Yuxuan Luo",
            "Zhengkun Rong",
            "Lizhen Wang",
            "Longhao Zhang",
            "Tianshu Hu",
            "Yongming Zhu"
        ],
        "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance",
        "abstract": "arXiv:2504.01724v1 Announce Type: new  Abstract: While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/.",
        "arxiv_id": "2504.01724",
        "ARXIVID": "2504.01724",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling in multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.01637": {
        "authors": [
            "Reo Abe",
            "Akifumi Ito",
            "Kanata Takayasu",
            "Satoshi Kurihara"
        ],
        "title": "LLM-mediated Dynamic Plan Generation with a Multi-Agent Approach",
        "abstract": "arXiv:2504.01637v1 Announce Type: new  Abstract: Planning methods with high adaptability to dynamic environments are crucial for the development of autonomous and versatile robots. We propose a method for leveraging a large language model (GPT-4o) to automatically generate networks capable of adapting to dynamic environments. The proposed method collects environmental \"status,\" representing conditions and goals, and uses them to generate agents. These agents are interconnected on the basis of specific conditions, resulting in networks that combine flexibility and generality. We conducted evaluation experiments to compare the networks automatically generated with the proposed method with manually constructed ones, confirming the comprehensiveness of the proposed method's networks and their higher generality. This research marks a significant advancement toward the development of versatile planning methods applicable to robotics, autonomous vehicles, smart systems, and other complex environments.",
        "arxiv_id": "2504.01637",
        "ARXIVID": "2504.01637",
        "COMMENT": "This paper does not match any specific criteria but discusses dynamic plan generation using large language models, which might be of general interest for embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.01872": {
        "authors": [
            "Jintao Zhang",
            "Zimin Xia",
            "Mingyue Dong",
            "Shuhan Shen",
            "Linwei Yue",
            "Xianwei Zheng"
        ],
        "title": "CoMatcher: Multi-View Collaborative Feature Matching",
        "abstract": "arXiv:2504.01872v1 Announce Type: new  Abstract: This paper proposes a multi-view collaborative matching strategy for reliable track construction in complex scenarios. We observe that the pairwise matching paradigms applied to image set matching often result in ambiguous estimation when the selected independent pairs exhibit significant occlusions or extreme viewpoint changes. This challenge primarily stems from the inherent uncertainty in interpreting intricate 3D structures based on limited two-view observations, as the 3D-to-2D projection leads to significant information loss. To address this, we introduce CoMatcher, a deep multi-view matcher to (i) leverage complementary context cues from different views to form a holistic 3D scene understanding and (ii) utilize cross-view projection consistency to infer a reliable global solution. Building on CoMatcher, we develop a groupwise framework that fully exploits cross-view relationships for large-scale matching tasks. Extensive experiments on various complex scenarios demonstrate the superiority of our method over the mainstream two-view matching paradigm.",
        "arxiv_id": "2504.01872",
        "ARXIVID": "2504.01872",
        "COMMENT": "This paper does not directly match any criteria but is related to multi-view collaborative feature matching, which might be of general interest for 3D scene understanding.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.01659": {
        "authors": [
            "Haosheng Li",
            "Yuecong Xu",
            "Junjie Chen",
            "Kemi Ding"
        ],
        "title": "Robust Unsupervised Domain Adaptation for 3D Point Cloud Segmentation Under Source Adversarial Attacks",
        "abstract": "arXiv:2504.01659v1 Announce Type: new  Abstract: Unsupervised domain adaptation (UDA) frameworks have shown good generalization capabilities for 3D point cloud semantic segmentation models on clean data. However, existing works overlook adversarial robustness when the source domain itself is compromised. To comprehensively explore the robustness of the UDA frameworks, we first design a stealthy adversarial point cloud generation attack that can significantly contaminate datasets with only minor perturbations to the point cloud surface. Based on that, we propose a novel dataset, AdvSynLiDAR, comprising synthesized contaminated LiDAR point clouds. With the generated corrupted data, we further develop the Adversarial Adaptation Framework (AAF) as the countermeasure. Specifically, by extending the key point sensitive (KPS) loss towards the Robust Long-Tail loss (RLT loss) and utilizing a decoder branch, our approach enables the model to focus on long-tail classes during the pre-training phase and leverages high-confidence decoded point cloud information to restore point cloud structures during the adaptation phase. We evaluated our AAF method on the AdvSynLiDAR dataset, where the results demonstrate that our AAF method can mitigate performance degradation under source adversarial perturbations for UDA in the 3D point cloud segmentation application.",
        "arxiv_id": "2504.01659",
        "ARXIVID": "2504.01659",
        "COMMENT": "This paper does not match any of the specific criteria but is related to 3D point cloud segmentation and domain adaptation, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.01053": {
        "authors": [
            "Chongyang Li",
            "Yanmei He",
            "Tianqian Zhang",
            "Mingjian He",
            "Shouyin Liu"
        ],
        "title": "Knowledge-Base based Semantic Image Transmission Using CLIP",
        "abstract": "arXiv:2504.01053v1 Announce Type: new  Abstract: This paper proposes a novel knowledge-Base (KB) assisted semantic communication framework for image transmission. At the receiver, a Facebook AI Similarity Search (FAISS) based vector database is constructed by extracting semantic embeddings from images using the Contrastive Language-Image Pre-Training (CLIP) model. During transmission, the transmitter first extracts a 512-dimensional semantic feature using the CLIP model, then compresses it with a lightweight neural network for transmission. After receiving the signal, the receiver reconstructs the feature back to 512 dimensions and performs similarity matching from the KB to retrieve the most semantically similar image. Semantic transmission success is determined by category consistency between the transmitted and retrieved images, rather than traditional metrics like Peak Signal-to-Noise Ratio (PSNR). The proposed system prioritizes semantic accuracy, offering a new evaluation paradigm for semantic-aware communication systems. Experimental validation on CIFAR100 demonstrates the effectiveness of the framework in achieving semantic image transmission.",
        "arxiv_id": "2504.01053",
        "ARXIVID": "2504.01053",
        "COMMENT": "Does not match any specific criterion but is related to semantic image transmission using CLIP, which may appeal to general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.01457": {
        "authors": [
            "Ting Meng",
            "Chunyun Fu",
            "Xiangyan Yan",
            "Zheng Liang",
            "Pan Ji",
            "Jianwen Wang",
            "Tao Huang"
        ],
        "title": "Deep LG-Track: An Enhanced Localization-Confidence-Guided Multi-Object Tracker",
        "abstract": "arXiv:2504.01457v1 Announce Type: new  Abstract: Multi-object tracking plays a crucial role in various applications, such as autonomous driving and security surveillance. This study introduces Deep LG-Track, a novel multi-object tracker that incorporates three key enhancements to improve the tracking accuracy and robustness. First, an adaptive Kalman filter is developed to dynamically update the covariance of measurement noise based on detection confidence and trajectory disappearance. Second, a novel cost matrix is formulated to adaptively fuse motion and appearance information, leveraging localization confidence and detection confidence as weighting factors. Third, a dynamic appearance feature updating strategy is introduced, adjusting the relative weighting of historical and current appearance features based on appearance clarity and localization accuracy. Comprehensive evaluations on the MOT17 and MOT20 datasets demonstrate that the proposed Deep LG-Track consistently outperforms state-of-the-art trackers across multiple performance metrics, highlighting its effectiveness in multi-object tracking tasks.",
        "arxiv_id": "2504.01457",
        "ARXIVID": "2504.01457",
        "COMMENT": "Does not match any specific criterion but is related to multi-object tracking, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.01396": {
        "authors": [
            "Zheng Yang",
            "Ruoxin Chen",
            "Zhiyuan Yan",
            "Ke-Yue Zhang",
            "Xinghe Fu",
            "Shuang Wu",
            "Xiujun Shu",
            "Taiping Yao",
            "Junchi Yan",
            "Shouhong Ding",
            "Xi Li"
        ],
        "title": "All Patches Matter, More Patches Better: Enhance AI-Generated Image Detection via Panoptic Patch Learning",
        "abstract": "arXiv:2504.01396v1 Announce Type: new  Abstract: The exponential growth of AI-generated images (AIGIs) underscores the urgent need for robust and generalizable detection methods. In this paper, we establish two key principles for AIGI detection through systematic analysis: \\textbf{(1) All Patches Matter:} Unlike conventional image classification where discriminative features concentrate on object-centric regions, each patch in AIGIs inherently contains synthetic artifacts due to the uniform generation process, suggesting that every patch serves as an important artifact source for detection. \\textbf{(2) More Patches Better}: Leveraging distributed artifacts across more patches improves detection robustness by capturing complementary forensic evidence and reducing over-reliance on specific patches, thereby enhancing robustness and generalization. However, our counterfactual analysis reveals an undesirable phenomenon: naively trained detectors often exhibit a \\textbf{Few-Patch Bias}, discriminating between real and synthetic images based on minority patches. We identify \\textbf{Lazy Learner} as the root cause: detectors preferentially learn conspicuous artifacts in limited patches while neglecting broader artifact distributions. To address this bias, we propose the \\textbf{P}anoptic \\textbf{P}atch \\textbf{L}earning (PPL) framework, involving: (1) Random Patch Replacement that randomly substitutes synthetic patches with real counterparts to compel models to identify artifacts in underutilized regions, encouraging the broader use of more patches; (2) Patch-wise Contrastive Learning that enforces consistent discriminative capability across all patches, ensuring uniform utilization of all patches. Extensive experiments across two different settings on several benchmarks verify the effectiveness of our approach.",
        "arxiv_id": "2504.01396",
        "ARXIVID": "2504.01396",
        "COMMENT": "Does not match any specific criterion but focuses on AI-generated image detection, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.01452": {
        "authors": [
            "Encheng Su",
            "Hu Cao",
            "Alois Knoll"
        ],
        "title": "BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting Binary Segmentation in Segment Anything Models",
        "abstract": "arXiv:2504.01452v1 Announce Type: new  Abstract: Accurate segmentation of polyps and skin lesions is essential for diagnosing colorectal and skin cancers. While various segmentation methods for polyps and skin lesions using fully supervised deep learning techniques have been developed, the pixel-level annotation of medical images by doctors is both time-consuming and costly. Foundational vision models like the Segment Anything Model (SAM) have demonstrated superior performance; however, directly applying SAM to medical segmentation may not yield satisfactory results due to the lack of domain-specific medical knowledge. In this paper, we propose BiSeg-SAM, a SAM-guided weakly supervised prompting and boundary refinement network for the segmentation of polyps and skin lesions. Specifically, we fine-tune SAM combined with a CNN module to learn local features. We introduce a WeakBox with two functions: automatically generating box prompts for the SAM model and using our proposed Multi-choice Mask-to-Box (MM2B) transformation for rough mask-to-box conversion, addressing the mismatch between coarse labels and precise predictions. Additionally, we apply scale consistency (SC) loss for prediction scale alignment. Our DetailRefine module enhances boundary precision and segmentation accuracy by refining coarse predictions using a limited amount of ground truth labels. This comprehensive approach enables BiSeg-SAM to achieve excellent multi-task segmentation performance. Our method demonstrates significant superiority over state-of-the-art (SOTA) methods when tested on five polyp datasets and one skin cancer dataset.",
        "arxiv_id": "2504.01452",
        "ARXIVID": "2504.01452",
        "COMMENT": "Does not match any specific criterion but is related to vision foundation models and segmentation tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}