{
    "2502.04507": {
        "authors": [
            "Peiyuan Zhang",
            "Yongqi Chen",
            "Runlong Su",
            "Hangliang Ding",
            "Ion Stoica",
            "Zhenghong Liu",
            "Hao Zhang"
        ],
        "title": "Fast Video Generation with Sliding Tile Attention",
        "abstract": "arXiv:2502.04507v1 Announce Type: new  Abstract: Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art video generation, but suffer from prohibitive compute cost -- when generating just a 5-second 720P video, attention alone takes 800 out of 945 seconds of total inference time. This paper introduces sliding tile attention (STA) to address this challenge. STA leverages the observation that attention scores in pretrained video diffusion models predominantly concentrate within localized 3D windows. By sliding and attending over the local spatial-temporal region, STA eliminates redundancy from full attention. Unlike traditional token-wise sliding window attention (SWA), STA operates tile-by-tile with a novel hardware-aware sliding window design, preserving expressiveness while being hardware-efficient. With careful kernel-level optimizations, STA offers the first efficient 2D/3D sliding-window-like attention implementation, achieving 58.79% MFU. Precisely, STA accelerates attention by 2.8-17x over FlashAttention-2 (FA2) and 1.6-10x over FlashAttention-3 (FA3). On the leading video DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s without quality degradation, requiring no training. Enabling finetuning further lowers latency to 268s with only a 0.09% drop on VBench.",
        "arxiv_id": "2502.04507",
        "ARXIVID": "2502.04507",
        "COMMENT": "Matches criterion 4 as it introduces a novel attention mechanism for video generation, relevant to vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.04363": {
        "authors": [
            "Bosung Kim",
            "Kyuhwan Lee",
            "Isu Jeong",
            "Jungmin Cheon",
            "Yeojin Lee",
            "Seulki Lee"
        ],
        "title": "On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices",
        "abstract": "arXiv:2502.04363v1 Announce Type: new  Abstract: We present On-device Sora, a first pioneering solution for diffusion-based on-device text-to-video generation that operates efficiently on smartphone-grade devices. Building on Open-Sora, On-device Sora applies three novel techniques to address the challenges of diffusion-based text-to-video generation on computation- and memory-limited mobile devices. First, Linear Proportional Leap (LPL) reduces the excessive denoising steps required in video diffusion through an efficient leap-based approach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive token-processing computation in attention layers by merging consecutive tokens along the temporal dimension. Third, Concurrent Inference with Dynamic Loading (CI-DL) dynamically partitions large models into smaller blocks and loads them into memory for concurrent model inference, effectively addressing the challenges of limited device memory. We implement On-device Sora on the iPhone 15 Pro, and the experimental evaluations demonstrate that it is capable of generating high-quality videos on the device, comparable to those produced by Open-Sora running on high-end GPUs. These results show that On-device Sora enables efficient and high-quality video generation on resource-constrained mobile devices, expanding accessibility, ensuring user privacy, reducing dependence on cloud infrastructure, and lowering associated costs. We envision the proposed On-device Sora as a significant first step toward democratizing state-of-the-art generative technologies, enabling video generation capabilities on commodity mobile and embedded devices. The code implementation is publicly available at an GitHub repository: https://github.com/eai-lab/On-device-Sora.",
        "arxiv_id": "2502.04363",
        "ARXIVID": "2502.04363",
        "COMMENT": "Matches criterion 4 as it introduces a novel approach for diffusion-based text-to-video generation on mobile devices, relevant to vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.05165": {
        "authors": [
            "Gemma Canet Tarr\\'es",
            "Zhe Lin",
            "Zhifei Zhang",
            "He Zhang",
            "Andrew Gilbert",
            "John Collomosse",
            "Soo Ye Kim"
        ],
        "title": "Multitwine: Multi-Object Compositing with Text and Layout Control",
        "abstract": "arXiv:2502.05165v1 Announce Type: new  Abstract: We introduce the first generative model capable of simultaneous multi-object compositing, guided by both text and layout. Our model allows for the addition of multiple objects within a scene, capturing a range of interactions from simple positional relations (e.g., next to, in front of) to complex actions requiring reposing (e.g., hugging, playing guitar). When an interaction implies additional props, like `taking a selfie', our model autonomously generates these supporting objects. By jointly training for compositing and subject-driven generation, also known as customization, we achieve a more balanced integration of textual and visual inputs for text-driven object compositing. As a result, we obtain a versatile model with state-of-the-art performance in both tasks. We further present a data generation pipeline leveraging visual and language models to effortlessly synthesize multimodal, aligned training data.",
        "arxiv_id": "2502.05165",
        "ARXIVID": "2502.05165",
        "COMMENT": "Matches criterion 4 as it introduces a generative model for multi-object compositing with text and layout control, relevant to vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.04843": {
        "authors": [
            "Feifei Li",
            "Qi Song",
            "Chi Zhang",
            "Hui Shuai",
            "Rui Huang"
        ],
        "title": "PoI: Pixel of Interest for Novel View Synthesis Assisted Scene Coordinate Regression",
        "abstract": "arXiv:2502.04843v1 Announce Type: new  Abstract: The task of estimating camera poses can be enhanced through novel view synthesis techniques such as NeRF and Gaussian Splatting to increase the diversity and extension of training data. However, these techniques often produce rendered images with issues like blurring and ghosting, which compromise their reliability. These issues become particularly pronounced for Scene Coordinate Regression (SCR) methods, which estimate 3D coordinates at the pixel level. To mitigate the problems associated with unreliable rendered images, we introduce a novel filtering approach, which selectively extracts well-rendered pixels while discarding the inferior ones. This filter simultaneously measures the SCR model's real-time reprojection loss and gradient during training. Building on this filtering technique, we also develop a new strategy to improve scene coordinate regression using sparse inputs, drawing on successful applications of sparse input techniques in novel view synthesis. Our experimental results validate the effectiveness of our method, demonstrating state-of-the-art performance on indoor and outdoor datasets.",
        "arxiv_id": "2502.04843",
        "ARXIVID": "2502.04843",
        "COMMENT": "Matches criterion 1 as it introduces a novel filtering approach for scene coordinate regression, enhancing spatial understanding in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.05178": {
        "authors": [
            "Yue Zhao",
            "Fuzhao Xue",
            "Scott Reed",
            "Linxi Fan",
            "Yuke Zhu",
            "Jan Kautz",
            "Zhiding Yu",
            "Philipp Kr\\\"ahenb\\\"uhl",
            "De-An Huang"
        ],
        "title": "QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation",
        "abstract": "arXiv:2502.05178v1 Announce Type: new  Abstract: We introduce Quantized Language-Image Pretraining (QLIP), a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding. QLIP trains a binary-spherical-quantization-based autoencoder with reconstruction and language-image alignment objectives. We are the first to show that the two objectives do not need to be at odds. We balance the two loss terms dynamically during training and show that a two-stage training pipeline effectively mixes the large-batch requirements of image-language pre-training with the memory bottleneck imposed by the reconstruction objective. We validate the effectiveness of QLIP for multimodal understanding and text-conditioned image generation with a single model. Specifically, QLIP serves as a drop-in replacement for the visual encoder for LLaVA and the image tokenizer for LlamaGen with comparable or even better performance. Finally, we demonstrate that QLIP enables a unified mixed-modality auto-regressive model for understanding and generation.",
        "arxiv_id": "2502.05178",
        "ARXIVID": "2502.05178",
        "COMMENT": "Matches criterion 2 as it introduces QLIP, a visual tokenization method for multimodal understanding and generation, relevant to advancements in MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.04896": {
        "authors": [
            "Shoufa Chen",
            "Chongjian Ge",
            "Yuqi Zhang",
            "Yida Zhang",
            "Fengda Zhu",
            "Hao Yang",
            "Hongxiang Hao",
            "Hui Wu",
            "Zhichao Lai",
            "Yifei Hu",
            "Ting-Che Lin",
            "Shilong Zhang",
            "Fu Li",
            "Chuan Li",
            "Xing Wang",
            "Yanghua Peng",
            "Peize Sun",
            "Ping Luo",
            "Yi Jiang",
            "Zehuan Yuan",
            "Bingyue Peng",
            "Xiaobing Liu"
        ],
        "title": "Goku: Flow Based Video Generative Foundation Models",
        "abstract": "arXiv:2502.04896v1 Announce Type: new  Abstract: This paper introduces Goku, a state-of-the-art family of joint image-and-video generation models leveraging rectified flow Transformers to achieve industry-leading performance. We detail the foundational elements enabling high-quality visual generation, including the data curation pipeline, model architecture design, flow formulation, and advanced infrastructure for efficient and robust large-scale training. The Goku models demonstrate superior performance in both qualitative and quantitative evaluations, setting new benchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and 83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for text-to-video tasks. We believe that this work provides valuable insights and practical advancements for the research community in developing joint image-and-video generation models.",
        "arxiv_id": "2502.04896",
        "ARXIVID": "2502.04896",
        "COMMENT": "Matches criterion 4 as it introduces a new visual foundation model for joint image-and-video generation with state-of-the-art performance.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.04412": {
        "authors": [
            "Ziyi Dong",
            "Yao Xiao",
            "Pengxu Wei",
            "Liang Lin"
        ],
        "title": "Decoder-Only LLMs are Better Controllers for Diffusion Models",
        "abstract": "arXiv:2502.04412v1 Announce Type: new  Abstract: Groundbreaking advancements in text-to-image generation have recently been achieved with the emergence of diffusion models. These models exhibit a remarkable ability to generate highly artistic and intricately detailed images based on textual prompts. However, obtaining desired generation outcomes often necessitates repetitive trials of manipulating text prompts just like casting spells on a magic mirror, and the reason behind that is the limited capability of semantic understanding inherent in current image generation models. Specifically, existing diffusion models encode the text prompt input with a pre-trained encoder structure, which is usually trained on a limited number of image-caption pairs. The state-of-the-art large language models (LLMs) based on the decoder-only structure have shown a powerful semantic understanding capability as their architectures are more suitable for training on very large-scale unlabeled data. In this work, we propose to enhance text-to-image diffusion models by borrowing the strength of semantic understanding from large language models, and devise a simple yet effective adapter to allow the diffusion models to be compatible with the decoder-only structure. Meanwhile, we also provide a supporting theoretical analysis with various architectures (e.g., encoder-only, encoder-decoder, and decoder-only), and conduct extensive empirical evaluations to verify its effectiveness. The experimental results show that the enhanced models with our adapter module are superior to the stat-of-the-art models in terms of text-to-image generation quality and reliability.",
        "arxiv_id": "2502.04412",
        "ARXIVID": "2502.04412",
        "COMMENT": "Matches criterion 2 as it enhances text-to-image diffusion models using decoder-only LLMs, aligning with advancements in multi-modal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2502.04371": {
        "authors": [
            "Zining Zhu",
            "Liang Zhao",
            "Kangheng Lin",
            "Jinze Yang",
            "En Yu",
            "Chenglong Liu",
            "Haoran Wei",
            "Jianjian Sun",
            "Zheng Ge",
            "Xiangyu Zhang"
        ],
        "title": "PerPO: Perceptual Preference Optimization via Discriminative Rewarding",
        "abstract": "arXiv:2502.04371v1 Announce Type: new  Abstract: This paper presents Perceptual Preference Optimization (PerPO), a perception alignment method aimed at addressing the visual discrimination challenges in generative pre-trained multimodal large language models (MLLMs). To align MLLMs with human visual perception process, PerPO employs discriminative rewarding to gather diverse negative samples, followed by listwise preference optimization to rank them.By utilizing the reward as a quantitative margin for ranking, our method effectively bridges generative preference optimization and discriminative empirical risk minimization. PerPO significantly enhances MLLMs' visual discrimination capabilities while maintaining their generative strengths, mitigates image-unconditional reward hacking, and ensures consistent performance across visual tasks. This work marks a crucial step towards more perceptually aligned and versatile MLLMs. We also hope that PerPO will encourage the community to rethink MLLM alignment strategies.",
        "arxiv_id": "2502.04371",
        "ARXIVID": "2502.04371",
        "COMMENT": "Matches criterion 2 as it proposes a novel method to enhance MLLMs' visual discrimination capabilities, aligning with advancements in multi-modal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2502.04804": {
        "authors": [
            "Mingxuan Yan",
            "Ruijie Zhang",
            "Xuedou Xiao",
            "Wei Wang"
        ],
        "title": "DetVPCC: RoI-based Point Cloud Sequence Compression for 3D Object Detection",
        "abstract": "arXiv:2502.04804v1 Announce Type: new  Abstract: While MPEG-standardized video-based point cloud compression (VPCC) achieves high compression efficiency for human perception, it struggles with a poor trade-off between bitrate savings and detection accuracy when supporting 3D object detectors. This limitation stems from VPCC's inability to prioritize regions of different importance within point clouds. To address this issue, we propose DetVPCC, a novel method integrating region-of-interest (RoI) encoding with VPCC for efficient point cloud sequence compression while preserving the 3D object detection accuracy. Specifically, we augment VPCC to support RoI-based compression by assigning spatially non-uniform quality levels. Then, we introduce a lightweight RoI detector to identify crucial regions that potentially contain objects. Experiments on the nuScenes dataset demonstrate that our approach significantly improves the detection accuracy. The code and demo video are available in supplementary materials.",
        "arxiv_id": "2502.04804",
        "ARXIVID": "2502.04804",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for point cloud sequence compression, focusing on 3D object detection, which is relevant to embodied AI benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.05066": {
        "authors": [
            "Aditya Kumar",
            "Tom Blanchard",
            "Adam Dziedzic",
            "Franziska Boenisch"
        ],
        "title": "Beautiful Images, Toxic Words: Understanding and Addressing Offensive Text in Generated Images",
        "abstract": "arXiv:2502.05066v1 Announce Type: new  Abstract: State-of-the-art visual generation models, such as Diffusion Models (DMs) and Vision Auto-Regressive Models (VARs), produce highly realistic images. While prior work has successfully mitigated Not Safe For Work (NSFW) content in the visual domain, we identify a novel threat: the generation of NSFW text embedded within images. This includes offensive language, such as insults, racial slurs, and sexually explicit terms, posing significant risks to users. We show that all state-of-the-art DMs (e.g., SD3, Flux, DeepFloyd IF) and VARs (e.g., Infinity) are vulnerable to this issue. Through extensive experiments, we demonstrate that existing mitigation techniques, effective for visual content, fail to prevent harmful text generation while substantially degrading benign text generation. As an initial step toward addressing this threat, we explore safety fine-tuning of the text encoder underlying major DM architectures using a customized dataset. Thereby, we suppress NSFW generation while preserving overall image and text generation quality. Finally, to advance research in this area, we introduce ToxicBench, an open-source benchmark for evaluating NSFW text generation in images. ToxicBench provides a curated dataset of harmful prompts, new metrics, and an evaluation pipeline assessing both NSFW-ness and generation quality. Our benchmark aims to guide future efforts in mitigating NSFW text generation in text-to-image models and is available at https://github.com/sprintml/ToxicBench",
        "arxiv_id": "2502.05066",
        "ARXIVID": "2502.05066",
        "COMMENT": "Matches criterion 4 as it addresses issues in vision foundation models and introduces a benchmark for evaluating NSFW text generation in images.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.04377": {
        "authors": [
            "Xiaoshuai Hao",
            "Yunfeng Diao",
            "Mengchuan Wei",
            "Yifan Yang",
            "Peng Hao",
            "Rong Yin",
            "Hui Zhang",
            "Weiming Li",
            "Shu Zhao",
            "Yu Liu"
        ],
        "title": "MapFusion: A Novel BEV Feature Fusion Network for Multi-modal Map Construction",
        "abstract": "arXiv:2502.04377v1 Announce Type: new  Abstract: Map construction task plays a vital role in providing precise and comprehensive static environmental information essential for autonomous driving systems. Primary sensors include cameras and LiDAR, with configurations varying between camera-only, LiDAR-only, or camera-LiDAR fusion, based on cost-performance considerations. While fusion-based methods typically perform best, existing approaches often neglect modality interaction and rely on simple fusion strategies, which suffer from the problems of misalignment and information loss. To address these issues, we propose MapFusion, a novel multi-modal Bird's-Eye View (BEV) feature fusion method for map construction. Specifically, to solve the semantic misalignment problem between camera and LiDAR BEV features, we introduce the Cross-modal Interaction Transform (CIT) module, enabling interaction between two BEV feature spaces and enhancing feature representation through a self-attention mechanism. Additionally, we propose an effective Dual Dynamic Fusion (DDF) module to adaptively select valuable information from different modalities, which can take full advantage of the inherent information between different modalities. Moreover, MapFusion is designed to be simple and plug-and-play, easily integrated into existing pipelines. We evaluate MapFusion on two map construction tasks, including High-definition (HD) map and BEV map segmentation, to show its versatility and effectiveness. Compared with the state-of-the-art methods, MapFusion achieves 3.6% and 6.2% absolute improvements on the HD map construction and BEV map segmentation tasks on the nuScenes dataset, respectively, demonstrating the superiority of our approach.",
        "arxiv_id": "2502.04377",
        "ARXIVID": "2502.04377",
        "COMMENT": "Matches criterion 3 as it introduces a novel BEV feature fusion network for multi-modal map construction, addressing overlooked issues like semantic misalignment.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.04483": {
        "authors": [
            "Nathan Louis",
            "Mahzad Khoshlessan",
            "Jason J. Corso"
        ],
        "title": "Measuring Physical Plausibility of 3D Human Poses Using Physics Simulation",
        "abstract": "arXiv:2502.04483v1 Announce Type: new  Abstract: Modeling humans in physical scenes is vital for understanding human-environment interactions for applications involving augmented reality or assessment of human actions from video (e.g. sports or physical rehabilitation). State-of-the-art literature begins with a 3D human pose, from monocular or multiple views, and uses this representation to ground the person within a 3D world space. While standard metrics for accuracy capture joint position errors, they do not consider physical plausibility of the 3D pose. This limitation has motivated researchers to propose other metrics evaluating jitter, floor penetration, and unbalanced postures. Yet, these approaches measure independent instances of errors and are not representative of balance or stability during motion. In this work, we propose measuring physical plausibility from within physics simulation. We introduce two metrics to capture the physical plausibility and stability of predicted 3D poses from any 3D Human Pose Estimation model. Using physics simulation, we discover correlations with existing plausibility metrics and measuring stability during motion. We evaluate and compare the performances of two state-of-the-art methods, a multi-view triangulated baseline, and ground truth 3D markers from the Human3.6m dataset.",
        "arxiv_id": "2502.04483",
        "ARXIVID": "2502.04483",
        "COMMENT": "Matches criterion 1 as it introduces new metrics for physical plausibility of 3D human poses using physics simulation, which is relevant to spatial understanding in embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.05173": {
        "authors": [
            "Xilin Wei",
            "Xiaoran Liu",
            "Yuhang Zang",
            "Xiaoyi Dong",
            "Pan Zhang",
            "Yuhang Cao",
            "Jian Tong",
            "Haodong Duan",
            "Qipeng Guo",
            "Jiaqi Wang",
            "Xipeng Qiu",
            "Dahua Lin"
        ],
        "title": "VideoRoPE: What Makes for Good Video Rotary Position Embedding?",
        "abstract": "arXiv:2502.05173v1 Announce Type: new  Abstract: While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge. This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work. As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH. The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors. Based on our analysis, we introduce \\textbf{VideoRoPE}, with a \\textit{3D structure} designed to preserve spatio-temporal relationships. VideoRoPE features \\textit{low-frequency temporal allocation} to mitigate periodic oscillations, a \\textit{diagonal layout} to maintain spatial symmetry, and \\textit{adjustable temporal spacing} to decouple temporal and spatial indexing. VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination. Our code will be available at \\href{https://github.com/Wiselnn570/VideoRoPE}{https://github.com/Wiselnn570/VideoRoPE}.",
        "arxiv_id": "2502.05173",
        "ARXIVID": "2502.05173",
        "COMMENT": "This paper matches criterion 4 as it focuses on a novel adaptation of Rotary Position Embedding (RoPE) for video understanding, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2502.05153": {
        "authors": [
            "Minh-Quan Le",
            "Gaurav Mittal",
            "Tianjian Meng",
            "A S M Iftekhar",
            "Vishwas Suryanarayanan",
            "Barun Patra",
            "Dimitris Samaras",
            "Mei Chen"
        ],
        "title": "Hummingbird: High Fidelity Image Generation via Multimodal Context Alignment",
        "abstract": "arXiv:2502.05153v1 Announce Type: new  Abstract: While diffusion models are powerful in generating high-quality, diverse synthetic data for object-centric tasks, existing methods struggle with scene-aware tasks such as Visual Question Answering (VQA) and Human-Object Interaction (HOI) Reasoning, where it is critical to preserve scene attributes in generated images consistent with a multimodal context, i.e. a reference image with accompanying text guidance query. To address this, we introduce Hummingbird, the first diffusion-based image generator which, given a multimodal context, generates highly diverse images w.r.t. the reference image while ensuring high fidelity by accurately preserving scene attributes, such as object interactions and spatial relationships from the text guidance. Hummingbird employs a novel Multimodal Context Evaluator that simultaneously optimizes our formulated Global Semantic and Fine-grained Consistency Rewards to ensure generated images preserve the scene attributes of reference images in relation to the text guidance while maintaining diversity. As the first model to address the task of maintaining both diversity and fidelity given a multimodal context, we introduce a new benchmark formulation incorporating MME Perception and Bongard HOI datasets. Benchmark experiments show Hummingbird outperforms all existing methods by achieving superior fidelity while maintaining diversity, validating Hummingbird's potential as a robust multimodal context-aligned image generator in complex visual tasks.",
        "arxiv_id": "2502.05153",
        "ARXIVID": "2502.05153",
        "COMMENT": "Matches criterion 2 as it introduces a new multimodal context-aligned image generator (Hummingbird) for scene-aware tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.04623": {
        "authors": [
            "Mengting Ma",
            "Yizhen Jiang",
            "Mengjiao Zhao",
            "Jiaxin Li",
            "Wei Zhang"
        ],
        "title": "HetSSNet: Spatial-Spectral Heterogeneous Graph Learning Network for Panchromatic and Multispectral Images Fusion",
        "abstract": "arXiv:2502.04623v1 Announce Type: new  Abstract: Remote sensing pansharpening aims to reconstruct spatial-spectral properties during the fusion of panchromatic (PAN) images and low-resolution multi-spectral (LR-MS) images, finally generating the high-resolution multi-spectral (HR-MS) images. In the mainstream modeling strategies, i.e., CNN and Transformer, the input images are treated as the equal-sized grid of pixels in the Euclidean space. They have limitations in facing remote sensing images with irregular ground objects. Graph is the more flexible structure, however, there are two major challenges when modeling spatial-spectral properties with graph: \\emph{1) constructing the customized graph structure for spatial-spectral relationship priors}; \\emph{2) learning the unified spatial-spectral representation through the graph}. To address these challenges, we propose the spatial-spectral heterogeneous graph learning network, named \\textbf{HetSSNet}. Specifically, HetSSNet initially constructs the heterogeneous graph structure for pansharpening, which explicitly describes pansharpening-specific relationships. Subsequently, the basic relationship pattern generation module is designed to extract the multiple relationship patterns from the heterogeneous graph. Finally, relationship pattern aggregation module is exploited to collaboratively learn unified spatial-spectral representation across different relationships among nodes with adaptive importance learning from local and global perspectives. Extensive experiments demonstrate the significant superiority and generalization of HetSSNet.",
        "arxiv_id": "2502.04623",
        "ARXIVID": "2502.04623",
        "COMMENT": "Matches criterion 1 as it introduces a novel graph-based method for spatial-spectral understanding in remote sensing.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.05091": {
        "authors": [
            "Gorkem Can Ates",
            "Kuang Gong",
            "Wei Shao"
        ],
        "title": "DCFormer: Efficient 3D Vision-Language Modeling with Decomposed Convolutions",
        "abstract": "arXiv:2502.05091v1 Announce Type: new  Abstract: Vision-language models (VLMs) align visual and textual representations, enabling high-performance zero-shot classification and image-text retrieval in 2D medical imaging. However, extending VLMs to 3D medical imaging remains computationally challenging. Existing 3D VLMs rely on Vision Transformers (ViTs), which are computationally expensive due to self-attention's quadratic complexity, or 3D convolutions, which demand excessive parameters and FLOPs as kernel size increases. We introduce DCFormer, an efficient 3D medical image encoder that factorizes 3D convolutions into three parallel 1D convolutions along depth, height, and width. This design preserves spatial information while significantly reducing computational cost. Integrated into a CLIP-based vision-language framework, DCFormer is evaluated on CT-RATE, a dataset of 50,188 paired 3D chest CT volumes and radiology reports, for zero-shot multi-abnormality detection across 18 pathologies. Compared to ViT, ConvNeXt, PoolFormer, and TransUNet, DCFormer achieves superior efficiency and accuracy, with DCFormer-Tiny reaching 62.0% accuracy and a 46.3% F1-score while using significantly fewer parameters. These results highlight DCFormer's potential for scalable, clinically deployable 3D medical VLMs. Our codes will be publicly available.",
        "arxiv_id": "2502.05091",
        "ARXIVID": "2502.05091",
        "COMMENT": "Matches criterion 2 as it introduces a new vision-language model (DCFormer) for 3D medical imaging.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.04847": {
        "authors": [
            "Qijun Gan",
            "Yi Ren",
            "Chen Zhang",
            "Zhenhui Ye",
            "Pan Xie",
            "Xiang Yin",
            "Zehuan Yuan",
            "Bingyue Peng",
            "Jianke Zhu"
        ],
        "title": "HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation",
        "abstract": "arXiv:2502.04847v1 Announce Type: new  Abstract: Human motion video generation has advanced significantly, while existing methods still struggle with accurately rendering detailed body parts like hands and faces, especially in long sequences and intricate motions. Current approaches also rely on fixed resolution and struggle to maintain visual consistency. To address these limitations, we propose HumanDiT, a pose-guided Diffusion Transformer (DiT)-based framework trained on a large and wild dataset containing 14,000 hours of high-quality video to produce high-fidelity videos with fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT, supports numerous video resolutions and variable sequence lengths, facilitating learning for long-sequence video generation; (ii) we introduce a prefix-latent reference strategy to maintain personalized characteristics across extended sequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to generate subsequent pose sequences, facilitating video continuation from static images or existing videos. It also utilizes a Pose Adapter to enable pose transfer with given sequences. Extensive experiments demonstrate its superior performance in generating long-form, pose-accurate videos across diverse scenarios.",
        "arxiv_id": "2502.04847",
        "ARXIVID": "2502.04847",
        "COMMENT": "Matches criterion 4 as it focuses on a vision foundation model (Diffusion Transformer) and its application to human motion video generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.04415": {
        "authors": [
            "Sergios-Anestis Kefalidis",
            "Konstantinos Plas",
            "Manolis Koubarakis"
        ],
        "title": "TerraQ: Spatiotemporal Question-Answering on Satellite Image Archives",
        "abstract": "arXiv:2502.04415v1 Announce Type: new  Abstract: TerraQ is a spatiotemporal question-answering engine for satellite image archives. It is a natural language processing system that is built to process requests for satellite images satisfying certain criteria. The requests can refer to image metadata and entities from a specialized knowledge base (e.g., the Emilia-Romagna region). With it, users can make requests like \"Give me a hundred images of rivers near ports in France, with less than 20% snow coverage and more than 10% cloud coverage\", thus making Earth Observation data more easily accessible, in-line with the current landscape of digital assistants.",
        "arxiv_id": "2502.04415",
        "ARXIVID": "2502.04415",
        "COMMENT": "Matches criterion 1 as it focuses on spatiotemporal understanding through a question-answering system for satellite image archives.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2502.04391": {
        "authors": [
            "Sophia J. Abraham",
            "Jonathan D. Hauenstein",
            "Walter J. Scheirer"
        ],
        "title": "Towards Fair and Robust Face Parsing for Generative AI: A Multi-Objective Approach",
        "abstract": "arXiv:2502.04391v1 Announce Type: new  Abstract: Face parsing is a fundamental task in computer vision, enabling applications such as identity verification, facial editing, and controllable image synthesis. However, existing face parsing models often lack fairness and robustness, leading to biased segmentation across demographic groups and errors under occlusions, noise, and domain shifts. These limitations affect downstream face synthesis, where segmentation biases can degrade generative model outputs. We propose a multi-objective learning framework that optimizes accuracy, fairness, and robustness in face parsing. Our approach introduces a homotopy-based loss function that dynamically adjusts the importance of these objectives during training. To evaluate its impact, we compare multi-objective and single-objective U-Net models in a GAN-based face synthesis pipeline (Pix2PixHD). Our results show that fairness-aware and robust segmentation improves photorealism and consistency in face generation. Additionally, we conduct preliminary experiments using ControlNet, a structured conditioning model for diffusion-based synthesis, to explore how segmentation quality influences guided image generation. Our findings demonstrate that multi-objective face parsing improves demographic consistency and robustness, leading to higher-quality GAN-based synthesis.",
        "arxiv_id": "2502.04391",
        "ARXIVID": "2502.04391",
        "COMMENT": "Matches criterion 4 as it discusses fairness and robustness in face parsing, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2502.05179": {
        "authors": [
            "Shilong Zhang",
            "Wenbo Li",
            "Shoufa Chen",
            "Chongjian Ge",
            "Peize Sun",
            "Yida Zhang",
            "Yi Jiang",
            "Zehuan Yuan",
            "Binyue Peng",
            "Ping Luo"
        ],
        "title": "FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation",
        "abstract": "arXiv:2502.05179v1 Announce Type: new  Abstract: DiT diffusion models have achieved great success in text-to-video generation, leveraging their scalability in model capacity and data scale. High content and motion fidelity aligned with text prompts, however, often require large model parameters and a substantial number of function evaluations (NFEs). Realistic and visually appealing details are typically reflected in high resolution outputs, further amplifying computational demands especially for single stage DiT models. To address these challenges, we propose a novel two stage framework, FlashVideo, which strategically allocates model capacity and NFEs across stages to balance generation fidelity and quality. In the first stage, prompt fidelity is prioritized through a low resolution generation process utilizing large parameters and sufficient NFEs to enhance computational efficiency. The second stage establishes flow matching between low and high resolutions, effectively generating fine details with minimal NFEs. Quantitative and visual results demonstrate that FlashVideo achieves state-of-the-art high resolution video generation with superior computational efficiency. Additionally, the two-stage design enables users to preview the initial output before committing to full resolution generation, thereby significantly reducing computational costs and wait times as well as enhancing commercial viability .",
        "arxiv_id": "2502.05179",
        "ARXIVID": "2502.05179",
        "COMMENT": "This paper does not directly match any of the criteria but is tangentially related to generative modeling in multi-modal learning (criterion 2).",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.04719": {
        "authors": [
            "Jun Dai",
            "Liqun Chen",
            "Xinge Yang",
            "Yuyao Hu",
            "Jinwei Gu",
            "Tianfan Xue"
        ],
        "title": "Tolerance-Aware Deep Optics",
        "abstract": "arXiv:2502.04719v1 Announce Type: new  Abstract: Deep optics has emerged as a promising approach by co-designing optical elements with deep learning algorithms. However, current research typically overlooks the analysis and optimization of manufacturing and assembly tolerances. This oversight creates a significant performance gap between designed and fabricated optical systems. To address this challenge, we present the first end-to-end tolerance-aware optimization framework that incorporates multiple tolerance types into the deep optics design pipeline. Our method combines physics-informed modelling with data-driven training to enhance optical design by accounting for and compensating for structural deviations in manufacturing and assembly. We validate our approach through computational imaging applications, demonstrating results in both simulations and real-world experiments. We further examine how our proposed solution improves the robustness of optical systems and vision algorithms against tolerances through qualitative and quantitative analyses. Code and additional visual results are available at openimaginglab.github.io/LensTolerance.",
        "arxiv_id": "2502.04719",
        "ARXIVID": "2502.04719",
        "COMMENT": "Does not match any specific criteria but is related to deep optics and computational imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.04870": {
        "authors": [
            "Xiao Yu",
            "Yan Fang",
            "Yao Zhao",
            "Yunchao Wei"
        ],
        "title": "IPSeg: Image Posterior Mitigates Semantic Drift in Class-Incremental Segmentation",
        "abstract": "arXiv:2502.04870v1 Announce Type: new  Abstract: Class incremental learning aims to enable models to learn from sequential, non-stationary data streams across different tasks without catastrophic forgetting. In class incremental semantic segmentation (CISS), the semantic content of image pixels evolves over incremental phases, known as semantic drift. In this work, we identify two critical challenges in CISS that contribute to semantic drift and degrade performance. First, we highlight the issue of separate optimization, where different parts of the model are optimized in distinct incremental stages, leading to misaligned probability scales. Second, we identify noisy semantics arising from inappropriate pseudo-labeling, which results in sub-optimal results. To address these challenges, we propose a novel and effective approach, Image Posterior and Semantics Decoupling for Segmentation (IPSeg). IPSeg introduces two key mechanisms: (1) leveraging image posterior probabilities to align optimization across stages and mitigate the effects of separate optimization, and (2) employing semantics decoupling to handle noisy semantics and tailor learning strategies for different semantics. Extensive experiments on the Pascal VOC 2012 and ADE20K datasets demonstrate that IPSeg achieves superior performance compared to state-of-the-art methods, particularly in challenging long-term incremental scenarios.",
        "arxiv_id": "2502.04870",
        "ARXIVID": "2502.04870",
        "COMMENT": "This paper does not match any of the criteria directly but is tangentially related to computer vision and machine learning through class incremental learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.04386": {
        "authors": [
            "Guangyao Zheng",
            "Michael A. Jacobs",
            "Vladimir Braverman",
            "Vishwa S. Parekh"
        ],
        "title": "Towards Fair Medical AI: Adversarial Debiasing of 3D CT Foundation Embeddings",
        "abstract": "arXiv:2502.04386v1 Announce Type: new  Abstract: Self-supervised learning has revolutionized medical imaging by enabling efficient and generalizable feature extraction from large-scale unlabeled datasets. Recently, self-supervised foundation models have been extended to three-dimensional (3D) computed tomography (CT) data, generating compact, information-rich embeddings with 1408 features that achieve state-of-the-art performance on downstream tasks such as intracranial hemorrhage detection and lung cancer risk forecasting. However, these embeddings have been shown to encode demographic information, such as age, sex, and race, which poses a significant risk to the fairness of clinical applications.   In this work, we propose a Variation Autoencoder (VAE) based adversarial debiasing framework to transform these embeddings into a new latent space where demographic information is no longer encoded, while maintaining the performance of critical downstream tasks. We validated our approach on the NLST lung cancer screening dataset, demonstrating that the debiased embeddings effectively eliminate multiple encoded demographic information and improve fairness without compromising predictive accuracy for lung cancer risk at 1-year and 2-year intervals. Additionally, our approach ensures the embeddings are robust against adversarial bias attacks. These results highlight the potential of adversarial debiasing techniques to ensure fairness and equity in clinical applications of self-supervised 3D CT embeddings, paving the way for their broader adoption in unbiased medical decision-making.",
        "arxiv_id": "2502.04386",
        "ARXIVID": "2502.04386",
        "COMMENT": "Does not match any specific criteria but is related to fairness in medical AI and self-supervised learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.04567": {
        "authors": [
            "Zhuotong Chen",
            "Fang Liu",
            "Xuan Zhu",
            "Yanjun Qi",
            "Mohammad Ghavamzadeh"
        ],
        "title": "Preference Optimization via Contrastive Divergence: Your Reward Model is Secretly an NLL Estimator",
        "abstract": "arXiv:2502.04567v1 Announce Type: new  Abstract: Existing studies on preference optimization (PO) have centered on constructing pairwise preference data following simple heuristics, such as maximizing the margin between preferred and dispreferred completions based on human (or AI) ranked scores. However, none of these heuristics has a full theoretical justification. In this work, we develop a novel PO framework that provides theoretical guidance to effectively sample dispreferred completions. To achieve this, we formulate PO as minimizing the negative log-likelihood (NLL) of a probability model and propose to estimate its normalization constant via a sampling strategy. As we will demonstrate, these estimative samples can act as dispreferred completions in PO. We then select contrastive divergence (CD) as the sampling strategy, and propose a novel MC-PO algorithm that applies the Monte Carlo (MC) kernel from CD to sample hard negatives w.r.t. the parameterized reward model. Finally, we propose the OnMC-PO algorithm, an extension of MC-PO to the online setting. On popular alignment benchmarks, MC-PO outperforms existing SOTA baselines, and OnMC-PO leads to further improvement.",
        "arxiv_id": "2502.04567",
        "ARXIVID": "2502.04567",
        "COMMENT": "Does not match any specific criteria but is related to preference optimization in machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.04946": {
        "authors": [
            "Craig Myles",
            "In Hwa Um",
            "Craig Marshall",
            "David Harris-Birtill",
            "David J. Harrison"
        ],
        "title": "SurGen: 1020 H&E-stained Whole Slide Images With Survival and Genetic Markers",
        "abstract": "arXiv:2502.04946v1 Announce Type: new  Abstract: $\\textbf{Background}$: Cancer remains one of the leading causes of morbidity and mortality worldwide. Comprehensive datasets that combine histopathological images with genetic and survival data across various tumour sites are essential for advancing computational pathology and personalised medicine. $\\textbf{Results}$: We present SurGen, a dataset comprising 1,020 H&E-stained whole slide images (WSIs) from 843 colorectal cancer cases. The dataset includes detailed annotations for key genetic mutations (KRAS, NRAS, BRAF) and mismatch repair status, as well as survival data for 426 cases. To demonstrate SurGen's practical utility, we conducted a proof-of-concept machine learning experiment predicting mismatch repair status from the WSIs, achieving a test AUROC of 0.8316. These preliminary results underscore the dataset's potential to facilitate research in biomarker discovery, prognostic modelling, and advanced machine learning applications in colorectal cancer. $\\textbf{Conclusions}$: SurGen offers a valuable resource for the scientific community, enabling studies that require high-quality WSIs linked with comprehensive clinical and genetic information on colorectal cancer. Our initial findings affirm the dataset's capacity to advance diagnostic precision and foster the development of personalised treatment strategies in colorectal oncology. Data available online at https://doi.org/10.6019/S-BIAD1285.",
        "arxiv_id": "2502.04946",
        "ARXIVID": "2502.04946",
        "COMMENT": "Does not match any specific criteria but is related to machine learning applications in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.04675": {
        "authors": [
            "Xueru Wen",
            "Jie Lou",
            "Xinyu Lu",
            "Junjie Yang",
            "Yanjiang Liu",
            "Yaojie Lu",
            "Debing Zhang",
            "XingYu"
        ],
        "title": "Scalable Oversight for Superhuman AI via Recursive Self-Critiquing",
        "abstract": "arXiv:2502.04675v1 Announce Type: new  Abstract: As AI capabilities increasingly surpass human proficiency in complex tasks, current alignment techniques including SFT and RLHF face fundamental challenges in ensuring reliable oversight. These methods rely on direct human assessment and become untenable when AI outputs exceed human cognitive thresholds. In response to this challenge, we explore two hypotheses: (1) critique of critique can be easier than critique itself, extending the widely-accepted observation that verification is easier than generation to the critique domain, as critique itself is a specialized form of generation; (2) this difficulty relationship is recursively held, suggesting that when direct evaluation is infeasible, performing high-order critiques (e.g., critique of critique of critique) offers a more tractable supervision pathway. To examine these hypotheses, we perform Human-Human, Human-AI, and AI-AI experiments across multiple tasks. Our results demonstrate encouraging evidence supporting these hypotheses and suggest that recursive self-critiquing is a promising direction for scalable oversight.",
        "arxiv_id": "2502.04675",
        "ARXIVID": "2502.04675",
        "COMMENT": "Does not match any specific criterion but explores an interesting oversight mechanism for AI, which is tangentially related to your friend's interest in clever statistical tricks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.04364": {
        "authors": [
            "Wenhao You",
            "Bryan Hooi",
            "Yiwei Wang",
            "Euijin Choo",
            "Ming-Hsuan Yang",
            "Junsong Yuan",
            "Zi Huang",
            "Yujun Cai"
        ],
        "title": "Lost in Edits? A $\\lambda$-Compass for AIGC Provenance",
        "abstract": "arXiv:2502.04364v1 Announce Type: new  Abstract: Recent advancements in diffusion models have driven the growth of text-guided image editing tools, enabling precise and iterative modifications of synthesized content. However, as these tools become increasingly accessible, they also introduce significant risks of misuse, emphasizing the critical need for robust attribution methods to ensure content authenticity and traceability. Despite the creative potential of such tools, they pose significant challenges for attribution, particularly in adversarial settings where edits can be layered to obscure an image's origins. We propose LambdaTracer, a novel latent-space attribution method that robustly identifies and differentiates authentic outputs from manipulated ones without requiring any modifications to generative or editing pipelines. By adaptively calibrating reconstruction losses, LambdaTracer remains effective across diverse iterative editing processes, whether automated through text-guided editing tools such as InstructPix2Pix and ControlNet or performed manually with editing software such as Adobe Photoshop. Extensive experiments reveal that our method consistently outperforms baseline approaches in distinguishing maliciously edited images, providing a practical solution to safeguard ownership, creativity, and credibility in the open, fast-evolving AI ecosystems.",
        "arxiv_id": "2502.04364",
        "ARXIVID": "2502.04364",
        "COMMENT": "Does not match any specific criterion but is relevant to diffusion models and image editing, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.04725": {
        "authors": [
            "Yujin Han",
            "Andi Han",
            "Wei Huang",
            "Chaochao Lu",
            "Difan Zou"
        ],
        "title": "Can Diffusion Models Learn Hidden Inter-Feature Rules Behind Images?",
        "abstract": "arXiv:2502.04725v1 Announce Type: new  Abstract: Despite the remarkable success of diffusion models (DMs) in data generation, they exhibit specific failure cases with unsatisfactory outputs. We focus on one such limitation: the ability of DMs to learn hidden rules between image features. Specifically, for image data with dependent features ($\\mathbf{x}$) and ($\\mathbf{y}$) (e.g., the height of the sun ($\\mathbf{x}$) and the length of the shadow ($\\mathbf{y}$)), we investigate whether DMs can accurately capture the inter-feature rule ($p(\\mathbf{y}|\\mathbf{x})$). Empirical evaluations on mainstream DMs (e.g., Stable Diffusion 3.5) reveal consistent failures, such as inconsistent lighting-shadow relationships and mismatched object-mirror reflections. Inspired by these findings, we design four synthetic tasks with strongly correlated features to assess DMs' rule-learning abilities. Extensive experiments show that while DMs can identify coarse-grained rules, they struggle with fine-grained ones. Our theoretical analysis demonstrates that DMs trained via denoising score matching (DSM) exhibit constant errors in learning hidden rules, as the DSM objective is not compatible with rule conformity. To mitigate this, we introduce a common technique - incorporating additional classifier guidance during sampling, which achieves (limited) improvements. Our analysis reveals that the subtle signals of fine-grained rules are challenging for the classifier to capture, providing insights for future exploration.",
        "arxiv_id": "2502.04725",
        "ARXIVID": "2502.04725",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and diffusion models, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.05147": {
        "authors": [
            "Zhengjian Kang",
            "Ye Zhang",
            "Xiaoyu Deng",
            "Xintao Li",
            "Yongzhe Zhang"
        ],
        "title": "LP-DETR: Layer-wise Progressive Relations for Object Detection",
        "abstract": "arXiv:2502.05147v1 Announce Type: new  Abstract: This paper presents LP-DETR (Layer-wise Progressive DETR), a novel approach that enhances DETR-based object detection through multi-scale relation modeling. Our method introduces learnable spatial relationships between object queries through a relation-aware self-attention mechanism, which adaptively learns to balance different scales of relations (local, medium and global) across decoder layers. This progressive design enables the model to effectively capture evolving spatial dependencies throughout the detection pipeline. Extensive experiments on COCO 2017 dataset demonstrate that our method improves both convergence speed and detection accuracy compared to standard self-attention module. The proposed method achieves competitive results, reaching 52.3\\% AP with 12 epochs and 52.5\\% AP with 24 epochs using ResNet-50 backbone, and further improving to 58.0\\% AP with Swin-L backbone. Furthermore, our analysis reveals an interesting pattern: the model naturally learns to prioritize local spatial relations in early decoder layers while gradually shifting attention to broader contexts in deeper layers, providing valuable insights for future research in object detection.",
        "arxiv_id": "2502.05147",
        "ARXIVID": "2502.05147",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and object detection, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}