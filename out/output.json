{
    "2601.02204": {
        "authors": [
            "Huichao Zhang",
            "Liao Qu",
            "Yiheng Liu",
            "Hang Chen",
            "Yangyang Song",
            "Yongsheng Dong",
            "Shikun Sun",
            "Xian Li",
            "Xu Wang",
            "Yi Jiang",
            "Hu Ye",
            "Bo Chen",
            "Yiming Gao",
            "Peng Liu",
            "Akide Liu",
            "Zhipeng Yang",
            "Qili Deng",
            "Linjie Xing",
            "Jiyang Liu",
            "Zhao Wang",
            "Yang Zhou",
            "Mingcong Liu",
            "Yi Zhang",
            "Qian He",
            "Xiwei Hu",
            "Zhongqi Qi",
            "Jie Shao",
            "Zhiye Fu",
            "Shuai Wang",
            "Fangmin Chen",
            "Xuezhi Chai",
            "Zhihua Wu",
            "Yitong Wang",
            "Zehuan Yuan",
            "Daniel K. Du",
            "Xinglong Wu"
        ],
        "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
        "abstract": "arXiv:2601.02204v1 Announce Type: new  Abstract: We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.",
        "arxiv_id": "2601.02204",
        "ARXIVID": "2601.02204",
        "COMMENT": "Matches criterion 2: Unified Diffusion Models for multiple vision tasks under a single architecture.",
        "RELEVANCE": 5,
        "NOVELTY": 8
    },
    "2601.02358": {
        "authors": [
            "Junyi Chen",
            "Tong He",
            "Zhoujie Fu",
            "Pengfei Wan",
            "Kun Gai",
            "Weicai Ye"
        ],
        "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
        "abstract": "arXiv:2601.02358v1 Announce Type: new  Abstract: We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.",
        "arxiv_id": "2601.02358",
        "ARXIVID": "2601.02358",
        "COMMENT": "Matches criterion 2: Unified Diffusion Models for multiple vision tasks under a single architecture.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2601.01222": {
        "authors": [
            "Mengfei Li",
            "Peng Li",
            "Zheng Zhang",
            "Jiahao Lu",
            "Chengfeng Zhao",
            "Wei Xue",
            "Qifeng Liu",
            "Sida Peng",
            "Wenxiao Zhang",
            "Wenhan Luo",
            "Yuan Liu",
            "Yike Guo"
        ],
        "title": "UniSH: Unifying Scene and Human Reconstruction in a Feed-Forward Pass",
        "abstract": "arXiv:2601.01222v1 Announce Type: new  Abstract: We present UniSH, a unified, feed-forward framework for joint metric-scale 3D scene and human reconstruction. A key challenge in this domain is the scarcity of large-scale, annotated real-world data, forcing a reliance on synthetic datasets. This reliance introduces a significant sim-to-real domain gap, leading to poor generalization, low-fidelity human geometry, and poor alignment on in-the-wild videos. To address this, we propose an innovative training paradigm that effectively leverages unlabeled in-the-wild data. Our framework bridges strong, disparate priors from scene reconstruction and HMR, and is trained with two core components: (1) a robust distillation strategy to refine human surface details by distilling high-frequency details from an expert depth model, and (2) a two-stage supervision scheme, which first learns coarse localization on synthetic data, then fine-tunes on real data by directly optimizing the geometric correspondence between the SMPL mesh and the human point cloud. This approach enables our feed-forward model to jointly recover high-fidelity scene geometry, human point clouds, camera parameters, and coherent, metric-scale SMPL bodies, all in a single forward pass. Extensive experiments demonstrate that our model achieves state-of-the-art performance on human-centric scene reconstruction and delivers highly competitive results on global human motion estimation, comparing favorably against both optimization-based frameworks and HMR-only methods. Project page: https://murphylmf.github.io/UniSH/",
        "arxiv_id": "2601.01222",
        "ARXIVID": "2601.01222",
        "COMMENT": "No specific criteria match.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    }
}