{
    "2512.16670": {
        "authors": [
            "Ole Beisswenger",
            "Jan-Niklas Dihlmann",
            "Hendrik P. A. Lensch"
        ],
        "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
        "abstract": "arXiv:2512.16670v1 Announce Type: new  Abstract: Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on a frame-by-frame basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. A three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches.",
        "arxiv_id": "2512.16670",
        "ARXIVID": "2512.16670",
        "COMMENT": "Matches criteria 2 closely as it introduces a diffusion model for multiple vision tasks including image generation and segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.16023": {
        "authors": [
            "Liudi Yang",
            "Yang Bai",
            "George Eskandar",
            "Fengyi Shen",
            "Mohammad Altillawi",
            "Dong Chen",
            "Ziyuan Liu",
            "Abhinav Valada"
        ],
        "title": "CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion",
        "abstract": "arXiv:2512.16023v1 Announce Type: new  Abstract: We present a method to generate video-action pairs that follow text instructions, starting from an initial image observation and the robot's joint states. Our approach automatically provides action labels for video diffusion models, overcoming the common lack of action annotations and enabling their full use for robotic policy learning. Existing methods either adopt two-stage pipelines, which limit tightly coupled cross-modal information sharing, or rely on adapting a single-modal diffusion model for a joint distribution that cannot fully leverage pretrained video knowledge. To overcome these limitations, we (1) extend a pretrained video diffusion model with a parallel, dedicated action diffusion model that preserves pretrained knowledge, (2) introduce a Bridge Attention mechanism to enable effective cross-modal interaction, and (3) design an action refinement module to convert coarse actions into precise controls for low-resolution datasets. Extensive evaluations on multiple public benchmarks and real-world datasets demonstrate that our method generates higher-quality videos, more accurate actions, and significantly outperforms existing baselines, offering a scalable framework for leveraging large-scale video data for robotic learning.",
        "arxiv_id": "2512.16023",
        "ARXIVID": "2512.16023",
        "COMMENT": "The paper does not match any of the specific criteria closely. It focuses on video-action pair generation for robotic manipulation using a multi-modal diffusion model, which does not align with the criteria of unified image/video generation and segmentation, unified diffusion models for multiple vision tasks, or image/video matting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    }
}