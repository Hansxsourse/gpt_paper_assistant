{
    "2505.05240": {
        "authors": [
            "Genghua Kou",
            "Fan Jia",
            "Weixin Mao",
            "Yingfei Liu",
            "Yucheng Zhao",
            "Ziheng Zhang",
            "Osamu Yoshie",
            "Tiancai Wang",
            "Ying Li",
            "Xiangyu Zhang"
        ],
        "title": "PADriver: Towards Personalized Autonomous Driving",
        "abstract": "arXiv:2505.05240v1 Announce Type: new  Abstract: In this paper, we propose PADriver, a novel closed-loop framework for personalized autonomous driving (PAD). Built upon Multi-modal Large Language Model (MLLM), PADriver takes streaming frames and personalized textual prompts as inputs. It autoaggressively performs scene understanding, danger level estimation and action decision. The predicted danger level reflects the risk of the potential action and provides an explicit reference for the final action, which corresponds to the preset personalized prompt. Moreover, we construct a closed-loop benchmark named PAD-Highway based on Highway-Env simulator to comprehensively evaluate the decision performance under traffic rules. The dataset contains 250 hours videos with high-quality annotation to facilitate the development of PAD behavior analysis. Experimental results on the constructed benchmark show that PADriver outperforms state-of-the-art approaches on different evaluation metrics, and enables various driving modes.",
        "arxiv_id": "2505.05240",
        "ARXIVID": "2505.05240",
        "COMMENT": "Matches criterion 2 as it introduces a Multi-modal Large Language Model (MLLM) for personalized autonomous driving.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2505.05446": {
        "authors": [
            "Han Xiao",
            "Yina Xie",
            "Guanxin Tan",
            "Yinghao Chen",
            "Rui Hu",
            "Ke Wang",
            "Aojun Zhou",
            "Hao Li",
            "Hao Shao",
            "Xudong Lu",
            "Peng Gao",
            "Yafei Wen",
            "Xiaoxin Chen",
            "Shuai Ren",
            "Hongsheng Li"
        ],
        "title": "Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding",
        "abstract": "arXiv:2505.05446v1 Announce Type: new  Abstract: Visual Document Understanding has become essential with the increase of text-rich visual content. This field poses significant challenges due to the need for effective integration of visual perception and textual comprehension, particularly across diverse document types with complex layouts. Moreover, existing fine-tuning datasets for this domain often fall short in providing the detailed contextual information for robust understanding, leading to hallucinations and limited comprehension of spatial relationships among visual elements. To address these challenges, we propose an innovative pipeline that utilizes adaptive generation of markup languages, such as Markdown, JSON, HTML, and TiKZ, to build highly structured document representations and deliver contextually-grounded responses. We introduce two fine-grained structured datasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs for document parsing, and DocMark-Instruct, featuring 624k fine-tuning data annotations for grounded instruction following. Extensive experiments demonstrate that our proposed model significantly outperforms existing state-of-theart MLLMs across a range of visual document understanding benchmarks, facilitating advanced reasoning and comprehension capabilities in complex visual scenarios. Our code and models are released at https://github. com/Euphoria16/DocMark.",
        "arxiv_id": "2505.05446",
        "ARXIVID": "2505.05446",
        "COMMENT": "Matches criterion 2 as it proposes a new pipeline for visual document understanding using multimodal large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2505.05212": {
        "authors": [
            "Xiaotong Yu",
            "Chang Wen Chen"
        ],
        "title": "HQC-NBV: A Hybrid Quantum-Classical View Planning Approach",
        "abstract": "arXiv:2505.05212v1 Announce Type: new  Abstract: Efficient view planning is a fundamental challenge in computer vision and robotic perception, critical for tasks ranging from search and rescue operations to autonomous navigation. While classical approaches, including sampling-based and deterministic methods, have shown promise in planning camera viewpoints for scene exploration, they often struggle with computational scalability and solution optimality in complex settings. This study introduces HQC-NBV, a hybrid quantum-classical framework for view planning that leverages quantum properties to efficiently explore the parameter space while maintaining robustness and scalability. We propose a specific Hamiltonian formulation with multi-component cost terms and a parameter-centric variational ansatz with bidirectional alternating entanglement patterns that capture the hierarchical dependencies between viewpoint parameters. Comprehensive experiments demonstrate that quantum-specific components provide measurable performance advantages. Compared to the classical methods, our approach achieves up to 49.2% higher exploration efficiency across diverse environments. Our analysis of entanglement architecture and coherence-preserving terms provides insights into the mechanisms of quantum advantage in robotic exploration tasks. This work represents a significant advancement in integrating quantum computing into robotic perception systems, offering a paradigm-shifting solution for various robot vision tasks.",
        "arxiv_id": "2505.05212",
        "ARXIVID": "2505.05212",
        "COMMENT": "Matches criterion 3 as it introduces a novel hybrid quantum-classical framework for robotic perception, focusing on view planning.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2505.04921": {
        "authors": [
            "Yunxin Li",
            "Zhenyu Liu",
            "Zitao Li",
            "Xuanyu Zhang",
            "Zhenran Xu",
            "Xinyu Chen",
            "Haoyuan Shi",
            "Shenyuan Jiang",
            "Xintong Wang",
            "Jifang Wang",
            "Shouzheng Huang",
            "Xinping Zhao",
            "Borui Jiang",
            "Lanqing Hong",
            "Longyue Wang",
            "Zhuotao Tian",
            "Baoxing Huai",
            "Wenhan Luo",
            "Weihua Luo",
            "Zheng Zhang",
            "Baotian Hu",
            "Min Zhang"
        ],
        "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models",
        "abstract": "arXiv:2505.04921v1 Announce Type: new  Abstract: Reasoning lies at the heart of intelligence, shaping the ability to make decisions, draw conclusions, and generalize across domains. In artificial intelligence, as systems increasingly operate in open, uncertain, and multimodal environments, reasoning becomes essential for enabling robust and adaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning. As research advances, multimodal reasoning has rapidly evolved from modular, perception-driven pipelines to unified, language-centric frameworks that offer more coherent cross-modal understanding. While instruction tuning and reinforcement learning have improved model reasoning, significant challenges remain in omni-modal generalization, reasoning depth, and agentic behavior. To address these issues, we present a comprehensive and structured survey of multimodal reasoning research, organized around a four-stage developmental roadmap that reflects the field's shifting design philosophies and emerging capabilities. First, we review early efforts based on task-specific modules, where reasoning was implicitly embedded across stages of representation, alignment, and fusion. Next, we examine recent approaches that unify reasoning into multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT) and multimodal reinforcement learning enabling richer and more structured reasoning chains. Finally, drawing on empirical insights from challenging benchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the conceptual direction of native large multimodal reasoning models (N-LMRMs), which aim to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments.",
        "arxiv_id": "2505.04921",
        "ARXIVID": "2505.04921",
        "COMMENT": "Matches criterion 2 as it discusses advancements in large multimodal reasoning models (LMRMs) and their integration into multimodal LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.04965": {
        "authors": [
            "Henry Zheng",
            "Hao Shi",
            "Qihang Peng",
            "Yong Xien Chng",
            "Rui Huang",
            "Yepeng Weng",
            "Zhongchao Shi",
            "Gao Huang"
        ],
        "title": "DenseGrounding: Improving Dense Language-Vision Semantics for Ego-Centric 3D Visual Grounding",
        "abstract": "arXiv:2505.04965v1 Announce Type: new  Abstract: Enabling intelligent agents to comprehend and interact with 3D environments through natural language is crucial for advancing robotics and human-computer interaction. A fundamental task in this field is ego-centric 3D visual grounding, where agents locate target objects in real-world 3D spaces based on verbal descriptions. However, this task faces two significant challenges: (1) loss of fine-grained visual semantics due to sparse fusion of point clouds with ego-centric multi-view images, (2) limited textual semantic context due to arbitrary language descriptions. We propose DenseGrounding, a novel approach designed to address these issues by enhancing both visual and textual semantics. For visual features, we introduce the Hierarchical Scene Semantic Enhancer, which retains dense semantics by capturing fine-grained global scene features and facilitating cross-modal alignment. For text descriptions, we propose a Language Semantic Enhancer that leverages large language models to provide rich context and diverse language descriptions with additional context during model training. Extensive experiments show that DenseGrounding significantly outperforms existing methods in overall accuracy, with improvements of 5.81% and 7.56% when trained on the comprehensive full dataset and smaller mini subset, respectively, further advancing the SOTA in egocentric 3D visual grounding. Our method also achieves 1st place and receives the Innovation Award in the CVPR 2024 Autonomous Grand Challenge Multi-view 3D Visual Grounding Track, validating its effectiveness and robustness.",
        "arxiv_id": "2505.04965",
        "ARXIVID": "2505.04965",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for ego-centric 3D visual grounding with enhanced semantics.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.05456": {
        "authors": [
            "Wenqi Wang",
            "Reuben Tan",
            "Pengyue Zhu",
            "Jianwei Yang",
            "Zhengyuan Yang",
            "Lijuan Wang",
            "Andrey Kolobov",
            "Jianfeng Gao",
            "Boqing Gong"
        ],
        "title": "SITE: towards Spatial Intelligence Thorough Evaluation",
        "abstract": "arXiv:2505.05456v1 Announce Type: new  Abstract: Spatial intelligence (SI) represents a cognitive ability encompassing the visualization, manipulation, and reasoning about spatial relationships, underpinning disciplines from neuroscience to robotics. We introduce SITE, a benchmark dataset towards SI Thorough Evaluation in a standardized format of multi-choice visual question-answering, designed to assess large vision-language models' spatial intelligence across diverse visual modalities (single-image, multi-image, and video) and SI factors (figural to environmental scales, spatial visualization and orientation, intrinsic and extrinsic, static and dynamic). Our approach to curating the benchmark combines a bottom-up survey about 31 existing datasets and a top-down strategy drawing upon three classification systems in cognitive science, which prompt us to design two novel types of tasks about view-taking and dynamic scenes. Extensive experiments reveal that leading models fall behind human experts especially in spatial orientation, a fundamental SI factor. Moreover, we demonstrate a positive correlation between a model's spatial reasoning proficiency and its performance on an embodied AI task.",
        "arxiv_id": "2505.05456",
        "ARXIVID": "2505.05456",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (SITE) for spatial intelligence evaluation in vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.04905": {
        "authors": [
            "Xi Yang",
            "Songsong Duan",
            "Nannan Wang",
            "Xinbo Gao"
        ],
        "title": "Pro2SAM: Mask Prompt to SAM with Grid Points for Weakly Supervised Object Localization",
        "abstract": "arXiv:2505.04905v1 Announce Type: new  Abstract: Weakly Supervised Object Localization (WSOL), which aims to localize objects by only using image-level labels, has attracted much attention because of its low annotation cost in real applications. Current studies focus on the Class Activation Map (CAM) of CNN and the self-attention map of transformer to identify the region of objects. However, both CAM and self-attention maps can not learn pixel-level fine-grained information on the foreground objects, which hinders the further advance of WSOL. To address this problem, we initiatively leverage the capability of zero-shot generalization and fine-grained segmentation in Segment Anything Model (SAM) to boost the activation of integral object regions. Further, to alleviate the semantic ambiguity issue accrued in single point prompt-based SAM, we propose an innovative mask prompt to SAM (Pro2SAM) network with grid points for WSOL task. First, we devise a Global Token Transformer (GTFormer) to generate a coarse-grained foreground map as a flexible mask prompt, where the GTFormer jointly embeds patch tokens and novel global tokens to learn foreground semantics. Secondly, we deliver grid points as dense prompts into SAM to maximize the probability of foreground mask, which avoids the lack of objects caused by a single point/box prompt. Finally, we propose a pixel-level similarity metric to come true the mask matching from mask prompt to SAM, where the mask with the highest score is viewed as the final localization map. Experiments show that the proposed Pro2SAM achieves state-of-the-art performance on both CUB-200-2011 and ILSVRC, with 84.03\\% and 66.85\\% Top-1 Loc, respectively.",
        "arxiv_id": "2505.04905",
        "ARXIVID": "2505.04905",
        "COMMENT": "Matches criterion 4 as it leverages the Segment Anything Model (SAM) for weakly supervised object localization, focusing on vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.05470": {
        "authors": [
            "Jie Liu",
            "Gongye Liu",
            "Jiajun Liang",
            "Yangguang Li",
            "Jiaheng Liu",
            "Xintao Wang",
            "Pengfei Wan",
            "Di Zhang",
            "Wanli Ouyang"
        ],
        "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
        "abstract": "arXiv:2505.05470v1 Announce Type: new  Abstract: We propose Flow-GRPO, the first method integrating online reinforcement learning (RL) into flow matching models. Our approach uses two key strategies: (1) an ODE-to-SDE conversion that transforms a deterministic Ordinary Differential Equation (ODE) into an equivalent Stochastic Differential Equation (SDE) that matches the original model's marginal distribution at all timesteps, enabling statistical sampling for RL exploration; and (2) a Denoising Reduction strategy that reduces training denoising steps while retaining the original inference timestep number, significantly improving sampling efficiency without performance degradation. Empirically, Flow-GRPO is effective across multiple text-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly perfect object counts, spatial relations, and fine-grained attributes, boosting GenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, its accuracy improves from $59\\%$ to $92\\%$, significantly enhancing text generation. Flow-GRPO also achieves substantial gains in human preference alignment. Notably, little to no reward hacking occurred, meaning rewards did not increase at the cost of image quality or diversity, and both remained stable in our experiments.",
        "arxiv_id": "2505.05470",
        "ARXIVID": "2505.05470",
        "COMMENT": "Matches criterion 4 as it discusses improvements in generative modeling for visual tasks using flow matching models and reinforcement learning.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2505.05023": {
        "authors": [
            "Jialei Chen",
            "Xu Zheng",
            "Dongyue Li",
            "Chong Yi",
            "Seigo Ito",
            "Danda Pani Paudel",
            "Luc Van Gool",
            "Hiroshi Murase",
            "Daisuke Deguchi"
        ],
        "title": "Split Matching for Inductive Zero-shot Semantic Segmentation",
        "abstract": "arXiv:2505.05023v1 Announce Type: new  Abstract: Zero-shot Semantic Segmentation (ZSS) aims to segment categories that are not annotated during training. While fine-tuning vision-language models has achieved promising results, these models often overfit to seen categories due to the lack of supervision for unseen classes. As an alternative to fully supervised approaches, query-based segmentation has shown great latent in ZSS, as it enables object localization without relying on explicit labels. However, conventional Hungarian matching, a core component in query-based frameworks, needs full supervision and often misclassifies unseen categories as background in the setting of ZSS. To address this issue, we propose Split Matching (SM), a novel assignment strategy that decouples Hungarian matching into two components: one for seen classes in annotated regions and another for latent classes in unannotated regions (referred to as unseen candidates). Specifically, we partition the queries into seen and candidate groups, enabling each to be optimized independently according to its available supervision. To discover unseen candidates, we cluster CLIP dense features to generate pseudo masks and extract region-level embeddings using CLS tokens. Matching is then conducted separately for the two groups based on both class-level similarity and mask-level consistency. Additionally, we introduce a Multi-scale Feature Enhancement (MFE) module that refines decoder features through residual multi-scale aggregation, improving the model's ability to capture spatial details across resolutions. SM is the first to introduce decoupled Hungarian matching under the inductive ZSS setting, and achieves state-of-the-art performance on two standard benchmarks.",
        "arxiv_id": "2505.05023",
        "ARXIVID": "2505.05023",
        "COMMENT": "Matches criterion 1 as it introduces a novel method (Split Matching) for improving spatial understanding in zero-shot semantic segmentation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.05467": {
        "authors": [
            "Haibo Wang",
            "Bo Feng",
            "Zhengfeng Lai",
            "Mingze Xu",
            "Shiyu Li",
            "Weifeng Ge",
            "Afshin Dehghan",
            "Meng Cao",
            "Ping Huang"
        ],
        "title": "StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant",
        "abstract": "arXiv:2505.05467v1 Announce Type: new  Abstract: We present StreamBridge, a simple yet effective framework that seamlessly transforms offline Video-LLMs into streaming-capable models. It addresses two fundamental challenges in adapting existing models into online scenarios: (1) limited capability for multi-turn real-time understanding, and (2) lack of proactive response mechanisms. Specifically, StreamBridge incorporates (1) a memory buffer combined with a round-decayed compression strategy, supporting long-context multi-turn interactions, and (2) a decoupled, lightweight activation model that can be effortlessly integrated into existing Video-LLMs, enabling continuous proactive responses. To further support StreamBridge, we construct Stream-IT, a large-scale dataset tailored for streaming video understanding, featuring interleaved video-text sequences and diverse instruction formats. Extensive experiments show that StreamBridge significantly improves the streaming understanding capabilities of offline Video-LLMs across various tasks, outperforming even proprietary models such as GPT-4o and Gemini 1.5 Pro. Simultaneously, it achieves competitive or superior performance on standard video understanding benchmarks.",
        "arxiv_id": "2505.05467",
        "ARXIVID": "2505.05467",
        "COMMENT": "Matches criterion 2 as it introduces a framework to adapt offline Video-LLMs into streaming-capable models, enhancing their real-time understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2505.05288": {
        "authors": [
            "Ahmed Abdelreheem",
            "Filippo Aleotti",
            "Jamie Watson",
            "Zawar Qureshi",
            "Abdelrahman Eldesokey",
            "Peter Wonka",
            "Gabriel Brostow",
            "Sara Vicente",
            "Guillermo Garcia-Hernando"
        ],
        "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes",
        "abstract": "arXiv:2505.05288v1 Announce Type: new  Abstract: We introduce the novel task of Language-Guided Object Placement in Real 3D Scenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual prompt broadly describing where the 3D asset should be placed. The task here is to find a valid placement for the 3D asset that respects the prompt. Compared with other language-guided localization tasks in 3D scenes such as grounding, this task has specific challenges: it is ambiguous because it has multiple valid solutions, and it requires reasoning about 3D geometric relationships and free space. We inaugurate this task by proposing a new benchmark and evaluation protocol. We also introduce a new dataset for training 3D LLMs on this task, as well as the first method to serve as a non-trivial baseline. We believe that this challenging task and our new benchmark could become part of the suite of benchmarks used to evaluate and compare generalist 3D LLM models.",
        "arxiv_id": "2505.05288",
        "ARXIVID": "2505.05288",
        "COMMENT": "Matches criterion 3. Proposes a new benchmark and dataset for language-guided object placement in 3D scenes, focusing on spatial reasoning and embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2505.05001": {
        "authors": [
            "Lang Nie",
            "Chunyu Lin",
            "Kang Liao",
            "Yun Zhang",
            "Shuaicheng Liu",
            "Yao Zhao"
        ],
        "title": "StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps",
        "abstract": "arXiv:2505.05001v1 Announce Type: new  Abstract: We retarget video stitching to an emerging issue, named warping shake, which unveils the temporal content shakes induced by sequentially unsmooth warps when extending image stitching to video stitching. Even if the input videos are stable, the stitched video can inevitably cause undesired warping shakes and affect the visual experience. To address this issue, we propose StabStitch++, a novel video stitching framework to realize spatial stitching and temporal stabilization with unsupervised learning simultaneously. First, different from existing learning-based image stitching solutions that typically warp one image to align with another, we suppose a virtual midplane between original image planes and project them onto it. Concretely, we design a differentiable bidirectional decomposition module to disentangle the homography transformation and incorporate it into our spatial warp, evenly spreading alignment burdens and projective distortions across two views. Then, inspired by camera paths in video stabilization, we derive the mathematical expression of stitching trajectories in video stitching by elaborately integrating spatial and temporal warps. Finally, a warp smoothing model is presented to produce stable stitched videos with a hybrid loss to simultaneously encourage content alignment, trajectory smoothness, and online collaboration. Compared with StabStitch that sacrifices alignment for stabilization, StabStitch++ makes no compromise and optimizes both of them simultaneously, especially in the online mode. To establish an evaluation benchmark and train the learning framework, we build a video stitching dataset with a rich diversity in camera motions and scenes. Experiments exhibit that StabStitch++ surpasses current solutions in stitching performance, robustness, and efficiency, offering compelling advancements in this field by building a real-time online video stitching system.",
        "arxiv_id": "2505.05001",
        "ARXIVID": "2505.05001",
        "COMMENT": "Matches criterion 3 as it introduces a novel video stitching framework with a focus on spatial and temporal stabilization, which is relevant to embodied AI benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2505.04917": {
        "authors": [
            "Chenxu Peng",
            "Chenxu Wang",
            "Minrui Zou",
            "Danyang Li",
            "Zhengpeng Yang",
            "Yimian Dai",
            "Ming-Ming Cheng",
            "Xiang Li"
        ],
        "title": "A Simple Detector with Frame Dynamics is a Strong Tracker",
        "abstract": "arXiv:2505.04917v1 Announce Type: new  Abstract: Infrared object tracking plays a crucial role in Anti-Unmanned Aerial Vehicle (Anti-UAV) applications. Existing trackers often depend on cropped template regions and have limited motion modeling capabilities, which pose challenges when dealing with tiny targets. To address this, we propose a simple yet effective infrared tiny-object tracker that enhances tracking performance by integrating global detection and motion-aware learning with temporal priors. Our method is based on object detection and achieves significant improvements through two key innovations. First, we introduce frame dynamics, leveraging frame difference and optical flow to encode both prior target features and motion characteristics at the input level, enabling the model to better distinguish the target from background clutter. Second, we propose a trajectory constraint filtering strategy in the post-processing stage, utilizing spatio-temporal priors to suppress false positives and enhance tracking robustness. Extensive experiments show that our method consistently outperforms existing approaches across multiple metrics in challenging infrared UAV tracking scenarios. Notably, we achieve state-of-the-art performance in the 4th Anti-UAV Challenge, securing 1st place in Track 1 and 2nd place in Track 2.",
        "arxiv_id": "2505.04917",
        "ARXIVID": "2505.04917",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for infrared object tracking with motion-aware learning and trajectory constraints.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.05376": {
        "authors": [
            "Rachmadio Noval Lazuardi",
            "Artem Sevastopolsky",
            "Egor Zakharov",
            "Matthias Niessner",
            "Vanessa Sklyarova"
        ],
        "title": "GeomHair: Reconstruction of Hair Strands from Colorless 3D Scans",
        "abstract": "arXiv:2505.05376v1 Announce Type: new  Abstract: We propose a novel method that reconstructs hair strands directly from colorless 3D scans by leveraging multi-modal hair orientation extraction. Hair strand reconstruction is a fundamental problem in computer vision and graphics that can be used for high-fidelity digital avatar synthesis, animation, and AR/VR applications. However, accurately recovering hair strands from raw scan data remains challenging due to human hair's complex and fine-grained structure. Existing methods typically rely on RGB captures, which can be sensitive to the environment and can be a challenging domain for extracting the orientation of guiding strands, especially in the case of challenging hairstyles. To reconstruct the hair purely from the observed geometry, our method finds sharp surface features directly on the scan and estimates strand orientation through a neural 2D line detector applied to the renderings of scan shading. Additionally, we incorporate a diffusion prior trained on a diverse set of synthetic hair scans, refined with an improved noise schedule, and adapted to the reconstructed contents via a scan-specific text prompt. We demonstrate that this combination of supervision signals enables accurate reconstruction of both simple and intricate hairstyles without relying on color information. To facilitate further research, we introduce Strands400, the largest publicly available dataset of hair strands with detailed surface geometry extracted from real-world data, which contains reconstructed hair strands from the scans of 400 subjects.",
        "arxiv_id": "2505.05376",
        "ARXIVID": "2505.05376",
        "COMMENT": "Matches criterion 4 as it focuses on reconstructing hair strands using multi-modal methods and diffusion priors, which relates to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.05209": {
        "authors": [
            "Haizhen Xie",
            "Kunpeng Du",
            "Qiangyu Yan",
            "Sen Lu",
            "Jianhong Han",
            "Hanting Chen",
            "Hailin Hu",
            "Jie Hu"
        ],
        "title": "EAM: Enhancing Anything with Diffusion Transformers for Blind Super-Resolution",
        "abstract": "arXiv:2505.05209v1 Announce Type: new  Abstract: Utilizing pre-trained Text-to-Image (T2I) diffusion models to guide Blind Super-Resolution (BSR) has become a predominant approach in the field. While T2I models have traditionally relied on U-Net architectures, recent advancements have demonstrated that Diffusion Transformers (DiT) achieve significantly higher performance in this domain. In this work, we introduce Enhancing Anything Model (EAM), a novel BSR method that leverages DiT and outperforms previous U-Net-based approaches. We introduce a novel block, $\\Psi$-DiT, which effectively guides the DiT to enhance image restoration. This block employs a low-resolution latent as a separable flow injection control, forming a triple-flow architecture that effectively leverages the prior knowledge embedded in the pre-trained DiT. To fully exploit the prior guidance capabilities of T2I models and enhance their generalization in BSR, we introduce a progressive Masked Image Modeling strategy, which also reduces training costs. Additionally, we propose a subject-aware prompt generation strategy that employs a robust multi-modal model in an in-context learning framework. This strategy automatically identifies key image areas, provides detailed descriptions, and optimizes the utilization of T2I diffusion priors. Our experiments demonstrate that EAM achieves state-of-the-art results across multiple datasets, outperforming existing methods in both quantitative metrics and visual quality.",
        "arxiv_id": "2505.05209",
        "ARXIVID": "2505.05209",
        "COMMENT": "Matches criterion 4. Proposes a novel method leveraging Diffusion Transformers for blind super-resolution, which is related to vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2505.04638": {
        "authors": [
            "Tianyu Liu",
            "Simeng Han",
            "Xiao Luo",
            "Hanchen Wang",
            "Pan Lu",
            "Biqing Zhu",
            "Yuge Wang",
            "Keyi Li",
            "Jiapeng Chen",
            "Rihao Qu",
            "Yufeng Liu",
            "Xinyue Cui",
            "Aviv Yaish",
            "Yuhang Chen",
            "Minsheng Hao",
            "Chuhan Li",
            "Kexing Li",
            "Arman Cohan",
            "Hua Xu",
            "Mark Gerstein",
            "James Zou",
            "Hongyu Zhao"
        ],
        "title": "Towards Artificial Intelligence Research Assistant for Expert-Involved Learning",
        "abstract": "arXiv:2505.04638v1 Announce Type: new  Abstract: Large Language Models (LLMs) and Large Multi-Modal Models (LMMs) have emerged as transformative tools in scientific research, yet their reliability and specific contributions to biomedical applications remain insufficiently characterized. In this study, we present \\textbf{AR}tificial \\textbf{I}ntelligence research assistant for \\textbf{E}xpert-involved \\textbf{L}earning (ARIEL), a multimodal dataset designed to benchmark and enhance two critical capabilities of LLMs and LMMs in biomedical research: summarizing extensive scientific texts and interpreting complex biomedical figures. To facilitate rigorous assessment, we create two open-source sets comprising biomedical articles and figures with designed questions. We systematically benchmark both open- and closed-source foundation models, incorporating expert-driven human evaluations conducted by doctoral-level experts. Furthermore, we improve model performance through targeted prompt engineering and fine-tuning strategies for summarizing research papers, and apply test-time computational scaling to enhance the reasoning capabilities of LMMs, achieving superior accuracy compared to human-expert corrections. We also explore the potential of using LMM Agents to generate scientific hypotheses from diverse multimodal inputs. Overall, our results delineate clear strengths and highlight significant limitations of current foundation models, providing actionable insights and guiding future advancements in deploying large-scale language and multi-modal models within biomedical research.",
        "arxiv_id": "2505.04638",
        "ARXIVID": "2505.04638",
        "COMMENT": "Matches criterion 2. Introduces ARIEL, a multimodal dataset and benchmarks for LLMs and LMMs in biomedical research, focusing on summarization and figure interpretation.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2505.05235": {
        "authors": [
            "Luca Marzari",
            "Isabella Mastroeni",
            "Alessandro Farinelli"
        ],
        "title": "Advancing Neural Network Verification through Hierarchical Safety Abstract Interpretation",
        "abstract": "arXiv:2505.05235v1 Announce Type: new  Abstract: Traditional methods for formal verification (FV) of deep neural networks (DNNs) are constrained by a binary encoding of safety properties, where a model is classified as either safe or unsafe (robust or not robust). This binary encoding fails to capture the nuanced safety levels within a model, often resulting in either overly restrictive or too permissive requirements. In this paper, we introduce a novel problem formulation called Abstract DNN-Verification, which verifies a hierarchical structure of unsafe outputs, providing a more granular analysis of the safety aspect for a given DNN. Crucially, by leveraging abstract interpretation and reasoning about output reachable sets, our approach enables assessing multiple safety levels during the FV process, requiring the same (in the worst case) or even potentially less computational effort than the traditional binary verification approach. Specifically, we demonstrate how this formulation allows rank adversarial inputs according to their abstract safety level violation, offering a more detailed evaluation of the model's safety and robustness. Our contributions include a theoretical exploration of the relationship between our novel abstract safety formulation and existing approaches that employ abstract interpretation for robustness verification, complexity analysis of the novel problem introduced, and an empirical evaluation considering both a complex deep reinforcement learning task (based on Habitat 3.0) and standard DNN-Verification benchmarks.",
        "arxiv_id": "2505.05235",
        "ARXIVID": "2505.05235",
        "COMMENT": "Matches criterion 3. The paper introduces a novel hierarchical safety verification method for neural networks, with an application in Habitat 3.0, a simulator for embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.05091": {
        "authors": [
            "Shashank Agnihotri",
            "Amaan Ansari",
            "Annika Dackermann",
            "Fabian R\\\"osch",
            "Margret Keuper"
        ],
        "title": "DispBench: Benchmarking Disparity Estimation to Synthetic Corruptions",
        "abstract": "arXiv:2505.05091v1 Announce Type: new  Abstract: Deep learning (DL) has surpassed human performance on standard benchmarks, driving its widespread adoption in computer vision tasks. One such task is disparity estimation, estimating the disparity between matching pixels in stereo image pairs, which is crucial for safety-critical applications like medical surgeries and autonomous navigation. However, DL-based disparity estimation methods are highly susceptible to distribution shifts and adversarial attacks, raising concerns about their reliability and generalization. Despite these concerns, a standardized benchmark for evaluating the robustness of disparity estimation methods remains absent, hindering progress in the field.   To address this gap, we introduce DispBench, a comprehensive benchmarking tool for systematically assessing the reliability of disparity estimation methods. DispBench evaluates robustness against synthetic image corruptions such as adversarial attacks and out-of-distribution shifts caused by 2D Common Corruptions across multiple datasets and diverse corruption scenarios. We conduct the most extensive performance and robustness analysis of disparity estimation methods to date, uncovering key correlations between accuracy, reliability, and generalization. Open-source code for DispBench: https://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation",
        "arxiv_id": "2505.05091",
        "ARXIVID": "2505.05091",
        "COMMENT": "Matches criterion 3. Introduces DispBench, a benchmark for evaluating disparity estimation methods under synthetic corruptions, focusing on robustness and generalization.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2505.04899": {
        "authors": [
            "Sifan Song",
            "Siyeop Yoon",
            "Pengfei Jin",
            "Sekeun Kim",
            "Matthew Tivnan",
            "Yujin Oh",
            "Runqi Meng",
            "Ling Chen",
            "Zhiliang Lyu",
            "Dufan Wu",
            "Ning Guo",
            "Xiang Li",
            "Quanzheng Li"
        ],
        "title": "OWT: A Foundational Organ-Wise Tokenization Framework for Medical Imaging",
        "abstract": "arXiv:2505.04899v1 Announce Type: new  Abstract: Recent advances in representation learning often rely on holistic, black-box embeddings that entangle multiple semantic components, limiting interpretability and generalization. These issues are especially critical in medical imaging. To address these limitations, we propose an Organ-Wise Tokenization (OWT) framework with a Token Group-based Reconstruction (TGR) training paradigm. Unlike conventional approaches that produce holistic features, OWT explicitly disentangles an image into separable token groups, each corresponding to a distinct organ or semantic entity. Our design ensures each token group encapsulates organ-specific information, boosting interpretability, generalization, and efficiency while allowing fine-grained control in downstream tasks. Experiments on CT and MRI datasets demonstrate the effectiveness of OWT in not only achieving strong image reconstruction and segmentation performance, but also enabling novel semantic-level generation and retrieval applications that are out of reach for standard holistic embedding methods. These findings underscore the potential of OWT as a foundational framework for semantically disentangled representation learning, offering broad scalability and applicability to real-world medical imaging scenarios and beyond.",
        "arxiv_id": "2505.04899",
        "ARXIVID": "2505.04899",
        "COMMENT": "Does not match any specific criteria but introduces a novel tokenization framework for medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.05318": {
        "authors": [
            "Agnese Chiatti",
            "Sara Bernardini",
            "Lara Shibelski Godoy Piccolo",
            "Viola Schiaffonati",
            "Matteo Matteucci"
        ],
        "title": "Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects",
        "abstract": "arXiv:2505.05318v1 Announce Type: new  Abstract: The rapid adoption of Vision Language Models (VLMs), pre-trained on large image-text and video-text datasets, calls for protecting and informing users about when to trust these systems. This survey reviews studies on trust dynamics in user-VLM interactions, through a multi-disciplinary taxonomy encompassing different cognitive science capabilities, collaboration modes, and agent behaviours. Literature insights and findings from a workshop with prospective VLM users inform preliminary requirements for future VLM trust studies.",
        "arxiv_id": "2505.05318",
        "ARXIVID": "2505.05318",
        "COMMENT": "Matches criterion 2. Discusses trust dynamics in Vision Language Models (VLMs), which aligns with the interest in VLLMs/MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2505.04979": {
        "authors": [
            "Zhuang Qi",
            "Sijin Zhou",
            "Lei Meng",
            "Han Hu",
            "Han Yu",
            "Xiangxu Meng"
        ],
        "title": "Federated Deconfounding and Debiasing Learning for Out-of-Distribution Generalization",
        "abstract": "arXiv:2505.04979v1 Announce Type: new  Abstract: Attribute bias in federated learning (FL) typically leads local models to optimize inconsistently due to the learning of non-causal associations, resulting degraded performance. Existing methods either use data augmentation for increasing sample diversity or knowledge distillation for learning invariant representations to address this problem. However, they lack a comprehensive analysis of the inference paths, and the interference from confounding factors limits their performance. To address these limitations, we propose the \\underline{Fed}erated \\underline{D}econfounding and \\underline{D}ebiasing \\underline{L}earning (FedDDL) method. It constructs a structured causal graph to analyze the model inference process, and performs backdoor adjustment to eliminate confounding paths. Specifically, we design an intra-client deconfounding learning module for computer vision tasks to decouple background and objects, generating counterfactual samples that establish a connection between the background and any label, which stops the model from using the background to infer the label. Moreover, we design an inter-client debiasing learning module to construct causal prototypes to reduce the proportion of the background in prototype components. Notably, it bridges the gap between heterogeneous representations via causal prototypical regularization. Extensive experiments on 2 benchmarking datasets demonstrate that \\methodname{} significantly enhances the model capability to focus on main objects in unseen data, leading to 4.5\\% higher Top-1 Accuracy on average over 9 state-of-the-art existing methods.",
        "arxiv_id": "2505.04979",
        "ARXIVID": "2505.04979",
        "COMMENT": "Does not match any specific criterion but is tangentially related to federated learning and generalization, which may be of peripheral interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.05007": {
        "authors": [
            "Xin Bi",
            "Zhichao Li",
            "Yuxuan Xia",
            "Panpan Tong",
            "Lijuan Zhang",
            "Yang Chen",
            "Junsheng Fu"
        ],
        "title": "Driving with Context: Online Map Matching for Complex Roads Using Lane Markings and Scenario Recognition",
        "abstract": "arXiv:2505.05007v1 Announce Type: new  Abstract: Accurate online map matching is fundamental to vehicle navigation and the activation of intelligent driving functions. Current online map matching methods are prone to errors in complex road networks, especially in multilevel road area. To address this challenge, we propose an online Standard Definition (SD) map matching method by constructing a Hidden Markov Model (HMM) with multiple probability factors. Our proposed method can achieve accurate map matching even in complex road networks by carefully leveraging lane markings and scenario recognition in the designing of the probability factors. First, the lane markings are generated by a multi-lane tracking method and associated with the SD map using HMM to build an enriched SD map. In areas covered by the enriched SD map, the vehicle can re-localize itself by performing Iterative Closest Point (ICP) registration for the lane markings. Then, the probability factor accounting for the lane marking detection can be obtained using the association probability between adjacent lanes and roads. Second, the driving scenario recognition model is applied to generate the emission probability factor of scenario recognition, which improves the performance of map matching on elevated roads and ordinary urban roads underneath them. We validate our method through extensive road tests in Europe and China, and the experimental results show that our proposed method effectively improves the online map matching accuracy as compared to other existing methods, especially in multilevel road area. Specifically, the experiments show that our proposed method achieves $F_1$ scores of 98.04% and 94.60% on the Zenseact Open Dataset and test data of multilevel road areas in Shanghai respectively, significantly outperforming benchmark methods. The implementation is available at https://github.com/TRV-Lab/LMSR-OMM.",
        "arxiv_id": "2505.05007",
        "ARXIVID": "2505.05007",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and intelligent driving applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.05321": {
        "authors": [
            "Chintan B. Maniyar",
            "Minakshi Kumar",
            "Gengchen Mai"
        ],
        "title": "Feature-Augmented Deep Networks for Multiscale Building Segmentation in High-Resolution UAV and Satellite Imagery",
        "abstract": "arXiv:2505.05321v1 Announce Type: new  Abstract: Accurate building segmentation from high-resolution RGB imagery remains challenging due to spectral similarity with non-building features, shadows, and irregular building geometries. In this study, we present a comprehensive deep learning framework for multiscale building segmentation using RGB aerial and satellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate a diverse, multi-sensor dataset and introduce feature-augmented inputs by deriving secondary representations including Principal Component Analysis (PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index (MBI), and Sobel edge filters from RGB channels. These features guide a Res-U-Net architecture in learning complex spatial patterns more effectively. We also propose training policies incorporating layer freezing, cyclical learning rates, and SuperConvergence to reduce training time and resource usage. Evaluated on a held-out WorldView-3 image, our model achieves an overall accuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of 0.80, outperforming existing RGB-based benchmarks. This study demonstrates the effectiveness of combining multi-resolution imagery, feature augmentation, and optimized training strategies for robust building segmentation in remote sensing applications.",
        "arxiv_id": "2505.05321",
        "ARXIVID": "2505.05321",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and machine learning in remote sensing applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.05307": {
        "authors": [
            "Ciyu Ruan",
            "Ruishan Guo",
            "Zihang Gong",
            "Jingao Xu",
            "Wenhan Yang",
            "Xinlei Chen"
        ],
        "title": "PRE-Mamba: A 4D State Space Model for Ultra-High-Frequent Event Camera Deraining",
        "abstract": "arXiv:2505.05307v1 Announce Type: new  Abstract: Event cameras excel in high temporal resolution and dynamic range but suffer from dense noise in rainy conditions. Existing event deraining methods face trade-offs between temporal precision, deraining effectiveness, and computational efficiency. In this paper, we propose PRE-Mamba, a novel point-based event camera deraining framework that fully exploits the spatiotemporal characteristics of raw event and rain. Our framework introduces a 4D event cloud representation that integrates dual temporal scales to preserve high temporal precision, a Spatio-Temporal Decoupling and Fusion module (STDF) that enhances deraining capability by enabling shallow decoupling and interaction of temporal and spatial information, and a Multi-Scale State Space Model (MS3M) that captures deeper rain dynamics across dual-temporal and multi-spatial scales with linear computational complexity. Enhanced by frequency-domain regularization, PRE-Mamba achieves superior performance (0.95 SR, 0.91 NR, and 0.4s/M events) with only 0.26M parameters on EventRain-27K, a comprehensive dataset with labeled synthetic and real-world sequences. Moreover, our method generalizes well across varying rain intensities, viewpoints, and even snowy conditions.",
        "arxiv_id": "2505.05307",
        "ARXIVID": "2505.05307",
        "COMMENT": "Does not match any specific criteria but introduces a novel framework for event camera deraining.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.04888": {
        "authors": [
            "Tharindu Fernando",
            "Clinton Fookes",
            "Sridha Sridharan",
            "Simon Denman"
        ],
        "title": "Cross-Branch Orthogonality for Improved Generalization in Face Deepfake Detection",
        "abstract": "arXiv:2505.04888v1 Announce Type: new  Abstract: Remarkable advancements in generative AI technology have given rise to a spectrum of novel deepfake categories with unprecedented leaps in their realism, and deepfakes are increasingly becoming a nuisance to law enforcement authorities and the general public. In particular, we observe alarming levels of confusion, deception, and loss of faith regarding multimedia content within society caused by face deepfakes, and existing deepfake detectors are struggling to keep up with the pace of improvements in deepfake generation. This is primarily due to their reliance on specific forgery artifacts, which limits their ability to generalise and detect novel deepfake types. To combat the spread of malicious face deepfakes, this paper proposes a new strategy that leverages coarse-to-fine spatial information, semantic information, and their interactions while ensuring feature distinctiveness and reducing the redundancy of the modelled features. A novel feature orthogonality-based disentanglement strategy is introduced to ensure branch-level and cross-branch feature disentanglement, which allows us to integrate multiple feature vectors without adding complexity to the feature space or compromising generalisation. Comprehensive experiments on three public benchmarks: FaceForensics++, Celeb-DF, and the Deepfake Detection Challenge (DFDC) show that these design choices enable the proposed approach to outperform current state-of-the-art methods by 5% on the Celeb-DF dataset and 7% on the DFDC dataset in a cross-dataset evaluation setting.",
        "arxiv_id": "2505.04888",
        "ARXIVID": "2505.04888",
        "COMMENT": "Does not match any specific criteria but proposes a novel strategy for face deepfake detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.05397": {
        "authors": [
            "Zhang Zhang",
            "Chao Sun",
            "Chao Yue",
            "Da Wen",
            "Tianze Wang",
            "Jianghao Leng"
        ],
        "title": "PillarMamba: Learning Local-Global Context for Roadside Point Cloud via Hybrid State Space Model",
        "abstract": "arXiv:2505.05397v1 Announce Type: new  Abstract: Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything (V2X) tasks, roadside perception has received increasing attention in recent years, as it can extend the perception range of connected vehicles and improve traffic safety. However, roadside point cloud oriented 3D object detection has not been effectively explored. To some extent, the key to the performance of a point cloud detector lies in the receptive field of the network and the ability to effectively utilize the scene context. The recent emergence of Mamba, based on State Space Model (SSM), has shaken up the traditional convolution and transformers that have long been the foundational building blocks, due to its efficient global receptive field. In this work, we introduce Mamba to pillar-based roadside point cloud perception and propose a framework based on Cross-stage State-space Group (CSG), called PillarMamba. It enhances the expressiveness of the network and achieves efficient computation through cross-stage feature fusion. However, due to the limitations of scan directions, state space model faces local connection disrupted and historical relationship forgotten. To address this, we propose the Hybrid State-space Block (HSB) to obtain the local-global context of roadside point cloud. Specifically, it enhances neighborhood connections through local convolution and preserves historical memory through residual attention. The proposed method outperforms the state-of-the-art methods on the popular large scale roadside benchmark: DAIR-V2X-I. The code will be released soon.",
        "arxiv_id": "2505.05397",
        "ARXIVID": "2505.05397",
        "COMMENT": "Does not match any specific criteria but focuses on roadside point cloud perception with novel methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.04915": {
        "authors": [
            "Tong Wang",
            "Ting Liu",
            "Xiaochao Qu",
            "Chengjing Wu",
            "Luoqi Liu",
            "Xiaolin Hu"
        ],
        "title": "GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing",
        "abstract": "arXiv:2505.04915v1 Announce Type: new  Abstract: Scene text editing, a subfield of image editing, requires modifying texts in images while preserving style consistency and visual coherence with the surrounding environment. While diffusion-based methods have shown promise in text generation, they still struggle to produce high-quality results. These methods often generate distorted or unrecognizable characters, particularly when dealing with complex characters like Chinese. In such systems, characters are composed of intricate stroke patterns and spatial relationships that must be precisely maintained. We present GlyphMastero, a specialized glyph encoder designed to guide the latent diffusion model for generating texts with stroke-level precision. Our key insight is that existing methods, despite using pretrained OCR models for feature extraction, fail to capture the hierarchical nature of text structures - from individual strokes to stroke-level interactions to overall character-level structure. To address this, our glyph encoder explicitly models and captures the cross-level interactions between local-level individual characters and global-level text lines through our novel glyph attention module. Meanwhile, our model implements a feature pyramid network to fuse the multi-scale OCR backbone features at the global-level. Through these cross-level and multi-scale fusions, we obtain more detailed glyph-aware guidance, enabling precise control over the scene text generation process. Our method achieves an 18.02\\% improvement in sentence accuracy over the state-of-the-art multi-lingual scene text editing baseline, while simultaneously reducing the text-region Fr\\'echet inception distance by 53.28\\%.",
        "arxiv_id": "2505.04915",
        "ARXIVID": "2505.04915",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling in scene text editing.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.05453": {
        "authors": [
            "Nataliia Klievtsova",
            "Timotheus Kampik",
            "Juergen Mangler",
            "Stefanie Rinderle-Ma"
        ],
        "title": "Conversational Process Model Redesign",
        "abstract": "arXiv:2505.05453v1 Announce Type: new  Abstract: With the recent success of large language models (LLMs), the idea of AI-augmented Business Process Management systems is becoming more feasible. One of their essential characteristics is the ability to be conversationally actionable, allowing humans to interact with the LLM effectively to perform crucial process life cycle tasks such as process model design and redesign. However, most current research focuses on single-prompt execution and evaluation of results, rather than on continuous interaction between the user and the LLM. In this work, we aim to explore the feasibility of using LLMs to empower domain experts in the creation and redesign of process models in an iterative and effective way. The proposed conversational process model redesign (CPD) approach receives as input a process model and a redesign request by the user in natural language. Instead of just letting the LLM make changes, the LLM is employed to (a) identify process change patterns from literature, (b) re-phrase the change request to be aligned with an expected wording for the identified pattern (i.e., the meaning), and then to (c) apply the meaning of the change to the process model. This multi-step approach allows for explainable and reproducible changes. In order to ensure the feasibility of the CPD approach, and to find out how well the patterns from literature can be handled by the LLM, we performed an extensive evaluation. The results show that some patterns are hard to understand by LLMs and by users. Within the scope of the study, we demonstrated that users need support to describe the changes clearly. Overall the evaluation shows that the LLMs can handle most changes well according to a set of completeness and correctness criteria.",
        "arxiv_id": "2505.05453",
        "ARXIVID": "2505.05453",
        "COMMENT": "Does not match any specific criterion but is tangentially related to large language models and their applications in process model redesign.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.04736": {
        "authors": [
            "Sutapa Dey Tithi",
            "Arun Kumar Ramesh",
            "Clara DiMarco",
            "Xiaoyi Tian",
            "Nazia Alam",
            "Kimia Fazeli",
            "Tiffany Barnes"
        ],
        "title": "The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems",
        "abstract": "arXiv:2505.04736v1 Announce Type: new  Abstract: Intelligent tutoring systems have demonstrated effectiveness in teaching formal propositional logic proofs, but their reliance on template-based explanations limits their ability to provide personalized student feedback. While large language models (LLMs) offer promising capabilities for dynamic feedback generation, they risk producing hallucinations or pedagogically unsound explanations. We evaluated the stepwise accuracy of LLMs in constructing multi-step symbolic logic proofs, comparing six prompting techniques across four state-of-the-art LLMs on 358 propositional logic problems. Results show that DeepSeek-V3 achieved superior performance with 84.4% accuracy on stepwise proof construction and excelled particularly in simpler rules. We further used the best-performing LLM to generate explanatory hints for 1,050 unique student problem-solving states from a logic ITS and evaluated them on 4 criteria with both an LLM grader and human expert ratings on a 20% sample. Our analysis finds that LLM-generated hints were 75% accurate and rated highly by human evaluators on consistency and clarity, but did not perform as well explaining why the hint was provided or its larger context. Our results demonstrate that LLMs may be used to augment tutoring systems with logic tutoring hints, but requires additional modifications to ensure accuracy and pedagogical appropriateness.",
        "arxiv_id": "2505.04736",
        "ARXIVID": "2505.04736",
        "COMMENT": "Does not match any specific criterion but is tangentially related to large language models and their applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.04946": {
        "authors": [
            "Xuyang Guo",
            "Jiayan Huo",
            "Zhenmei Shi",
            "Zhao Song",
            "Jiahao Zhang",
            "Jiale Zhao"
        ],
        "title": "T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models",
        "abstract": "arXiv:2505.04946v1 Announce Type: new  Abstract: Thanks to recent advancements in scalable deep architectures and large-scale pretraining, text-to-video generation has achieved unprecedented capabilities in producing high-fidelity, instruction-following content across a wide range of styles, enabling applications in advertising, entertainment, and education. However, these models' ability to render precise on-screen text, such as captions or mathematical formulas, remains largely untested, posing significant challenges for applications requiring exact textual accuracy. In this work, we introduce T2VTextBench, the first human-evaluation benchmark dedicated to evaluating on-screen text fidelity and temporal consistency in text-to-video models. Our suite of prompts integrates complex text strings with dynamic scene changes, testing each model's ability to maintain detailed instructions across frames. We evaluate ten state-of-the-art systems, ranging from open-source solutions to commercial offerings, and find that most struggle to generate legible, consistent text. These results highlight a critical gap in current video generators and provide a clear direction for future research aimed at enhancing textual manipulation in video synthesis.",
        "arxiv_id": "2505.04946",
        "ARXIVID": "2505.04946",
        "COMMENT": "Does not match any specific criterion but is tangentially related to vision-language models and evaluation benchmarks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.05229": {
        "authors": [
            "Andrea Asperti",
            "Leonardo Dess\\`i",
            "Maria Chiara Tonetti",
            "Nico Wu"
        ],
        "title": "Does CLIP perceive art the same way we do?",
        "abstract": "arXiv:2505.05229v1 Announce Type: new  Abstract: CLIP has emerged as a powerful multimodal model capable of connecting images and text through joint embeddings, but to what extent does it \"see\" the same way humans do - especially when interpreting artworks? In this paper, we investigate CLIP's ability to extract high-level semantic and stylistic information from paintings, including both human-created and AI-generated imagery. We evaluate its perception across multiple dimensions: content, scene understanding, artistic style, historical period, and the presence of visual deformations or artifacts. By designing targeted probing tasks and comparing CLIP's responses to human annotations and expert benchmarks, we explore its alignment with human perceptual and contextual understanding. Our findings reveal both strengths and limitations in CLIP's visual representations, particularly in relation to aesthetic cues and artistic intent. We further discuss the implications of these insights for using CLIP as a guidance mechanism during generative processes, such as style transfer or prompt-based image synthesis. Our work highlights the need for deeper interpretability in multimodal systems, especially when applied to creative domains where nuance and subjectivity play a central role.",
        "arxiv_id": "2505.05229",
        "ARXIVID": "2505.05229",
        "COMMENT": "Does not match any specific criteria but investigates CLIP's perception of art, which is tangentially related to vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.04835": {
        "authors": [
            "Shashank Agnihotri",
            "David Schader",
            "Nico Sharei",
            "Mehmet Ege Ka\\c{c}ar",
            "Margret Keuper"
        ],
        "title": "Are Synthetic Corruptions A Reliable Proxy For Real-World Corruptions?",
        "abstract": "arXiv:2505.04835v1 Announce Type: new  Abstract: Deep learning (DL) models are widely used in real-world applications but remain vulnerable to distribution shifts, especially due to weather and lighting changes. Collecting diverse real-world data for testing the robustness of DL models is resource-intensive, making synthetic corruptions an attractive alternative for robustness testing. However, are synthetic corruptions a reliable proxy for real-world corruptions? To answer this, we conduct the largest benchmarking study on semantic segmentation models, comparing performance on real-world corruptions and synthetic corruptions datasets. Our results reveal a strong correlation in mean performance, supporting the use of synthetic corruptions for robustness evaluation. We further analyze corruption-specific correlations, providing key insights to understand when synthetic corruptions succeed in representing real-world corruptions. Open-source Code: https://github.com/shashankskagnihotri/benchmarking_robustness/tree/segmentation_david/semantic_segmentation",
        "arxiv_id": "2505.04835",
        "ARXIVID": "2505.04835",
        "COMMENT": "Does not match any specific criteria but provides insights into synthetic vs real-world corruptions for robustness testing.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.04758": {
        "authors": [
            "Songsong Duan",
            "Xi Yang",
            "Nannan Wang",
            "Xinbo Gao"
        ],
        "title": "Lightweight RGB-D Salient Object Detection from a Speed-Accuracy Tradeoff Perspective",
        "abstract": "arXiv:2505.04758v1 Announce Type: new  Abstract: Current RGB-D methods usually leverage large-scale backbones to improve accuracy but sacrifice efficiency. Meanwhile, several existing lightweight methods are difficult to achieve high-precision performance. To balance the efficiency and performance, we propose a Speed-Accuracy Tradeoff Network (SATNet) for Lightweight RGB-D SOD from three fundamental perspectives: depth quality, modality fusion, and feature representation. Concerning depth quality, we introduce the Depth Anything Model to generate high-quality depth maps,which effectively alleviates the multi-modal gaps in the current datasets. For modality fusion, we propose a Decoupled Attention Module (DAM) to explore the consistency within and between modalities. Here, the multi-modal features are decoupled into dual-view feature vectors to project discriminable information of feature maps. For feature representation, we develop a Dual Information Representation Module (DIRM) with a bi-directional inverted framework to enlarge the limited feature space generated by the lightweight backbones. DIRM models texture features and saliency features to enrich feature space, and employ two-way prediction heads to optimal its parameters through a bi-directional backpropagation. Finally, we design a Dual Feature Aggregation Module (DFAM) in the decoder to aggregate texture and saliency features. Extensive experiments on five public RGB-D SOD datasets indicate that the proposed SATNet excels state-of-the-art (SOTA) CNN-based heavyweight models and achieves a lightweight framework with 5.2 M parameters and 415 FPS.",
        "arxiv_id": "2505.04758",
        "ARXIVID": "2505.04758",
        "COMMENT": "Does not match any specific criteria but is related to lightweight models and efficiency in RGB-D tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}