{
    "2509.20360": {
        "authors": [
            "Xuan Ju",
            "Tianyu Wang",
            "Yuqian Zhou",
            "He Zhang",
            "Qing Liu",
            "Nanxuan Zhao",
            "Zhifei Zhang",
            "Yijun Li",
            "Yuanhao Cai",
            "Shaoteng Liu",
            "Daniil Pakhomov",
            "Zhe Lin",
            "Soo Ye Kim",
            "Qiang Xu"
        ],
        "title": "EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning",
        "abstract": "arXiv:2509.20360v1 Announce Type: new  Abstract: Recent advances in foundation models highlight a clear trend toward unification and scaling, showing emergent capabilities across diverse domains. While image generation and editing have rapidly transitioned from task-specific to unified frameworks, video generation and editing remain fragmented due to architectural limitations and data scarcity. In this work, we introduce EditVerse, a unified framework for image and video generation and editing within a single model. By representing all modalities, i.e., text, image, and video, as a unified token sequence, EditVerse leverages self-attention to achieve robust in-context learning, natural cross-modal knowledge transfer, and flexible handling of inputs and outputs with arbitrary resolutions and durations. To address the lack of video editing training data, we design a scalable data pipeline that curates 232K video editing samples and combines them with large-scale image and video datasets for joint training. Furthermore, we present EditVerseBench, the first benchmark for instruction-based video editing covering diverse tasks and resolutions. Extensive experiments and user studies demonstrate that EditVerse achieves state-of-the-art performance, surpassing existing open-source and commercial models, while exhibiting emergent editing and generation abilities across modalities.",
        "arxiv_id": "2509.20360",
        "ARXIVID": "2509.20360",
        "COMMENT": "The paper introduces a unified framework for image and video generation and editing, which aligns with the interest in unified pipelines for generation and segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.20251": {
        "authors": [
            "Hao Lu",
            "Zhuang Ma",
            "Guangfeng Jiang",
            "Wenhang Ge",
            "Bohan Li",
            "Yuzhan Cai",
            "Wenzhao Zheng",
            "Yunpeng Zhang",
            "Yingcong Chen"
        ],
        "title": "4D Driving Scene Generation With Stereo Forcing",
        "abstract": "arXiv:2509.20251v1 Announce Type: new  Abstract: Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. Bridging generation and novel view synthesis remains a major challenge. We present PhiGenesis, a unified framework for 4D scene generation that extends video generation techniques with geometric and temporal consistency. Given multi-view image sequences and camera parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting representations along target 3D trajectories. In its first stage, PhiGenesis leverages a pre-trained video VAE with a novel range-view adapter to enable feed-forward 4D reconstruction from multi-view images. This architecture supports single-frame or video inputs and outputs complete 4D scenes including geometry, semantics, and motion. In the second stage, PhiGenesis introduces a geometric-guided video diffusion model, using rendered historical 4D scenes as priors to generate future views conditioned on trajectories. To address geometric exposure bias in novel views, we propose Stereo Forcing, a novel conditioning strategy that integrates geometric uncertainty during denoising. This method enhances temporal coherence by dynamically adjusting generative influence based on uncertainty-aware perturbations. Our experimental results demonstrate that our method achieves state-of-the-art performance in both appearance and geometric reconstruction, temporal generation and novel view synthesis (NVS) tasks, while simultaneously delivering competitive performance in downstream evaluations. Homepage is at \\href{https://jiangxb98.github.io/PhiGensis}{PhiGensis}.",
        "arxiv_id": "2509.20251",
        "ARXIVID": "2509.20251",
        "COMMENT": "Matches criteria 1 closely as it proposes a unified framework for 4D scene generation that includes geometry, semantics, and motion, integrating video generation and novel view synthesis.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2509.20295": {
        "authors": [
            "Xichen Xu",
            "Yanshu Wang",
            "Jinbao Wang",
            "Xiaoning Lei",
            "Guoyang Xie",
            "Guannan Jiang",
            "Zhichao Lu"
        ],
        "title": "FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis",
        "abstract": "arXiv:2509.20295v1 Announce Type: new  Abstract: Industrial anomaly segmentation relies heavily on pixel-level annotations, yet real-world anomalies are often scarce, diverse, and costly to label. Segmentation-oriented industrial anomaly synthesis (SIAS) has emerged as a promising alternative; however, existing methods struggle to balance sampling efficiency and generation quality. Moreover, most approaches treat all spatial regions uniformly, overlooking the distinct statistical differences between anomaly and background areas. This uniform treatment hinders the synthesis of controllable, structure-specific anomalies tailored for segmentation tasks. In this paper, we propose FAST, a foreground-aware diffusion framework featuring two novel modules: the Anomaly-Informed Accelerated Sampling (AIAS) and the Foreground-Aware Reconstruction Module (FARM). AIAS is a training-free sampling algorithm specifically designed for segmentation-oriented industrial anomaly synthesis, which accelerates the reverse process through coarse-to-fine aggregation and enables the synthesis of state-of-the-art segmentation-oriented anomalies in as few as 10 steps. Meanwhile, FARM adaptively adjusts the anomaly-aware noise within the masked foreground regions at each sampling step, preserving localized anomaly signals throughout the denoising trajectory. Extensive experiments on multiple industrial benchmarks demonstrate that FAST consistently outperforms existing anomaly synthesis methods in downstream segmentation tasks. We release the code at: https://anonymous.4open.science/r/NeurIPS-938.",
        "arxiv_id": "2509.20295",
        "ARXIVID": "2509.20295",
        "COMMENT": "The paper proposes a diffusion framework for segmentation-oriented anomaly synthesis, which aligns with the interest in unified diffusion models for multiple vision tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.20358": {
        "authors": [
            "Chen Wang",
            "Chuhao Chen",
            "Yiming Huang",
            "Zhiyang Dou",
            "Yuan Liu",
            "Jiatao Gu",
            "Lingjie Liu"
        ],
        "title": "PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation",
        "abstract": "arXiv:2509.20358v1 Announce Type: new  Abstract: Existing video generation models excel at producing photo-realistic videos from text or images, but often lack physical plausibility and 3D controllability. To overcome these limitations, we introduce PhysCtrl, a novel framework for physics-grounded image-to-video generation with physical parameters and force control. At its core is a generative physics network that learns the distribution of physical dynamics across four materials (elastic, sand, plasticine, and rigid) via a diffusion model conditioned on physics parameters and applied forces. We represent physical dynamics as 3D point trajectories and train on a large-scale synthetic dataset of 550K animations generated by physics simulators. We enhance the diffusion model with a novel spatiotemporal attention block that emulates particle interactions and incorporates physics-based constraints during training to enforce physical plausibility. Experiments show that PhysCtrl generates realistic, physics-grounded motion trajectories which, when used to drive image-to-video models, yield high-fidelity, controllable videos that outperform existing methods in both visual quality and physical plausibility. Project Page: https://cwchenwang.github.io/physctrl",
        "arxiv_id": "2509.20358",
        "ARXIVID": "2509.20358",
        "COMMENT": "Does not match any specific criteria closely. Focuses on physics-grounded video generation, not on joint generation and segmentation or multi-task diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.19965": {
        "authors": [
            "Phyo Thet Yee",
            "Dimitrios Kollias",
            "Sudeepta Mishra",
            "Abhinav Dhall"
        ],
        "title": "SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding",
        "abstract": "arXiv:2509.19965v1 Announce Type: new  Abstract: Audio-driven talking face generation has received growing interest, particularly for applications requiring expressive and natural human-avatar interaction. However, most existing emotion-aware methods rely on a single modality (either audio or image) for emotion embedding, limiting their ability to capture nuanced affective cues. Additionally, most methods condition on a single reference image, restricting the model's ability to represent dynamic changes in actions or attributes across time. To address these issues, we introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion embedding by combining emotional signals from text (via sentiment analysis) and audio (via speech-based emotion recognition and audio-derived valence-arousal features), enabling the generation of talking face videos with richer and more authentic emotional expressiveness and fidelity. To ensure natural head motion and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M) module that generates motion frames aligned with the input audio. Finally, SynchroRaMa incorporates scene descriptions generated by Large Language Model (LLM) as additional textual input, enabling it to capture dynamic actions and high-level semantic attributes. Conditioning the model on both visual and textual cues enhances temporal consistency and visual realism. Quantitative and qualitative experiments on benchmark datasets demonstrate that SynchroRaMa outperforms the state-of-the-art, achieving improvements in image quality, expression preservation, and motion realism. A user study further confirms that SynchroRaMa achieves higher subjective ratings than competing methods in overall naturalness, motion diversity, and video smoothness. Our project page is available at .",
        "arxiv_id": "2509.19965",
        "ARXIVID": "2509.19965",
        "COMMENT": "Does not match any specific criteria closely. Focuses on talking face generation with emotion embedding, not on joint generation and segmentation or multi-task diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.19749": {
        "authors": [
            "Shao-Yu Chang",
            "Jingyi Xu",
            "Hieu Le",
            "Dimitris Samaras"
        ],
        "title": "Talking Head Generation via AU-Guided Landmark Prediction",
        "abstract": "arXiv:2509.19749v1 Announce Type: new  Abstract: We propose a two-stage framework for audio-driven talking head generation with fine-grained expression control via facial Action Units (AUs). Unlike prior methods relying on emotion labels or implicit AU conditioning, our model explicitly maps AUs to 2D facial landmarks, enabling physically grounded, per-frame expression control. In the first stage, a variational motion generator predicts temporally coherent landmark sequences from audio and AU intensities. In the second stage, a diffusion-based synthesizer generates realistic, lip-synced videos conditioned on these landmarks and a reference image. This separation of motion and appearance improves expression accuracy, temporal stability, and visual realism. Experiments on the MEAD dataset show that our method outperforms state-of-the-art baselines across multiple metrics, demonstrating the effectiveness of explicit AU-to-landmark modeling for expressive talking head generation.",
        "arxiv_id": "2509.19749",
        "ARXIVID": "2509.19749",
        "COMMENT": "Does not match any specific criteria closely. Focuses on talking head generation with expression control, not on joint generation and segmentation or multi-task diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}