{
    "2506.21270": {
        "authors": [
            "Cheng Zou",
            "Senlin Cheng",
            "Bolei Xu",
            "Dandan Zheng",
            "Xiaobo Li",
            "Jingdong Chen",
            "Ming Yang"
        ],
        "title": "Video Virtual Try-on with Conditional Diffusion Transformer Inpainter",
        "abstract": "arXiv:2506.21270v1 Announce Type: new  Abstract: Video virtual try-on aims to naturally fit a garment to a target person in consecutive video frames. It is a challenging task, on the one hand, the output video should be in good spatial-temporal consistency, on the other hand, the details of the given garment need to be preserved well in all the frames. Naively using image-based try-on methods frame by frame can get poor results due to severe inconsistency. Recent diffusion-based video try-on methods, though very few, happen to coincide with a similar solution: inserting temporal attention into image-based try-on model to adapt it for video try-on task, which have shown improvements but there still exist inconsistency problems. In this paper, we propose ViTI (Video Try-on Inpainter), formulate and implement video virtual try-on as a conditional video inpainting task, which is different from previous methods. In this way, we start with a video generation problem instead of an image-based try-on problem, which from the beginning has a better spatial-temporal consistency. Specifically, at first we build a video inpainting framework based on Diffusion Transformer with full 3D spatial-temporal attention, and then we progressively adapt it for video garment inpainting, with a collection of masking strategies and multi-stage training. After these steps, the model can inpaint the masked garment area with appropriate garment pixels according to the prompt with good spatial-temporal consistency. Finally, as other try-on methods, garment condition is added to the model to make sure the inpainted garment appearance and details are as expected. Both quantitative and qualitative experimental results show that ViTI is superior to previous works.",
        "arxiv_id": "2506.21270",
        "ARXIVID": "2506.21270",
        "COMMENT": "The paper does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.20756": {
        "authors": [
            "Haodong Li",
            "Chen Wang",
            "Jiahui Lei",
            "Kostas Daniilidis",
            "Lingjie Liu"
        ],
        "title": "StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation",
        "abstract": "arXiv:2506.20756v1 Announce Type: new  Abstract: Recent video depth estimation methods achieve great performance by following the paradigm of image depth estimation, i.e., typically fine-tuning pre-trained video diffusion models with massive data. However, we argue that video depth estimation is not a naive extension of image depth estimation. The temporal consistency requirements for dynamic and static regions in videos are fundamentally different. Consistent video depth in static regions, typically backgrounds, can be more effectively achieved via stereo matching across all frames, which provides much stronger global 3D cues. While the consistency for dynamic regions still should be learned from large-scale video depth data to ensure smooth transitions, due to the violation of triangulation constraints. Based on these insights, we introduce StereoDiff, a two-stage video depth estimator that synergizes stereo matching for mainly the static areas with video depth diffusion for maintaining consistent depth transitions in dynamic areas. We mathematically demonstrate how stereo matching and video depth diffusion offer complementary strengths through frequency domain analysis, highlighting the effectiveness of their synergy in capturing the advantages of both. Experimental results on zero-shot, real-world, dynamic video depth benchmarks, both indoor and outdoor, demonstrate StereoDiff's SoTA performance, showcasing its superior consistency and accuracy in video depth estimation.",
        "arxiv_id": "2506.20756",
        "ARXIVID": "2506.20756",
        "COMMENT": "The paper does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}