{
    "2512.11464": {
        "authors": [
            "Han Lin",
            "Xichen Pan",
            "Ziqi Huang",
            "Ji Hou",
            "Jialiang Wang",
            "Weifeng Chen",
            "Zecheng He",
            "Felix Juefei-Xu",
            "Junzhe Sun",
            "Zhipeng Fan",
            "Ali Thabet",
            "Mohit Bansal",
            "Chu Wang"
        ],
        "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas",
        "abstract": "arXiv:2512.11464v1 Announce Type: new  Abstract: Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.",
        "arxiv_id": "2512.11464",
        "ARXIVID": "2512.11464",
        "COMMENT": "Matches criterion 2: Unified Diffusion Models for multiple vision tasks",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.11715": {
        "authors": [
            "Wei Chow",
            "Linfeng Li",
            "Lingdong Kong",
            "Zefeng Li",
            "Qi Xu",
            "Hang Song",
            "Tian Ye",
            "Xian Wang",
            "Jinbin Bai",
            "Shilin Xu",
            "Xiangtai Li",
            "Junting Pan",
            "Shaoteng Liu",
            "Ran Zhou",
            "Tianshu Yang",
            "Songhua Liu"
        ],
        "title": "EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing",
        "abstract": "arXiv:2512.11715v1 Announce Type: new  Abstract: Recent advances in diffusion models (DMs) have achieved exceptional visual quality in image editing tasks. However, the global denoising dynamics of DMs inherently conflate local editing targets with the full-image context, leading to unintended modifications in non-target regions. In this paper, we shift our attention beyond DMs and turn to Masked Generative Transformers (MGTs) as an alternative approach to tackle this challenge. By predicting multiple masked tokens rather than holistic refinement, MGTs exhibit a localized decoding paradigm that endows them with the inherent capacity to explicitly preserve non-relevant regions during the editing process. Building upon this insight, we introduce the first MGT-based image editing framework, termed EditMGT. We first demonstrate that MGT's cross-attention maps provide informative localization signals for localizing edit-relevant regions and devise a multi-layer attention consolidation scheme that refines these maps to achieve fine-grained and precise localization. On top of these adaptive localization results, we introduce region-hold sampling, which restricts token flipping within low-attention areas to suppress spurious edits, thereby confining modifications to the intended target regions and preserving the integrity of surrounding non-target areas. To train EditMGT, we construct CrispEdit-2M, a high-resolution dataset spanning seven diverse editing categories. Without introducing additional parameters, we adapt a pre-trained text-to-image MGT into an image editing model through attention injection. Extensive experiments across four standard benchmarks demonstrate that, with fewer than 1B parameters, our model achieves similarity performance while enabling 6 times faster editing. Moreover, it delivers comparable or superior editing quality, with improvements of 3.6% and 17.6% on style change and style transfer tasks, respectively.",
        "arxiv_id": "2512.11715",
        "ARXIVID": "2512.11715",
        "COMMENT": "Does not match any specific criteria",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.11141": {
        "authors": [
            "Yiwei Lyu",
            "Chenhui Zhao",
            "Soumyanil Banerjee",
            "Shixuan Liu",
            "Akshay Rao",
            "Akhil Kondepudi",
            "Honglak Lee",
            "Todd C. Hollon"
        ],
        "title": "Learning complete and explainable visual representations from itemized text supervision",
        "abstract": "arXiv:2512.11141v1 Announce Type: new  Abstract: Training vision models with language supervision enables general and transferable representations. However, many visual domains, especially non-object-centric domains such as medical imaging and remote sensing, contain itemized text annotations: multiple text items describing distinct and semantically independent findings within a single image. Such supervision differs from standard multi-caption supervision, where captions are redundant or highly overlapping. Here, we introduce ItemizedCLIP, a framework for learning complete and explainable visual representations from itemized text supervision. ItemizedCLIP employs a cross-attention module to produce text item-conditioned visual embeddings and a set of tailored objectives that jointly enforce item independence (distinct regions for distinct items) and representation completeness (coverage of all items). Across four domains with naturally itemized text supervision (brain MRI, head CT, chest CT, remote sensing) and one additional synthetically itemized dataset, ItemizedCLIP achieves substantial improvements in zero-shot performance and fine-grained interpretability over baselines. The resulting ItemizedCLIP representations are semantically grounded, item-differentiable, complete, and visually interpretable. Our code is available at https://github.com/MLNeurosurg/ItemizedCLIP.",
        "arxiv_id": "2512.11141",
        "ARXIVID": "2512.11141",
        "COMMENT": "Does not match any specific criteria",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.11722": {
        "authors": [
            "Lin Bai",
            "Xiaoyang Li",
            "Liqiang Huang",
            "Quynh Nguyen",
            "Hien Van Nguyen",
            "Saurabh Prasad",
            "Dragan Maric",
            "John Redell",
            "Pramod Dash",
            "Badrinath Roysam"
        ],
        "title": "Weak-to-Strong Generalization Enables Fully Automated De Novo Training of Multi-head Mask-RCNN Model for Segmenting Densely Overlapping Cell Nuclei in Multiplex Whole-slice Brain Images",
        "abstract": "arXiv:2512.11722v1 Announce Type: new  Abstract: We present a weak to strong generalization methodology for fully automated training of a multi-head extension of the Mask-RCNN method with efficient channel attention for reliable segmentation of overlapping cell nuclei in multiplex cyclic immunofluorescent (IF) whole-slide images (WSI), and present evidence for pseudo-label correction and coverage expansion, the key phenomena underlying weak to strong generalization. This method can learn to segment de novo a new class of images from a new instrument and/or a new imaging protocol without the need for human annotations. We also present metrics for automated self-diagnosis of segmentation quality in production environments, where human visual proofreading of massive WSI images is unaffordable. Our method was benchmarked against five current widely used methods and showed a significant improvement. The code, sample WSI images, and high-resolution segmentation results are provided in open form for community adoption and adaptation.",
        "arxiv_id": "2512.11722",
        "ARXIVID": "2512.11722",
        "COMMENT": "Does not match any specific criteria",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}