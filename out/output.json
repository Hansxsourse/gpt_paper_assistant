{
    "2510.06308": {
        "authors": [
            "Yi Xin",
            "Qi Qin",
            "Siqi Luo",
            "Kaiwen Zhu",
            "Juncheng Yan",
            "Yan Tai",
            "Jiayi Lei",
            "Yuewen Cao",
            "Keqi Wang",
            "Yibin Wang",
            "Jinbin Bai",
            "Qian Yu",
            "Dengyang Jiang",
            "Yuandong Pu",
            "Haoxing Chen",
            "Le Zhuo",
            "Junjun He",
            "Gen Luo",
            "Tianbin Li",
            "Ming Hu",
            "Jin Ye",
            "Shenglong Ye",
            "Bo Zhang",
            "Chang Xu",
            "Wenhai Wang",
            "Hongsheng Li",
            "Guangtao Zhai",
            "Tianfan Xue",
            "Bin Fu",
            "Xiaohong Liu",
            "Yu Qiao",
            "Yihao Liu"
        ],
        "title": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding",
        "abstract": "arXiv:2510.06308v1 Announce Type: new  Abstract: We introduce Lumina-DiMOO, an open-source foundational model for seamless multi-modal generation and understanding. Lumina-DiMOO sets itself apart from prior unified models by utilizing a fully discrete diffusion modeling to handle inputs and outputs across various modalities. This innovative approach allows Lumina-DiMOO to achieve higher sampling efficiency compared to previous autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a broad spectrum of multi-modal tasks, including text-to-image generation, image-to-image generation (e.g., image editing, subject-driven generation, and image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves state-of-the-art performance on multiple benchmarks, surpassing existing open-source unified multi-modal models. To foster further advancements in multi-modal and discrete diffusion model research, we release our code and checkpoints to the community. Project Page: https://synbol.github.io/Lumina-DiMOO.",
        "arxiv_id": "2510.06308",
        "ARXIVID": "2510.06308",
        "COMMENT": "Matches criteria 2 with a unified diffusion model for multi-modal tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2510.06590": {
        "authors": [
            "Ziyuan Huang",
            "DanDan Zheng",
            "Cheng Zou",
            "Rui Liu",
            "Xiaolong Wang",
            "Kaixiang Ji",
            "Weilong Chai",
            "Jianxin Sun",
            "Libin Wang",
            "Yongjie Lv",
            "Taozhi Huang",
            "Jiajia Liu",
            "Qingpei Guo",
            "Ming Yang",
            "Jingdong Chen",
            "Jun Zhou"
        ],
        "title": "Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer",
        "abstract": "arXiv:2510.06590v1 Announce Type: new  Abstract: Visual tokenization remains a core challenge in unifying visual understanding and generation within the autoregressive paradigm. Existing methods typically employ tokenizers in discrete latent spaces to align with the tokens from large language models, where the quantization errors can limit semantic expressiveness and degrade the capability of vision-language understanding. To address this, we introduce MingTok, a new family of visual tokenizers with a continuous latent space, for unified autoregressive generation and understanding. While understanding tasks favor discriminative high-dimensional features, generation tasks prefer compact low-level codes. Thus, to reconcile these competing demands, MingTok adopts a three-stage sequential architecture involving low-level encoding, semantic expansion, and visual reconstruction. Built on top of it, Ming-UniVision eliminates the need for task-specific visual representations, and unifies diverse vision-language tasks under a single autoregrsssive prediction paradigm. By formulating both understanding and generation as next-token prediction in a shared continuous space, it seamlessly supports multi-round, in-context tasks such as iterative understanding, generation and editing. Empirically, we find that using a unified continuous visual representation reconciles the competing requirements on the tokenizers by the understanding and generation tasks, thereby leading to state-of-the-art level performance across both domains. We hope our findings will facilitate unified visual tokenization in the continuous domain. Inference code and model weights are released to benefit community.",
        "arxiv_id": "2510.06590",
        "ARXIVID": "2510.06590",
        "COMMENT": "Matches criteria 1 closely with a unified framework for image understanding and generation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2510.07310": {
        "authors": [
            "Siyoon Jin",
            "Seongchan Kim",
            "Dahyun Chung",
            "Jaeho Lee",
            "Hyunwook Choi",
            "Jisu Nam",
            "Jiyoung Kim",
            "Seungryong Kim"
        ],
        "title": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation",
        "abstract": "arXiv:2510.07310v1 Announce Type: new  Abstract: Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-object interactions. This raises a key question: How do these models internally represent interactions? To answer this, we curate MATRIX-11K, a video dataset with interaction-aware captions and multi-instance mask tracks. Using this dataset, we conduct a systematic analysis that formalizes two perspectives of video DiTs: semantic grounding, via video-to-text attention, which evaluates whether noun and verb tokens capture instances and their relations; and semantic propagation, via video-to-video attention, which assesses whether instance bindings persist across frames. We find both effects concentrate in a small subset of interaction-dominant layers. Motivated by this, we introduce MATRIX, a simple and effective regularization that aligns attention in specific layers of video DiTs with multi-instance mask tracks from the MATRIX-11K dataset, enhancing both grounding and propagation. We further propose InterGenEval, an evaluation protocol for interaction-aware video generation. In experiments, MATRIX improves both interaction fidelity and semantic alignment while reducing drift and hallucination. Extensive ablations validate our design choices. Codes and weights will be released.",
        "arxiv_id": "2510.07310",
        "ARXIVID": "2510.07310",
        "COMMENT": "The paper proposes a framework for interaction-aware video generation using multi-instance mask tracks, which aligns with the criteria of unified image/video generation and segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.07316": {
        "authors": [
            "Gangwei Xu",
            "Haotong Lin",
            "Hongcheng Luo",
            "Xianqi Wang",
            "Jingfeng Yao",
            "Lianghui Zhu",
            "Yuechuan Pu",
            "Cheng Chi",
            "Haiyang Sun",
            "Bing Wang",
            "Guang Chen",
            "Hangjun Ye",
            "Sida Peng",
            "Xin Yang"
        ],
        "title": "Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers",
        "abstract": "arXiv:2510.07316v1 Announce Type: new  Abstract: This paper presents Pixel-Perfect Depth, a monocular depth estimation model based on pixel-space diffusion generation that produces high-quality, flying-pixel-free point clouds from estimated depth maps. Current generative depth estimation models fine-tune Stable Diffusion and achieve impressive performance. However, they require a VAE to compress depth maps into latent space, which inevitably introduces \\textit{flying pixels} at edges and details. Our model addresses this challenge by directly performing diffusion generation in the pixel space, avoiding VAE-induced artifacts. To overcome the high complexity associated with pixel-space generation, we introduce two novel designs: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which incorporate semantic representations from vision foundation models into DiT to prompt the diffusion process, thereby preserving global semantic consistency while enhancing fine-grained visual details; and 2) Cascade DiT Design that progressively increases the number of tokens to further enhance efficiency and accuracy. Our model achieves the best performance among all published generative models across five benchmarks, and significantly outperforms all other models in edge-aware point cloud evaluation.",
        "arxiv_id": "2510.07316",
        "ARXIVID": "2510.07316",
        "COMMENT": "Matches criteria 2 with a diffusion model for depth estimation and segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.06460": {
        "authors": [
            "Piyush Dashpute",
            "Niki Nezakati",
            "Wolfgang Heidrich",
            "Vishwanath Saragadam"
        ],
        "title": "TDiff: Thermal Plug-And-Play Prior with Patch-Based Diffusion",
        "abstract": "arXiv:2510.06460v1 Announce Type: new  Abstract: Thermal images from low-cost cameras often suffer from low resolution, fixed pattern noise, and other localized degradations. Available datasets for thermal imaging are also limited in both size and diversity. To address these challenges, we propose a patch-based diffusion framework (TDiff) that leverages the local nature of these distortions by training on small thermal patches. In this approach, full-resolution images are restored by denoising overlapping patches and blending them using smooth spatial windowing. To our knowledge, this is the first patch-based diffusion framework that models a learned prior for thermal image restoration across multiple tasks. Experiments on denoising, super-resolution, and deblurring demonstrate strong results on both simulated and real thermal data, establishing our method as a unified restoration pipeline.",
        "arxiv_id": "2510.06460",
        "ARXIVID": "2510.06460",
        "COMMENT": "Matches criteria 2 with a unified diffusion model for multiple tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    }
}