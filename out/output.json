{
    "2506.08632": {
        "authors": [
            "Yang Bai",
            "Liudi Yang",
            "George Eskandar",
            "Fengyi Shen",
            "Dong Chen",
            "Mohammad Altillawi",
            "Ziyuan Liu",
            "Gitta Kutyniok"
        ],
        "title": "RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping",
        "abstract": "arXiv:2506.08632v1 Announce Type: new  Abstract: Recent advancements in generative models have revolutionized video synthesis and editing. However, the scarcity of diverse, high-quality datasets continues to hinder video-conditioned robotic learning, limiting cross-platform generalization. In this work, we address the challenge of swapping a robotic arm in one video with another: a key step for crossembodiment learning. Unlike previous methods that depend on paired video demonstrations in the same environmental settings, our proposed framework, RoboSwap, operates on unpaired data from diverse environments, alleviating the data collection needs. RoboSwap introduces a novel video editing pipeline integrating both GANs and diffusion models, combining their isolated advantages. Specifically, we segment robotic arms from their backgrounds and train an unpaired GAN model to translate one robotic arm to another. The translated arm is blended with the original video background and refined with a diffusion model to enhance coherence, motion realism and object interaction. The GAN and diffusion stages are trained independently. Our experiments demonstrate that RoboSwap outperforms state-of-the-art video and image editing models on three benchmarks in terms of both structural coherence and motion consistency, thereby offering a robust solution for generating reliable, cross-embodiment data in robotic learning.",
        "arxiv_id": "2506.08632",
        "ARXIVID": "2506.08632",
        "COMMENT": "Criteria 1: The paper describes a framework that integrates GANs and diffusion models for video editing, involving segmentation and generation tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.08351": {
        "authors": [
            "Huixuan Zhang",
            "Junzhe Zhang",
            "Xiaojun Wan"
        ],
        "title": "How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models",
        "abstract": "arXiv:2506.08351v1 Announce Type: new  Abstract: With the rapid development of text-to-vision generation diffusion models, classifier-free guidance has emerged as the most prevalent method for conditioning. However, this approach inherently requires twice as many steps for model forwarding compared to unconditional generation, resulting in significantly higher costs. While previous study has introduced the concept of adaptive guidance, it lacks solid analysis and empirical results, making previous method unable to be applied to general diffusion models. In this work, we present another perspective of applying adaptive guidance and propose Step AG, which is a simple, universally applicable adaptive guidance strategy. Our evaluations focus on both image quality and image-text alignment. whose results indicate that restricting classifier-free guidance to the first several denoising steps is sufficient for generating high-quality, well-conditioned images, achieving an average speedup of 20% to 30%. Such improvement is consistent across different settings such as inference steps, and various models including video generation models, highlighting the superiority of our method.",
        "arxiv_id": "2506.08351",
        "ARXIVID": "2506.08351",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}