{
    "2510.20803": {
        "authors": [
            "Xiaolong Wang",
            "Lixiang Ru",
            "Ziyuan Huang",
            "Kaixiang Ji",
            "Dandan Zheng",
            "Jingdong Chen",
            "Jun Zhou"
        ],
        "title": "ARGenSeg: Image Segmentation with Autoregressive Image Generation Model",
        "abstract": "arXiv:2510.20803v1 Announce Type: new  Abstract: We propose a novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within a unified framework. Prior works integrating image segmentation into multimodal large language models (MLLMs) typically employ either boundary points representation or dedicated segmentation heads. These methods rely on discrete representations or semantic prompts fed into task-specific decoders, which limits the ability of the MLLM to capture fine-grained visual details. To address these challenges, we introduce a segmentation framework for MLLM based on image generation, which naturally produces dense masks for target objects. We leverage MLLM to output visual tokens and detokenize them into images using an universal VQ-VAE, making the segmentation fully dependent on the pixel-level understanding of the MLLM. To reduce inference latency, we employ a next-scale-prediction strategy to generate required visual tokens in parallel. Extensive experiments demonstrate that our method surpasses prior state-of-the-art approaches on multiple segmentation datasets with a remarkable boost in inference speed, while maintaining strong understanding capabilities.",
        "arxiv_id": "2510.20803",
        "ARXIVID": "2510.20803",
        "COMMENT": "Matches criteria 1 closely: proposes a unified framework for image generation and segmentation.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2510.20819": {
        "authors": [
            "Nimrod Berman",
            "Omkar Joglekar",
            "Eitan Kosman",
            "Dotan Di Castro",
            "Omri Azencot"
        ],
        "title": "Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge",
        "abstract": "arXiv:2510.20819v1 Announce Type: new  Abstract: Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: https://sites.google.com/view/lddbm/home.",
        "arxiv_id": "2510.20819",
        "ARXIVID": "2510.20819",
        "COMMENT": "Matches criteria 2 closely: introduces a diffusion model for multiple tasks including modality translation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    }
}