{
    "2503.03663": {
        "authors": [
            "Wei Li",
            "Bing Hu",
            "Rui Shao",
            "Leyang Shen",
            "Liqiang Nie"
        ],
        "title": "LION-FS: Fast & Slow Video-Language Thinker as Online Video Assistant",
        "abstract": "arXiv:2503.03663v1 Announce Type: new  Abstract: First-person video assistants are highly anticipated to enhance our daily lives through online video dialogue. However, existing online video assistants often sacrifice assistant efficacy for real-time efficiency by processing low-frame-rate videos with coarse-grained visual features.To overcome the trade-off between efficacy and efficiency, we propose \"Fast & Slow Video-Language Thinker\" as an onLIne videO assistaNt, LION-FS, achieving real-time, proactive, temporally accurate, and contextually precise responses. LION-FS adopts a two-stage optimization strategy: 1)Fast Path: Routing-Based Response Determination evaluates frame-by-frame whether an immediate response is necessary. To enhance response determination accuracy and handle higher frame-rate inputs efficiently, we employ Token Aggregation Routing to dynamically fuse spatiotemporal features without increasing token numbers, while utilizing Token Dropping Routing to eliminate redundant features. 2)Slow Path: Multi-granularity Keyframe Augmentation optimizes keyframes during response generation. To provide comprehensive and detailed responses beyond atomic actions constrained by training data, fine-grained spatial features and human-environment interaction features are extracted through multi-granular pooling. These features are further integrated into a meticulously designed multimodal Thinking Template to guide more precise response generation. Comprehensive evaluations on online video tasks demonstrate that LION-FS achieves state-of-the-art efficacy and efficiency.",
        "arxiv_id": "2503.03663",
        "ARXIVID": "2503.03663",
        "COMMENT": "Matches criterion 2 as it introduces a novel video-language model (LION-FS) for online video assistance, focusing on multimodal reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2503.03307": {
        "authors": [
            "Ji Zhao",
            "Banglei Guan",
            "Zibin Liu",
            "Laurent Kneip"
        ],
        "title": "Full-DoF Egomotion Estimation for Event Cameras Using Geometric Solvers",
        "abstract": "arXiv:2503.03307v1 Announce Type: new  Abstract: For event cameras, current sparse geometric solvers for egomotion estimation assume that the rotational displacements are known, such as those provided by an IMU. Thus, they can only recover the translational motion parameters. Recovering full-DoF motion parameters using a sparse geometric solver is a more challenging task, and has not yet been investigated. In this paper, we propose several solvers to estimate both rotational and translational velocities within a unified framework. Our method leverages event manifolds induced by line segments. The problem formulations are based on either an incidence relation for lines or a novel coplanarity relation for normal vectors. We demonstrate the possibility of recovering full-DoF egomotion parameters for both angular and linear velocities without requiring extra sensor measurements or motion priors. To achieve efficient optimization, we exploit the Adam framework with a first-order approximation of rotations for quick initialization. Experiments on both synthetic and real-world data demonstrate the effectiveness of our method. The code is available at https://github.com/jizhaox/relpose-event.",
        "arxiv_id": "2503.03307",
        "ARXIVID": "2503.03307",
        "COMMENT": "Matches criterion 1 as it proposes a novel method for spatial understanding in embodied agents using event cameras for full-DoF egomotion estimation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2503.03562": {
        "authors": [
            "Wenqiao Li",
            "Yao Gu",
            "Xintao Chen",
            "Xiaohao Xu",
            "Ming Hu",
            "Xiaonan Huang",
            "Yingna Wu"
        ],
        "title": "Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection",
        "abstract": "arXiv:2503.03562v1 Announce Type: new  Abstract: Humans detect real-world object anomalies by perceiving, interacting, and reasoning based on object-conditioned physical knowledge. The long-term goal of Industrial Anomaly Detection (IAD) is to enable machines to autonomously replicate this skill. However, current IAD algorithms are largely developed and tested on static, semantically simple datasets, which diverge from real-world scenarios where physical understanding and reasoning are essential.To bridge this gap, we introduce the Physics Anomaly Detection (Phys-AD) dataset, the first large-scale, real-world, physics-grounded video dataset for industrial anomaly detection. Collected using a real robot arm and motor, Phys-AD provides a diverse set of dynamic, semantically rich scenarios. The dataset includes more than 6400 videos across 22 real-world object categories, interacting with robot arms and motors, and exhibits 47 types of anomalies. Anomaly detection in Phys-AD requires visual reasoning, combining both physical knowledge and video content to determine object abnormality.We benchmark state-of-the-art anomaly detection methods under three settings: unsupervised AD, weakly-supervised AD, and video-understanding AD, highlighting their limitations in handling physics-grounded anomalies. Additionally, we introduce the Physics Anomaly Explanation (PAEval) metric, designed to assess the ability of visual-language foundation models to not only detect anomalies but also provide accurate explanations for their underlying physical causes. Our dataset and benchmark will be publicly available.",
        "arxiv_id": "2503.03562",
        "ARXIVID": "2503.03562",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (Phys-AD dataset) for anomaly detection in embodied AI scenarios, focusing on physical reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.03535": {
        "authors": [
            "Po-Chien Luan",
            "Yang Gao",
            "Celine Demonsant",
            "Alexandre Alahi"
        ],
        "title": "Unified Human Localization and Trajectory Prediction with Monocular Vision",
        "abstract": "arXiv:2503.03535v1 Announce Type: new  Abstract: Conventional human trajectory prediction models rely on clean curated data, requiring specialized equipment or manual labeling, which is often impractical for robotic applications. The existing predictors tend to overfit to clean observation affecting their robustness when used with noisy inputs. In this work, we propose MonoTransmotion (MT), a Transformer-based framework that uses only a monocular camera to jointly solve localization and prediction tasks. Our framework has two main modules: Bird's Eye View (BEV) localization and trajectory prediction. The BEV localization module estimates the position of a person using 2D human poses, enhanced by a novel directional loss for smoother sequential localizations. The trajectory prediction module predicts future motion from these estimates. We show that by jointly training both tasks with our unified framework, our method is more robust in real-world scenarios made of noisy inputs. We validate our MT network on both curated and non-curated datasets. On the curated dataset, MT achieves around 12% improvement over baseline models on BEV localization and trajectory prediction. On real-world non-curated dataset, experimental results indicate that MT maintains similar performance levels, highlighting its robustness and generalization capability. The code is available at https://github.com/vita-epfl/MonoTransmotion.",
        "arxiv_id": "2503.03535",
        "ARXIVID": "2503.03535",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for embodied AI tasks, focusing on joint localization and trajectory prediction using monocular vision.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2503.03190": {
        "authors": [
            "Jingzhou Luo",
            "Yang Liu",
            "Weixing Chen",
            "Zhen Li",
            "Yaowei Wang",
            "Guanbin Li",
            "Liang Lin"
        ],
        "title": "DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering",
        "abstract": "arXiv:2503.03190v1 Announce Type: new  Abstract: 3D Question Answering (3D QA) requires the model to comprehensively understand its situated 3D scene described by the text, then reason about its surrounding environment and answer a question under that situation. However, existing methods usually rely on global scene perception from pure 3D point clouds and overlook the importance of rich local texture details from multi-view images. Moreover, due to the inherent noise in camera poses and complex occlusions, there exists significant feature degradation and reduced feature robustness problems when aligning 3D point cloud with multi-view images. In this paper, we propose a Dual-vision Scene Perception Network (DSPNet), to comprehensively integrate multi-view and point cloud features to improve robustness in 3D QA. Our Text-guided Multi-view Fusion (TGMF) module prioritizes image views that closely match the semantic content of the text. To adaptively fuse back-projected multi-view images with point cloud features, we design the Adaptive Dual-vision Perception (ADVP) module, enhancing 3D scene comprehension. Additionally, our Multimodal Context-guided Reasoning (MCGR) module facilitates robust reasoning by integrating contextual information across visual and linguistic modalities. Experimental results on SQA3D and ScanQA datasets demonstrate the superiority of our DSPNet. Codes will be available at https://github.com/LZ-CH/DSPNet.",
        "arxiv_id": "2503.03190",
        "ARXIVID": "2503.03190",
        "COMMENT": "This paper aligns with Criterion 3 as it proposes a new method for 3D question answering with a focus on dual-vision scene perception, which is relevant to embodied AI and spatial understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.03708": {
        "authors": [
            "Nianzu Yang",
            "Pandeng Li",
            "Liming Zhao",
            "Yang Li",
            "Chen-Wei Xie",
            "Yehui Tang",
            "Xudong Lu",
            "Zhihang Liu",
            "Yun Zheng",
            "Yu Liu",
            "Junchi Yan"
        ],
        "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
        "abstract": "arXiv:2503.03708v1 Announce Type: new  Abstract: Video tokenizers, which transform videos into compact latent representations, are key to video generation. Existing video tokenizers are based on the VAE architecture and follow a paradigm where an encoder compresses videos into compact latents, and a deterministic decoder reconstructs the original videos from these latents. In this paper, we propose a novel \\underline{\\textbf{C}}onditioned \\underline{\\textbf{D}}iffusion-based video \\underline{\\textbf{T}}okenizer entitled \\textbf{\\ourmethod}, which departs from previous methods by replacing the deterministic decoder with a 3D causal diffusion model. The reverse diffusion generative process of the decoder is conditioned on the latent representations derived via the encoder. With a feature caching and sampling acceleration, the framework efficiently reconstructs high-fidelity videos of arbitrary lengths. Results show that {\\ourmethod} achieves state-of-the-art performance in video reconstruction tasks using just a single-step sampling. Even a smaller version of {\\ourmethod} still achieves reconstruction results on par with the top two baselines. Furthermore, the latent video generation model trained using {\\ourmethod} also shows superior performance.",
        "arxiv_id": "2503.03708",
        "ARXIVID": "2503.03708",
        "COMMENT": "Matches criterion 4 as it introduces a novel diffusion-based video tokenizer, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.03689": {
        "authors": [
            "Zhao Yang",
            "Zezhong Qian",
            "Xiaofan Li",
            "Weixiang Xu",
            "Gongpeng Zhao",
            "Ruohong Yu",
            "Lingsi Zhu",
            "Longjun Liu"
        ],
        "title": "DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with Reward Guidance",
        "abstract": "arXiv:2503.03689v1 Announce Type: new  Abstract: Accurate and high-fidelity driving scene reconstruction demands the effective utilization of comprehensive scene information as conditional inputs. Existing methods predominantly rely on 3D bounding boxes and BEV road maps for foreground and background control, which fail to capture the full complexity of driving scenes and adequately integrate multimodal information. In this work, we present DualDiff, a dual-branch conditional diffusion model designed to enhance driving scene generation across multiple views and video sequences. Specifically, we introduce Occupancy Ray-shape Sampling (ORS) as a conditional input, offering rich foreground and background semantics alongside 3D spatial geometry to precisely control the generation of both elements. To improve the synthesis of fine-grained foreground objects, particularly complex and distant ones, we propose a Foreground-Aware Mask (FGM) denoising loss function. Additionally, we develop the Semantic Fusion Attention (SFA) mechanism to dynamically prioritize relevant information and suppress noise, enabling more effective multimodal fusion. Finally, to ensure high-quality image-to-video generation, we introduce the Reward-Guided Diffusion (RGD) framework, which maintains global consistency and semantic coherence in generated videos. Extensive experiments demonstrate that DualDiff achieves state-of-the-art (SOTA) performance across multiple datasets. On the NuScenes dataset, DualDiff reduces the FID score by 4.09% compared to the best baseline. In downstream tasks, such as BEV segmentation, our method improves vehicle mIoU by 4.50% and road mIoU by 1.70%, while in BEV 3D object detection, the foreground mAP increases by 1.46%. Code will be made available at https://github.com/yangzhaojason/DualDiff.",
        "arxiv_id": "2503.03689",
        "ARXIVID": "2503.03689",
        "COMMENT": "Matches criterion 4 as it focuses on a dual-branch diffusion model for video generation, leveraging multimodal information.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.03751": {
        "authors": [
            "Xuanchi Ren",
            "Tianchang Shen",
            "Jiahui Huang",
            "Huan Ling",
            "Yifan Lu",
            "Merlin Nimier-David",
            "Thomas M\\\"uller",
            "Alexander Keller",
            "Sanja Fidler",
            "Jun Gao"
        ],
        "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
        "abstract": "arXiv:2503.03751v1 Announce Type: new  Abstract: We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if implemented at all, is imprecise, because camera parameters are mere inputs to the neural network which must then infer how the video depends on the camera. In contrast, GEN3C is guided by a 3D cache: point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames. When generating the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with the new camera trajectory provided by the user. Crucially, this means that GEN3C neither has to remember what it previously generated nor does it have to infer the image structure from the camera pose. The model, instead, can focus all its generative power on previously unobserved regions, as well as advancing the scene state to the next frame. Our results demonstrate more precise camera control than prior work, as well as state-of-the-art results in sparse-view novel view synthesis, even in challenging settings such as driving scenes and monocular dynamic video. Results are best viewed in videos. Check out our webpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
        "arxiv_id": "2503.03751",
        "ARXIVID": "2503.03751",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models and their application in video generation with 3D consistency.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.02897": {
        "authors": [
            "Hong Lu",
            "Yali Bian",
            "Rahul C. Shah"
        ],
        "title": "ClipGrader: Leveraging Vision-Language Models for Robust Label Quality Assessment in Object Detection",
        "abstract": "arXiv:2503.02897v1 Announce Type: new  Abstract: High-quality annotations are essential for object detection models, but ensuring label accuracy - especially for bounding boxes - remains both challenging and costly. This paper introduces ClipGrader, a novel approach that leverages vision-language models to automatically assess the accuracy of bounding box annotations. By adapting CLIP (Contrastive Language-Image Pre-training) to evaluate both class label correctness and spatial precision of bounding box, ClipGrader offers an effective solution for grading object detection labels. Tested on modified object detection datasets with artificially disturbed bounding boxes, ClipGrader achieves 91% accuracy on COCO with a 1.8% false positive rate. Moreover, it maintains 87% accuracy with a 2.1% false positive rate when trained on just 10% of the COCO data. ClipGrader also scales effectively to larger datasets such as LVIS, achieving 79% accuracy across 1,203 classes. Our experiments demonstrate ClipGrader's ability to identify errors in existing COCO annotations, highlighting its potential for dataset refinement. When integrated into a semi-supervised object detection (SSOD) model, ClipGrader readily improves the pseudo label quality, helping achieve higher mAP (mean Average Precision) throughout the training process. ClipGrader thus provides a scalable AI-assisted tool for enhancing annotation quality control and verifying annotations in large-scale object detection datasets.",
        "arxiv_id": "2503.02897",
        "ARXIVID": "2503.02897",
        "COMMENT": "This paper aligns with Criterion 2 as it leverages vision-language models (CLIP) for label quality assessment in object detection, which is a novel application of VLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.03196": {
        "authors": [
            "Zhiyuan Huang",
            "Ziming Cheng",
            "Junting Pan",
            "Zhaohui Hou",
            "Mingjie Zhan"
        ],
        "title": "SpiritSight Agent: Advanced GUI Agent with One Look",
        "abstract": "arXiv:2503.03196v1 Announce Type: new  Abstract: Graphical User Interface (GUI) agents show amazing abilities in assisting human-computer interaction, automating human user's navigation on digital devices. An ideal GUI agent is expected to achieve high accuracy, low latency, and compatibility for different GUI platforms. Recent vision-based approaches have shown promise by leveraging advanced Vision Language Models (VLMs). While they generally meet the requirements of compatibility and low latency, these vision-based GUI agents tend to have low accuracy due to their limitations in element grounding. To address this issue, we propose $\\textbf{SpiritSight}$, a vision-based, end-to-end GUI agent that excels in GUI navigation tasks across various GUI platforms. First, we create a multi-level, large-scale, high-quality GUI dataset called $\\textbf{GUI-Lasagne}$ using scalable methods, empowering SpiritSight with robust GUI understanding and grounding capabilities. Second, we introduce the $\\textbf{Universal Block Parsing (UBP)}$ method to resolve the ambiguity problem in dynamic high-resolution of visual inputs, further enhancing SpiritSight's ability to ground GUI objects. Through these efforts, SpiritSight agent outperforms other advanced methods on diverse GUI benchmarks, demonstrating its superior capability and compatibility in GUI navigation tasks. Models are available at $\\href{https://huggingface.co/SenseLLM/SpiritSight-Agent-8B}{this\\ URL}$.",
        "arxiv_id": "2503.03196",
        "ARXIVID": "2503.03196",
        "COMMENT": "Matches criterion 2 as it discusses a vision-based GUI agent leveraging advanced vision language models (VLMs).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.03215": {
        "authors": [
            "Wentao Li",
            "Congcong Wang",
            "Xiaoxiao Cui",
            "Zhi Liu",
            "Wei Guo",
            "Lizhen Cui"
        ],
        "title": "COSINT-Agent: A Knowledge-Driven Multimodal Agent for Chinese Open Source Intelligence",
        "abstract": "arXiv:2503.03215v1 Announce Type: new  Abstract: Open Source Intelligence (OSINT) requires the integration and reasoning of diverse multimodal data, presenting significant challenges in deriving actionable insights. Traditional approaches, including multimodal large language models (MLLMs), often struggle to infer complex contextual relationships or deliver comprehensive intelligence from unstructured data sources. In this paper, we introduce COSINT-Agent, a knowledge-driven multimodal agent tailored to address the challenges of OSINT in the Chinese domain. COSINT-Agent seamlessly integrates the perceptual capabilities of fine-tuned MLLMs with the structured reasoning power of the Entity-Event-Scene Knowledge Graph (EES-KG). Central to COSINT-Agent is the innovative EES-Match framework, which bridges COSINT-MLLM and EES-KG, enabling systematic extraction, reasoning, and contextualization of multimodal insights. This integration facilitates precise entity recognition, event interpretation, and context retrieval, effectively transforming raw multimodal data into actionable intelligence. Extensive experiments validate the superior performance of COSINT-Agent across core OSINT tasks, including entity recognition, EES generation, and context matching. These results underscore its potential as a robust and scalable solution for advancing automated multimodal reasoning and enhancing the effectiveness of OSINT methodologies.",
        "arxiv_id": "2503.03215",
        "ARXIVID": "2503.03215",
        "COMMENT": "Matches criterion 2 as it introduces a multimodal large language model (MLLM) tailored for OSINT tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.03299": {
        "authors": [
            "Julia Hindel",
            "Rohit Mohan",
            "Jelena Bratuli\\`c",
            "Daniele Cattaneo",
            "Thomas Brox",
            "Abhinav Valada"
        ],
        "title": "Label-Efficient LiDAR Semantic Segmentation with 2D-3D Vision Transformer Adapters",
        "abstract": "arXiv:2503.03299v1 Announce Type: new  Abstract: LiDAR semantic segmentation models are typically trained from random initialization as universal pre-training is hindered by the lack of large, diverse datasets. Moreover, most point cloud segmentation architectures incorporate custom network layers, limiting the transferability of advances from vision-based architectures. Inspired by recent advances in universal foundation models, we propose BALViT, a novel approach that leverages frozen vision models as amodal feature encoders for learning strong LiDAR encoders. Specifically, BALViT incorporates both range-view and bird's-eye-view LiDAR encoding mechanisms, which we combine through a novel 2D-3D adapter. While the range-view features are processed through a frozen image backbone, our bird's-eye-view branch enhances them through multiple cross-attention interactions. Thereby, we continuously improve the vision network with domain-dependent knowledge, resulting in a strong label-efficient LiDAR encoding mechanism. Extensive evaluations of BALViT on the SemanticKITTI and nuScenes benchmarks demonstrate that it outperforms state-of-the-art methods on small data regimes. We make the code and models publicly available at: http://balvit.cs.uni-freiburg.de.",
        "arxiv_id": "2503.03299",
        "ARXIVID": "2503.03299",
        "COMMENT": "Matches criterion 4 as it discusses leveraging vision foundation models for LiDAR semantic segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.03280": {
        "authors": [
            "Hiep Truong Cong",
            "Ajay Kumar Sigatapu",
            "Arindam Das",
            "Yashwanth Sharma",
            "Venkatesh Satagopan",
            "Ganesh Sistu",
            "Ciaran Eising"
        ],
        "title": "BEVMOSNet: Multimodal Fusion for BEV Moving Object Segmentation",
        "abstract": "arXiv:2503.03280v1 Announce Type: new  Abstract: Accurate motion understanding of the dynamic objects within the scene in bird's-eye-view (BEV) is critical to ensure a reliable obstacle avoidance system and smooth path planning for autonomous vehicles. However, this task has received relatively limited exploration when compared to object detection and segmentation with only a few recent vision-based approaches presenting preliminary findings that significantly deteriorate in low-light, nighttime, and adverse weather conditions such as rain. Conversely, LiDAR and radar sensors remain almost unaffected in these scenarios, and radar provides key velocity information of the objects. Therefore, we introduce BEVMOSNet, to our knowledge, the first end-to-end multimodal fusion leveraging cameras, LiDAR, and radar to precisely predict the moving objects in BEV. In addition, we perform a deeper analysis to find out the optimal strategy for deformable cross-attention-guided sensor fusion for cross-sensor knowledge sharing in BEV. While evaluating BEVMOSNet on the nuScenes dataset, we show an overall improvement in IoU score of 36.59% compared to the vision-based unimodal baseline BEV-MoSeg (Sigatapu et al., 2023), and 2.35% compared to the multimodel SimpleBEV (Harley et al., 2022), extended for the motion segmentation task, establishing this method as the state-of-the-art in BEV motion segmentation.",
        "arxiv_id": "2503.03280",
        "ARXIVID": "2503.03280",
        "COMMENT": "Matches criterion 3 as it introduces a novel multimodal fusion method for BEV motion segmentation, which is relevant to embodied AI benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.03278": {
        "authors": [
            "Jun Li",
            "Che Liu",
            "Wenjia Bai",
            "Rossella Arcucci",
            "Cosmin I. Bercea",
            "Julia A. Schnabel"
        ],
        "title": "Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions",
        "abstract": "arXiv:2503.03278v1 Announce Type: new  Abstract: Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks. However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains underexplored. A major challenge is the complex and abstract nature of medical terminology, which makes it difficult to directly associate pathological anomaly terms with their corresponding visual features. In this work, we introduce a novel approach to enhance VLM performance in medical abnormality detection and localization by leveraging decomposed medical knowledge. Instead of directly prompting models to recognize specific abnormalities, we focus on breaking down medical concepts into fundamental attributes and common visual patterns. This strategy promotes a stronger alignment between textual descriptions and visual features, improving both the recognition and localization of abnormalities in medical images.We evaluate our method on the 0.23B Florence-2 base model and demonstrate that it achieves comparable performance in abnormality grounding to significantly larger 7B LLaVA-based medical VLMs, despite being trained on only 1.5% of the data used for such models. Experimental results also demonstrate the effectiveness of our approach in both known and previously unseen abnormalities, suggesting its strong generalization capabilities.",
        "arxiv_id": "2503.03278",
        "ARXIVID": "2503.03278",
        "COMMENT": "Matches criterion 2 as it discusses enhancing visual language models (VLMs) for medical abnormality detection.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.03743": {
        "authors": [
            "Yuqi Zhou",
            "Shuai Wang",
            "Sunhao Dai",
            "Qinglin Jia",
            "Zhaocheng Du",
            "Zhenhua Dong",
            "Jun Xu"
        ],
        "title": "CHOP: Mobile Operating Assistant with Constrained High-frequency Optimized Subtask Planning",
        "abstract": "arXiv:2503.03743v1 Announce Type: new  Abstract: The advancement of visual language models (VLMs) has enhanced mobile device operations, allowing simulated human-like actions to address user requirements. Current VLM-based mobile operating assistants can be structured into three levels: task, subtask, and action. The subtask level, linking high-level goals with low-level executable actions, is crucial for task completion but faces two challenges: ineffective subtasks that lower-level agent cannot execute and inefficient subtasks that fail to contribute to the completion of the higher-level task. These challenges stem from VLM's lack of experience in decomposing subtasks within GUI scenarios in multi-agent architecture. To address these, we propose a new mobile assistant architecture with constrained high-frequency o}ptimized planning (CHOP). Our approach overcomes the VLM's deficiency in GUI scenarios planning by using human-planned subtasks as the basis vector. We evaluate our architecture in both English and Chinese contexts across 20 Apps, demonstrating significant improvements in both effectiveness and efficiency. Our dataset and code is available at https://github.com/Yuqi-Zhou/CHOP",
        "arxiv_id": "2503.03743",
        "ARXIVID": "2503.03743",
        "COMMENT": "Matches criterion 3 as it introduces a new architecture for mobile operating assistants with a novel planning approach.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.03321": {
        "authors": [
            "Seil Kang",
            "Jinyeong Kim",
            "Junhyeok Kim",
            "Seong Jae Hwang"
        ],
        "title": "See What You Are Told: Visual Attention Sink in Large Multimodal Models",
        "abstract": "arXiv:2503.03321v1 Announce Type: new  Abstract: Large multimodal models (LMMs) \"see\" images by leveraging the attention mechanism between text and visual tokens in the transformer decoder. Ideally, these models should focus on key visual information relevant to the text token. However, recent findings indicate that LMMs have an extraordinary tendency to consistently allocate high attention weights to specific visual tokens, even when these tokens are irrelevant to the corresponding text. In this study, we investigate the property behind the appearance of these irrelevant visual tokens and examine their characteristics. Our findings show that this behavior arises due to the massive activation of certain hidden state dimensions, which resembles the attention sink found in language models. Hence, we refer to this phenomenon as the visual attention sink. In particular, our analysis reveals that removing the irrelevant visual sink tokens does not impact model performance, despite receiving high attention weights. Consequently, we recycle the attention to these tokens as surplus resources, redistributing the attention budget to enhance focus on the image. To achieve this, we introduce Visual Attention Redistribution (VAR), a method that redistributes attention in image-centric heads, which we identify as innately focusing on visual information. VAR can be seamlessly applied across different LMMs to improve performance on a wide range of tasks, including general vision-language tasks, visual hallucination tasks, and vision-centric tasks, all without the need for additional training, models, or inference steps. Experimental results demonstrate that VAR enables LMMs to process visual information more effectively by adjusting their internal attention mechanisms, offering a new direction to enhancing the multimodal capabilities of LMMs.",
        "arxiv_id": "2503.03321",
        "ARXIVID": "2503.03321",
        "COMMENT": "Matches criterion 2 as it investigates and improves attention mechanisms in large multimodal models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.03556": {
        "authors": [
            "Xiaomeng Zhu",
            "Yuyang Li",
            "Leiyao Cui",
            "Pengfei Li",
            "Huan-ang Gao",
            "Yixin Zhu",
            "Hao Zhao"
        ],
        "title": "Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented Manipulation",
        "abstract": "arXiv:2503.03556v1 Announce Type: new  Abstract: Object affordance reasoning, the ability to infer object functionalities based on physical properties, is fundamental for task-oriented planning and activities in both humans and Artificial Intelligence (AI). This capability, required for planning and executing daily activities in a task-oriented manner, relies on commonsense knowledge of object physics and functionalities, extending beyond simple object recognition. Current computational models for affordance reasoning from perception lack generalizability, limiting their applicability in novel scenarios. Meanwhile, comprehensive Large Language Models (LLMs) with emerging reasoning capabilities are challenging to deploy on local devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a large-scale dataset comprising 1,496 tasks and 119k images, designed to enhance the generalizability of affordance reasoning from perception. Utilizing this dataset, we develop Afford-X, an end-to-end trainable affordance reasoning model that incorporates Verb Attention and Bi-Fusion modules to improve multi-modal understanding. This model achieves up to a 12.1% performance improvement over the best-reported results from non-LLM methods, while also demonstrating a 1.2% enhancement compared to our previous conference paper. Additionally, it maintains a compact 187M parameter size and infers nearly 50 times faster than the GPT-4V API. Our work demonstrates the potential for efficient, generalizable affordance reasoning models that can be deployed on local devices for task-oriented manipulations. We showcase Afford-X's effectiveness in enabling task-oriented manipulations for robots across various tasks and environments, underscoring its efficiency and broad implications for advancing robotics and AI systems in real-world applications.",
        "arxiv_id": "2503.03556",
        "ARXIVID": "2503.03556",
        "COMMENT": "Matches criterion 1 as it introduces a new method for affordance reasoning, improving spatial understanding in embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.02950": {
        "authors": [
            "Danqing Zhang",
            "Balaji Rama",
            "Jingyi Ni",
            "Shiying He",
            "Fu Zhao",
            "Kunyu Chen",
            "Arnold Chen",
            "Junyu Cao"
        ],
        "title": "LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications",
        "abstract": "arXiv:2503.02950v1 Announce Type: new  Abstract: We introduce LiteWebAgent, an open-source suite for VLM-based web agent applications. Our framework addresses a critical gap in the web agent ecosystem with a production-ready solution that combines minimal serverless backend configuration, intuitive user and browser interfaces, and extensible research capabilities in agent planning, memory, and tree search. For the core LiteWebAgent agent framework, we implemented a simple yet effective baseline using recursive function calling, providing with decoupled action generation and action grounding. In addition, we integrate advanced research components such as agent planning, agent workflow memory, and tree search in a modular and extensible manner. We then integrate the LiteWebAgent agent framework with frontend and backend as deployed systems in two formats: (1) a production Vercel-based web application, which provides users with an agent-controlled remote browser, (2) a Chrome extension leveraging LiteWebAgent's API to control an existing Chrome browser via CDP (Chrome DevTools Protocol). The LiteWebAgent framework is available at https://github.com/PathOnAI/LiteWebAgent, with deployed frontend at https://lite-web-agent.vercel.app/.",
        "arxiv_id": "2503.02950",
        "ARXIVID": "2503.02950",
        "COMMENT": "Matches criterion 2 as it introduces a VLM-based framework for web-agent applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.03132": {
        "authors": [
            "Awais Nizamani",
            "Hamid Laga",
            "Guanjin Wang",
            "Farid Boussaid",
            "Mohammed Bennamoun",
            "Anuj Srivastava"
        ],
        "title": "Dynamic Neural Surfaces for Elastic 4D Shape Representation and Analysis",
        "abstract": "arXiv:2503.03132v1 Announce Type: new  Abstract: We propose a novel framework for the statistical analysis of genus-zero 4D surfaces, i.e., 3D surfaces that deform and evolve over time. This problem is particularly challenging due to the arbitrary parameterizations of these surfaces and their varying deformation speeds, necessitating effective spatiotemporal registration. Traditionally, 4D surfaces are discretized, in space and time, before computing their spatiotemporal registrations, geodesics, and statistics. However, this approach may result in suboptimal solutions and, as we demonstrate in this paper, is not necessary. In contrast, we treat 4D surfaces as continuous functions in both space and time. We introduce Dynamic Spherical Neural Surfaces (D-SNS), an efficient smooth and continuous spatiotemporal representation for genus-0 4D surfaces. We then demonstrate how to perform core 4D shape analysis tasks such as spatiotemporal registration, geodesics computation, and mean 4D shape estimation, directly on these continuous representations without upfront discretization and meshing. By integrating neural representations with classical Riemannian geometry and statistical shape analysis techniques, we provide the building blocks for enabling full functional shape analysis. We demonstrate the efficiency of the framework on 4D human and face datasets. The source code and additional results are available at https://4d-dsns.github.io/DSNS/.",
        "arxiv_id": "2503.03132",
        "ARXIVID": "2503.03132",
        "COMMENT": "This paper does not match any specific criteria but introduces a novel framework for 4D shape analysis, which might be tangentially interesting for vision applications.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.03329": {
        "authors": [
            "Yiqiong Yang",
            "Yitian Yuan",
            "Baoxing Ren",
            "Ye Wu",
            "Yanqiu Feng",
            "Xinyuan Zhang"
        ],
        "title": "Deep Learning-Based Diffusion MRI Tractography: Integrating Spatial and Anatomical Information",
        "abstract": "arXiv:2503.03329v1 Announce Type: new  Abstract: Diffusion MRI tractography technique enables non-invasive visualization of the white matter pathways in the brain. It plays a crucial role in neuroscience and clinical fields by facilitating the study of brain connectivity and neurological disorders. However, the accuracy of reconstructed tractograms has been a longstanding challenge. Recently, deep learning methods have been applied to improve tractograms for better white matter coverage, but often comes at the expense of generating excessive false-positive connections. This is largely due to their reliance on local information to predict long range streamlines. To improve the accuracy of streamline propagation predictions, we introduce a novel deep learning framework that integrates image-domain spatial information and anatomical information along tracts, with the former extracted through convolutional layers and the later modeled via a Transformer-decoder. Additionally, we employ a weighted loss function to address fiber class imbalance encountered during training. We evaluate the proposed method on the simulated ISMRM 2015 Tractography Challenge dataset, achieving a valid streamline rate of 66.2%, white matter coverage of 63.8%, and successfully reconstructing 24 out of 25 bundles. Furthermore, on the multi-site Tractoinferno dataset, the proposed method demonstrates its ability to handle various diffusion MRI acquisition schemes, achieving a 5.7% increase in white matter coverage and a 4.1% decrease in overreach compared to RNN-based methods.",
        "arxiv_id": "2503.03329",
        "ARXIVID": "2503.03329",
        "COMMENT": "This paper partially aligns with Criterion 1 as it integrates spatial and anatomical information for diffusion MRI tractography, which involves spatial understanding, but it is not directly related to embodied agents.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.03128": {
        "authors": [
            "Chenhui Xu",
            "Dancheng Liu",
            "Jiajie Li",
            "Amir Nassereldine",
            "Zhaohui Li",
            "Jinjun Xiong"
        ],
        "title": "Towards Understanding Multi-Round Large Language Model Reasoning: Approximability, Learnability and Generalizability",
        "abstract": "arXiv:2503.03128v1 Announce Type: new  Abstract: Recent advancements in cognitive science and multi-round reasoning techniques for Large Language Models (LLMs) suggest that iterative thinking processes improve problem-solving performance in complex tasks. Inspired by this, approaches like Chain-of-Thought, debating, and self-refinement have been applied to auto-regressive LLMs, achieving significant successes in tasks such as mathematical reasoning, commonsense reasoning, and multi-hop question answering. Despite these successes, the theoretical basis for how multi-round reasoning enhances problem-solving abilities remains underexplored. In this work, we investigate the approximation, learnability, and generalization properties of multi-round auto-regressive models. We show that Transformers with finite context windows are universal approximators for steps of Turing-computable functions and can approximate any Turing-computable sequence-to-sequence function through multi-round reasoning. We extend PAC learning to sequence generation and demonstrate that multi-round generation is learnable even when the sequence length exceeds the model's context window. Finally, we examine how generalization error propagates across rounds, and show how the aforementioned approaches can help constrain this error, ensuring outputs stay within an expectation boundary. This work sheds light on the systemic theoretical foundations of multi-round sequence learning and reasoning, emphasizing its role in inference complexity.",
        "arxiv_id": "2503.03128",
        "ARXIVID": "2503.03128",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.03270": {
        "authors": [
            "Beilin Chu",
            "Xuan Xu",
            "Yufei Zhang",
            "Weike You",
            "Linna Zhou"
        ],
        "title": "Reduced Spatial Dependency for More General Video-level Deepfake Detection",
        "abstract": "arXiv:2503.03270v1 Announce Type: new  Abstract: As one of the prominent AI-generated content, Deepfake has raised significant safety concerns. Although it has been demonstrated that temporal consistency cues offer better generalization capability, existing methods based on CNNs inevitably introduce spatial bias, which hinders the extraction of intrinsic temporal features. To address this issue, we propose a novel method called Spatial Dependency Reduction (SDR), which integrates common temporal consistency features from multiple spatially-perturbed clusters, to reduce the dependency of the model on spatial information. Specifically, we design multiple Spatial Perturbation Branch (SPB) to construct spatially-perturbed feature clusters. Subsequently, we utilize the theory of mutual information and propose a Task-Relevant Feature Integration (TRFI) module to capture temporal features residing in similar latent space from these clusters. Finally, the integrated feature is fed into a temporal transformer to capture long-range dependencies. Extensive benchmarks and ablation studies demonstrate the effectiveness and rationale of our approach.",
        "arxiv_id": "2503.03270",
        "ARXIVID": "2503.03270",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.03222": {
        "authors": [
            "Zhumei Wang",
            "Zechen Hu",
            "Ruoxi Guo",
            "Huaijin Pi",
            "Ziyong Feng",
            "Sida Peng",
            "Xiaowei Zhou"
        ],
        "title": "Mocap-2-to-3: Lifting 2D Diffusion-Based Pretrained Models for 3D Motion Capture",
        "abstract": "arXiv:2503.03222v1 Announce Type: new  Abstract: Recovering absolute poses in the world coordinate system from monocular views presents significant challenges. Two primary issues arise in this context. Firstly, existing methods rely on 3D motion data for training, which requires collection in limited environments. Acquiring such 3D labels for new actions in a timely manner is impractical, severely restricting the model's generalization capabilities. In contrast, 2D poses are far more accessible and easier to obtain. Secondly, estimating a person's absolute position in metric space from a single viewpoint is inherently more complex. To address these challenges, we introduce Mocap-2-to-3, a novel framework that decomposes intricate 3D motions into 2D poses, leveraging 2D data to enhance 3D motion reconstruction in diverse scenarios and accurately predict absolute positions in the world coordinate system. We initially pretrain a single-view diffusion model with extensive 2D data, followed by fine-tuning a multi-view diffusion model for view consistency using publicly available 3D data. This strategy facilitates the effective use of large-scale 2D data. Additionally, we propose an innovative human motion representation that decouples local actions from global movements and encodes geometric priors of the ground, ensuring the generative model learns accurate motion priors from 2D data. During inference, this allows for the gradual recovery of global movements, resulting in more plausible positioning. We evaluate our model's performance on real-world datasets, demonstrating superior accuracy in motion and absolute human positioning compared to state-of-the-art methods, along with enhanced generalization and scalability. Our code will be made publicly available.",
        "arxiv_id": "2503.03222",
        "ARXIVID": "2503.03222",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and motion capture, which aligns with general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.03430": {
        "authors": [
            "Junhao Xu",
            "Yanan Zhang",
            "Zhi Cai",
            "Di Huang"
        ],
        "title": "CoSDH: Communication-Efficient Collaborative Perception via Supply-Demand Awareness and Intermediate-Late Hybridization",
        "abstract": "arXiv:2503.03430v1 Announce Type: new  Abstract: Multi-agent collaborative perception enhances perceptual capabilities by utilizing information from multiple agents and is considered a fundamental solution to the problem of weak single-vehicle perception in autonomous driving. However, existing collaborative perception methods face a dilemma between communication efficiency and perception accuracy. To address this issue, we propose a novel communication-efficient collaborative perception framework based on supply-demand awareness and intermediate-late hybridization, dubbed as \\mymethodname. By modeling the supply-demand relationship between agents, the framework refines the selection of collaboration regions, reducing unnecessary communication cost while maintaining accuracy. In addition, we innovatively introduce the intermediate-late hybrid collaboration mode, where late-stage collaboration compensates for the performance degradation in collaborative perception under low communication bandwidth. Extensive experiments on multiple datasets, including both simulated and real-world scenarios, demonstrate that \\mymethodname~ achieves state-of-the-art detection accuracy and optimal bandwidth trade-offs, delivering superior detection precision under real communication bandwidths, thus proving its effectiveness and practical applicability. The code will be released at https://github.com/Xu2729/CoSDH.",
        "arxiv_id": "2503.03430",
        "ARXIVID": "2503.03430",
        "COMMENT": "This paper does not match any specific criteria but discusses collaborative perception in autonomous driving, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.03137": {
        "authors": [
            "Changliang Zhou",
            "Xi Lin",
            "Zhenkun Wang",
            "Qingfu Zhang"
        ],
        "title": "L2R: Learning to Reduce Search Space for Generalizable Neural Routing Solver",
        "abstract": "arXiv:2503.03137v1 Announce Type: new  Abstract: Constructive neural combinatorial optimization (NCO) has attracted growing research attention due to its ability to solve complex routing problems without relying on handcrafted rules. However, existing NCO methods face significant challenges in generalizing to large-scale problems due to high computational complexity and inefficient capture of structural patterns. To address this issue, we propose a novel learning-based search space reduction method that adaptively selects a small set of promising candidate nodes at each step of the constructive NCO process. Unlike traditional methods that rely on fixed heuristics, our selection model dynamically prioritizes nodes based on learned patterns, significantly reducing the search space while maintaining solution quality. Experimental results demonstrate that our method, trained solely on 100-node instances from uniform distribution, generalizes remarkably well to large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) instances with up to 1 million nodes from the uniform distribution and over 80K nodes from other distributions.",
        "arxiv_id": "2503.03137",
        "ARXIVID": "2503.03137",
        "COMMENT": "This paper does not match any specific criteria but discusses neural combinatorial optimization, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.03115": {
        "authors": [
            "Kun Yang",
            "Yuxiang Liu",
            "Zeyu Cui",
            "Yu Liu",
            "Maojun Zhang",
            "Shen Yan",
            "Qing Wang"
        ],
        "title": "NTR-Gaussian: Nighttime Dynamic Thermal Reconstruction with 4D Gaussian Splatting Based on Thermodynamics",
        "abstract": "arXiv:2503.03115v1 Announce Type: new  Abstract: Thermal infrared imaging offers the advantage of all-weather capability, enabling non-intrusive measurement of an object's surface temperature. Consequently, thermal infrared images are employed to reconstruct 3D models that accurately reflect the temperature distribution of a scene, aiding in applications such as building monitoring and energy management. However, existing approaches predominantly focus on static 3D reconstruction for a single time period, overlooking the impact of environmental factors on thermal radiation and failing to predict or analyze temperature variations over time. To address these challenges, we propose the NTR-Gaussian method, which treats temperature as a form of thermal radiation, incorporating elements like convective heat transfer and radiative heat dissipation. Our approach utilizes neural networks to predict thermodynamic parameters such as emissivity, convective heat transfer coefficient, and heat capacity. By integrating these predictions, we can accurately forecast thermal temperatures at various times throughout a nighttime scene. Furthermore, we introduce a dynamic dataset specifically for nighttime thermal imagery. Extensive experiments and evaluations demonstrate that NTR-Gaussian significantly outperforms comparison methods in thermal reconstruction, achieving a predicted temperature error within 1 degree Celsius.",
        "arxiv_id": "2503.03115",
        "ARXIVID": "2503.03115",
        "COMMENT": "This paper does not match any specific criteria but discusses thermal imaging and reconstruction, which might be tangentially interesting for vision applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.03528": {
        "authors": [
            "Qiqi Guo",
            "Zhuowen Zheng",
            "Guanghua Yang",
            "Zhiquan Liu",
            "Xiaofan Li",
            "Jianqing Li",
            "Jinyu Tian",
            "Xueyuan Gong"
        ],
        "title": "AdaSin: Enhancing Hard Sample Metrics with Dual Adaptive Penalty for Face Recognition",
        "abstract": "arXiv:2503.03528v1 Announce Type: new  Abstract: In recent years, the emergence of deep convolutional neural networks has positioned face recognition as a prominent research focus in computer vision. Traditional loss functions, such as margin-based, hard-sample mining-based, and hybrid approaches, have achieved notable performance improvements, with some leveraging curriculum learning to optimize training. However, these methods often fall short in effectively quantifying the difficulty of hard samples. To address this, we propose Adaptive Sine (AdaSin) loss function, which introduces the sine of the angle between a sample's embedding feature and its ground-truth class center as a novel difficulty metric. This metric enables precise and effective penalization of hard samples. By incorporating curriculum learning, the model dynamically adjusts classification boundaries across different training stages. Unlike previous adaptive-margin loss functions, AdaSin introduce a dual adaptive penalty, applied to both the positive and negative cosine similarities of hard samples. This design imposes stronger constraints, enhancing intra-class compactness and inter-class separability. The combination of the dual adaptive penalty and curriculum learning is guided by a well-designed difficulty metric. It enables the model to focus more effectively on hard samples in later training stages, and lead to the extraction of highly discriminative face features. Extensive experiments across eight benchmarks demonstrate that AdaSin achieves superior accuracy compared to other state-of-the-art methods.",
        "arxiv_id": "2503.03528",
        "ARXIVID": "2503.03528",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.03453": {
        "authors": [
            "Patryk Rygiel",
            "Julian Suk",
            "Kak Khee Yeung",
            "Christoph Brune",
            "Jelmer M. Wolterink"
        ],
        "title": "Active Learning for Deep Learning-Based Hemodynamic Parameter Estimation",
        "abstract": "arXiv:2503.03453v1 Announce Type: new  Abstract: Hemodynamic parameters such as pressure and wall shear stress play an important role in diagnosis, prognosis, and treatment planning in cardiovascular diseases. These parameters can be accurately computed using computational fluid dynamics (CFD), but CFD is computationally intensive. Hence, deep learning methods have been adopted as a surrogate to rapidly estimate CFD outcomes. A drawback of such data-driven models is the need for time-consuming reference CFD simulations for training. In this work, we introduce an active learning framework to reduce the number of CFD simulations required for the training of surrogate models, lowering the barriers to their deployment in new applications. We propose three distinct querying strategies to determine for which unlabeled samples CFD simulations should be obtained. These querying strategies are based on geometrical variance, ensemble uncertainty, and adherence to the physics governing fluid dynamics. We benchmark these methods on velocity field estimation in synthetic coronary artery bifurcations and find that they allow for substantial reductions in annotation cost. Notably, we find that our strategies reduce the number of samples required by up to 50% and make the trained models more robust to difficult cases. Our results show that active learning is a feasible strategy to increase the potential of deep learning-based CFD surrogates.",
        "arxiv_id": "2503.03453",
        "ARXIVID": "2503.03453",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.03446": {
        "authors": [
            "Iris Dominguez-Catena",
            "Daniel Paternain",
            "Mikel Galar",
            "MaryBeth Defrance",
            "Maarten Buyl",
            "Tijl De Bie"
        ],
        "title": "Biased Heritage: How Datasets Shape Models in Facial Expression Recognition",
        "abstract": "arXiv:2503.03446v1 Announce Type: new  Abstract: In recent years, the rapid development of artificial intelligence (AI) systems has raised concerns about our ability to ensure their fairness, that is, how to avoid discrimination based on protected characteristics such as gender, race, or age. While algorithmic fairness is well-studied in simple binary classification tasks on tabular data, its application to complex, real-world scenarios-such as Facial Expression Recognition (FER)-remains underexplored. FER presents unique challenges: it is inherently multiclass, and biases emerge across intersecting demographic variables, each potentially comprising multiple protected groups. We present a comprehensive framework to analyze bias propagation from datasets to trained models in image-based FER systems, while introducing new bias metrics specifically designed for multiclass problems with multiple demographic groups. Our methodology studies bias propagation by (1) inducing controlled biases in FER datasets, (2) training models on these biased datasets, and (3) analyzing the correlation between dataset bias metrics and model fairness notions. Our findings reveal that stereotypical biases propagate more strongly to model predictions than representational biases, suggesting that preventing emotion-specific demographic patterns should be prioritized over general demographic balance in FER datasets. Additionally, we observe that biased datasets lead to reduced model accuracy, challenging the assumed fairness-accuracy trade-off.",
        "arxiv_id": "2503.03446",
        "ARXIVID": "2503.03446",
        "COMMENT": "Does not match any specific criteria but is related to fairness in computer vision, which aligns with general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.03655": {
        "authors": [
            "Thomas P\\\"ollabauer",
            "Michael Gasser",
            "Tristan Wirth",
            "Sarah Berkei",
            "Volker Knauthe",
            "Arjan Kuijper"
        ],
        "title": "Improving 6D Object Pose Estimation of metallic Household and Industry Objects",
        "abstract": "arXiv:2503.03655v1 Announce Type: new  Abstract: 6D object pose estimation suffers from reduced accuracy when applied to metallic objects. We set out to improve the state-of-the-art by addressing challenges such as reflections and specular highlights in industrial applications. Our novel BOP-compatible dataset, featuring a diverse set of metallic objects (cans, household, and industrial items) under various lighting and background conditions, provides additional geometric and visual cues. We demonstrate that these cues can be effectively leveraged to enhance overall performance. To illustrate the usefulness of the additional features, we improve upon the GDRNPP algorithm by introducing an additional keypoint prediction and material estimator head in order to improve spatial scene understanding. Evaluations on the new dataset show improved accuracy for metallic objects, supporting the hypothesis that additional geometric and visual cues can improve learning.",
        "arxiv_id": "2503.03655",
        "ARXIVID": "2503.03655",
        "COMMENT": "Does not match any specific criteria but is related to improving spatial understanding in object pose estimation.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.03365": {
        "authors": [
            "Juan Miguel Valverde",
            "Motoya Koga",
            "Nijihiko Otsuka",
            "Anders Bjorholm Dahl"
        ],
        "title": "TopoMortar: A dataset to evaluate image segmentation methods focused on topology accuracy",
        "abstract": "arXiv:2503.03365v1 Announce Type: new  Abstract: We present TopoMortar, a brick wall dataset that is the first dataset specifically designed to evaluate topology-focused image segmentation methods, such as topology loss functions. TopoMortar enables to investigate in two ways whether methods incorporate prior topological knowledge. First, by eliminating challenges seen in real-world data, such as small training set, noisy labels, and out-of-distribution test-set images, that, as we show, impact the effectiveness of topology losses. Second, by allowing to assess in the same dataset topology accuracy across dataset challenges, isolating dataset-related effects from the effect of incorporating prior topological knowledge. In these two experiments, it is deliberately difficult to improve topology accuracy without actually using topology information, thus, permitting to attribute an improvement in topology accuracy to the incorporation of prior topological knowledge. To this end, TopoMortar includes three types of labels (accurate, noisy, pseudo-labels), two fixed training sets (large and small), and in-distribution and out-of-distribution test-set images. We compared eight loss functions on TopoMortar, and we found that clDice achieved the most topologically accurate segmentations, Skeleton Recall loss performed best particularly with noisy labels, and the relative advantageousness of the other loss functions depended on the experimental setting. Additionally, we show that simple methods, such as data augmentation and self-distillation, can elevate Cross entropy Dice loss to surpass most topology loss functions, and that those simple methods can enhance topology loss functions as well. clDice and Skeleton Recall loss, both skeletonization-based loss functions, were also the fastest to train, making this type of loss function a promising research direction. TopoMortar and our code can be found at https://github.com/jmlipman/TopoMortar",
        "arxiv_id": "2503.03365",
        "ARXIVID": "2503.03365",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and dataset creation.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.03042": {
        "authors": [
            "Yan Han",
            "Soumava Kumar Roy",
            "Mehrtash Harandi",
            "Lars Petersson"
        ],
        "title": "Learning from Noisy Labels with Contrastive Co-Transformer",
        "abstract": "arXiv:2503.03042v1 Announce Type: new  Abstract: Deep learning with noisy labels is an interesting challenge in weakly supervised learning. Despite their significant learning capacity, CNNs have a tendency to overfit in the presence of samples with noisy labels. Alleviating this issue, the well known Co-Training framework is used as a fundamental basis for our work. In this paper, we introduce a Contrastive Co-Transformer framework, which is simple and fast, yet able to improve the performance by a large margin compared to the state-of-the-art approaches. We argue the robustness of transformers when dealing with label noise. Our Contrastive Co-Transformer approach is able to utilize all samples in the dataset, irrespective of whether they are clean or noisy. Transformers are trained by a combination of contrastive loss and classification loss. Extensive experimental results on corrupted data from six standard benchmark datasets including Clothing1M, demonstrate that our Contrastive Co-Transformer is superior to existing state-of-the-art methods.",
        "arxiv_id": "2503.03042",
        "ARXIVID": "2503.03042",
        "COMMENT": "This paper does not match any specific criteria but discusses learning with noisy labels, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.03519": {
        "authors": [
            "Shunxin Wang",
            "Raymond Veldhuis",
            "Nicola Strisciuglio"
        ],
        "title": "Do ImageNet-trained models learn shortcuts? The impact of frequency shortcuts on generalization",
        "abstract": "arXiv:2503.03519v1 Announce Type: new  Abstract: Frequency shortcuts refer to specific frequency patterns that models heavily rely on for correct classification. Previous studies have shown that models trained on small image datasets often exploit such shortcuts, potentially impairing their generalization performance. However, existing methods for identifying frequency shortcuts require expensive computations and become impractical for analyzing models trained on large datasets. In this work, we propose the first approach to more efficiently analyze frequency shortcuts at a larger scale. We show that both CNN and transformer models learn frequency shortcuts on ImageNet. We also expose that frequency shortcut solutions can yield good performance on out-of-distribution (OOD) test sets which largely retain texture information. However, these shortcuts, mostly aligned with texture patterns, hinder model generalization on rendition-based OOD test sets. These observations suggest that current OOD evaluations often overlook the impact of frequency shortcuts on model generalization. Future benchmarks could thus benefit from explicitly assessing and accounting for these shortcuts to build models that generalize across a broader range of OOD scenarios.",
        "arxiv_id": "2503.03519",
        "ARXIVID": "2503.03519",
        "COMMENT": "This paper does not match any of the specific criteria but discusses generalization issues in computer vision models, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}