{
    "2505.15879": {
        "authors": [
            "Yue Fan",
            "Xuehai He",
            "Diji Yang",
            "Kaizhi Zheng",
            "Ching-Chen Kuo",
            "Yuting Zheng",
            "Sravana Jyothi Narayanaraju",
            "Xinze Guan",
            "Xin Eric Wang"
        ],
        "title": "GRIT: Teaching MLLMs to Think with Images",
        "abstract": "arXiv:2505.15879v1 Announce Type: new  Abstract: Recent studies have demonstrated the efficacy of using Reinforcement Learning (RL) in building reasoning models that articulate chains of thoughts prior to producing final answers. However, despite ongoing advances that aim at enabling reasoning for vision-language tasks, existing open-source visual reasoning models typically generate reasoning content with pure natural language, lacking explicit integration of visual information. This limits their ability to produce clearly articulated and visually grounded reasoning chains. To this end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method for training MLLMs to think with images. GRIT introduces a grounded reasoning paradigm, in which models generate reasoning chains that interleave natural language and explicit bounding box coordinates. These coordinates point to regions of the input image that the model consults during its reasoning process. Additionally, GRIT is equipped with a reinforcement learning approach, GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused on the final answer accuracy and format of the grounded reasoning output, which eliminates the need for data with reasoning chain annotations or explicit bounding box labels. As a result, GRIT achieves exceptional data efficiency, requiring as few as 20 image-question-answer triplets from existing datasets. Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to produce coherent and visually grounded reasoning chains, showing a successful unification of reasoning and grounding abilities.",
        "arxiv_id": "2505.15879",
        "ARXIVID": "2505.15879",
        "COMMENT": "Matches criterion 2 as it introduces a novel method for training MLLMs to think with images, focusing on grounded reasoning with images and texts.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2505.17015": {
        "authors": [
            "Runsen Xu",
            "Weiyao Wang",
            "Hao Tang",
            "Xingyu Chen",
            "Xiaodong Wang",
            "Fu-Jen Chu",
            "Dahua Lin",
            "Matt Feiszli",
            "Kevin J. Liang"
        ],
        "title": "Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models",
        "abstract": "arXiv:2505.17015v1 Announce Type: new  Abstract: Multi-modal large language models (MLLMs) have rapidly advanced in visual tasks, yet their spatial understanding remains limited to single images, leaving them ill-suited for robotics and other real-world applications that require multi-frame reasoning. In this paper, we propose a framework to equip MLLMs with robust multi-frame spatial understanding by integrating depth perception, visual correspondence, and dynamic perception. Central to our approach is the MultiSPA dataset, a novel, large-scale collection of more than 27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we introduce a comprehensive benchmark that tests a wide spectrum of spatial tasks under uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves significant gains over baselines and proprietary systems, demonstrating scalable, generalizable multi-frame reasoning. We further observe multi-task benefits and early indications of emergent capabilities in challenging scenarios, and showcase how our model can serve as a multi-frame reward annotator for robotics.",
        "arxiv_id": "2505.17015",
        "ARXIVID": "2505.17015",
        "COMMENT": "Matches criteria 1 and 2 as it introduces a new MLLM framework for multi-frame spatial understanding, addressing spatial intelligence in embodied agents.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2505.16770": {
        "authors": [
            "Meng-Hao Guo",
            "Xuanyu Chu",
            "Qianrui Yang",
            "Zhe-Han Mo",
            "Yiqing Shen",
            "Pei-lin Li",
            "Xinjie Lin",
            "Jinnian Zhang",
            "Xin-Sheng Chen",
            "Yi Zhang",
            "Kiyohiro Nakayama",
            "Zhengyang Geng",
            "Houwen Peng",
            "Han Hu",
            "Shi-Nin Hu"
        ],
        "title": "RBench-V: A Primary Assessment for Visual Reasoning Models with Multi-modal Outputs",
        "abstract": "arXiv:2505.16770v1 Announce Type: new  Abstract: The rapid advancement of native multi-modal models and omni-models, exemplified by GPT-4o, Gemini, and o3, with their capability to process and generate content across modalities such as text and images, marks a significant milestone in the evolution of intelligence. Systematic evaluation of their multi-modal output capabilities in visual thinking processes (also known as multi-modal chain of thought, M-CoT) becomes critically important. However, existing benchmarks for evaluating multi-modal models primarily focus on assessing multi-modal inputs and text-only reasoning while neglecting the importance of reasoning through multi-modal outputs. In this paper, we present a benchmark, dubbed RBench-V, designed to assess models' vision-indispensable reasoning abilities. To construct RBench-V, we carefully hand-pick 803 questions covering math, physics, counting, and games. Unlike previous benchmarks that typically specify certain input modalities, RBench-V presents problems centered on multi-modal outputs, which require image manipulation such as generating novel images and constructing auxiliary lines to support the reasoning process. We evaluate numerous open- and closed-source models on RBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the best-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below the human score of 82.3%, highlighting that current models struggle to leverage multi-modal reasoning. Data and code are available at https://evalmodels.github.io/rbenchv",
        "arxiv_id": "2505.16770",
        "ARXIVID": "2505.16770",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (RBench-V) for evaluating multi-modal reasoning models, focusing on novel multi-modal outputs.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2505.16707": {
        "authors": [
            "Yongliang Wu",
            "Zonghui Li",
            "Xinting Hu",
            "Xinyu Ye",
            "Xianfang Zeng",
            "Gang Yu",
            "Wenbo Zhu",
            "Bernt Schiele",
            "Ming-Hsuan Yang",
            "Xu Yang"
        ],
        "title": "KRIS-Bench: Benchmarking Next-Level Intelligent Image Editing Models",
        "abstract": "arXiv:2505.16707v1 Announce Type: new  Abstract: Recent advances in multi-modal generative models have enabled significant progress in instruction-based image editing. However, while these models produce visually plausible outputs, their capacity for knowledge-based reasoning editing tasks remains under-explored. In this paper, we introduce KRIS-Bench (Knowledge-based Reasoning in Image-editing Systems Benchmark), a diagnostic benchmark designed to assess models through a cognitively informed lens. Drawing from educational theory, KRIS-Bench categorizes editing tasks across three foundational knowledge types: Factual, Conceptual, and Procedural. Based on this taxonomy, we design 22 representative tasks spanning 7 reasoning dimensions and release 1,267 high-quality annotated editing instances. To support fine-grained evaluation, we propose a comprehensive protocol that incorporates a novel Knowledge Plausibility metric, enhanced by knowledge hints and calibrated through human studies. Empirical results on 10 state-of-the-art models reveal significant gaps in reasoning performance, highlighting the need for knowledge-centric benchmarks to advance the development of intelligent image editing systems.",
        "arxiv_id": "2505.16707",
        "ARXIVID": "2505.16707",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (KRIS-Bench) for knowledge-based reasoning in image editing systems, focusing on a novel angle of reasoning tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2505.16673": {
        "authors": [
            "Huanjin Yao",
            "Qixiang Yin",
            "Jingyi Zhang",
            "Min Yang",
            "Yibo Wang",
            "Wenhao Wu",
            "Fei Su",
            "Li Shen",
            "Minghui Qiu",
            "Dacheng Tao",
            "Jiaxing Huang"
        ],
        "title": "R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO",
        "abstract": "arXiv:2505.16673v1 Announce Type: new  Abstract: In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during RL. To this end, we propose Share-GRPO, a novel RL approach that tackle these issues by exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, Share-GRPO first expands the question space for a given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during RL. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training. Extensive evaluations over six widely-used reasoning benchmarks showcase the superior performance of our method. Code will be available at https://github.com/HJYao00/R1-ShareVL.",
        "arxiv_id": "2505.16673",
        "ARXIVID": "2505.16673",
        "COMMENT": "Matches criterion 2 as it proposes a novel reinforcement learning approach to improve reasoning in multimodal large language models (MLLMs).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2505.16146": {
        "authors": [
            "Zhenglin Hua",
            "Jinghan He",
            "Zijun Yao",
            "Tianxu Han",
            "Haiyun Guo",
            "Yuheng Jia",
            "Junfeng Fang"
        ],
        "title": "Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation",
        "abstract": "arXiv:2505.16146v1 Announce Type: new  Abstract: Large vision-language models (LVLMs) have achieved remarkable performance on multimodal tasks such as visual question answering (VQA) and image captioning. However, they still suffer from hallucinations, generating text inconsistent with visual input, posing significant risks in real-world applications. Existing approaches to address this issue focus on incorporating external knowledge bases, alignment training, or decoding strategies, all of which require substantial computational cost and time. Recent works try to explore more efficient alternatives by adjusting LVLMs' internal representations. Although promising, these methods may cause hallucinations to be insufficiently suppressed or lead to excessive interventions that negatively affect normal semantics. In this work, we leverage sparse autoencoders (SAEs) to identify semantic directions closely associated with either hallucinations or actuality, realizing more precise and direct hallucination-related representations. Our analysis demonstrates that interventions along the faithful direction we identified can mitigate hallucinations, while those along the hallucinatory direction can exacerbate them. Building on these insights, we propose Steering LVLMs via SAE Latent Directions (SSL), a training-free method based on SAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive experiments demonstrate that SSL significantly outperforms existing decoding approaches in mitigating hallucinations, while maintaining transferability across different model architectures with negligible additional time overhead.",
        "arxiv_id": "2505.16146",
        "ARXIVID": "2505.16146",
        "COMMENT": "Matches criterion 2 as it focuses on mitigating hallucinations in large vision-language models (LVLMs), which is a key challenge in VLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.16761": {
        "authors": [
            "Jian Liu",
            "Jing Xu",
            "Song Guo",
            "Jing Li",
            "Jingfeng Guo",
            "Jiaao Yu",
            "Haohan Weng",
            "Biwen Lei",
            "Xianghui Yang",
            "Zhuo Chen",
            "Fangqi Zhu",
            "Tao Han",
            "Chunchao Guo"
        ],
        "title": "Mesh-RFT: Enhancing Mesh Generation via Fine-grained Reinforcement Fine-Tuning",
        "abstract": "arXiv:2505.16761v1 Announce Type: new  Abstract: Existing pretrained models for 3D mesh generation often suffer from data biases and produce low-quality results, while global reinforcement learning (RL) methods rely on object-level rewards that struggle to capture local structure details. To address these challenges, we present \\textbf{Mesh-RFT}, a novel fine-grained reinforcement fine-tuning framework that employs Masked Direct Preference Optimization (M-DPO) to enable localized refinement via quality-aware face masking. To facilitate efficient quality evaluation, we introduce an objective topology-aware scoring system to evaluate geometric integrity and topological regularity at both object and face levels through two metrics: Boundary Edge Ratio (BER) and Topology Score (TS). By integrating these metrics into a fine-grained RL strategy, Mesh-RFT becomes the first method to optimize mesh quality at the granularity of individual faces, resolving localized errors while preserving global coherence. Experiment results show that our M-DPO approach reduces Hausdorff Distance (HD) by 24.6\\% and improves Topology Score (TS) by 3.8\\% over pre-trained models, while outperforming global DPO methods with a 17.4\\% HD reduction and 4.9\\% TS gain. These results demonstrate Mesh-RFT's ability to improve geometric integrity and topological regularity, achieving new state-of-the-art performance in production-ready mesh generation. Project Page: \\href{https://hitcslj.github.io/mesh-rft/}{this https URL}.",
        "arxiv_id": "2505.16761",
        "ARXIVID": "2505.16761",
        "COMMENT": "Matches criterion 1 as it introduces a novel reinforcement fine-tuning framework for 3D mesh generation, improving spatial intelligence in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.16793": {
        "authors": [
            "Xiang Li",
            "Yong Tao",
            "Siyuan Zhang",
            "Siwei Liu",
            "Zhitong Xiong",
            "Chunbo Luo",
            "Lu Liu",
            "Mykola Pechenizkiy",
            "Xiao Xiang Zhu",
            "Tianjin Huang"
        ],
        "title": "REOBench: Benchmarking Robustness of Earth Observation Foundation Models",
        "abstract": "arXiv:2505.16793v1 Announce Type: new  Abstract: Earth observation foundation models have shown strong generalization across multiple Earth observation tasks, but their robustness under real-world perturbations remains underexplored. To bridge this gap, we introduce REOBench, the first comprehensive benchmark for evaluating the robustness of Earth observation foundation models across six tasks and twelve types of image corruptions, including both appearance-based and geometric perturbations. To ensure realistic and fine-grained evaluation, our benchmark focuses on high-resolution optical remote sensing images, which are widely used in critical applications such as urban planning and disaster response. We conduct a systematic evaluation of a broad range of models trained using masked image modeling, contrastive learning, and vision-language pre-training paradigms. Our results reveal that (1) existing Earth observation foundation models experience significant performance degradation when exposed to input corruptions. (2) The severity of degradation varies across tasks, model architectures, backbone sizes, and types of corruption, with performance drop varying from less than 1% to over 20%. (3) Vision-language models show enhanced robustness, particularly in multimodal tasks. REOBench underscores the vulnerability of current Earth observation foundation models to real-world corruptions and provides actionable insights for developing more robust and reliable models.",
        "arxiv_id": "2505.16793",
        "ARXIVID": "2505.16793",
        "COMMENT": "Matches criterion 4 as it evaluates the robustness of Earth observation foundation models, which are a type of vision foundation model.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.16165": {
        "authors": [
            "Yechan Park",
            "Gyuhyeon Pak",
            "Euntai Kim"
        ],
        "title": "RE-TRIP : Reflectivity Instance Augmented Triangle Descriptor for 3D Place Recognition",
        "abstract": "arXiv:2505.16165v1 Announce Type: new  Abstract: While most people associate LiDAR primarily with its ability to measure distances and provide geometric information about the environment (via point clouds), LiDAR also captures additional data, including reflectivity or intensity values. Unfortunately, when LiDAR is applied to Place Recognition (PR) in mobile robotics, most previous works on LiDAR-based PR rely only on geometric measurements, neglecting the additional reflectivity information that LiDAR provides. In this paper, we propose a novel descriptor for 3D PR, named RE-TRIP (REflectivity-instance augmented TRIangle descriPtor). This new descriptor leverages both geometric measurements and reflectivity to enhance robustness in challenging scenarios such as geometric degeneracy, high geometric similarity, and the presence of dynamic objects. To implement RE-TRIP in real-world applications, we further propose (1) a keypoint extraction method, (2) a key instance segmentation method, (3) a RE-TRIP matching method, and (4) a reflectivity-combined loop verification method. Finally, we conduct a series of experiments to demonstrate the effectiveness of RE-TRIP. Applied to public datasets (i.e., HELIPR, FusionPortable) containing diverse scenarios such as long corridors, bridges, large-scale urban areas, and highly dynamic environments -- our experimental results show that the proposed method outperforms existing state-of-the-art methods in terms of Scan Context, Intensity Scan Context, and STD.",
        "arxiv_id": "2505.16165",
        "ARXIVID": "2505.16165",
        "COMMENT": "Matches criterion 3 as it proposes a novel descriptor for 3D place recognition using LiDAR, focusing on previously overlooked reflectivity information.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.15929": {
        "authors": [
            "Hui Shen",
            "Taiqiang Wu",
            "Qi Han",
            "Yunta Hsieh",
            "Jizhou Wang",
            "Yuyue Zhang",
            "Yuxin Cheng",
            "Zijian Hao",
            "Yuansheng Ni",
            "Xin Wang",
            "Zhongwei Wan",
            "Kai Zhang",
            "Wendong Xu",
            "Jing Xiong",
            "Ping Luo",
            "Wenhu Chen",
            "Chaofan Tao",
            "Zhuoqing Mao",
            "Ngai Wong"
        ],
        "title": "PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?",
        "abstract": "arXiv:2505.15929v1 Announce Type: new  Abstract: Existing benchmarks fail to capture a crucial aspect of intelligence: physical reasoning, the integrated ability to combine domain knowledge, symbolic reasoning, and understanding of real-world constraints. To address this gap, we introduce PhyX: the first large-scale benchmark designed to assess models capacity for physics-grounded reasoning in visual scenarios. PhyX includes 3K meticulously curated multimodal questions spanning 6 reasoning types across 25 sub-domains and 6 core physics domains: thermodynamics, electromagnetism, mechanics, modern physics, optics, and wave\\&acoustics. In our comprehensive evaluation, even state-of-the-art models struggle significantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and GPT-o4-mini achieve only 32.5\\%, 42.2\\%, and 45.8\\% accuracy respectively-performance gaps exceeding 29\\% compared to human experts. Our analysis exposes critical limitations in current models: over-reliance on memorized disciplinary knowledge, excessive dependence on mathematical formulations, and surface-level visual pattern matching rather than genuine physical understanding. We provide in-depth analysis through fine-grained statistics, detailed case studies, and multiple evaluation paradigms to thoroughly examine physical reasoning capabilities. To ensure reproducibility, we implement a compatible evaluation protocol based on widely-used toolkits such as VLMEvalKit, enabling one-click evaluation.",
        "arxiv_id": "2505.15929",
        "ARXIVID": "2505.15929",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for physical reasoning in visual scenarios, relevant to embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.16974": {
        "authors": [
            "Zongyan Han",
            "Jiale Cao",
            "Shuo Chen",
            "Tong Wang",
            "Jorma Laaksonen",
            "Rao Muhammad Anwer"
        ],
        "title": "OpenSeg-R: Improving Open-Vocabulary Segmentation via Step-by-Step Visual Reasoning",
        "abstract": "arXiv:2505.16974v1 Announce Type: new  Abstract: Open-Vocabulary Segmentation (OVS) has drawn increasing attention for its capacity to generalize segmentation beyond predefined categories. However, existing methods typically predict segmentation masks with simple forward inference, lacking explicit reasoning and interpretability. This makes it challenging for OVS model to distinguish similar categories in open-world settings due to the lack of contextual understanding and discriminative visual cues. To address this limitation, we propose a step-by-step visual reasoning framework for open-vocabulary segmentation, named OpenSeg-R. The proposed OpenSeg-R leverages Large Multimodal Models (LMMs) to perform hierarchical visual reasoning before segmentation. Specifically, we generate both generic and image-specific reasoning for each image, forming structured triplets that explain the visual reason for objects in a coarse-to-fine manner. Based on these reasoning steps, we can compose detailed description prompts, and feed them to the segmentor to produce more accurate segmentation masks. To the best of our knowledge, OpenSeg-R is the first framework to introduce explicit step-by-step visual reasoning into OVS. Experimental results demonstrate that OpenSeg-R significantly outperforms state-of-the-art methods on open-vocabulary semantic segmentation across five benchmark datasets. Moreover, it achieves consistent gains across all metrics on open-vocabulary panoptic segmentation. Qualitative results further highlight the effectiveness of our reasoning-guided framework in improving both segmentation precision and interpretability. Our code is publicly available at https://github.com/Hanzy1996/OpenSeg-R.",
        "arxiv_id": "2505.16974",
        "ARXIVID": "2505.16974",
        "COMMENT": "Matches criterion 2 as it introduces a new framework leveraging LMMs for open-vocabulary segmentation with step-by-step visual reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.17022": {
        "authors": [
            "Chengqi Duan",
            "Rongyao Fang",
            "Yuqing Wang",
            "Kun Wang",
            "Linjiang Huang",
            "Xingyu Zeng",
            "Hongsheng Li",
            "Xihui Liu"
        ],
        "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning",
        "abstract": "arXiv:2505.17022v1 Announce Type: new  Abstract: Visual generation models have made remarkable progress in creating realistic images from text prompts, yet struggle with complex prompts that specify multiple objects with precise spatial relationships and attributes. Effective handling of such prompts requires explicit reasoning about the semantic content and spatial layout. We present GoT-R1, a framework that applies reinforcement learning to enhance semantic-spatial reasoning in visual generation. Building upon the Generation Chain-of-Thought approach, GoT-R1 enables models to autonomously discover effective reasoning strategies beyond predefined templates through carefully designed reinforcement learning. To achieve this, we propose a dual-stage multi-dimensional reward framework that leverages MLLMs to evaluate both the reasoning process and final output, enabling effective supervision across the entire generation pipeline. The reward system assesses semantic alignment, spatial accuracy, and visual quality in a unified approach. Experimental results demonstrate significant improvements on T2I-CompBench benchmark, particularly in compositional tasks involving precise spatial relationships and attribute binding. GoT-R1 advances the state-of-the-art in image generation by successfully transferring sophisticated reasoning capabilities to the visual generation domain. To facilitate future research, we make our code and pretrained models publicly available at https://github.com/gogoduan/GoT-R1.",
        "arxiv_id": "2505.17022",
        "ARXIVID": "2505.17022",
        "COMMENT": "Matches criterion 2 as it discusses a new MLLM framework for visual generation with reinforcement learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.16815": {
        "authors": [
            "Chunyi Li",
            "Jiaohao Xiao",
            "Jianbo Zhang",
            "Farong Wen",
            "Zicheng Zhang",
            "Yuan Tian",
            "Xiangyang Zhu",
            "Xiaohong Liu",
            "Zhengxue Cheng",
            "Weisi Lin",
            "Guangtao Zhai"
        ],
        "title": "Perceptual Quality Assessment for Embodied AI",
        "abstract": "arXiv:2505.16815v1 Announce Type: new  Abstract: Embodied AI has developed rapidly in recent years, but it is still mainly deployed in laboratories, with various distortions in the Real-world limiting its application. Traditionally, Image Quality Assessment (IQA) methods are applied to predict human preferences for distorted images; however, there is no IQA method to assess the usability of an image in embodied tasks, namely, the perceptual quality for robots. To provide accurate and reliable quality indicators for future embodied scenarios, we first propose the topic: IQA for Embodied AI. Specifically, we (1) based on the Mertonian system and meta-cognitive theory, constructed a perception-cognition-decision-execution pipeline and defined a comprehensive subjective score collection process; (2) established the Embodied-IQA database, containing over 36k reference/distorted image pairs, with more than 5m fine-grained annotations provided by Vision Language Models/Vision Language Action-models/Real-world robots; (3) trained and validated the performance of mainstream IQA methods on Embodied-IQA, demonstrating the need to develop more accurate quality indicators for Embodied AI. We sincerely hope that through evaluation, we can promote the application of Embodied AI under complex distortions in the Real-world. Project page: https://github.com/lcysyzxdxc/EmbodiedIQA",
        "arxiv_id": "2505.16815",
        "ARXIVID": "2505.16815",
        "COMMENT": "Matches criterion 3. Proposes a new benchmark and evaluation framework for perceptual quality assessment in embodied AI, focusing on novel angles for real-world distortions.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.16456": {
        "authors": [
            "Siwei Meng",
            "Yawei Luo",
            "Ping Liu"
        ],
        "title": "MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM",
        "abstract": "arXiv:2505.16456v1 Announce Type: new  Abstract: Recent advances in static 3D generation have intensified the demand for physically consistent dynamic 3D content. However, existing video generation models, including diffusion-based methods, often prioritize visual realism while neglecting physical plausibility, resulting in implausible object dynamics. Prior approaches for physics-aware dynamic generation typically rely on large-scale annotated datasets or extensive model fine-tuning, which imposes significant computational and data collection burdens and limits scalability across scenarios. To address these challenges, we present MAGIC, a training-free framework for single-image physical property inference and dynamic generation, integrating pretrained image-to-video diffusion models with iterative LLM-based reasoning. Our framework generates motion-rich videos from a static image and closes the visual-to-physical gap through a confidence-driven LLM feedback loop that adaptively steers the diffusion model toward physics-relevant motion. To translate visual dynamics into controllable physical behavior, we further introduce a differentiable MPM simulator operating directly on 3D Gaussians reconstructed from the single image, enabling physically grounded, simulation-ready outputs without any supervision or model tuning. Experiments show that MAGIC outperforms existing physics-aware generative methods in inference accuracy and achieves greater temporal coherence than state-of-the-art video diffusion models.",
        "arxiv_id": "2505.16456",
        "ARXIVID": "2505.16456",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for spatial understanding and dynamic 3D generation using a combination of diffusion models and LLM-based reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.17011": {
        "authors": [
            "Yan Li",
            "Changyao Tian",
            "Renqiu Xia",
            "Ning Liao",
            "Weiwei Guo",
            "Junchi Yan",
            "Hongsheng Li",
            "Jifeng Dai",
            "Hao Li",
            "Xue Yang"
        ],
        "title": "Learning Adaptive and Temporally Causal Video Tokenization in a 1D Latent Space",
        "abstract": "arXiv:2505.17011v1 Announce Type: new  Abstract: We propose AdapTok, an adaptive temporal causal video tokenizer that can flexibly allocate tokens for different frames based on video content. AdapTok is equipped with a block-wise masking strategy that randomly drops tail tokens of each block during training, and a block causal scorer to predict the reconstruction quality of video frames using different numbers of tokens. During inference, an adaptive token allocation strategy based on integer linear programming is further proposed to adjust token usage given predicted scores. Such design allows for sample-wise, content-aware, and temporally dynamic token allocation under a controllable overall budget. Extensive experiments for video reconstruction and generation on UCF-101 and Kinetics-600 demonstrate the effectiveness of our approach. Without additional image data, AdapTok consistently improves reconstruction quality and generation performance under different token budgets, allowing for more scalable and token-efficient generative video modeling.",
        "arxiv_id": "2505.17011",
        "ARXIVID": "2505.17011",
        "COMMENT": "Matches criterion 4 as it introduces a novel adaptive video tokenization method, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.16463": {
        "authors": [
            "Jiquan Shan",
            "Junxiao Wang",
            "Lifeng Zhao",
            "Liang Cai",
            "Hongyuan Zhang",
            "Ioannis Liritzis"
        ],
        "title": "AnchorFormer: Differentiable Anchor Attention for Efficient Vision Transformer",
        "abstract": "arXiv:2505.16463v1 Announce Type: new  Abstract: Recently, vision transformers (ViTs) have achieved excellent performance on vision tasks by measuring the global self-attention among the image patches. Given $n$ patches, they will have quadratic complexity such as $\\mathcal{O}(n^2)$ and the time cost is high when splitting the input image with a small granularity. Meanwhile, the pivotal information is often randomly gathered in a few regions of an input image, some tokens may not be helpful for the downstream tasks. To handle this problem, we introduce an anchor-based efficient vision transformer (AnchorFormer), which employs the anchor tokens to learn the pivotal information and accelerate the inference. Firstly, by estimating the bipartite attention between the anchors and tokens, the complexity will be reduced from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(mn)$, where $m$ is an anchor number and $m < n$. Notably, by representing the anchors with the neurons in a neural layer, we can differentiable learn these distributions and approximate global self-attention through the Markov process. Moreover, we extend the proposed model to three downstream tasks including classification, detection, and segmentation. Extensive experiments show the effectiveness of our AnchorFormer, e.g., achieving up to a 9.0% higher accuracy or 46.7% FLOPs reduction on ImageNet classification, 81.3% higher mAP on COCO detection under comparable FLOPs, as compared to the current baselines.",
        "arxiv_id": "2505.16463",
        "ARXIVID": "2505.16463",
        "COMMENT": "Matches criterion 4 as it proposes an efficient vision transformer with novel anchor-based attention, relevant to vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.16971": {
        "authors": [
            "Himangi Mittal",
            "Peiye Zhuang",
            "Hsin-Ying Lee",
            "Shubham Tulsiani"
        ],
        "title": "UniPhy: Learning a Unified Constitutive Model for Inverse Physics Simulation",
        "abstract": "arXiv:2505.16971v1 Announce Type: new  Abstract: We propose UniPhy, a common latent-conditioned neural constitutive model that can encode the physical properties of diverse materials. At inference UniPhy allows `inverse simulation' i.e. inferring material properties by optimizing the scene-specific latent to match the available observations via differentiable simulation. In contrast to existing methods that treat such inference as system identification, UniPhy does not rely on user-specified material type information. Compared to prior neural constitutive modeling approaches which learn instance specific networks, the shared training across materials improves both, robustness and accuracy of the estimates. We train UniPhy using simulated trajectories across diverse geometries and materials -- elastic, plasticine, sand, and fluids (Newtonian & non-Newtonian). At inference, given an object with unknown material properties, UniPhy can infer the material properties via latent optimization to match the motion observations, and can then allow re-simulating the object under diverse scenarios. We compare UniPhy against prior inverse simulation methods, and show that the inference from UniPhy enables more accurate replay and re-simulation under novel conditions.",
        "arxiv_id": "2505.16971",
        "ARXIVID": "2505.16971",
        "COMMENT": "Matches criterion 3 as it focuses on embodied AI with a novel method for inverse physics simulation, which is a unique angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.15880": {
        "authors": [
            "Zhiyuan Xu",
            "Bohan Li",
            "Huan-ang Gao",
            "Mingju Gao",
            "Yong Chen",
            "Ming Liu",
            "Chenxu Yan",
            "Hang Zhao",
            "Shuo Feng",
            "Hao Zhao"
        ],
        "title": "Challenger: Affordable Adversarial Driving Video Generation",
        "abstract": "arXiv:2505.15880v1 Announce Type: new  Abstract: Generating photorealistic driving videos has seen significant progress recently, but current methods largely focus on ordinary, non-adversarial scenarios. Meanwhile, efforts to generate adversarial driving scenarios often operate on abstract trajectory or BEV representations, falling short of delivering realistic sensor data that can truly stress-test autonomous driving (AD) systems. In this work, we introduce Challenger, a framework that produces physically plausible yet photorealistic adversarial driving videos. Generating such videos poses a fundamental challenge: it requires jointly optimizing over the space of traffic interactions and high-fidelity sensor observations. Challenger makes this affordable through two techniques: (1) a physics-aware multi-round trajectory refinement process that narrows down candidate adversarial maneuvers, and (2) a tailored trajectory scoring function that encourages realistic yet adversarial behavior while maintaining compatibility with downstream video synthesis. As tested on the nuScenes dataset, Challenger generates a diverse range of aggressive driving scenarios-including cut-ins, sudden lane changes, tailgating, and blind spot intrusions-and renders them into multiview photorealistic videos. Extensive evaluations show that these scenarios significantly increase the collision rate of state-of-the-art end-to-end AD models (UniAD, VAD, SparseDrive, and DiffusionDrive), and importantly, adversarial behaviors discovered for one model often transfer to others.",
        "arxiv_id": "2505.15880",
        "ARXIVID": "2505.15880",
        "COMMENT": "Matches criterion 3. Introduces a new framework for generating adversarial driving videos, focusing on novel methods for embodied AI and simulation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2505.16411": {
        "authors": [
            "Sreetama Sarkar",
            "Yue Che",
            "Alex Gavin",
            "Peter A. Beerel",
            "Souvik Kundu"
        ],
        "title": "Mitigating Hallucinations in Vision-Language Models through Image-Guided Head Suppression",
        "abstract": "arXiv:2505.16411v1 Announce Type: new  Abstract: Despite their remarkable progress in multimodal understanding tasks, large vision language models (LVLMs) often suffer from \"hallucinations\", generating texts misaligned with the visual context. Existing methods aimed at reducing hallucinations through inference time intervention incur a significant increase in latency. To mitigate this, we present SPIN, a task-agnostic attention-guided head suppression strategy that can be seamlessly integrated during inference, without incurring any significant compute or latency overhead. We investigate whether hallucination in LVLMs can be linked to specific model components. Our analysis suggests that hallucinations can be attributed to a dynamic subset of attention heads in each layer. Leveraging this insight, for each text query token, we selectively suppress attention heads that exhibit low attention to image tokens, keeping the top-K attention heads intact. Extensive evaluations on visual question answering and image description tasks demonstrate the efficacy of SPIN in reducing hallucination scores up to 2.7x while maintaining F1, and improving throughput by 1.8x compared to existing alternatives. Code is available at https://github.com/YUECHE77/SPIN.",
        "arxiv_id": "2505.16411",
        "ARXIVID": "2505.16411",
        "COMMENT": "Matches criterion 2 as it proposes a method to mitigate hallucinations in vision-language models (LVLMs) through attention-guided head suppression.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2505.15865": {
        "authors": [
            "Ingeol Baek",
            "Hwan Chang",
            "Sunghyun Ryu",
            "Hwanhee Lee"
        ],
        "title": "How Do Large Vision-Language Models See Text in Image? Unveiling the Distinctive Role of OCR Heads",
        "abstract": "arXiv:2505.15865v1 Announce Type: new  Abstract: Despite significant advancements in Large Vision Language Models (LVLMs), a gap remains, particularly regarding their interpretability and how they locate and interpret textual information within images. In this paper, we explore various LVLMs to identify the specific heads responsible for recognizing text from images, which we term the Optical Character Recognition Head (OCR Head). Our findings regarding these heads are as follows: (1) Less Sparse: Unlike previous retrieval heads, a large number of heads are activated to extract textual information from images. (2) Qualitatively Distinct: OCR heads possess properties that differ significantly from general retrieval heads, exhibiting low similarity in their characteristics. (3) Statically Activated: The frequency of activation for these heads closely aligns with their OCR scores. We validate our findings in downstream tasks by applying Chain-of-Thought (CoT) to both OCR and conventional retrieval heads and by masking these heads. We also demonstrate that redistributing sink-token values within the OCR heads improves performance. These insights provide a deeper understanding of the internal mechanisms LVLMs employ in processing embedded textual information in images.",
        "arxiv_id": "2505.15865",
        "ARXIVID": "2505.15865",
        "COMMENT": "Matches criterion 2 as it explores the internal mechanisms of large vision-language models (LVLMs) and their OCR capabilities.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2505.16080": {
        "authors": [
            "Jiayue Liu",
            "Zhongchao Yi",
            "Zhengyang Zhou",
            "Qihe Huang",
            "Kuo Yang",
            "Xu Wang",
            "Yang Wang"
        ],
        "title": "SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation",
        "abstract": "arXiv:2505.16080v1 Announce Type: new  Abstract: Discovering regularities from spatiotemporal systems can benefit various scientific and social planning. Current spatiotemporal learners usually train an independent model from a specific source data that leads to limited transferability among sources, where even correlated tasks requires new design and training. The key towards increasing cross-domain knowledge is to enable collective intelligence and model evolution. In this paper, inspired by neuroscience theories, we theoretically derive the increased information boundary via learning cross-domain collective intelligence and propose a Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the model independence and enables cross-domain knowledge to be shared and aggregated. Specifically, we first re-order the sample groups to imitate the human curriculum learning, and devise two complementary learners, elastic common container and task-independent extractor to allow model growth and task-wise commonality and personality disentanglement. Then an adaptive dynamic coupler with a new difference metric determines whether the new sample group should be incorporated into common container to achieve model evolution under various domains. Experiments show that SynEVO improves the generalization capacity by at most 42% under cross-domain scenarios and SynEVO provides a paradigm of NeuroAI for knowledge transfer and adaptation.",
        "arxiv_id": "2505.16080",
        "ARXIVID": "2505.16080",
        "COMMENT": "Matches criterion 1 as it proposes a neuro-inspired spatiotemporal framework for cross-domain adaptation, which relates to spatial intelligence.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2505.16679": {
        "authors": [
            "Jordan Dotzel",
            "Tony Montes",
            "Mohamed S. Abdelfattah",
            "Zhiru Zhang"
        ],
        "title": "Semantic Compression of 3D Objects for Open and Collaborative Virtual Worlds",
        "abstract": "arXiv:2505.16679v1 Announce Type: new  Abstract: Traditional methods for 3D object compression operate only on structural information within the object vertices, polygons, and textures. These methods are effective at compression rates up to 10x for standard object sizes but quickly deteriorate at higher compression rates with texture artifacts, low-polygon counts, and mesh gaps. In contrast, semantic compression ignores structural information and operates directly on the core concepts to push to extreme levels of compression. In addition, it uses natural language as its storage format, which makes it natively human-readable and a natural fit for emerging applications built around large-scale, collaborative projects within augmented and virtual reality. It deprioritizes structural information like location, size, and orientation and predicts the missing information with state-of-the-art deep generative models. In this work, we construct a pipeline for 3D semantic compression from public generative models and explore the quality-compression frontier for 3D object compression. We apply this pipeline to achieve rates as high as 105x for 3D objects taken from the Objaverse dataset and show that semantic compression can outperform traditional methods in the important quality-preserving region around 100x compression.",
        "arxiv_id": "2505.16679",
        "ARXIVID": "2505.16679",
        "COMMENT": "Matches criterion 1 as it discusses semantic compression for 3D objects, which involves spatial understanding and generative modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.17017": {
        "authors": [
            "Chengzhuo Tong",
            "Ziyu Guo",
            "Renrui Zhang",
            "Wenyu Shan",
            "Xinyu Wei",
            "Zhenghao Xing",
            "Hongsheng Li",
            "Pheng-Ann Heng"
        ],
        "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO",
        "abstract": "arXiv:2505.17017v1 Announce Type: new  Abstract: Recent advancements underscore the significant role of Reinforcement Learning (RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large language models (LLMs). Two prominent RL algorithms, Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central to these developments, showcasing different pros and cons. Autoregressive image generation, also interpretable as a sequential CoT reasoning process, presents unique challenges distinct from LLM-based CoT reasoning. These encompass ensuring text-image consistency, improving image aesthetic quality, and designing sophisticated reward models, rather than relying on simpler rule-based rewards. While recent efforts have extended RL to this domain, these explorations typically lack an in-depth analysis of the domain-specific challenges and the characteristics of different RL strategies. To bridge this gap, we provide the first comprehensive investigation of the GRPO and DPO algorithms in autoregressive image generation, evaluating their in-domain performance and out-of-domain generalization, while scrutinizing the impact of different reward models on their respective capabilities. Our findings reveal that GRPO and DPO exhibit distinct advantages, and crucially, that reward models possessing stronger intrinsic generalization capabilities potentially enhance the generalization potential of the applied RL algorithms. Furthermore, we systematically explore three prevalent scaling strategies to enhance both their in-domain and out-of-domain proficiency, deriving unique insights into efficiently scaling performance for each paradigm. We hope our study paves a new path for inspiring future work on developing more effective RL algorithms to achieve robust CoT reasoning in the realm of autoregressive image generation. Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT",
        "arxiv_id": "2505.17017",
        "ARXIVID": "2505.17017",
        "COMMENT": "Matches criterion 2 as it explores reinforcement learning for image generation with Chain-of-Thought reasoning, relevant to VLLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.16282": {
        "authors": [
            "Fanbin Lu",
            "Zhisheng Zhong",
            "Shu Liu",
            "Chi-Wing Fu",
            "Jiaya Jia"
        ],
        "title": "ARPO:End-to-End Policy Optimization for GUI Agents with Experience Replay",
        "abstract": "arXiv:2505.16282v1 Announce Type: new  Abstract: Training large language models (LLMs) as interactive agents for controlling graphical user interfaces (GUIs) presents a unique challenge to optimize long-horizon action sequences with multimodal feedback from complex environments. While recent works have advanced multi-turn reinforcement learning (RL) for reasoning and tool-using capabilities in LLMs, their application to GUI-based agents remains relatively underexplored due to the difficulty of sparse rewards, delayed feedback, and high rollout costs. In this paper, we investigate end-to-end policy optimization for vision-language-based GUI agents with the aim of improving performance on complex, long-horizon computer tasks. We propose Agentic Replay Policy Optimization (ARPO), an end-to-end RL approach that augments Group Relative Policy Optimization (GRPO) with a replay buffer to reuse the successful experience across training iterations. To further stabilize the training process, we propose a task selection strategy that filters tasks based on baseline agent performance, allowing the agent to focus on learning from informative interactions. Additionally, we compare ARPO with offline preference optimization approaches, highlighting the advantages of policy-based methods in GUI environments. Experiments on the OSWorld benchmark demonstrate that ARPO achieves competitive results, establishing a new performance baseline for LLM-based GUI agents trained via reinforcement learning. Our findings underscore the effectiveness of reinforcement learning for training multi-turn, vision-language GUI agents capable of managing complex real-world UI interactions. Codes and models:https://github.com/dvlab-research/ARPO.git.",
        "arxiv_id": "2505.16282",
        "ARXIVID": "2505.16282",
        "COMMENT": "Matches criterion 3 as it focuses on a new method for training GUI agents with reinforcement learning, which is relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.15963": {
        "authors": [
            "Shujun Liu",
            "Siyuan Wang",
            "Zejun Li",
            "Jianxiang Wang",
            "Cheng Zeng",
            "Zhongyu Wei"
        ],
        "title": "OViP: Online Vision-Language Preference Learning",
        "abstract": "arXiv:2505.15963v1 Announce Type: new  Abstract: Large vision-language models (LVLMs) remain vulnerable to hallucination, often generating content misaligned with visual inputs. While recent approaches advance multi-modal Direct Preference Optimization (DPO) to mitigate hallucination, they typically rely on predefined or randomly edited negative samples that fail to reflect actual model errors, limiting training efficacy. In this work, we propose an Online Vision-language Preference Learning (OViP) framework that dynamically constructs contrastive training data based on the model's own hallucinated outputs. By identifying semantic differences between sampled response pairs and synthesizing negative images using a diffusion model, OViP generates more relevant supervision signals in real time. This failure-driven training enables adaptive alignment of both textual and visual preferences. Moreover, we refine existing evaluation protocols to better capture the trade-off between hallucination suppression and expressiveness. Experiments on hallucination and general benchmarks demonstrate that OViP effectively reduces hallucinations while preserving core multi-modal capabilities.",
        "arxiv_id": "2505.15963",
        "ARXIVID": "2505.15963",
        "COMMENT": "Matches criterion 2 as it proposes OViP, a framework for improving vision-language models by reducing hallucinations through dynamic preference learning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.17020": {
        "authors": [
            "Shilin Yan",
            "Jiaming Han",
            "Joey Tsai",
            "Hongwei Xue",
            "Rongyao Fang",
            "Lingyi Hong",
            "Ziyu Guo",
            "Ray Zhang"
        ],
        "title": "CrossLMM: Decoupling Long Video Sequences from LMMs via Dual Cross-Attention Mechanisms",
        "abstract": "arXiv:2505.17020v1 Announce Type: new  Abstract: The advent of Large Multimodal Models (LMMs) has significantly enhanced Large Language Models (LLMs) to process and interpret diverse data modalities (e.g., image and video). However, as input complexity increases, particularly with long video sequences, the number of required tokens has grown significantly, leading to quadratically computational costs. This has made the efficient compression of video tokens in LMMs, while maintaining performance integrity, a pressing research challenge. In this paper, we introduce CrossLMM, decoupling long video sequences from LMMs via a dual cross-attention mechanism, which substantially reduces visual token quantity with minimal performance degradation. Specifically, we first implement a significant token reduction from pretrained visual encoders through a pooling methodology. Then, within LLM layers, we employ a visual-to-visual cross-attention mechanism, wherein the pooled visual tokens function as queries against the original visual token set. This module enables more efficient token utilization while retaining fine-grained informational fidelity. In addition, we introduce a text-to-visual cross-attention mechanism, for which the text tokens are enhanced through interaction with the original visual tokens, enriching the visual comprehension of the text tokens. Comprehensive empirical evaluation demonstrates that our approach achieves comparable or superior performance across diverse video-based LMM benchmarks, despite utilizing substantially fewer computational resources.",
        "arxiv_id": "2505.17020",
        "ARXIVID": "2505.17020",
        "COMMENT": "Matches criterion 2 as it introduces CrossLMM, a method for improving efficiency in Large Multimodal Models (LMMs) for video processing.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.16685": {
        "authors": [
            "Corentin Dufourg",
            "Charlotte Pelletier",
            "St\\'ephane May",
            "S\\'ebastien Lef\\`evre"
        ],
        "title": "On the use of Graphs for Satellite Image Time Series",
        "abstract": "arXiv:2505.16685v1 Announce Type: new  Abstract: The Earth's surface is subject to complex and dynamic processes, ranging from large-scale phenomena such as tectonic plate movements to localized changes associated with ecosystems, agriculture, or human activity. Satellite images enable global monitoring of these processes with extensive spatial and temporal coverage, offering advantages over in-situ methods. In particular, resulting satellite image time series (SITS) datasets contain valuable information. To handle their large volume and complexity, some recent works focus on the use of graph-based techniques that abandon the regular Euclidean structure of satellite data to work at an object level. Besides, graphs enable modelling spatial and temporal interactions between identified objects, which are crucial for pattern detection, classification and regression tasks. This paper is an effort to examine the integration of graph-based methods in spatio-temporal remote-sensing analysis. In particular, it aims to present a versatile graph-based pipeline to tackle SITS analysis. It focuses on the construction of spatio-temporal graphs from SITS and their application to downstream tasks. The paper includes a comprehensive review and two case studies, which highlight the potential of graph-based approaches for land cover mapping and water resource forecasting. It also discusses numerous perspectives to resolve current limitations and encourage future developments.",
        "arxiv_id": "2505.16685",
        "ARXIVID": "2505.16685",
        "COMMENT": "Matches criterion 3 as it discusses a novel graph-based approach for spatio-temporal analysis in satellite image time series, which could be relevant for embodied AI benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.16805": {
        "authors": [
            "Xuesong Chen",
            "Linjiang Huang",
            "Tao Ma",
            "Rongyao Fang",
            "Shaoshuai Shi",
            "Hongsheng Li"
        ],
        "title": "SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous Driving",
        "abstract": "arXiv:2505.16805v1 Announce Type: new  Abstract: The integration of Vision-Language Models (VLMs) into autonomous driving systems has shown promise in addressing key challenges such as learning complexity, interpretability, and common-sense reasoning. However, existing approaches often struggle with efficient integration and realtime decision-making due to computational demands. In this paper, we introduce SOLVE, an innovative framework that synergizes VLMs with end-to-end (E2E) models to enhance autonomous vehicle planning. Our approach emphasizes knowledge sharing at the feature level through a shared visual encoder, enabling comprehensive interaction between VLM and E2E components. We propose a Trajectory Chain-of-Thought (T-CoT) paradigm, which progressively refines trajectory predictions, reducing uncertainty and improving accuracy. By employing a temporal decoupling strategy, SOLVE achieves efficient cooperation by aligning high-quality VLM outputs with E2E real-time performance. Evaluated on the nuScenes dataset, our method demonstrates significant improvements in trajectory prediction accuracy, paving the way for more robust and reliable autonomous driving systems.",
        "arxiv_id": "2505.16805",
        "ARXIVID": "2505.16805",
        "COMMENT": "Matches criterion 2 as it integrates vision-language models (VLMs) into autonomous driving systems, showcasing a novel application.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.17021": {
        "authors": [
            "Sara Ghaboura",
            "Ketan More",
            "Wafa Alghallabi",
            "Omkar Thawakar",
            "Jorma Laaksonen",
            "Hisham Cholakkal",
            "Salman Khan",
            "Rao Muhammad Anwer"
        ],
        "title": "ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark",
        "abstract": "arXiv:2505.17021v1 Announce Type: new  Abstract: As Large Multimodal Models (LMMs) become more capable, there is growing interest in evaluating their reasoning processes alongside their final outputs. However, most benchmarks remain focused on English, overlooking languages with rich linguistic and cultural contexts, such as Arabic. To address this gap, we introduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the first benchmark designed to evaluate step-by-step reasoning in Arabic across both textual and visual modalities. ARB spans 11 diverse domains, including visual reasoning, document understanding, OCR, scientific analysis, and cultural interpretation. It comprises 1,356 multimodal samples paired with 5,119 human-curated reasoning steps and corresponding actions. We evaluated 12 state-of-the-art open- and closed-source LMMs and found persistent challenges in coherence, faithfulness, and cultural grounding. ARB offers a structured framework for diagnosing multimodal reasoning in underrepresented languages and marks a critical step toward inclusive, transparent, and culturally aware AI systems. We release the benchmark, rubric, and evaluation suit to support future research and reproducibility. Code available at: https://github.com/mbzuai-oryx/ARB",
        "arxiv_id": "2505.17021",
        "ARXIVID": "2505.17021",
        "COMMENT": "Matches criterion 2. Introduces a benchmark for multimodal reasoning in Arabic, focusing on LMMs and their evaluation.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2505.16025": {
        "authors": [
            "Wen Wen",
            "Yaohong Wu",
            "Yue Sheng",
            "Neil Birkbeck",
            "Balu Adsumilli",
            "Yilin Wang"
        ],
        "title": "CP-LLM: Context and Pixel Aware Large Language Model for Video Quality Assessment",
        "abstract": "arXiv:2505.16025v1 Announce Type: new  Abstract: Video quality assessment (VQA) is a challenging research topic with broad applications. Effective VQA necessitates sensitivity to pixel-level distortions and a comprehensive understanding of video context to accurately determine the perceptual impact of distortions. Traditional hand-crafted and learning-based VQA models mainly focus on pixel-level distortions and lack contextual understanding, while recent LLM-based models struggle with sensitivity to small distortions or handle quality scoring and description as separate tasks. To address these shortcomings, we introduce CP-LLM: a Context and Pixel aware Large Language Model. CP-LLM is a novel multimodal LLM architecture featuring dual vision encoders designed to independently analyze perceptual quality at both high-level (video context) and low-level (pixel distortion) granularity, along with a language decoder subsequently reasons about the interplay between these aspects. This design enables CP-LLM to simultaneously produce robust quality scores and interpretable quality descriptions, with enhanced sensitivity to pixel distortions (e.g. compression artifacts). The model is trained via a multi-task pipeline optimizing for score prediction, description generation, and pairwise comparisons. Experiment results demonstrate that CP-LLM achieves state-of-the-art cross-dataset performance on established VQA benchmarks and superior robustness to pixel distortions, confirming its efficacy for comprehensive and practical video quality assessment in real-world scenarios.",
        "arxiv_id": "2505.16025",
        "ARXIVID": "2505.16025",
        "COMMENT": "Matches criterion 2 as it introduces a multimodal LLM for video quality assessment, which is a novel application of VLLMs.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2505.16540": {
        "authors": [
            "Inbal Cohen",
            "Boaz Meivar",
            "Peihan Tu",
            "Shai Avidan",
            "Gal Oren"
        ],
        "title": "TextureSAM: Towards a Texture Aware Foundation Model for Segmentation",
        "abstract": "arXiv:2505.16540v1 Announce Type: new  Abstract: Segment Anything Models (SAM) have achieved remarkable success in object segmentation tasks across diverse datasets. However, these models are predominantly trained on large-scale semantic segmentation datasets, which introduce a bias toward object shape rather than texture cues in the image. This limitation is critical in domains such as medical imaging, material classification, and remote sensing, where texture changes define object boundaries. In this study, we investigate SAM's bias toward semantics over textures and introduce a new texture-aware foundation model, TextureSAM, which performs superior segmentation in texture-dominant scenarios. To achieve this, we employ a novel fine-tuning approach that incorporates texture augmentation techniques, incrementally modifying training images to emphasize texture features. By leveraging a novel texture-alternation of the ADE20K dataset, we guide TextureSAM to prioritize texture-defined regions, thereby mitigating the inherent shape bias present in the original SAM model. Our extensive experiments demonstrate that TextureSAM significantly outperforms SAM-2 on both natural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation datasets. The code and texture-augmented dataset will be publicly available.",
        "arxiv_id": "2505.16540",
        "ARXIVID": "2505.16540",
        "COMMENT": "Matches criterion 4. Proposes a texture-aware foundation model for segmentation, which is related to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.16459": {
        "authors": [
            "Guiyao Tie",
            "Xueyang Zhou",
            "Tianhe Gu",
            "Ruihang Zhang",
            "Chaoran Hu",
            "Sizhe Zhang",
            "Mengqu Sun",
            "Yan Zhang",
            "Pan Zhou",
            "Lichao Sun"
        ],
        "title": "MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks",
        "abstract": "arXiv:2505.16459v1 Announce Type: new  Abstract: Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, vision, and structured inputs, opening the door to complex tasks such as logical deduction, spatial reasoning, and scientific analysis. Despite their promise, the reasoning capabilities of MLLMs, particularly those augmented with intermediate thinking traces (MLLMs-T), remain poorly understood and lack standardized evaluation benchmarks. Existing work focuses primarily on perception or final answer correctness, offering limited insight into how models reason or fail across modalities. To address this gap, we introduce the MMMR, a new benchmark designed to rigorously evaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a high-difficulty dataset of 1,083 questions spanning six diverse reasoning types with symbolic depth and multi-hop demands and 2) a modular Reasoning Trace Evaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy through metrics like relevance, consistency, and structured error annotations. Empirical results show that MLLMs-T overall outperform non-thinking counterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro suffer from reasoning pathologies such as inconsistency and overthinking. This benchmark reveals persistent gaps between accuracy and reasoning quality and provides an actionable evaluation pipeline for future model development. Overall, the MMMR offers a scalable foundation for evaluating, comparing, and improving the next generation of multi-modal reasoning systems.",
        "arxiv_id": "2505.16459",
        "ARXIVID": "2505.16459",
        "COMMENT": "Matches criterion 3 as it introduces MMMR, a new benchmark for multi-modal reasoning tasks with explicit thinking, focusing on reasoning quality.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.16875": {
        "authors": [
            "Zhehao Huang",
            "Yuhang Liu",
            "Yixin Lou",
            "Zhengbao He",
            "Mingzhen He",
            "Wenxing Zhou",
            "Tao Li",
            "Kehan Li",
            "Zeyi Huang",
            "Xiaolin Huang"
        ],
        "title": "T2I-ConBench: Text-to-Image Benchmark for Continual Post-training",
        "abstract": "arXiv:2505.16875v1 Announce Type: new  Abstract: Continual post-training adapts a single text-to-image diffusion model to learn new tasks without incurring the cost of separate models, but naive post-training causes forgetting of pretrained knowledge and undermines zero-shot compositionality. We observe that the absence of a standardized evaluation protocol hampers related research for continual post-training. To address this, we introduce T2I-ConBench, a unified benchmark for continual post-training of text-to-image models. T2I-ConBench focuses on two practical scenarios, item customization and domain enhancement, and analyzes four dimensions: (1) retention of generality, (2) target-task performance, (3) catastrophic forgetting, and (4) cross-task generalization. It combines automated metrics, human-preference modeling, and vision-language QA for comprehensive assessment. We benchmark ten representative methods across three realistic task sequences and find that no approach excels on all fronts. Even joint \"oracle\" training does not succeed for every task, and cross-task generalization remains unsolved. We release all datasets, code, and evaluation tools to accelerate research in continual post-training for text-to-image models.",
        "arxiv_id": "2505.16875",
        "ARXIVID": "2505.16875",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (T2I-ConBench) for continual post-training in text-to-image models, focusing on novel evaluation protocols.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.16321": {
        "authors": [
            "Jie Zhao",
            "Xin Chen",
            "Yongsheng Yuan",
            "Michael Felsberg",
            "Dong Wang",
            "Huchuan Lu"
        ],
        "title": "Efficient Motion Prompt Learning for Robust Visual Tracking",
        "abstract": "arXiv:2505.16321v1 Announce Type: new  Abstract: Due to the challenges of processing temporal information, most trackers depend solely on visual discriminability and overlook the unique temporal coherence of video data. In this paper, we propose a lightweight and plug-and-play motion prompt tracking method. It can be easily integrated into existing vision-based trackers to build a joint tracking framework leveraging both motion and vision cues, thereby achieving robust tracking through efficient prompt learning. A motion encoder with three different positional encodings is proposed to encode the long-term motion trajectory into the visual embedding space, while a fusion decoder and an adaptive weight mechanism are designed to dynamically fuse visual and motion features. We integrate our motion module into three different trackers with five models in total. Experiments on seven challenging tracking benchmarks demonstrate that the proposed motion module significantly improves the robustness of vision-based trackers, with minimal training costs and negligible speed sacrifice. Code is available at https://github.com/zj5559/Motion-Prompt-Tracking.",
        "arxiv_id": "2505.16321",
        "ARXIVID": "2505.16321",
        "COMMENT": "Matches criterion 3 as it introduces a motion prompt learning method for robust visual tracking, which could be relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2505.16335": {
        "authors": [
            "Renjie Wei",
            "Songqiang Xu",
            "Qingyu Guo",
            "Meng Li"
        ],
        "title": "FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design",
        "abstract": "arXiv:2505.16335v1 Announce Type: new  Abstract: Visual autoregressive (VAR) modeling has marked a paradigm shift in image generation from next-token prediction to next-scale prediction. VAR predicts a set of tokens at each step from coarse to fine scale, leading to better image quality and faster inference speed compared to existing diffusion models. However, the large parameter size and computation cost hinder its deployment on edge devices. To reduce the memory and computation cost, we propose FPQVAR, an efficient post-training floating-point (FP) quantization framework for VAR featuring algorithm and hardware co-design. At the algorithm level, we first identify the challenges of quantizing VAR. To address them, we propose Dual Format Quantization for the highly imbalanced input activation. We further propose Group-wise Hadamard Transformation and GHT-Aware Learnable Transformation to address the time-varying outlier channels. At the hardware level, we design the first low-bit FP quantizer and multiplier with lookup tables on FPGA and propose the first FPGA-based VAR accelerator featuring low-bit FP computation and an elaborate two-level pipeline. Extensive experiments show that compared to the state-of-the-art quantization method, our proposed FPQVAR significantly improves Fr\\'echet Inception Distance (FID) from 10.83 to 3.58, Inception Score (IS) from 175.9 to 241.5 under 4-bit quantization. FPQVAR also significantly improves the performance of 6-bit quantized VAR, bringing it on par with the FP16 model. Our accelerator on AMD-Xilinx VCK190 FPGA achieves a throughput of 1.1 image/s, which is 3.1x higher than the integer-based accelerator. It also demonstrates 3.6x and 2.8x higher energy efficiency compared to the integer-based accelerator and GPU baseline, respectively.",
        "arxiv_id": "2505.16335",
        "ARXIVID": "2505.16335",
        "COMMENT": "Does not match any specific criteria but is generally relevant to visual autoregressive models and hardware co-design.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.16174": {
        "authors": [
            "Ping Liu",
            "Chi Zhang"
        ],
        "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility",
        "abstract": "arXiv:2505.16174v1 Announce Type: new  Abstract: To what extent does concept erasure eliminate generative capacity in diffusion models? While prior evaluations have primarily focused on measuring concept suppression under specific textual prompts, we explore a complementary and fundamental question: do current concept erasure techniques genuinely remove the ability to generate targeted concepts, or do they merely achieve superficial, prompt-specific suppression? We systematically evaluate the robustness and reversibility of two representative concept erasure methods, Unified Concept Editing and Erased Stable Diffusion, by probing their ability to eliminate targeted generative behaviors in text-to-image models. These methods attempt to suppress undesired semantic concepts by modifying internal model parameters, either through targeted attention edits or model-level fine-tuning strategies. To rigorously assess whether these techniques truly erase generative capacity, we propose an instance-level evaluation strategy that employs lightweight fine-tuning to explicitly test the reactivation potential of erased concepts. Through quantitative metrics and qualitative analyses, we show that erased concepts often reemerge with substantial visual fidelity after minimal adaptation, indicating that current methods suppress latent generative representations without fully eliminating them. Our findings reveal critical limitations in existing concept erasure approaches and highlight the need for deeper, representation-level interventions and more rigorous evaluation standards to ensure genuine, irreversible removal of concepts from generative models.",
        "arxiv_id": "2505.16174",
        "ARXIVID": "2505.16174",
        "COMMENT": "Does not match any specific criteria but is generally relevant to generative modeling and concept erasure in diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.16864": {
        "authors": [
            "Yuechen Zhang",
            "Jinbo Xing",
            "Bin Xia",
            "Shaoteng Liu",
            "Bohao Peng",
            "Xin Tao",
            "Pengfei Wan",
            "Eric Lo",
            "Jiaya Jia"
        ],
        "title": "Training-Free Efficient Video Generation via Dynamic Token Carving",
        "abstract": "arXiv:2505.16864v1 Announce Type: new  Abstract: Despite the remarkable generation quality of video Diffusion Transformer (DiT) models, their practical deployment is severely hindered by extensive computational requirements. This inefficiency stems from two key challenges: the quadratic complexity of self-attention with respect to token length and the multi-step nature of diffusion models. To address these limitations, we present Jenga, a novel inference pipeline that combines dynamic attention carving with progressive resolution generation. Our approach leverages two key insights: (1) early denoising steps do not require high-resolution latents, and (2) later steps do not require dense attention. Jenga introduces a block-wise attention mechanism that dynamically selects relevant token interactions using 3D space-filling curves, alongside a progressive resolution strategy that gradually increases latent resolution during generation. Experimental results demonstrate that Jenga achieves substantial speedups across multiple state-of-the-art video diffusion models while maintaining comparable generation quality (8.83$\\times$ speedup with 0.01\\% performance drop on VBench). As a plug-and-play solution, Jenga enables practical, high-quality video generation on modern hardware by reducing inference time from minutes to seconds -- without requiring model retraining. Code: https://github.com/dvlab-research/Jenga",
        "arxiv_id": "2505.16864",
        "ARXIVID": "2505.16864",
        "COMMENT": "Does not match any specific criteria but is generally relevant to generative modeling and efficiency improvements in video generation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.16687": {
        "authors": [
            "Naifu Xue",
            "Zhaoyang Jia",
            "Jiahao Li",
            "Bin Li",
            "Yuan Zhang",
            "Yan Lu"
        ],
        "title": "One-Step Diffusion-Based Image Compression with Semantic Distillation",
        "abstract": "arXiv:2505.16687v1 Announce Type: new  Abstract: While recent diffusion-based generative image codecs have shown impressive performance, their iterative sampling process introduces unpleasing latency. In this work, we revisit the design of a diffusion-based codec and argue that multi-step sampling is not necessary for generative compression. Based on this insight, we propose OneDC, a One-step Diffusion-based generative image Codec -- that integrates a latent compression module with a one-step diffusion generator. Recognizing the critical role of semantic guidance in one-step diffusion, we propose using the hyperprior as a semantic signal, overcoming the limitations of text prompts in representing complex visual content. To further enhance the semantic capability of the hyperprior, we introduce a semantic distillation mechanism that transfers knowledge from a pretrained generative tokenizer to the hyperprior codec. Additionally, we adopt a hybrid pixel- and latent-domain optimization to jointly enhance both reconstruction fidelity and perceptual realism. Extensive experiments demonstrate that OneDC achieves SOTA perceptual quality even with one-step generation, offering over 40% bitrate reduction and 20x faster decoding compared to prior multi-step diffusion-based codecs. Code will be released later.",
        "arxiv_id": "2505.16687",
        "ARXIVID": "2505.16687",
        "COMMENT": "Does not match any specific criteria. Focuses on image compression using diffusion models, which is not directly related to spatial understanding, VLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.16938": {
        "authors": [
            "NovelSeek Team",
            "Bo Zhang",
            "Shiyang Feng",
            "Xiangchao Yan",
            "Jiakang Yuan",
            "Zhiyin Yu",
            "Xiaohan He",
            "Songtao Huang",
            "Shaowei Hou",
            "Zheng Nie",
            "Zhilong Wang",
            "Jinyao Liu",
            "Runmin Ma",
            "Tianshuo Peng",
            "Peng Ye",
            "Dongzhan Zhou",
            "Shufei Zhang",
            "Xiaosong Wang",
            "Yilan Zhang",
            "Meng Li",
            "Zhongying Tu",
            "Xiangyu Yue",
            "Wangli Ouyang",
            "Bowen Zhou",
            "Lei Bai"
        ],
        "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification",
        "abstract": "arXiv:2505.16938v1 Announce Type: new  Abstract: Artificial Intelligence (AI) is accelerating the transformation of scientific research paradigms, not only enhancing research efficiency but also driving innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework to conduct Autonomous Scientific Research (ASR) across various scientific research fields, enabling researchers to tackle complicated problems in these fields with unprecedented speed and precision. NovelSeek highlights three key advantages: 1) Scalability: NovelSeek has demonstrated its versatility across 12 scientific research tasks, capable of generating innovative ideas to enhance the performance of baseline code. 2) Interactivity: NovelSeek provides an interface for human expert feedback and multi-agent interaction in automated end-to-end processes, allowing for the seamless integration of domain expert knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in several scientific fields with significantly less time cost compared to human efforts. For instance, in reaction yield prediction, it increased from 27.6% to 35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from 0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation, precision advanced from 78.8% to 81.0% in a mere 30 hours.",
        "arxiv_id": "2505.16938",
        "ARXIVID": "2505.16938",
        "COMMENT": "Does not match any specific criteria but discusses a multi-agent framework for autonomous scientific research, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.16324": {
        "authors": [
            "Cheng Cheng",
            "Lin Song",
            "Yicheng Xiao",
            "Yuxin Chen",
            "Xuchong Zhang",
            "Hongbin Sun",
            "Ying Shan"
        ],
        "title": "TensorAR: Refinement is All You Need in Autoregressive Image Generation",
        "abstract": "arXiv:2505.16324v1 Announce Type: new  Abstract: Autoregressive (AR) image generators offer a language-model-friendly approach to image generation by predicting discrete image tokens in a causal sequence. However, unlike diffusion models, AR models lack a mechanism to refine previous predictions, limiting their generation quality. In this paper, we introduce TensorAR, a new AR paradigm that reformulates image generation from next-token prediction to next-tensor prediction. By generating overlapping windows of image patches (tensors) in a sliding fashion, TensorAR enables iterative refinement of previously generated content. To prevent information leakage during training, we propose a discrete tensor noising scheme, which perturbs input tokens via codebook-indexed noise. TensorAR is implemented as a plug-and-play module compatible with existing AR models. Extensive experiments on LlamaGEN, Open-MAGVIT2, and RAR demonstrate that TensorAR significantly improves the generation performance of autoregressive models.",
        "arxiv_id": "2505.16324",
        "ARXIVID": "2505.16324",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.16157": {
        "authors": [
            "Yuang Ai",
            "Huaibo Huang",
            "Tao Wu",
            "Qihang Fan",
            "Ran He"
        ],
        "title": "Breaking Complexity Barriers: High-Resolution Image Restoration with Rank Enhanced Linear Attention",
        "abstract": "arXiv:2505.16157v1 Announce Type: new  Abstract: Transformer-based models have made remarkable progress in image restoration (IR) tasks. However, the quadratic complexity of self-attention in Transformer hinders its applicability to high-resolution images. Existing methods mitigate this issue with sparse or window-based attention, yet inherently limit global context modeling. Linear attention, a variant of softmax attention, demonstrates promise in global context modeling while maintaining linear complexity, offering a potential solution to the above challenge. Despite its efficiency benefits, vanilla linear attention suffers from a significant performance drop in IR, largely due to the low-rank nature of its attention map. To counter this, we propose Rank Enhanced Linear Attention (RELA), a simple yet effective method that enriches feature representations by integrating a lightweight depthwise convolution. Building upon RELA, we propose an efficient and effective image restoration Transformer, named LAformer. LAformer achieves effective global perception by integrating linear attention and channel attention, while also enhancing local fitting capabilities through a convolutional gated feed-forward network. Notably, LAformer eliminates hardware-inefficient operations such as softmax and window shifting, enabling efficient processing of high-resolution images. Extensive experiments across 7 IR tasks and 21 benchmarks demonstrate that LAformer outperforms SOTA methods and offers significant computational advantages.",
        "arxiv_id": "2505.16157",
        "ARXIVID": "2505.16157",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of computer vision and efficient modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16039": {
        "authors": [
            "Shuvashis Sarker",
            "Shamim Rahim Refat",
            "Faika Fairuj Preotee",
            "Shifat Islam",
            "Tashreef Muhammad",
            "Mohammad Ashraful Hoque"
        ],
        "title": "An Exploratory Approach Towards Investigating and Explaining Vision Transformer and Transfer Learning for Brain Disease Detection",
        "abstract": "arXiv:2505.16039v1 Announce Type: new  Abstract: The brain is a highly complex organ that manages many important tasks, including movement, memory and thinking. Brain-related conditions, like tumors and degenerative disorders, can be hard to diagnose and treat. Magnetic Resonance Imaging (MRI) serves as a key tool for identifying these conditions, offering high-resolution images of brain structures. Despite this, interpreting MRI scans can be complicated. This study tackles this challenge by conducting a comparative analysis of Vision Transformer (ViT) and Transfer Learning (TL) models such as VGG16, VGG19, Resnet50V2, MobilenetV2 for classifying brain diseases using MRI data from Bangladesh based dataset. ViT, known for their ability to capture global relationships in images, are particularly effective for medical imaging tasks. Transfer learning helps to mitigate data constraints by fine-tuning pre-trained models. Furthermore, Explainable AI (XAI) methods such as GradCAM, GradCAM++, LayerCAM, ScoreCAM, and Faster-ScoreCAM are employed to interpret model predictions. The results demonstrate that ViT surpasses transfer learning models, achieving a classification accuracy of 94.39%. The integration of XAI methods enhances model transparency, offering crucial insights to aid medical professionals in diagnosing brain diseases with greater precision.",
        "arxiv_id": "2505.16039",
        "ARXIVID": "2505.16039",
        "COMMENT": "Does not match any specific criteria but is generally relevant to computer vision and medical imaging using Vision Transformers.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16294": {
        "authors": [
            "Yufei Yin",
            "Lechao Cheng",
            "Wengang Zhou",
            "Jiajun Deng",
            "Zhou Yu",
            "Houqiang Li"
        ],
        "title": "Self-Classification Enhancement and Correction for Weakly Supervised Object Detection",
        "abstract": "arXiv:2505.16294v1 Announce Type: new  Abstract: In recent years, weakly supervised object detection (WSOD) has attracted much attention due to its low labeling cost. The success of recent WSOD models is often ascribed to the two-stage multi-class classification (MCC) task, i.e., multiple instance learning and online classification refinement. Despite achieving non-trivial progresses, these methods overlook potential classification ambiguities between these two MCC tasks and fail to leverage their unique strengths. In this work, we introduce a novel WSOD framework to ameliorate these two issues. For one thing, we propose a self-classification enhancement module that integrates intra-class binary classification (ICBC) to bridge the gap between the two distinct MCC tasks. The ICBC task enhances the network's discrimination between positive and mis-located samples in a class-wise manner and forges a mutually reinforcing relationship with the MCC task. For another, we propose a self-classification correction algorithm during inference, which combines the results of both MCC tasks to effectively reduce the mis-classified predictions. Extensive experiments on the prevalent VOC 2007 & 2012 datasets demonstrate the superior performance of our framework.",
        "arxiv_id": "2505.16294",
        "ARXIVID": "2505.16294",
        "COMMENT": "Does not match any specific criteria but is generally relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16792": {
        "authors": [
            "Ziqiao Wang",
            "Wangbo Zhao",
            "Yuhao Zhou",
            "Zekai Li",
            "Zhiyuan Liang",
            "Mingjia Shi",
            "Xuanlei Zhao",
            "Pengfei Zhou",
            "Kaipeng Zhang",
            "Zhangyang Wang",
            "Kai Wang",
            "Yang You"
        ],
        "title": "REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training",
        "abstract": "arXiv:2505.16792v1 Announce Type: new  Abstract: Diffusion Transformers (DiTs) deliver state-of-the-art image quality, yet their training remains notoriously slow. A recent remedy -- representation alignment (REPA) that matches DiT hidden features to those of a non-generative teacher (e.g. DINO) -- dramatically accelerates the early epochs but plateaus or even degrades performance later. We trace this failure to a capacity mismatch: once the generative student begins modelling the joint data distribution, the teacher's lower-dimensional embeddings and attention patterns become a straitjacket rather than a guide. We then introduce HASTE (Holistic Alignment with Stage-wise Termination for Efficient training), a two-phase schedule that keeps the help and drops the hindrance. Phase I applies a holistic alignment loss that simultaneously distills attention maps (relational priors) and feature projections (semantic anchors) from the teacher into mid-level layers of the DiT, yielding rapid convergence. Phase II then performs one-shot termination that deactivates the alignment loss, once a simple trigger such as a fixed iteration is hit, freeing the DiT to focus on denoising and exploit its generative capacity. HASTE speeds up training of diverse DiTs without architecture changes. On ImageNet 256X256, it reaches the vanilla SiT-XL/2 baseline FID in 50 epochs and matches REPA's best FID in 500 epochs, amounting to a 28X reduction in optimization steps. HASTE also improves text-to-image DiTs on MS-COCO, demonstrating to be a simple yet principled recipe for efficient diffusion training across various tasks. Our code is available at https://github.com/NUS-HPC-AI-Lab/HASTE .",
        "arxiv_id": "2505.16792",
        "ARXIVID": "2505.16792",
        "COMMENT": "Does not match any specific criterion but discusses efficient training for diffusion transformers, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16402": {
        "authors": [
            "Yuanhao Huang",
            "Yilong Ren",
            "Jinlei Wang",
            "Lujia Huo",
            "Xuesong Bai",
            "Jinchuan Zhang",
            "Haiyan Yu"
        ],
        "title": "AdvReal: Adversarial Patch Generation Framework with Application to Adversarial Safety Evaluation of Object Detection Systems",
        "abstract": "arXiv:2505.16402v1 Announce Type: new  Abstract: Autonomous vehicles are typical complex intelligent systems with artificial intelligence at their core. However, perception methods based on deep learning are extremely vulnerable to adversarial samples, resulting in safety accidents. How to generate effective adversarial examples in the physical world and evaluate object detection systems is a huge challenge. In this study, we propose a unified joint adversarial training framework for both 2D and 3D samples to address the challenges of intra-class diversity and environmental variations in real-world scenarios. Building upon this framework, we introduce an adversarial sample reality enhancement approach that incorporates non-rigid surface modeling and a realistic 3D matching mechanism. We compare with 5 advanced adversarial patches and evaluate their attack performance on 8 object detecotrs, including single-stage, two-stage, and transformer-based models. Extensive experiment results in digital and physical environments demonstrate that the adversarial textures generated by our method can effectively mislead the target detection model. Moreover, proposed method demonstrates excellent robustness and transferability under multi-angle attacks, varying lighting conditions, and different distance in the physical world. The demo video and code can be obtained at https://github.com/Huangyh98/AdvReal.git.",
        "arxiv_id": "2505.16402",
        "ARXIVID": "2505.16402",
        "COMMENT": "Does not match any specific criteria. Focuses on adversarial patch generation for object detection systems, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16314": {
        "authors": [
            "Shuhao Han",
            "Haotian Fan",
            "Fangyuan Kong",
            "Wenjie Liao",
            "Chunle Guo",
            "Chongyi Li",
            "Radu Timofte",
            "Liang Li",
            "Tao Li",
            "Junhui Cui",
            "Yunqiu Wang",
            "Yang Tai",
            "Jingwei Sun",
            "Jianhui Sun",
            "Xinli Yue",
            "Tianyi Wang",
            "Huan Hou",
            "Junda Lu",
            "Xinyang Huang",
            "Zitang Zhou",
            "Zijian Zhang",
            "Xuhui Zheng",
            "Xuecheng Wu",
            "Chong Peng",
            "Xuezhi Cao",
            "Trong-Hieu Nguyen-Mau",
            "Minh-Hoang Le",
            "Minh-Khoa Le-Phan",
            "Duy-Nam Ly",
            "Hai-Dang Nguyen",
            "Minh-Triet Tran",
            "Yukang Lin",
            "Yan Hong",
            "Chuanbiao Song",
            "Siyuan Li",
            "Jun Lan",
            "Zhichao Zhang",
            "Xinyue Li",
            "Wei Sun",
            "Zicheng Zhang",
            "Yunhao Li",
            "Xiaohong Liu",
            "Guangtao Zhai",
            "Zitong Xu",
            "Huiyu Duan",
            "Jiarui Wang",
            "Guangji Ma",
            "Liu Yang",
            "Lu Liu",
            "Qiang Hu",
            "Xiongkuo Min",
            "Zichuan Wang",
            "Zhenchen Tang",
            "Bo Peng",
            "Jing Dong",
            "Fengbin Guan",
            "Zihao Yu",
            "Yiting Lu",
            "Wei Luo",
            "Xin Li",
            "Minhao Lin",
            "Haofeng Chen",
            "Xuanxuan He",
            "Kele Xu",
            "Qisheng Xu",
            "Zijian Gao",
            "Tianjiao Wan",
            "Bo-Cheng Qiu",
            "Chih-Chung Hsu",
            "Chia-ming Lee",
            "Yu-Fan Lin",
            "Bo Yu",
            "Zehao Wang",
            "Da Mu",
            "Mingxiu Chen",
            "Junkang Fang",
            "Huamei Sun",
            "Wending Zhao",
            "Zhiyu Wang",
            "Wang Liu",
            "Weikang Yu",
            "Puhong Duan",
            "Bin Sun",
            "Xudong Kang",
            "Shutao Li",
            "Shuai He",
            "Lingzhi Fu",
            "Heng Cong",
            "Rongyu Zhang",
            "Jiarong He",
            "Zhishan Qiao",
            "Yongqing Huang",
            "Zewen Chen",
            "Zhe Pang",
            "Juan Wang",
            "Jian Guo",
            "Zhizhuo Shao",
            "Ziyu Feng",
            "Bing Li",
            "Weiming Hu",
            "Hesong Li",
            "Dehua Liu",
            "Zeming Liu",
            "Qingsong Xie",
            "Ruichen Wang",
            "Zhihao Li",
            "Yuqi Liang",
            "Jianqi Bi",
            "Jun Luo",
            "Junfeng Yang",
            "Can Li",
            "Jing Fu",
            "Hongwei Xu",
            "Mingrui Long",
            "Lulin Tang"
        ],
        "title": "NTIRE 2025 challenge on Text to Image Generation Model Quality Assessment",
        "abstract": "arXiv:2505.16314v1 Announce Type: new  Abstract: This paper reports on the NTIRE 2025 challenge on Text to Image (T2I) generation model quality assessment, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2025. The aim of this challenge is to address the fine-grained quality assessment of text-to-image generation models. This challenge evaluates text-to-image models from two aspects: image-text alignment and image structural distortion detection, and is divided into the alignment track and the structural track. The alignment track uses the EvalMuse-40K, which contains around 40K AI-Generated Images (AIGIs) generated by 20 popular generative models. The alignment track has a total of 371 registered participants. A total of 1,883 submissions are received in the development phase, and 507 submissions are received in the test phase. Finally, 12 participating teams submitted their models and fact sheets. The structure track uses the EvalMuse-Structure, which contains 10,000 AI-Generated Images (AIGIs) with corresponding structural distortion mask. A total of 211 participants have registered in the structure track. A total of 1155 submissions are received in the development phase, and 487 submissions are received in the test phase. Finally, 8 participating teams submitted their models and fact sheets. Almost all methods have achieved better results than baseline methods, and the winning methods in both tracks have demonstrated superior prediction performance on T2I model quality assessment.",
        "arxiv_id": "2505.16314",
        "ARXIVID": "2505.16314",
        "COMMENT": "Does not match any specific criteria. Focuses on quality assessment for text-to-image generation models, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16221": {
        "authors": [
            "Yifan Zhang",
            "Xinkui Zhao",
            "Zuxin Wang",
            "Guanjie Cheng",
            "Yueshen Xu",
            "Shuiguang Deng",
            "Jianwei Yin"
        ],
        "title": "LightRouter: Towards Efficient LLM Collaboration with Minimal Overhead",
        "abstract": "arXiv:2505.16221v1 Announce Type: new  Abstract: The rapid advancement of large language models has unlocked remarkable capabilities across a diverse array of natural language processing tasks. However, the considerable differences among available LLMs-in terms of cost, performance, and computational demands-pose significant challenges for users aiming to identify the most suitable model for specific tasks. In this work, we present LightRouter, a novel framework designed to systematically select and integrate a small subset of LLMs from a larger pool, with the objective of jointly optimizing both task performance and cost efficiency. LightRouter leverages an adaptive selection mechanism to identify models that require only a minimal number of boot tokens, thereby reducing costs, and further employs an effective integration strategy to combine their outputs. Extensive experiments across multiple benchmarks demonstrate that LightRouter matches or outperforms widely-used ensemble baselines, achieving up to a 25% improvement in accuracy. Compared with leading high-performing models, LightRouter achieves comparable performance while reducing inference costs by up to 27%. Importantly, our framework operates without any prior knowledge of individual models and relies exclusively on inexpensive, lightweight models. This work introduces a practical approach for efficient LLM selection and provides valuable insights into optimal strategies for model combination.",
        "arxiv_id": "2505.16221",
        "ARXIVID": "2505.16221",
        "COMMENT": "Does not match any specific criteria. Focuses on efficient LLM collaboration, which is not directly related to spatial understanding, VLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16756": {
        "authors": [
            "Hailong Ning",
            "Siying Wang",
            "Tao Lei",
            "Xiaopeng Cao",
            "Huanmin Dou",
            "Bin Zhao",
            "Asoke K. Nandi",
            "Petia Radeva"
        ],
        "title": "Representation Discrepancy Bridging Method for Remote Sensing Image-Text Retrieval",
        "abstract": "arXiv:2505.16756v1 Announce Type: new  Abstract: Remote Sensing Image-Text Retrieval (RSITR) plays a critical role in geographic information interpretation, disaster monitoring, and urban planning by establishing semantic associations between image and textual descriptions. Existing Parameter-Efficient Fine-Tuning (PEFT) methods for Vision-and-Language Pre-training (VLP) models typically adopt symmetric adapter structures for exploring cross-modal correlations. However, the strong discriminative nature of text modality may dominate the optimization process and inhibits image representation learning. The nonnegligible imbalanced cross-modal optimization remains a bottleneck to enhancing the model performance. To address this issue, this study proposes a Representation Discrepancy Bridging (RDB) method for the RSITR task. On the one hand, a Cross-Modal Asymmetric Adapter (CMAA) is designed to enable modality-specific optimization and improve feature alignment. The CMAA comprises a Visual Enhancement Adapter (VEA) and a Text Semantic Adapter (TSA). VEA mines fine-grained image features by Differential Attention (DA) mechanism, while TSA identifies key textual semantics through Hierarchical Attention (HA) mechanism. On the other hand, this study extends the traditional single-task retrieval framework to a dual-task optimization framework and develops a Dual-Task Consistency Loss (DTCL). The DTCL improves cross-modal alignment robustness through an adaptive weighted combination of cross-modal, classification, and exponential moving average consistency constraints. Experiments on RSICD and RSITMD datasets show that the proposed RDB method achieves a 6%-11% improvement in mR metrics compared to state-of-the-art PEFT methods and a 1.15%-2% improvement over the full fine-tuned GeoRSCLIP model.",
        "arxiv_id": "2505.16756",
        "ARXIVID": "2505.16756",
        "COMMENT": "Does not match any specific criteria. Focuses on remote sensing image-text retrieval, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16166": {
        "authors": [
            "Yuhao Xue",
            "Zhifei Zhang",
            "Xinyang Jiang",
            "Yifei Shen",
            "Junyao Gao",
            "Wentao Gu",
            "Jiale Zhao",
            "Miaojing Shi",
            "Cairong Zhao"
        ],
        "title": "TRAIL: Transferable Robust Adversarial Images via Latent diffusion",
        "abstract": "arXiv:2505.16166v1 Announce Type: new  Abstract: Adversarial attacks exploiting unrestricted natural perturbations present severe security risks to deep learning systems, yet their transferability across models remains limited due to distribution mismatches between generated adversarial features and real-world data. While recent works utilize pre-trained diffusion models as adversarial priors, they still encounter challenges due to the distribution shift between the distribution of ideal adversarial samples and the natural image distribution learned by the diffusion model. To address the challenge, we propose Transferable Robust Adversarial Images via Latent Diffusion (TRAIL), a test-time adaptation framework that enables the model to generate images from a distribution of images with adversarial features and closely resembles the target images. To mitigate the distribution shift, during attacks, TRAIL updates the diffusion U-Net's weights by combining adversarial objectives (to mislead victim models) and perceptual constraints (to preserve image realism). The adapted model then generates adversarial samples through iterative noise injection and denoising guided by these objectives. Experiments demonstrate that TRAIL significantly outperforms state-of-the-art methods in cross-model attack transferability, validating that distribution-aligned adversarial feature synthesis is critical for practical black-box attacks.",
        "arxiv_id": "2505.16166",
        "ARXIVID": "2505.16166",
        "COMMENT": "Does not match any specific criteria. Focuses on adversarial attacks and transferability, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16659": {
        "authors": [
            "Kaiyu Guo",
            "Tan Pan",
            "Chen Jiang",
            "Zijian Wang",
            "Brian C. Lovell",
            "Limei Han",
            "Yuan Cheng",
            "Mahsa Baktashmotlagh"
        ],
        "title": "SD-MAD: Sign-Driven Few-shot Multi-Anomaly Detection in Medical Images",
        "abstract": "arXiv:2505.16659v1 Announce Type: new  Abstract: Medical anomaly detection (AD) is crucial for early clinical intervention, yet it faces challenges due to limited access to high-quality medical imaging data, caused by privacy concerns and data silos. Few-shot learning has emerged as a promising approach to alleviate these limitations by leveraging the large-scale prior knowledge embedded in vision-language models (VLMs). Recent advancements in few-shot medical AD have treated normal and abnormal cases as a one-class classification problem, often overlooking the distinction among multiple anomaly categories. Thus, in this paper, we propose a framework tailored for few-shot medical anomaly detection in the scenario where the identification of multiple anomaly categories is required. To capture the detailed radiological signs of medical anomaly categories, our framework incorporates diverse textual descriptions for each category generated by a Large-Language model, under the assumption that different anomalies in medical images may share common radiological signs in each category. Specifically, we introduce SD-MAD, a two-stage Sign-Driven few-shot Multi-Anomaly Detection framework: (i) Radiological signs are aligned with anomaly categories by amplifying inter-anomaly discrepancy; (ii) Aligned signs are selected further to mitigate the effect of the under-fitting and uncertain-sample issue caused by limited medical data, employing an automatic sign selection strategy at inference. Moreover, we propose three protocols to comprehensively quantify the performance of multi-anomaly detection. Extensive experiments illustrate the effectiveness of our method.",
        "arxiv_id": "2505.16659",
        "ARXIVID": "2505.16659",
        "COMMENT": "Does not match any specific criteria. Focuses on few-shot multi-anomaly detection in medical images, which is outside the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16877": {
        "authors": [
            "Yuqicheng Zhu",
            "Daniel Hern\\'andez",
            "Yuan He",
            "Zifeng Ding",
            "Bo Xiong",
            "Evgeny Kharlamov",
            "Steffen Staab"
        ],
        "title": "Predicate-Conditional Conformalized Answer Sets for Knowledge Graph Embeddings",
        "abstract": "arXiv:2505.16877v1 Announce Type: new  Abstract: Uncertainty quantification in Knowledge Graph Embedding (KGE) methods is crucial for ensuring the reliability of downstream applications. A recent work applies conformal prediction to KGE methods, providing uncertainty estimates by generating a set of answers that is guaranteed to include the true answer with a predefined confidence level. However, existing methods provide probabilistic guarantees averaged over a reference set of queries and answers (marginal coverage guarantee). In high-stakes applications such as medical diagnosis, a stronger guarantee is often required: the predicted sets must provide consistent coverage per query (conditional coverage guarantee). We propose CondKGCP, a novel method that approximates predicate-conditional coverage guarantees while maintaining compact prediction sets. CondKGCP merges predicates with similar vector representations and augments calibration with rank information. We prove the theoretical guarantees and demonstrate empirical effectiveness of CondKGCP by comprehensive evaluations.",
        "arxiv_id": "2505.16877",
        "ARXIVID": "2505.16877",
        "COMMENT": "Does not match any specific criteria. Focuses on uncertainty quantification in knowledge graph embeddings, which is outside the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16283": {
        "authors": [
            "Lijian Li",
            "Yuanpeng He",
            "Chi-Man Pun"
        ],
        "title": "Efficient Prototype Consistency Learning in Medical Image Segmentation via Joint Uncertainty and Data Augmentation",
        "abstract": "arXiv:2505.16283v1 Announce Type: new  Abstract: Recently, prototype learning has emerged in semi-supervised medical image segmentation and achieved remarkable performance. However, the scarcity of labeled data limits the expressiveness of prototypes in previous methods, potentially hindering the complete representation of prototypes for class embedding. To overcome this issue, we propose an efficient prototype consistency learning via joint uncertainty quantification and data augmentation (EPCL-JUDA) to enhance the semantic expression of prototypes based on the framework of Mean-Teacher. The concatenation of original and augmented labeled data is fed into student network to generate expressive prototypes. Then, a joint uncertainty quantification method is devised to optimize pseudo-labels and generate reliable prototypes for original and augmented unlabeled data separately. High-quality global prototypes for each class are formed by fusing labeled and unlabeled prototypes, which are utilized to generate prototype-to-features to conduct consistency learning. Notably, a prototype network is proposed to reduce high memory requirements brought by the introduction of augmented data. Extensive experiments on Left Atrium, Pancreas-NIH, Type B Aortic Dissection datasets demonstrate EPCL-JUDA's superiority over previous state-of-the-art approaches, confirming the effectiveness of our framework. The code will be released soon.",
        "arxiv_id": "2505.16283",
        "ARXIVID": "2505.16283",
        "COMMENT": "Does not match any specific criteria but focuses on prototype learning for medical image segmentation, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16256": {
        "authors": [
            "Yan Zhao",
            "Zhengxue Cheng",
            "Junxuan Zhang",
            "Qunshan Gu",
            "Qi Wang",
            "Li Song"
        ],
        "title": "DualComp: End-to-End Learning of a Unified Dual-Modality Lossless Compressor",
        "abstract": "arXiv:2505.16256v1 Announce Type: new  Abstract: Most learning-based lossless compressors are designed for a single modality, requiring separate models for multi-modal data and lacking flexibility. However, different modalities vary significantly in format and statistical properties, making it ineffective to use compressors that lack modality-specific adaptations. While multi-modal large language models (MLLMs) offer a potential solution for modality-unified compression, their excessive complexity hinders practical deployment. To address these challenges, we focus on the two most common modalities, image and text, and propose DualComp, the first unified and lightweight learning-based dual-modality lossless compressor. Built on a lightweight backbone, DualComp incorporates three key structural enhancements to handle modality heterogeneity: modality-unified tokenization, modality-switching contextual learning, and modality-routing mixture-of-experts. A reparameterization training strategy is also used to boost compression performance. DualComp integrates both modality-specific and shared parameters for efficient parameter utilization, enabling near real-time inference (200KB/s) on desktop CPUs. With much fewer parameters, DualComp achieves compression performance on par with the SOTA LLM-based methods for both text and image datasets. Its simplified single-modality variant surpasses the previous best image compressor on the Kodak dataset by about 9% using just 1.2% of the model size.",
        "arxiv_id": "2505.16256",
        "ARXIVID": "2505.16256",
        "COMMENT": "Does not match any specific criteria but discusses a dual-modality lossless compressor, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16318": {
        "authors": [
            "Hossein Khalili",
            "Seongbin Park",
            "Venkat Bollapragada",
            "Nader Sehatbakhsh"
        ],
        "title": "SuperPure: Efficient Purification of Localized and Distributed Adversarial Patches via Super-Resolution GAN Models",
        "abstract": "arXiv:2505.16318v1 Announce Type: new  Abstract: As vision-based machine learning models are increasingly integrated into autonomous and cyber-physical systems, concerns about (physical) adversarial patch attacks are growing. While state-of-the-art defenses can achieve certified robustness with minimal impact on utility against highly-concentrated localized patch attacks, they fall short in two important areas: (i) State-of-the-art methods are vulnerable to low-noise distributed patches where perturbations are subtly dispersed to evade detection or masking, as shown recently by the DorPatch attack; (ii) Achieving high robustness with state-of-the-art methods is extremely time and resource-consuming, rendering them impractical for latency-sensitive applications in many cyber-physical systems.   To address both robustness and latency issues, this paper proposes a new defense strategy for adversarial patch attacks called SuperPure. The key novelty is developing a pixel-wise masking scheme that is robust against both distributed and localized patches. The masking involves leveraging a GAN-based super-resolution scheme to gradually purify the image from adversarial patches. Our extensive evaluations using ImageNet and two standard classifiers, ResNet and EfficientNet, show that SuperPure advances the state-of-the-art in three major directions: (i) it improves the robustness against conventional localized patches by more than 20%, on average, while also improving top-1 clean accuracy by almost 10%; (ii) It achieves 58% robustness against distributed patch attacks (as opposed to 0% in state-of-the-art method, PatchCleanser); (iii) It decreases the defense end-to-end latency by over 98% compared to PatchCleanser. Our further analysis shows that SuperPure is robust against white-box attacks and different patch sizes. Our code is open-source.",
        "arxiv_id": "2505.16318",
        "ARXIVID": "2505.16318",
        "COMMENT": "Does not match any specific criteria but focuses on adversarial patch defense using GANs, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16477": {
        "authors": [
            "Yanbo Zhang",
            "Sumeer A. Khan",
            "Adnan Mahmud",
            "Huck Yang",
            "Alexander Lavin",
            "Michael Levin",
            "Jeremy Frey",
            "Jared Dunnmon",
            "James Evans",
            "Alan Bundy",
            "Saso Dzeroski",
            "Jesper Tegner",
            "Hector Zenil"
        ],
        "title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
        "abstract": "arXiv:2505.16477v1 Announce Type: new  Abstract: With recent Nobel Prizes recognising AI contributions to science, Large Language Models (LLMs) are transforming scientific research by enhancing productivity and reshaping the scientific method. LLMs are now involved in experimental design, data analysis, and workflows, particularly in chemistry and biology. However, challenges such as hallucinations and reliability persist. In this contribution, we review how Large Language Models (LLMs) are redefining the scientific method and explore their potential applications across different stages of the scientific cycle, from hypothesis testing to discovery. We conclude that, for LLMs to serve as relevant and effective creative engines and productivity enhancers, their deep integration into all steps of the scientific process should be pursued in collaboration and alignment with human scientific goals, with clear evaluation metrics. The transition to AI-driven science raises ethical questions about creativity, oversight, and responsibility. With careful guidance, LLMs could evolve into creative engines, driving transformative breakthroughs across scientific disciplines responsibly and effectively. However, the scientific community must also decide how much it leaves to LLMs to drive science, even when associations with 'reasoning', mostly currently undeserved, are made in exchange for the potential to explore hypothesis and solution regions that might otherwise remain unexplored by human exploration alone.",
        "arxiv_id": "2505.16477",
        "ARXIVID": "2505.16477",
        "COMMENT": "Does not match any specific criteria but discusses the role of LLMs in scientific discovery, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16778": {
        "authors": [
            "Xianing Chen",
            "Si Huo",
            "Borui Jiang",
            "Hailin Hu",
            "Xinghao Chen"
        ],
        "title": "Single Domain Generalization for Few-Shot Counting via Universal Representation Matching",
        "abstract": "arXiv:2505.16778v1 Announce Type: new  Abstract: Few-shot counting estimates the number of target objects in an image using only a few annotated exemplars. However, domain shift severely hinders existing methods to generalize to unseen scenarios. This falls into the realm of single domain generalization that remains unexplored in few-shot counting. To solve this problem, we begin by analyzing the main limitations of current methods, which typically follow a standard pipeline that extract the object prototypes from exemplars and then match them with image feature to construct the correlation map. We argue that existing methods overlook the significance of learning highly generalized prototypes. Building on this insight, we propose the first single domain generalization few-shot counting model, Universal Representation Matching, termed URM. Our primary contribution is the discovery that incorporating universal vision-language representations distilled from a large scale pretrained vision-language model into the correlation construction process substantially improves robustness to domain shifts without compromising in domain performance. As a result, URM achieves state-of-the-art performance on both in domain and the newly introduced domain generalization setting.",
        "arxiv_id": "2505.16778",
        "ARXIVID": "2505.16778",
        "COMMENT": "Does not match any specific criteria but is relevant to domain generalization in few-shot counting, which is tangentially related to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16448": {
        "authors": [
            "Renfei Dang",
            "Shujian Huang",
            "Jiajun Chen"
        ],
        "title": "Internal Bias in Reasoning Models leads to Overthinking",
        "abstract": "arXiv:2505.16448v1 Announce Type: new  Abstract: While current reasoning models possess strong exploratory capabilities, they are often criticized for overthinking due to redundant and unnecessary reflections. In this work, we reveal for the first time that overthinking in reasoning models may stem from their internal bias towards input texts. Upon encountering a reasoning problem, the model immediately forms a preliminary guess about the answer, which we term as an internal bias since it is not derived through actual reasoning. When this guess conflicts with its reasoning result, the model tends to engage in reflection, leading to the waste of computational resources. Through further interpretability experiments, we find that this behavior is largely driven by the model's excessive attention to the input section, which amplifies the influence of internal bias on its decision-making process. Additionally, by masking out the original input section, the affect of internal bias can be effectively alleviated and the reasoning length could be reduced by 31%-53% across different complex reasoning tasks. Notably, in most cases, this approach also leads to improvements in accuracy. These findings demonstrate a causal relationship between internal bias and overthinking.",
        "arxiv_id": "2505.16448",
        "ARXIVID": "2505.16448",
        "COMMENT": "Does not match any specific criteria but is relevant to reasoning models and their behavior, which is tangentially related to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16067": {
        "authors": [
            "Zidi Xiong",
            "Yuping Lin",
            "Wenya Xie",
            "Pengfei He",
            "Jiliang Tang",
            "Himabindu Lakkaraju",
            "Zhen Xiang"
        ],
        "title": "How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior",
        "abstract": "arXiv:2505.16067v1 Announce Type: new  Abstract: Memory is a critical component in large language model (LLM)-based agents, enabling them to store and retrieve past executions to improve task performance over time. In this paper, we conduct an empirical study on how memory management choices impact the LLM agents' behavior, especially their long-term performance. Specifically, we focus on two fundamental memory operations that are widely used by many agent frameworks-addition, which incorporates new experiences into the memory base, and deletion, which selectively removes past experiences-to systematically study their impact on the agent behavior. Through our quantitative analysis, we find that LLM agents display an experience-following property: high similarity between a task input and the input in a retrieved memory record often results in highly similar agent outputs. Our analysis further reveals two significant challenges associated with this property: error propagation, where inaccuracies in past experiences compound and degrade future performance, and misaligned experience replay, where outdated or irrelevant experiences negatively influence current tasks. Through controlled experiments, we show that combining selective addition and deletion strategies can help mitigate these negative effects, yielding an average absolute performance gain of 10% compared to naive memory growth. Furthermore, we highlight how memory management choices affect agents' behavior under challenging conditions such as task distribution shifts and constrained memory resources. Our findings offer insights into the behavioral dynamics of LLM agent memory systems and provide practical guidance for designing memory components that support robust, long-term agent performance. We also release our code to facilitate further study.",
        "arxiv_id": "2505.16067",
        "ARXIVID": "2505.16067",
        "COMMENT": "Does not match any specific criteria but is relevant to memory management in LLM agents, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16475": {
        "authors": [
            "Jiaqi Li",
            "Xinyi Dong",
            "Yang Liu",
            "Zhizhuo Yang",
            "Quansen Wang",
            "Xiaobo Wang",
            "SongChun Zhu",
            "Zixia Jia",
            "Zilong Zheng"
        ],
        "title": "ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection",
        "abstract": "arXiv:2505.16475v1 Announce Type: new  Abstract: We present a novel pipeline, ReflectEvo, to demonstrate that small language models (SLMs) can enhance meta introspection through reflection learning. This process iteratively generates self-reflection for self-training, fostering a continuous and self-evolving process. Leveraging this pipeline, we construct ReflectEvo-460k, a large-scale, comprehensive, self-generated reflection dataset with broadened instructions and diverse multi-domain tasks. Building upon this dataset, we demonstrate the effectiveness of reflection learning to improve SLMs' reasoning abilities using SFT and DPO with remarkable performance, substantially boosting Llama-3 from 52.4% to 71.2% and Mistral from 44.4% to 71.1%. It validates that ReflectEvo can rival or even surpass the reasoning capability of the three prominent open-sourced models on BIG-bench without distillation from superior models or fine-grained human annotation. We further conduct a deeper analysis of the high quality of self-generated reflections and their impact on error localization and correction. Our work highlights the potential of continuously enhancing the reasoning performance of SLMs through iterative reflection learning in the long run.",
        "arxiv_id": "2505.16475",
        "ARXIVID": "2505.16475",
        "COMMENT": "Does not match any specific criteria but is relevant to advancements in reasoning for language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.16161": {
        "authors": [
            "Liyan Wang",
            "Weixiang Zhou",
            "Cong Wang",
            "Kin-Man Lam",
            "Zhixun Su",
            "Jinshan Pan"
        ],
        "title": "Deep Learning-Driven Ultra-High-Definition Image Restoration: A Survey",
        "abstract": "arXiv:2505.16161v1 Announce Type: new  Abstract: Ultra-high-definition (UHD) image restoration aims to specifically solve the problem of quality degradation in ultra-high-resolution images. Recent advancements in this field are predominantly driven by deep learning-based innovations, including enhancements in dataset construction, network architecture, sampling strategies, prior knowledge integration, and loss functions. In this paper, we systematically review recent progress in UHD image restoration, covering various aspects ranging from dataset construction to algorithm design. This serves as a valuable resource for understanding state-of-the-art developments in the field. We begin by summarizing degradation models for various image restoration subproblems, such as super-resolution, low-light enhancement, deblurring, dehazing, deraining, and desnowing, and emphasizing the unique challenges of their application to UHD image restoration. We then highlight existing UHD benchmark datasets and organize the literature according to degradation types and dataset construction methods. Following this, we showcase major milestones in deep learning-driven UHD image restoration, reviewing the progression of restoration tasks, technological developments, and evaluations of existing methods. We further propose a classification framework based on network architectures and sampling strategies, helping to clearly organize existing methods. Finally, we share insights into the current research landscape and propose directions for further advancements. A related repository is available at https://github.com/wlydlut/UHD-Image-Restoration-Survey.",
        "arxiv_id": "2505.16161",
        "ARXIVID": "2505.16161",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of computer vision and image restoration.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.16899": {
        "authors": [
            "Kerem Oktar",
            "Katherine M. Collins",
            "Jose Hernandez-Orallo",
            "Diane Coyle",
            "Stephen Cave",
            "Adrian Weller",
            "Ilia Sucholutsky"
        ],
        "title": "Identifying, Evaluating, and Mitigating Risks of AI Thought Partnerships",
        "abstract": "arXiv:2505.16899v1 Announce Type: new  Abstract: Artificial Intelligence (AI) systems have historically been used as tools that execute narrowly defined tasks. Yet recent advances in AI have unlocked possibilities for a new class of models that genuinely collaborate with humans in complex reasoning, from conceptualizing problems to brainstorming solutions. Such AI thought partners enable novel forms of collaboration and extended cognition, yet they also pose major risks-including and beyond risks of typical AI tools and agents. In this commentary, we systematically identify risks of AI thought partners through a novel framework that identifies risks at multiple levels of analysis, including Real-time, Individual, and Societal risks arising from collaborative cognition (RISc). We leverage this framework to propose concrete metrics for risk evaluation, and finally suggest specific mitigation strategies for developers and policymakers. As AI thought partners continue to proliferate, these strategies can help prevent major harms and ensure that humans actively benefit from productive thought partnerships.",
        "arxiv_id": "2505.16899",
        "ARXIVID": "2505.16899",
        "COMMENT": "Does not match any specific criterion but discusses risks of AI thought partnerships, which is tangentially related to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.16561": {
        "authors": [
            "Jannis Becktepe",
            "Leona Hennig",
            "Steffen Oeltze-Jafra",
            "Marius Lindauer"
        ],
        "title": "Auto-nnU-Net: Towards Automated Medical Image Segmentation",
        "abstract": "arXiv:2505.16561v1 Announce Type: new  Abstract: Medical Image Segmentation (MIS) includes diverse tasks, from bone to organ segmentation, each with its own challenges in finding the best segmentation model. The state-of-the-art AutoML-related MIS-framework nnU-Net automates many aspects of model configuration but remains constrained by fixed hyperparameters and heuristic design choices. As a full-AutoML framework for MIS, we propose Auto-nnU-Net, a novel nnU-Net variant enabling hyperparameter optimization (HPO), neural architecture search (NAS), and hierarchical NAS (HNAS). Additionally, we propose Regularized PriorBand to balance model accuracy with the computational resources required for training, addressing the resource constraints often faced in real-world medical settings that limit the feasibility of extensive training procedures. We evaluate our approach across diverse MIS datasets from the well-established Medical Segmentation Decathlon, analyzing the impact of AutoML techniques on segmentation performance, computational efficiency, and model design choices. The results demonstrate that our AutoML approach substantially improves the segmentation performance of nnU-Net on 6 out of 10 datasets and is on par on the other datasets while maintaining practical resource requirements. Our code is available at https://github.com/LUH-AI/AutonnUNet.",
        "arxiv_id": "2505.16561",
        "ARXIVID": "2505.16561",
        "COMMENT": "Does not match any specific criteria. Focuses on automated medical image segmentation, which is outside the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.16384": {
        "authors": [
            "Haoming Huang",
            "Musen Zhang",
            "Jianxin Yang",
            "Zhen Li",
            "Jinkai Li",
            "Yao Guo"
        ],
        "title": "MAGE: A Multi-task Architecture for Gaze Estimation with an Efficient Calibration Module",
        "abstract": "arXiv:2505.16384v1 Announce Type: new  Abstract: Eye gaze can provide rich information on human psychological activities, and has garnered significant attention in the field of Human-Robot Interaction (HRI). However, existing gaze estimation methods merely predict either the gaze direction or the Point-of-Gaze (PoG) on the screen, failing to provide sufficient information for a comprehensive six Degree-of-Freedom (DoF) gaze analysis in 3D space. Moreover, the variations of eye shape and structure among individuals also impede the generalization capability of these methods. In this study, we propose MAGE, a Multi-task Architecture for Gaze Estimation with an efficient calibration module, to predict the 6-DoF gaze information that is applicable for the real-word HRI. Our basic model encodes both the directional and positional features from facial images, and predicts gaze results with dedicated information flow and multiple decoders. To reduce the impact of individual variations, we propose a novel calibration module, namely Easy-Calibration, to fine-tune the basic model with subject-specific data, which is efficient to implement without the need of a screen. Experimental results demonstrate that our method achieves state-of-the-art performance on the public MPIIFaceGaze, EYEDIAP, and our built IMRGaze datasets.",
        "arxiv_id": "2505.16384",
        "ARXIVID": "2505.16384",
        "COMMENT": "Does not match any specific criteria. Focuses on gaze estimation for human-robot interaction, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.15825": {
        "authors": [
            "Ammar Chouchane",
            "Mohcene Bessaoudi",
            "Hamza Kheddar",
            "Abdelmalik Ouamane",
            "Tiago Vieira",
            "Mahmoud Hassaballah"
        ],
        "title": "Multilinear subspace learning for person re-identification based fusion of high order tensor features",
        "abstract": "arXiv:2505.15825v1 Announce Type: new  Abstract: Video surveillance image analysis and processing is a challenging field in computer vision, with one of its most difficult tasks being Person Re-Identification (PRe-ID). PRe-ID aims to identify and track target individuals who have already been detected in a network of cameras, using a robust description of their pedestrian images. The success of recent research in person PRe-ID is largely due to effective feature extraction and representation, as well as the powerful learning of these features to reliably discriminate between pedestrian images. To this end, two powerful features, Convolutional Neural Networks (CNN) and Local Maximal Occurrence (LOMO), are modeled on multidimensional data using the proposed method, High-Dimensional Feature Fusion (HDFF). Specifically, a new tensor fusion scheme is introduced to leverage and combine these two types of features in a single tensor, even though their dimensions are not identical. To enhance the system's accuracy, we employ Tensor Cross-View Quadratic Analysis (TXQDA) for multilinear subspace learning, followed by cosine similarity for matching. TXQDA efficiently facilitates learning while reducing the high dimensionality inherent in high-order tensor data. The effectiveness of our approach is verified through experiments on three widely-used PRe-ID datasets: VIPeR, GRID, and PRID450S. Extensive experiments demonstrate that our approach outperforms recent state-of-the-art methods.",
        "arxiv_id": "2505.15825",
        "ARXIVID": "2505.15825",
        "COMMENT": "Does not match any specific criteria. Focuses on person re-identification and tensor fusion, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}