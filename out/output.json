{
    "2504.12048": {
        "authors": [
            "Zirui Pan",
            "Xin Wang",
            "Yipeng Zhang",
            "Hong Chen",
            "Kwan Man Cheng",
            "Yaofei Wu",
            "Wenwu Zhu"
        ],
        "title": "Modular-Cam: Modular Dynamic Camera-view Video Generation with LLM",
        "abstract": "arXiv:2504.12048v1 Announce Type: new  Abstract: Text-to-Video generation, which utilizes the provided text prompt to generate high-quality videos, has drawn increasing attention and achieved great success due to the development of diffusion models recently. Existing methods mainly rely on a pre-trained text encoder to capture the semantic information and perform cross attention with the encoded text prompt to guide the generation of video. However, when it comes to complex prompts that contain dynamic scenes and multiple camera-view transformations, these methods can not decompose the overall information into separate scenes, as well as fail to smoothly change scenes based on the corresponding camera-views. To solve these problems, we propose a novel method, i.e., Modular-Cam. Specifically, to better understand a given complex prompt, we utilize a large language model to analyze user instructions and decouple them into multiple scenes together with transition actions. To generate a video containing dynamic scenes that match the given camera-views, we incorporate the widely-used temporal transformer into the diffusion model to ensure continuity within a single scene and propose CamOperator, a modular network based module that well controls the camera movements. Moreover, we propose AdaControlNet, which utilizes ControlNet to ensure consistency across scenes and adaptively adjusts the color tone of the generated video. Extensive qualitative and quantitative experiments prove our proposed Modular-Cam's strong capability of generating multi-scene videos together with its ability to achieve fine-grained control of camera movements. Generated results are available at https://modular-cam.github.io.",
        "arxiv_id": "2504.12048",
        "ARXIVID": "2504.12048",
        "COMMENT": "Matches criterion 2 as it proposes a novel method for text-to-video generation using LLMs and diffusion models, which are VLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2504.11739": {
        "authors": [
            "Bingjie Gao",
            "Xinyu Gao",
            "Xiaoxue Wu",
            "Yujie Zhou",
            "Yu Qiao",
            "Li Niu",
            "Xinyuan Chen",
            "Yaohui Wang"
        ],
        "title": "The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation",
        "abstract": "arXiv:2504.11739v1 Announce Type: new  Abstract: The evolution of Text-to-video (T2V) generative models, trained on large-scale datasets, has been marked by significant progress. However, the sensitivity of T2V generative models to input prompts highlights the critical role of prompt design in influencing generative outcomes. Prior research has predominantly relied on Large Language Models (LLMs) to align user-provided prompts with the distribution of training prompts, albeit without tailored guidance encompassing prompt vocabulary and sentence structure nuances. To this end, we introduce \\textbf{RAPO}, a novel \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{P}rompt \\textbf{O}ptimization framework. In order to address potential inaccuracies and ambiguous details generated by LLM-generated prompts. RAPO refines the naive prompts through dual optimization branches, selecting the superior prompt for T2V generation. The first branch augments user prompts with diverse modifiers extracted from a learned relational graph, refining them to align with the format of training prompts via a fine-tuned LLM. Conversely, the second branch rewrites the naive prompt using a pre-trained LLM following a well-defined instruction set. Extensive experiments demonstrate that RAPO can effectively enhance both the static and dynamic dimensions of generated videos, demonstrating the significance of prompt optimization for user-provided prompts. Project website: \\href{https://whynothaha.github.io/Prompt_optimizer/RAPO.html}{GitHub}.",
        "arxiv_id": "2504.11739",
        "ARXIVID": "2504.11739",
        "COMMENT": "Matches criterion 2 as it proposes a novel framework for optimizing prompts in text-to-video generation, leveraging multi-modal models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2504.11477": {
        "authors": [
            "Yunkai Zhang",
            "Shiyin Wei",
            "Yong Huang",
            "Yawu Su",
            "Shanshan Lu",
            "Hui Li"
        ],
        "title": "SDIGLM: Leveraging Large Language Models and Multi-Modal Chain of Thought for Structural Damage Identification",
        "abstract": "arXiv:2504.11477v1 Announce Type: new  Abstract: Existing computer vision(CV)-based structural damage identification models demonstrate notable accuracy in categorizing and localizing damage. However, these models present several critical limitations that hinder their practical application in civil engineering(CE). Primarily, their ability to recognize damage types remains constrained, preventing comprehensive analysis of the highly varied and complex conditions encountered in real-world CE structures. Second, these models lack linguistic capabilities, rendering them unable to articulate structural damage characteristics through natural language descriptions. With the continuous advancement of artificial intelligence(AI), large multi-modal models(LMMs) have emerged as a transformative solution, enabling the unified encoding and alignment of textual and visual data. These models can autonomously generate detailed descriptive narratives of structural damage while demonstrating robust generalization across diverse scenarios and tasks. This study introduces SDIGLM, an innovative LMM for structural damage identification, developed based on the open-source VisualGLM-6B architecture. To address the challenge of adapting LMMs to the intricate and varied operating conditions in CE, this work integrates a U-Net-based semantic segmentation module to generate defect segmentation maps as visual Chain of Thought(CoT). Additionally, a multi-round dialogue fine-tuning dataset is constructed to enhance logical reasoning, complemented by a language CoT formed through prompt engineering. By leveraging this multi-modal CoT, SDIGLM surpasses general-purpose LMMs in structural damage identification, achieving an accuracy of 95.24% across various infrastructure types. Moreover, the model effectively describes damage characteristics such as hole size, crack direction, and corrosion severity.",
        "arxiv_id": "2504.11477",
        "ARXIVID": "2504.11477",
        "COMMENT": "Matches criterion 2 as it introduces a multi-modal large language model (SDIGLM) for structural damage identification.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2504.11930": {
        "authors": [
            "Hairui Ren",
            "Fan Tang",
            "He Zhao",
            "Zixuan Wang",
            "Dandan Guo",
            "Yi Chang"
        ],
        "title": "Beyond Words: Augmenting Discriminative Richness via Diffusions in Unsupervised Prompt Learning",
        "abstract": "arXiv:2504.11930v1 Announce Type: new  Abstract: Fine-tuning vision-language models (VLMs) with large amounts of unlabeled data has recently garnered significant interest. However, a key challenge remains the lack of high-quality pseudo-labeled data. Current pseudo-labeling strategies often struggle with mismatches between semantic and visual information, leading to sub-optimal performance of unsupervised prompt learning (UPL) methods. In this paper, we introduce a simple yet effective approach called \\textbf{A}ugmenting D\\textbf{i}scriminative \\textbf{R}ichness via Diffusions (AiR), toward learning a richer discriminating way to represent the class comprehensively and thus facilitate classification. Specifically, our approach includes a pseudo-label generation module that leverages high-fidelity synthetic samples to create an auxiliary classifier, which captures richer visual variation, bridging text-image-pair classification to a more robust image-image-pair classification. Additionally, we exploit the diversity of diffusion-based synthetic samples to enhance prompt learning, providing greater information for semantic-visual alignment. Extensive experiments on five public benchmarks, including RESISC45 and Flowers102, and across three learning paradigms-UL, SSL, and TRZSL-demonstrate that AiR achieves substantial and consistent performance improvements over state-of-the-art unsupervised prompt learning methods.",
        "arxiv_id": "2504.11930",
        "ARXIVID": "2504.11930",
        "COMMENT": "This paper proposes a method to improve vision-language models (VLMs) using diffusion-based synthetic samples, aligning with criterion 2.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.12167": {
        "authors": [
            "Yuan Luo",
            "Rudolf Hoffmann",
            "Yan Xia",
            "Olaf Wysocki",
            "Benedikt Schwab",
            "Thomas H. Kolbe",
            "Daniel Cremers"
        ],
        "title": "RADLER: Radar Object Detection Leveraging Semantic 3D City Models and Self-Supervised Radar-Image Learning",
        "abstract": "arXiv:2504.12167v1 Announce Type: new  Abstract: Semantic 3D city models are worldwide easy-accessible, providing accurate, object-oriented, and semantic-rich 3D priors. To date, their potential to mitigate the noise impact on radar object detection remains under-explored. In this paper, we first introduce a unique dataset, RadarCity, comprising 54K synchronized radar-image pairs and semantic 3D city models. Moreover, we propose a novel neural network, RADLER, leveraging the effectiveness of contrastive self-supervised learning (SSL) and semantic 3D city models to enhance radar object detection of pedestrians, cyclists, and cars. Specifically, we first obtain the robust radar features via a SSL network in the radar-image pretext task. We then use a simple yet effective feature fusion strategy to incorporate semantic-depth features from semantic 3D city models. Having prior 3D information as guidance, RADLER obtains more fine-grained details to enhance radar object detection. We extensively evaluate RADLER on the collected RadarCity dataset and demonstrate average improvements of 5.46% in mean avarage precision (mAP) and 3.51% in mean avarage recall (mAR) over previous radar object detection methods. We believe this work will foster further research on semantic-guided and map-supported radar object detection. Our project page is publicly available athttps://gpp-communication.github.io/RADLER .",
        "arxiv_id": "2504.12167",
        "ARXIVID": "2504.12167",
        "COMMENT": "Matches criterion 1 as it leverages semantic 3D city models for radar object detection, focusing on spatial understanding and multi-modal integration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.11893": {
        "authors": [
            "Wei Sun",
            "Yanzhao Zhou",
            "Jianbin Jiao",
            "Yuan Li"
        ],
        "title": "CAGS: Open-Vocabulary 3D Scene Understanding with Context-Aware Gaussian Splatting",
        "abstract": "arXiv:2504.11893v1 Announce Type: new  Abstract: Open-vocabulary 3D scene understanding is crucial for applications requiring natural language-driven spatial interpretation, such as robotics and augmented reality. While 3D Gaussian Splatting (3DGS) offers a powerful representation for scene reconstruction, integrating it with open-vocabulary frameworks reveals a key challenge: cross-view granularity inconsistency. This issue, stemming from 2D segmentation methods like SAM, results in inconsistent object segmentations across views (e.g., a \"coffee set\" segmented as a single entity in one view but as \"cup + coffee + spoon\" in another). Existing 3DGS-based methods often rely on isolated per-Gaussian feature learning, neglecting the spatial context needed for cohesive object reasoning, leading to fragmented representations. We propose Context-Aware Gaussian Splatting (CAGS), a novel framework that addresses this challenge by incorporating spatial context into 3DGS. CAGS constructs local graphs to propagate contextual features across Gaussians, reducing noise from inconsistent granularity, employs mask-centric contrastive learning to smooth SAM-derived features across views, and leverages a precomputation strategy to reduce computational cost by precomputing neighborhood relationships, enabling efficient training in large-scale scenes. By integrating spatial context, CAGS significantly improves 3D instance segmentation and reduces fragmentation errors on datasets like LERF-OVS and ScanNet, enabling robust language-guided 3D scene understanding.",
        "arxiv_id": "2504.11893",
        "ARXIVID": "2504.11893",
        "COMMENT": "Matches criterion 1 as it proposes a novel method (CAGS) for spatial understanding in 3D scene reconstruction and open-vocabulary frameworks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.12284": {
        "authors": [
            "Aditya Prakash",
            "Benjamin Lundell",
            "Dmitry Andreychuk",
            "David Forsyth",
            "Saurabh Gupta",
            "Harpreet Sawhney"
        ],
        "title": "How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions",
        "abstract": "arXiv:2504.12284v1 Announce Type: new  Abstract: We tackle the novel problem of predicting 3D hand motion and contact maps (or Interaction Trajectories) given a single RGB view, action text, and a 3D contact point on the object as input. Our approach consists of (1) Interaction Codebook: a VQVAE model to learn a latent codebook of hand poses and contact points, effectively tokenizing interaction trajectories, (2) Interaction Predictor: a transformer-decoder module to predict the interaction trajectory from test time inputs by using an indexer module to retrieve a latent affordance from the learned codebook. To train our model, we develop a data engine that extracts 3D hand poses and contact trajectories from the diverse HoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger than existing works, in terms of diversity of objects and interactions observed, and test for generalization of the model across object categories, action categories, tasks, and scenes. Experimental results show the effectiveness of our approach over transformer & diffusion baselines across all settings.",
        "arxiv_id": "2504.12284",
        "ARXIVID": "2504.12284",
        "COMMENT": "Matches criterion 3. Proposes a novel benchmark and method for embodied AI focusing on 3D hand motion and contact prediction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.11845": {
        "authors": [
            "Jie Zhu",
            "Bo Peng",
            "Zhe Zhang",
            "Bingzheng Liu",
            "Jianjun Lei"
        ],
        "title": "Boosting Multi-View Stereo with Depth Foundation Model in the Absence of Real-World Labels",
        "abstract": "arXiv:2504.11845v1 Announce Type: new  Abstract: Learning-based Multi-View Stereo (MVS) methods have made remarkable progress in recent years. However, how to effectively train the network without using real-world labels remains a challenging problem. In this paper, driven by the recent advancements of vision foundation models, a novel method termed DFM-MVS, is proposed to leverage the depth foundation model to generate the effective depth prior, so as to boost MVS in the absence of real-world labels. Specifically, a depth prior-based pseudo-supervised training mechanism is developed to simulate realistic stereo correspondences using the generated depth prior, thereby constructing effective supervision for the MVS network. Besides, a depth prior-guided error correction strategy is presented to leverage the depth prior as guidance to mitigate the error propagation problem inherent in the widely-used coarse-to-fine network structure. Experimental results on DTU and Tanks & Temples datasets demonstrate that the proposed DFM-MVS significantly outperforms existing MVS methods without using real-world labels.",
        "arxiv_id": "2504.11845",
        "ARXIVID": "2504.11845",
        "COMMENT": "Matches criterion 4 as it leverages a vision foundation model for improving multi-view stereo without real-world labels.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.12021": {
        "authors": [
            "Mohamad Dalal",
            "Artur Xarles",
            "Anthony Cioppa",
            "Silvio Giancola",
            "Marc Van Droogenbroeck",
            "Bernard Ghanem",
            "Albert Clap\\'es",
            "Sergio Escalera",
            "Thomas B. Moeslund"
        ],
        "title": "Action Anticipation from SoccerNet Football Video Broadcasts",
        "abstract": "arXiv:2504.12021v1 Announce Type: new  Abstract: Artificial intelligence has revolutionized the way we analyze sports videos, whether to understand the actions of games in long untrimmed videos or to anticipate the player's motion in future frames. Despite these efforts, little attention has been given to anticipating game actions before they occur. In this work, we introduce the task of action anticipation for football broadcast videos, which consists in predicting future actions in unobserved future frames, within a five- or ten-second anticipation window. To benchmark this task, we release a new dataset, namely the SoccerNet Ball Action Anticipation dataset, based on SoccerNet Ball Action Spotting. Additionally, we propose a Football Action ANticipation TRAnsformer (FAANTRA), a baseline method that adapts FUTR, a state-of-the-art action anticipation model, to predict ball-related actions. To evaluate action anticipation, we introduce new metrics, including mAP@$\\delta$, which evaluates the temporal precision of predicted future actions, as well as mAP@$\\infty$, which evaluates their occurrence within the anticipation window. We also conduct extensive ablation studies to examine the impact of various task settings, input configurations, and model architectures. Experimental results highlight both the feasibility and challenges of action anticipation in football videos, providing valuable insights into the design of predictive models for sports analytics. By forecasting actions before they unfold, our work will enable applications in automated broadcasting, tactical analysis, and player decision-making. Our dataset and code are publicly available at https://github.com/MohamadDalal/FAANTRA.",
        "arxiv_id": "2504.12021",
        "ARXIVID": "2504.12021",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset and method for action anticipation in football videos.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.12299": {
        "authors": [
            "Marko Tot",
            "Shu Ishida",
            "Abdelhak Lemkhenter",
            "David Bignell",
            "Pallavi Choudhury",
            "Chris Lovett",
            "Luis Fran\\c{c}a",
            "Matheus Ribeiro Furtado de Mendon\\c{c}a",
            "Tarun Gupta",
            "Darren Gehring",
            "Sam Devlin",
            "Sergio Valcarcel Macua",
            "Raluca Georgescu"
        ],
        "title": "Adapting a World Model for Trajectory Following in a 3D Game",
        "abstract": "arXiv:2504.12299v1 Announce Type: new  Abstract: Imitation learning is a powerful tool for training agents by leveraging expert knowledge, and being able to replicate a given trajectory is an integral part of it. In complex environments, like modern 3D video games, distribution shift and stochasticity necessitate robust approaches beyond simple action replay. In this study, we apply Inverse Dynamics Models (IDM) with different encoders and policy heads to trajectory following in a modern 3D video game -- Bleeding Edge. Additionally, we investigate several future alignment strategies that address the distribution shift caused by the aleatoric uncertainty and imperfections of the agent. We measure both the trajectory deviation distance and the first significant deviation point between the reference and the agent's trajectory and show that the optimal configuration depends on the chosen setting. Our results show that in a diverse data setting, a GPT-style policy head with an encoder trained from scratch performs the best, DINOv2 encoder with the GPT-style policy head gives the best results in the low data regime, and both GPT-style and MLP-style policy heads had comparable results when pre-trained on a diverse setting and fine-tuned for a specific behaviour setting.",
        "arxiv_id": "2504.12299",
        "ARXIVID": "2504.12299",
        "COMMENT": "Matches criterion 3 as it focuses on embodied AI with trajectory following in a 3D game, exploring new methods for imitation learning and trajectory alignment.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2504.11914": {
        "authors": [
            "Yuhao Chao",
            "Jie Liu",
            "Jie Tang",
            "Gangshan Wu"
        ],
        "title": "AnomalyR1: A GRPO-based End-to-end MLLM for Industrial Anomaly Detection",
        "abstract": "arXiv:2504.11914v1 Announce Type: new  Abstract: Industrial Anomaly Detection (IAD) poses a formidable challenge due to the scarcity of defective samples, making it imperative to deploy models capable of robust generalization to detect unseen anomalies effectively. Traditional approaches, often constrained by hand-crafted features or domain-specific expert models, struggle to address this limitation, underscoring the need for a paradigm shift. We introduce AnomalyR1, a pioneering framework that leverages VLM-R1, a Multimodal Large Language Model (MLLM) renowned for its exceptional generalization and interpretability, to revolutionize IAD. By integrating MLLM with Group Relative Policy Optimization (GRPO), enhanced by our novel Reasoned Outcome Alignment Metric (ROAM), AnomalyR1 achieves a fully end-to-end solution that autonomously processes inputs of image and domain knowledge, reasons through analysis, and generates precise anomaly localizations and masks. Based on the latest multimodal IAD benchmark, our compact 3-billion-parameter model outperforms existing methods, establishing state-of-the-art results. As MLLM capabilities continue to advance, this study is the first to deliver an end-to-end VLM-based IAD solution that demonstrates the transformative potential of ROAM-enhanced GRPO, positioning our framework as a forward-looking cornerstone for next-generation intelligent anomaly detection systems in industrial applications with limited defective data.",
        "arxiv_id": "2504.11914",
        "ARXIVID": "2504.11914",
        "COMMENT": "Matches criterion 2 as it introduces a multimodal large language model (MLLM) for industrial anomaly detection.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2504.12027": {
        "authors": [
            "Bingyan Liu",
            "Chengyu Wang",
            "Tongtong Su",
            "Huan Ten",
            "Jun Huang",
            "Kailing Guo",
            "Kui Jia"
        ],
        "title": "Understanding Attention Mechanism in Video Diffusion Models",
        "abstract": "arXiv:2504.12027v1 Announce Type: new  Abstract: Text-to-video (T2V) synthesis models, such as OpenAI's Sora, have garnered significant attention due to their ability to generate high-quality videos from a text prompt. In diffusion-based T2V models, the attention mechanism is a critical component. However, it remains unclear what intermediate features are learned and how attention blocks in T2V models affect various aspects of video synthesis, such as image quality and temporal consistency. In this paper, we conduct an in-depth perturbation analysis of the spatial and temporal attention blocks of T2V models using an information-theoretic approach. Our results indicate that temporal and spatial attention maps affect not only the timing and layout of the videos but also the complexity of spatiotemporal elements and the aesthetic quality of the synthesized videos. Notably, high-entropy attention maps are often key elements linked to superior video quality, whereas low-entropy attention maps are associated with the video's intra-frame structure. Based on our findings, we propose two novel methods to enhance video quality and enable text-guided video editing. These methods rely entirely on lightweight manipulation of the attention matrices in T2V models. The efficacy and effectiveness of our methods are further validated through experimental evaluation across multiple datasets.",
        "arxiv_id": "2504.12027",
        "ARXIVID": "2504.12027",
        "COMMENT": "Matches criterion 2 as it explores attention mechanisms in text-to-video diffusion models, which are a type of VLLM.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2504.11686": {
        "authors": [
            "Yiran He",
            "Yun Cao",
            "Bowen Yang",
            "Zeyu Zhang"
        ],
        "title": "Can GPT tell us why these images are synthesized? Empowering Multimodal Large Language Models for Forensics",
        "abstract": "arXiv:2504.11686v1 Announce Type: new  Abstract: The rapid development of generative AI facilitates content creation and makes image manipulation easier and more difficult to detect. While multimodal Large Language Models (LLMs) have encoded rich world knowledge, they are not inherently tailored for combating AI-generated Content (AIGC) and struggle to comprehend local forgery details. In this work, we investigate the application of multimodal LLMs in forgery detection. We propose a framework capable of evaluating image authenticity, localizing tampered regions, providing evidence, and tracing generation methods based on semantic tampering clues. Our method demonstrates that the potential of LLMs in forgery analysis can be effectively unlocked through meticulous prompt engineering and the application of few-shot learning techniques. We conduct qualitative and quantitative experiments and show that GPT4V can achieve an accuracy of 92.1% in Autosplice and 86.3% in LaMa, which is competitive with state-of-the-art AIGC detection methods. We further discuss the limitations of multimodal LLMs in such tasks and propose potential improvements.",
        "arxiv_id": "2504.11686",
        "ARXIVID": "2504.11686",
        "COMMENT": "Matches criterion 2 as it explores the application of multi-modal large language models for forgery detection.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2504.11763": {
        "authors": [
            "Aoran Liu",
            "Kun Hu",
            "Clinton Mo",
            "Changyang Li",
            "Zhiyong Wang"
        ],
        "title": "Extended Short- and Long-Range Mesh Learning for Fast and Generalized Garment Simulation",
        "abstract": "arXiv:2504.11763v1 Announce Type: new  Abstract: 3D garment simulation is a critical component for producing cloth-based graphics. Recent advancements in graph neural networks (GNNs) offer a promising approach for efficient garment simulation. However, GNNs require extensive message-passing to propagate information such as physical forces and maintain contact awareness across the entire garment mesh, which becomes computationally inefficient at higher resolutions. To address this, we devise a novel GNN-based mesh learning framework with two key components to extend the message-passing range with minimal overhead, namely the Laplacian-Smoothed Dual Message-Passing (LSDMP) and the Geodesic Self-Attention (GSA) modules. LSDMP enhances message-passing with a Laplacian features smoothing process, which efficiently propagates the impact of each vertex to nearby vertices. Concurrently, GSA introduces geodesic distance embeddings to represent the spatial relationship between vertices and utilises attention mechanisms to capture global mesh information. The two modules operate in parallel to ensure both short- and long-range mesh modelling. Extensive experiments demonstrate the state-of-the-art performance of our method, requiring fewer layers and lower inference latency.",
        "arxiv_id": "2504.11763",
        "ARXIVID": "2504.11763",
        "COMMENT": "This paper introduces a novel GNN-based framework for efficient garment simulation, which involves spatial understanding and intelligence in 3D meshes. It aligns with criterion 1.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.11773": {
        "authors": [
            "Yiran Wang",
            "Jiaqi Li",
            "Chaoyi Hong",
            "Ruibo Li",
            "Liusheng Sun",
            "Xiao Song",
            "Zhe Wang",
            "Zhiguo Cao",
            "Guosheng Lin"
        ],
        "title": "TacoDepth: Towards Efficient Radar-Camera Depth Estimation with One-stage Fusion",
        "abstract": "arXiv:2504.11773v1 Announce Type: new  Abstract: Radar-Camera depth estimation aims to predict dense and accurate metric depth by fusing input images and Radar data. Model efficiency is crucial for this task in pursuit of real-time processing on autonomous vehicles and robotic platforms. However, due to the sparsity of Radar returns, the prevailing methods adopt multi-stage frameworks with intermediate quasi-dense depth, which are time-consuming and not robust. To address these challenges, we propose TacoDepth, an efficient and accurate Radar-Camera depth estimation model with one-stage fusion. Specifically, the graph-based Radar structure extractor and the pyramid-based Radar fusion module are designed to capture and integrate the graph structures of Radar point clouds, delivering superior model efficiency and robustness without relying on the intermediate depth results. Moreover, TacoDepth can be flexible for different inference modes, providing a better balance of speed and accuracy. Extensive experiments are conducted to demonstrate the efficacy of our method. Compared with the previous state-of-the-art approach, TacoDepth improves depth accuracy and processing speed by 12.8% and 91.8%. Our work provides a new perspective on efficient Radar-Camera depth estimation.",
        "arxiv_id": "2504.11773",
        "ARXIVID": "2504.11773",
        "COMMENT": "Matches criterion 1 as it proposes a novel one-stage fusion method (TacoDepth) for Radar-Camera depth estimation, focusing on spatial understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.11543": {
        "authors": [
            "Divyansh Garg",
            "Shaun VanWeelden",
            "Diego Caples",
            "Andis Draguns",
            "Nikil Ravi",
            "Pranav Putta",
            "Naman Garg",
            "Tomas Abraham",
            "Michael Lara",
            "Federico Lopez",
            "James Liu",
            "Atharva Gundawar",
            "Prannay Hebbar",
            "Youngchul Joo",
            "Charles London",
            "Christian Schroeder de Witt",
            "Sumeet Motwani"
        ],
        "title": "REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites",
        "abstract": "arXiv:2504.11543v1 Announce Type: new  Abstract: We introduce REAL, a benchmark and framework for multi-turn agent evaluations on deterministic simulations of real-world websites. REAL comprises high-fidelity, deterministic replicas of 11 widely-used websites across domains such as e-commerce, travel, communication, and professional networking. We also release a benchmark consisting of 112 practical tasks that mirror everyday complex user interactions requiring both accurate information retrieval and state-changing actions. All interactions occur within this fully controlled setting, eliminating safety risks and enabling robust, reproducible evaluation of agent capability and reliability. Our novel evaluation framework combines programmatic checks of website state for action-based tasks with rubric-guided LLM-based judgments for information retrieval. The framework supports both open-source and proprietary agent systems through a flexible evaluation harness that accommodates black-box commands within browser environments, allowing research labs to test agentic systems without modification. Our empirical results show that frontier language models achieve at most a 41% success rate on REAL, highlighting critical gaps in autonomous web navigation and task completion capabilities. Our framework supports easy integration of new tasks, reproducible evaluation, and scalable data generation for training web agents. The websites, framework, and leaderboard are available at https://realevals.xyz and https://github.com/agi-inc/REAL.",
        "arxiv_id": "2504.11543",
        "ARXIVID": "2504.11543",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (REAL) for evaluating autonomous agents in deterministic simulations, focusing on novel evaluation frameworks.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2504.11786": {
        "authors": [
            "Sang-Jun Park",
            "Keun-Soo Heo",
            "Dong-Hee Shin",
            "Young-Han Son",
            "Ji-Hye Oh",
            "Tae-Eui Kam"
        ],
        "title": "DART: Disease-aware Image-Text Alignment and Self-correcting Re-alignment for Trustworthy Radiology Report Generation",
        "abstract": "arXiv:2504.11786v1 Announce Type: new  Abstract: The automatic generation of radiology reports has emerged as a promising solution to reduce a time-consuming task and accurately capture critical disease-relevant findings in X-ray images. Previous approaches for radiology report generation have shown impressive performance. However, there remains significant potential to improve accuracy by ensuring that retrieved reports contain disease-relevant findings similar to those in the X-ray images and by refining generated reports. In this study, we propose a Disease-aware image-text Alignment and self-correcting Re-alignment for Trustworthy radiology report generation (DART) framework. In the first stage, we generate initial reports based on image-to-text retrieval with disease-matching, embedding both images and texts in a shared embedding space through contrastive learning. This approach ensures the retrieval of reports with similar disease-relevant findings that closely align with the input X-ray images. In the second stage, we further enhance the initial reports by introducing a self-correction module that re-aligns them with the X-ray images. Our proposed framework achieves state-of-the-art results on two widely used benchmarks, surpassing previous approaches in both report generation and clinical efficacy metrics, thereby enhancing the trustworthiness of radiology reports.",
        "arxiv_id": "2504.11786",
        "ARXIVID": "2504.11786",
        "COMMENT": "Matches criterion 2. Proposes a novel framework for radiology report generation using vision-language alignment.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.11999": {
        "authors": [
            "Mengyu Wang",
            "Hanbo Bi",
            "Yingchao Feng",
            "Linlin Xin",
            "Shuo Gong",
            "Tianqi Wang",
            "Zhiyuan Yan",
            "Peijin Wang",
            "Wenhui Diao",
            "Xian Sun"
        ],
        "title": "A Complex-valued SAR Foundation Model Based on Physically Inspired Representation Learning",
        "abstract": "arXiv:2504.11999v1 Announce Type: new  Abstract: Vision foundation models in remote sensing have been extensively studied due to their superior generalization on various downstream tasks. Synthetic Aperture Radar (SAR) offers all-day, all-weather imaging capabilities, providing significant advantages for Earth observation. However, establishing a foundation model for SAR image interpretation inevitably encounters the challenges of insufficient information utilization and poor interpretability. In this paper, we propose a remote sensing foundation model based on complex-valued SAR data, which simulates the polarimetric decomposition process for pre-training, i.e., characterizing pixel scattering intensity as a weighted combination of scattering bases and scattering coefficients, thereby endowing the foundation model with physical interpretability. Specifically, we construct a series of scattering queries, each representing an independent and meaningful scattering basis, which interact with SAR features in the scattering query decoder and output the corresponding scattering coefficient. To guide the pre-training process, polarimetric decomposition loss and power self-supervision loss are constructed. The former aligns the predicted coefficients with Yamaguchi coefficients, while the latter reconstructs power from the predicted coefficients and compares it to the input image's power. The performance of our foundation model is validated on six typical downstream tasks, achieving state-of-the-art results. Notably, the foundation model can extract stable feature representations and exhibits strong generalization, even in data-scarce conditions.",
        "arxiv_id": "2504.11999",
        "ARXIVID": "2504.11999",
        "COMMENT": "Matches criterion 4. Proposes a vision foundation model for SAR data with physically inspired representation learning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.11733": {
        "authors": [
            "Li Yu",
            "Situo Wang",
            "Wei Zhou",
            "Moncef Gabbouj"
        ],
        "title": "DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment",
        "abstract": "arXiv:2504.11733v1 Announce Type: new  Abstract: Inspired by the dual-stream theory of the human visual system (HVS) - where the ventral stream is responsible for object recognition and detail analysis, while the dorsal stream focuses on spatial relationships and motion perception - an increasing number of video quality assessment (VQA) works built upon this framework are proposed. Recent advancements in large multi-modal models, notably Contrastive Language-Image Pretraining (CLIP), have motivated researchers to incorporate CLIP into dual-stream-based VQA methods. This integration aims to harness the model's superior semantic understanding capabilities to replicate the object recognition and detail analysis in ventral stream, as well as spatial relationship analysis in dorsal stream. However, CLIP is originally designed for images and lacks the ability to capture temporal and motion information inherent in videos. %Furthermore, existing feature fusion strategies in no-reference video quality assessment (NR-VQA) often rely on fixed weighting schemes, which fail to adaptively adjust feature importance. To address the limitation, this paper propose a Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment (DVLTA-VQA), which decouples CLIP's visual and textual components, and integrates them into different stages of the NR-VQA pipeline.",
        "arxiv_id": "2504.11733",
        "ARXIVID": "2504.11733",
        "COMMENT": "Matches criterion 2 as it discusses the integration of multi-modal large language models (CLIP) for video quality assessment.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.12104": {
        "authors": [
            "Shuo Li",
            "Fang Liu",
            "Zehua Hao",
            "Xinyi Wang",
            "Lingling Li",
            "Xu Liu",
            "Puhua Chen",
            "Wenping Ma"
        ],
        "title": "Logits DeConfusion with CLIP for Few-Shot Learning",
        "abstract": "arXiv:2504.12104v1 Announce Type: new  Abstract: With its powerful visual-language alignment capability, CLIP performs well in zero-shot and few-shot learning tasks. However, we found in experiments that CLIP's logits suffer from serious inter-class confusion problems in downstream tasks, and the ambiguity between categories seriously affects the accuracy. To address this challenge, we propose a novel method called Logits DeConfusion, which effectively learns and eliminates inter-class confusion in logits by combining our Multi-level Adapter Fusion (MAF) module with our Inter-Class Deconfusion (ICD) module. Our MAF extracts features from different levels and fuses them uniformly to enhance feature representation. Our ICD learnably eliminates inter-class confusion in logits with a residual structure. Experimental results show that our method can significantly improve the classification performance and alleviate the inter-class confusion problem. The code is available at https://github.com/LiShuo1001/LDC.",
        "arxiv_id": "2504.12104",
        "ARXIVID": "2504.12104",
        "COMMENT": "Matches criterion 4 as it focuses on improving CLIP's performance in few-shot learning, which is related to vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2504.11571": {
        "authors": [
            "Dayeon Ki",
            "Tianyi Zhou",
            "Marine Carpuat",
            "Gang Wu",
            "Puneet Mathur",
            "Viswanathan Swaminathan"
        ],
        "title": "GraphicBench: A Planning Benchmark for Graphic Design with Language Agents",
        "abstract": "arXiv:2504.11571v1 Announce Type: new  Abstract: Large Language Model (LLM)-powered agents have unlocked new possibilities for automating human tasks. While prior work has focused on well-defined tasks with specified goals, the capabilities of agents in creative design tasks with open-ended goals remain underexplored. We introduce GraphicBench, a new planning benchmark for graphic design that covers 1,079 user queries and input images across four design types. We further present GraphicTown, an LLM agent framework with three design experts and 46 actions (tools) to choose from for executing each step of the planned workflows in web environments. Experiments with six LLMs demonstrate their ability to generate workflows that integrate both explicit design constraints from user queries and implicit commonsense constraints. However, these workflows often do not lead to successful execution outcomes, primarily due to challenges in: (1) reasoning about spatial relationships, (2) coordinating global dependencies across experts, and (3) retrieving the most appropriate action per step. We envision GraphicBench as a challenging yet valuable testbed for advancing LLM-agent planning and execution in creative design tasks.",
        "arxiv_id": "2504.11571",
        "ARXIVID": "2504.11571",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (GraphicBench) for creative design tasks with LLM agents, focusing on spatial reasoning and planning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.12029": {
        "authors": [
            "Bingjie Gao",
            "Bo Zhang",
            "Li Niu"
        ],
        "title": "Object Placement for Anything",
        "abstract": "arXiv:2504.12029v1 Announce Type: new  Abstract: Object placement aims to determine the appropriate placement (\\emph{e.g.}, location and size) of a foreground object when placing it on the background image. Most previous works are limited by small-scale labeled dataset, which hinders the real-world application of object placement. In this work, we devise a semi-supervised framework which can exploit large-scale unlabeled dataset to promote the generalization ability of discriminative object placement models. The discriminative models predict the rationality label for each foreground placement given a foreground-background pair. To better leverage the labeled data, under the semi-supervised framework, we further propose to transfer the knowledge of rationality variation, \\emph{i.e.}, whether the change of foreground placement would result in the change of rationality label, from labeled data to unlabeled data. Extensive experiments demonstrate that our framework can effectively enhance the generalization ability of discriminative object placement models.",
        "arxiv_id": "2504.12029",
        "ARXIVID": "2504.12029",
        "COMMENT": "This paper focuses on object placement using semi-supervised learning, which involves spatial reasoning but does not directly align with any specific criterion.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2504.12197": {
        "authors": [
            "Mahdi Alehdaghi",
            "Rajarshi Bhattacharya",
            "Pourya Shamsolmoali",
            "Rafael M. O. Cruz",
            "Maguelonne Heritier",
            "Eric Granger"
        ],
        "title": "Beyond Patches: Mining Interpretable Part-Prototypes for Explainable AI",
        "abstract": "arXiv:2504.12197v1 Announce Type: new  Abstract: Deep learning has provided considerable advancements for multimedia systems, yet the interpretability of deep models remains a challenge. State-of-the-art post-hoc explainability methods, such as GradCAM, provide visual interpretation based on heatmaps but lack conceptual clarity. Prototype-based approaches, like ProtoPNet and PIPNet, offer a more structured explanation but rely on fixed patches, limiting their robustness and semantic consistency.   To address these limitations, a part-prototypical concept mining network (PCMNet) is proposed that dynamically learns interpretable prototypes from meaningful regions. PCMNet clusters prototypes into concept groups, creating semantically grounded explanations without requiring additional annotations. Through a joint process of unsupervised part discovery and concept activation vector extraction, PCMNet effectively captures discriminative concepts and makes interpretable classification decisions.   Our extensive experiments comparing PCMNet against state-of-the-art methods on multiple datasets show that it can provide a high level of interpretability, stability, and robustness under clean and occluded scenarios.",
        "arxiv_id": "2504.12197",
        "ARXIVID": "2504.12197",
        "COMMENT": "Does not match any specific criterion but is related to explainable AI, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.12129": {
        "authors": [
            "Songping Wang",
            "Yueming Lyu",
            "Shiqi Liu",
            "Ning Li",
            "Tong Tong",
            "Hao Sun",
            "Caifeng Shan"
        ],
        "title": "Anti-Aesthetics: Protecting Facial Privacy against Customized Text-to-Image Synthesis",
        "abstract": "arXiv:2504.12129v1 Announce Type: new  Abstract: The rise of customized diffusion models has spurred a boom in personalized visual content creation, but also poses risks of malicious misuse, severely threatening personal privacy and copyright protection. Some studies show that the aesthetic properties of images are highly positively correlated with human perception of image quality. Inspired by this, we approach the problem from a novel and intriguing aesthetic perspective to degrade the generation quality of maliciously customized models, thereby achieving better protection of facial identity. Specifically, we propose a Hierarchical Anti-Aesthetic (HAA) framework to fully explore aesthetic cues, which consists of two key branches: 1) Global Anti-Aesthetics: By establishing a global anti-aesthetic reward mechanism and a global anti-aesthetic loss, it can degrade the overall aesthetics of the generated content; 2) Local Anti-Aesthetics: A local anti-aesthetic reward mechanism and a local anti-aesthetic loss are designed to guide adversarial perturbations to disrupt local facial identity. By seamlessly integrating both branches, our HAA effectively achieves the goal of anti-aesthetics from a global to a local level during customized generation. Extensive experiments show that HAA outperforms existing SOTA methods largely in identity removal, providing a powerful tool for protecting facial privacy and copyright.",
        "arxiv_id": "2504.12129",
        "ARXIVID": "2504.12129",
        "COMMENT": "Does not match any specific criterion but is related to privacy protection in generative models, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.11741": {
        "authors": [
            "Yiyou Sun",
            "Georgia Zhou",
            "Hao Wang",
            "Dacheng Li",
            "Nouha Dziri",
            "Dawn Song"
        ],
        "title": "Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?",
        "abstract": "arXiv:2504.11741v1 Announce Type: new  Abstract: Recent supervised fine-tuning (SFT) approaches have significantly improved language models' performance on mathematical reasoning tasks, even when models are trained at a small scale. However, the specific capabilities enhanced through such fine-tuning remain poorly understood. In this paper, we conduct a detailed analysis of model performance on the AIME24 dataset to understand how reasoning capabilities evolve. We discover a ladder-like structure in problem difficulty, categorize questions into four tiers (Easy, Medium, Hard, and Extremely Hard (Exh)), and identify the specific requirements for advancing between tiers. We find that progression from Easy to Medium tier requires adopting an R1 reasoning style with minimal SFT (500-1K instances), while Hard-level questions suffer from frequent model's errors at each step of the reasoning chain, with accuracy plateauing at around 65% despite logarithmic scaling. Exh-level questions present a fundamentally different challenge; they require unconventional problem-solving skills that current models uniformly struggle with. Additional findings reveal that carefully curated small-scale datasets offer limited advantage-scaling dataset size proves far more effective. Our analysis provides a clearer roadmap for advancing language model capabilities in mathematical reasoning.",
        "arxiv_id": "2504.11741",
        "ARXIVID": "2504.11741",
        "COMMENT": "This paper analyzes reasoning capabilities in language models, which is not directly related to any of the specified criteria but may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11844": {
        "authors": [
            "Tom Everitt",
            "Cristina Garbacea",
            "Alexis Bellot",
            "Jonathan Richens",
            "Henry Papadatos",
            "Sim\\'eon Campos",
            "Rohin Shah"
        ],
        "title": "Evaluating the Goal-Directedness of Large Language Models",
        "abstract": "arXiv:2504.11844v1 Announce Type: new  Abstract: To what extent do LLMs use their capabilities towards their given goal? We take this as a measure of their goal-directedness. We evaluate goal-directedness on tasks that require information gathering, cognitive effort, and plan execution, where we use subtasks to infer each model's relevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI, and Anthropic show that goal-directedness is relatively consistent across tasks, differs from task performance, and is only moderately sensitive to motivational prompts. Notably, most models are not fully goal-directed. We hope our goal-directedness evaluations will enable better monitoring of LLM progress, and enable more deliberate design choices of agentic properties in LLMs.",
        "arxiv_id": "2504.11844",
        "ARXIVID": "2504.11844",
        "COMMENT": "Does not match any specific criteria but discusses goal-directedness in LLMs, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11705": {
        "authors": [
            "Adriano D'Alessandro",
            "Ali Mahdavi-Amiri",
            "Ghassan Hamarneh"
        ],
        "title": "Learning What NOT to Count",
        "abstract": "arXiv:2504.11705v1 Announce Type: new  Abstract: Few/zero-shot object counting methods reduce the need for extensive annotations but often struggle to distinguish between fine-grained categories, especially when multiple similar objects appear in the same scene. To address this limitation, we propose an annotation-free approach that enables the seamless integration of new fine-grained categories into existing few/zero-shot counting models. By leveraging latent generative models, we synthesize high-quality, category-specific crowded scenes, providing a rich training source for adapting to new categories without manual labeling. Our approach introduces an attention prediction network that identifies fine-grained category boundaries trained using only synthetic pseudo-annotated data. At inference, these fine-grained attention estimates refine the output of existing few/zero-shot counting networks. To benchmark our method, we further introduce the FGTC dataset, a taxonomy-specific fine-grained object counting dataset for natural images. Our method substantially enhances pre-trained state-of-the-art models on fine-grained taxon counting tasks, while using only synthetic data. Code and data to be released upon acceptance.",
        "arxiv_id": "2504.11705",
        "ARXIVID": "2504.11705",
        "COMMENT": "Does not match any specific criteria but discusses generative modeling for fine-grained object counting, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11919": {
        "authors": [
            "Qianjin Yu",
            "Keyu Wu",
            "Zihan Chen",
            "Chushu Zhang",
            "Manlin Mei",
            "Lingjun Huang",
            "Fang Tan",
            "Yongsheng Du",
            "Kunlin Liu",
            "Yurui Zhu"
        ],
        "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading",
        "abstract": "arXiv:2504.11919v1 Announce Type: new  Abstract: Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its excellent reasoning ability in complex tasks and has publiclyshared its methodology. This provides potentially high-quality chain-of-thought (CoT) data for stimulating the reasoning abilities of small-sized large language models (LLMs). To generate high-quality CoT data for different LLMs, we seek an efficient method for generating high-quality CoT data with LLM-Adaptive questiondifficulty levels. First, we grade the difficulty of the questions according to the reasoning ability of the LLMs themselves and construct a LLM-Adaptive question database. Second, we sample the problem database based on a distribution of difficulty levels of the questions and then use DeepSeek-R1 (671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality CoT data with correct answers. Thanks to the construction of CoT data with LLM-Adaptive difficulty levels, we have significantly reduced the cost of data generation and enhanced the efficiency of model supervised fine-tuning (SFT). Finally, we have validated the effectiveness and generalizability of the proposed method in the fields of complex mathematical competitions and code generation tasks. Notably, with only 2k high-quality mathematical CoT data, our ZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly, with only 2k high-quality code CoT data, our ZCode-32B surpasses DeepSeek-Distill-32B in code reasoning tasks.",
        "arxiv_id": "2504.11919",
        "ARXIVID": "2504.11919",
        "COMMENT": "Does not match any specific criteria but discusses reasoning abilities and CoT data generation, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11896": {
        "authors": [
            "Xingxing Yang",
            "Jie Chen",
            "Zaifeng Yang"
        ],
        "title": "Learning Physics-Informed Color-Aware Transforms for Low-Light Image Enhancement",
        "abstract": "arXiv:2504.11896v1 Announce Type: new  Abstract: Image decomposition offers deep insights into the imaging factors of visual data and significantly enhances various advanced computer vision tasks. In this work, we introduce a novel approach to low-light image enhancement based on decomposed physics-informed priors. Existing methods that directly map low-light to normal-light images in the sRGB color space suffer from inconsistent color predictions and high sensitivity to spectral power distribution (SPD) variations, resulting in unstable performance under diverse lighting conditions. To address these challenges, we introduce a Physics-informed Color-aware Transform (PiCat), a learning-based framework that converts low-light images from the sRGB color space into deep illumination-invariant descriptors via our proposed Color-aware Transform (CAT). This transformation enables robust handling of complex lighting and SPD variations. Complementing this, we propose the Content-Noise Decomposition Network (CNDN), which refines the descriptor distributions to better align with well-lit conditions by mitigating noise and other distortions, thereby effectively restoring content representations to low-light images. The CAT and the CNDN collectively act as a physical prior, guiding the transformation process from low-light to normal-light domains. Our proposed PiCat framework demonstrates superior performance compared to state-of-the-art methods across five benchmark datasets.",
        "arxiv_id": "2504.11896",
        "ARXIVID": "2504.11896",
        "COMMENT": "Does not match any specific criteria. Focuses on low-light image enhancement using physics-informed priors, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.12103": {
        "authors": [
            "Tao Wen",
            "Jiepeng Wang",
            "Yabo Chen",
            "Shugong Xu",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "title": "Metric-Solver: Sliding Anchored Metric Depth Estimation from a Single Image",
        "abstract": "arXiv:2504.12103v1 Announce Type: new  Abstract: Accurate and generalizable metric depth estimation is crucial for various computer vision applications but remains challenging due to the diverse depth scales encountered in indoor and outdoor environments. In this paper, we introduce Metric-Solver, a novel sliding anchor-based metric depth estimation method that dynamically adapts to varying scene scales. Our approach leverages an anchor-based representation, where a reference depth serves as an anchor to separate and normalize the scene depth into two components: scaled near-field depth and tapered far-field depth. The anchor acts as a normalization factor, enabling the near-field depth to be normalized within a consistent range while mapping far-field depth smoothly toward zero. Through this approach, any depth from zero to infinity in the scene can be represented within a unified representation, effectively eliminating the need to manually account for scene scale variations. More importantly, for the same scene, the anchor can slide along the depth axis, dynamically adjusting to different depth scales. A smaller anchor provides higher resolution in the near-field, improving depth precision for closer objects while a larger anchor improves depth estimation in far regions. This adaptability enables the model to handle depth predictions at varying distances and ensure strong generalization across datasets. Our design enables a unified and adaptive depth representation across diverse environments. Extensive experiments demonstrate that Metric-Solver outperforms existing methods in both accuracy and cross-dataset generalization.",
        "arxiv_id": "2504.12103",
        "ARXIVID": "2504.12103",
        "COMMENT": "Does not match any specific criteria. Focuses on metric depth estimation, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11781": {
        "authors": [
            "Guanchun Wang",
            "Xiangrong Zhang",
            "Yifei Zhang",
            "Zelin Peng",
            "Tianyang Zhang",
            "Xu Tang",
            "Licheng Jiao"
        ],
        "title": "ACMamba: Fast Unsupervised Anomaly Detection via An Asymmetrical Consensus State Space Model",
        "abstract": "arXiv:2504.11781v1 Announce Type: new  Abstract: Unsupervised anomaly detection in hyperspectral images (HSI), aiming to detect unknown targets from backgrounds, is challenging for earth surface monitoring. However, current studies are hindered by steep computational costs due to the high-dimensional property of HSI and dense sampling-based training paradigm, constraining their rapid deployment. Our key observation is that, during training, not all samples within the same homogeneous area are indispensable, whereas ingenious sampling can provide a powerful substitute for reducing costs. Motivated by this, we propose an Asymmetrical Consensus State Space Model (ACMamba) to significantly reduce computational costs without compromising accuracy. Specifically, we design an asymmetrical anomaly detection paradigm that utilizes region-level instances as an efficient alternative to dense pixel-level samples. In this paradigm, a low-cost Mamba-based module is introduced to discover global contextual attributes of regions that are essential for HSI reconstruction. Additionally, we develop a consensus learning strategy from the optimization perspective to simultaneously facilitate background reconstruction and anomaly compression, further alleviating the negative impact of anomaly reconstruction. Theoretical analysis and extensive experiments across eight benchmarks verify the superiority of ACMamba, demonstrating a faster speed and stronger performance over the state-of-the-art.",
        "arxiv_id": "2504.11781",
        "ARXIVID": "2504.11781",
        "COMMENT": "Does not match any specific criteria. Focuses on anomaly detection in hyperspectral images, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.12110": {
        "authors": [
            "Chia Hsiang Kao",
            "Wenting Zhao",
            "Shreelekha Revankar",
            "Samuel Speas",
            "Snehal Bhagat",
            "Rajeev Datta",
            "Cheng Perng Phoo",
            "Utkarsh Mall",
            "Carl Vondrick",
            "Kavita Bala",
            "Bharath Hariharan"
        ],
        "title": "Towards LLM Agents for Earth Observation",
        "abstract": "arXiv:2504.12110v1 Announce Type: new  Abstract: Earth Observation (EO) provides critical planetary data for environmental monitoring, disaster management, climate science, and other scientific domains. Here we ask: Are AI systems ready for reliable Earth Observation? We introduce \\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth Observatory articles across 13 topics and 17 satellite sensors. Using Google Earth Engine API as a tool, LLM agents can only achieve an accuracy of 33% because the code fails to run over 58% of the time. We improve the failure rate for open models by fine-tuning synthetic data, allowing much smaller models (Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g., DeepSeek-R1). Taken together, our findings identify significant challenges to be solved before AI agents can automate earth observation, and suggest paths forward. The project page is available at https://iandrover.github.io/UnivEarth.",
        "arxiv_id": "2504.12110",
        "ARXIVID": "2504.12110",
        "COMMENT": "Does not match any specific criteria. Focuses on Earth observation using LLM agents, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.12222": {
        "authors": [
            "Yike Liu",
            "Jianhui Zhang",
            "Haipeng Li",
            "Shuaicheng Liu",
            "Bing Zeng"
        ],
        "title": "Coding-Prior Guided Diffusion Network for Video Deblurring",
        "abstract": "arXiv:2504.12222v1 Announce Type: new  Abstract: While recent video deblurring methods have advanced significantly, they often overlook two valuable prior information: (1) motion vectors (MVs) and coding residuals (CRs) from video codecs, which provide efficient inter-frame alignment cues, and (2) the rich real-world knowledge embedded in pre-trained diffusion generative models. We present CPGDNet, a novel two-stage framework that effectively leverages both coding priors and generative diffusion priors for high-quality deblurring. First, our coding-prior feature propagation (CPFP) module utilizes MVs for efficient frame alignment and CRs to generate attention masks, addressing motion inaccuracies and texture variations. Second, a coding-prior controlled generation (CPC) module network integrates coding priors into a pretrained diffusion model, guiding it to enhance critical regions and synthesize realistic details. Experiments demonstrate our method achieves state-of-the-art perceptual quality with up to 30% improvement in IQA metrics. Both the code and the codingprior-augmented dataset will be open-sourced.",
        "arxiv_id": "2504.12222",
        "ARXIVID": "2504.12222",
        "COMMENT": "Does not match any specific criteria. Focuses on video deblurring using coding priors and diffusion models, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11470": {
        "authors": [
            "Huaxiang Zhang",
            "Hao Zhang",
            "Aoran Mei",
            "Zhongxue Gan",
            "Guo-Niu Zhu"
        ],
        "title": "SO-DETR: Leveraging Dual-Domain Features and Knowledge Distillation for Small Object Detection",
        "abstract": "arXiv:2504.11470v1 Announce Type: new  Abstract: Detection Transformer-based methods have achieved significant advancements in general object detection. However, challenges remain in effectively detecting small objects. One key difficulty is that existing encoders struggle to efficiently fuse low-level features. Additionally, the query selection strategies are not effectively tailored for small objects. To address these challenges, this paper proposes an efficient model, Small Object Detection Transformer (SO-DETR). The model comprises three key components: a dual-domain hybrid encoder, an enhanced query selection mechanism, and a knowledge distillation strategy. The dual-domain hybrid encoder integrates spatial and frequency domains to fuse multi-scale features effectively. This approach enhances the representation of high-resolution features while maintaining relatively low computational overhead. The enhanced query selection mechanism optimizes query initialization by dynamically selecting high-scoring anchor boxes using expanded IoU, thereby improving the allocation of query resources. Furthermore, by incorporating a lightweight backbone network and implementing a knowledge distillation strategy, we develop an efficient detector for small objects. Experimental results on the VisDrone-2019-DET and UAVVaste datasets demonstrate that SO-DETR outperforms existing methods with similar computational demands. The project page is available at https://github.com/ValiantDiligent/SO_DETR.",
        "arxiv_id": "2504.11470",
        "ARXIVID": "2504.11470",
        "COMMENT": "Does not match any specific criteria. Focuses on small object detection using transformers, which is unrelated to the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11749": {
        "authors": [
            "Zongye Zhang",
            "Wenrui Cai",
            "Qingjie Liu",
            "Yunhong Wang"
        ],
        "title": "SkeletonX: Data-Efficient Skeleton-based Action Recognition via Cross-sample Feature Aggregation",
        "abstract": "arXiv:2504.11749v1 Announce Type: new  Abstract: While current skeleton action recognition models demonstrate impressive performance on large-scale datasets, their adaptation to new application scenarios remains challenging. These challenges are particularly pronounced when facing new action categories, diverse performers, and varied skeleton layouts, leading to significant performance degeneration. Additionally, the high cost and difficulty of collecting skeleton data make large-scale data collection impractical. This paper studies one-shot and limited-scale learning settings to enable efficient adaptation with minimal data. Existing approaches often overlook the rich mutual information between labeled samples, resulting in sub-optimal performance in low-data scenarios. To boost the utility of labeled data, we identify the variability among performers and the commonality within each action as two key attributes. We present SkeletonX, a lightweight training pipeline that integrates seamlessly with existing GCN-based skeleton action recognizers, promoting effective training under limited labeled data. First, we propose a tailored sample pair construction strategy on two key attributes to form and aggregate sample pairs. Next, we develop a concise and effective feature aggregation module to process these pairs. Extensive experiments are conducted on NTU RGB+D, NTU RGB+D 120, and PKU-MMD with various GCN backbones, demonstrating that the pipeline effectively improves performance when trained from scratch with limited data. Moreover, it surpasses previous state-of-the-art methods in the one-shot setting, with only 1/10 of the parameters and much fewer FLOPs. The code and data are available at: https://github.com/zzysteve/SkeletonX",
        "arxiv_id": "2504.11749",
        "ARXIVID": "2504.11749",
        "COMMENT": "Does not match any specific criteria. Focuses on skeleton-based action recognition with limited data, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11922": {
        "authors": [
            "Lvpan Cai",
            "Haowei Wang",
            "Jiayi Ji",
            "YanShu ZhouMen",
            "Yiwei Ma",
            "Xiaoshuai Sun",
            "Liujuan Cao",
            "Rongrong Ji"
        ],
        "title": "Zooming In on Fakes: A Novel Dataset for Localized AI-Generated Image Detection with Forgery Amplification Approach",
        "abstract": "arXiv:2504.11922v1 Announce Type: new  Abstract: The rise of AI-generated image editing tools has made localized forgeries increasingly realistic, posing challenges for visual content integrity. Although recent efforts have explored localized AIGC detection, existing datasets predominantly focus on object-level forgeries while overlooking broader scene edits in regions such as sky or ground. To address these limitations, we introduce \\textbf{BR-Gen}, a large-scale dataset of 150,000 locally forged images with diverse scene-aware annotations, which are based on semantic calibration to ensure high-quality samples. BR-Gen is constructed through a fully automated Perception-Creation-Evaluation pipeline to ensure semantic coherence and visual realism. In addition, we further propose \\textbf{NFA-ViT}, a Noise-guided Forgery Amplification Vision Transformer that enhances the detection of localized forgeries by amplifying forgery-related features across the entire image. NFA-ViT mines heterogeneous regions in images, \\emph{i.e.}, potential edited areas, by noise fingerprints. Subsequently, attention mechanism is introduced to compel the interaction between normal and abnormal features, thereby propagating the generalization traces throughout the entire image, allowing subtle forgeries to influence a broader context and improving overall detection robustness. Extensive experiments demonstrate that BR-Gen constructs entirely new scenarios that are not covered by existing methods. Take a step further, NFA-ViT outperforms existing methods on BR-Gen and generalizes well across current benchmarks. All data and codes are available at https://github.com/clpbc/BR-Gen.",
        "arxiv_id": "2504.11922",
        "ARXIVID": "2504.11922",
        "COMMENT": "Does not match any specific criteria. Focuses on localized AI-generated image detection, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11473": {
        "authors": [
            "Warren Zhu",
            "Aida Ramezani",
            "Yang Xu"
        ],
        "title": "Visual moral inference and communication",
        "abstract": "arXiv:2504.11473v1 Announce Type: new  Abstract: Humans can make moral inferences from multiple sources of input. In contrast, automated moral inference in artificial intelligence typically relies on language models with textual input. However, morality is conveyed through modalities beyond language. We present a computational framework that supports moral inference from natural images, demonstrated in two related tasks: 1) inferring human moral judgment toward visual images and 2) analyzing patterns in moral content communicated via images from public news. We find that models based on text alone cannot capture the fine-grained human moral judgment toward visual stimuli, but language-vision fusion models offer better precision in visual moral inference. Furthermore, applications of our framework to news data reveal implicit biases in news categories and geopolitical discussions. Our work creates avenues for automating visual moral inference and discovering patterns of visual moral communication in public media.",
        "arxiv_id": "2504.11473",
        "ARXIVID": "2504.11473",
        "COMMENT": "Does not match any specific criteria. Focuses on moral inference from visual data, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11704": {
        "authors": [
            "Marina Danilevsky",
            "Kristjan Greenewald",
            "Chulaka Gunasekara",
            "Maeda Hanafi",
            "Lihong He",
            "Yannis Katsis",
            "Krishnateja Killamsetty",
            "Yatin Nandwani",
            "Lucian Popa",
            "Dinesh Raghu",
            "Frederick Reiss",
            "Vraj Shah",
            "Khoi-Nguyen Tran",
            "Huaiyu Zhu",
            "Luis Lastras"
        ],
        "title": "A Library of LLM Intrinsics for Retrieval-Augmented Generation",
        "abstract": "arXiv:2504.11704v1 Announce Type: new  Abstract: In the developer community for large language models (LLMs), there is not yet a clean pattern analogous to a software library, to support very large scale collaboration. Even for the commonplace use case of Retrieval-Augmented Generation (RAG), it is not currently possible to write a RAG application against a well-defined set of APIs that are agreed upon by different LLM providers. Inspired by the idea of compiler intrinsics, we propose some elements of such a concept through introducing a library of LLM Intrinsics for RAG. An LLM intrinsic is defined as a capability that can be invoked through a well-defined API that is reasonably stable and independent of how the LLM intrinsic itself is implemented. The intrinsics in our library are released as LoRA adapters on HuggingFace, and through a software interface with clear structured input/output characteristics on top of vLLM as an inference platform, accompanied in both places with documentation and code. This article describes the intended usage, training details, and evaluations for each intrinsic, as well as compositions of multiple intrinsics.",
        "arxiv_id": "2504.11704",
        "ARXIVID": "2504.11704",
        "COMMENT": "Does not match any specific criteria. Focuses on retrieval-augmented generation and LLM intrinsics, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.11515": {
        "authors": [
            "Kangsheng Wang",
            "Chengwei Ye",
            "Huanzhen Zhang",
            "Linuo Xu",
            "Shuyan Liu"
        ],
        "title": "Graph-Driven Multimodal Feature Learning Framework for Apparent Personality Assessment",
        "abstract": "arXiv:2504.11515v1 Announce Type: new  Abstract: Predicting personality traits automatically has become a challenging problem in computer vision. This paper introduces an innovative multimodal feature learning framework for personality analysis in short video clips. For visual processing, we construct a facial graph and design a Geo-based two-stream network incorporating an attention mechanism, leveraging both Graph Convolutional Networks (GCN) and Convolutional Neural Networks (CNN) to capture static facial expressions. Additionally, ResNet18 and VGGFace networks are employed to extract global scene and facial appearance features at the frame level. To capture dynamic temporal information, we integrate a BiGRU with a temporal attention module for extracting salient frame representations. To enhance the model's robustness, we incorporate the VGGish CNN for audio-based features and XLM-Roberta for text-based features. Finally, a multimodal channel attention mechanism is introduced to integrate different modalities, and a Multi-Layer Perceptron (MLP) regression model is used to predict personality traits. Experimental results confirm that our proposed framework surpasses existing state-of-the-art approaches in performance.",
        "arxiv_id": "2504.11515",
        "ARXIVID": "2504.11515",
        "COMMENT": "Does not match any specific criteria. Focuses on personality assessment using multimodal features, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}