{
    "2502.19128": {
        "authors": [
            "Junlong Ren",
            "Hao Wu",
            "Hui Xiong",
            "Hao Wang"
        ],
        "title": "SCA3D: Enhancing Cross-modal 3D Retrieval via 3D Shape and Caption Paired Data Augmentation",
        "abstract": "arXiv:2502.19128v1 Announce Type: new  Abstract: The cross-modal 3D retrieval task aims to achieve mutual matching between text descriptions and 3D shapes. This has the potential to enhance the interaction between natural language and the 3D environment, especially within the realms of robotics and embodied artificial intelligence (AI) applications. However, the scarcity and expensiveness of 3D data constrain the performance of existing cross-modal 3D retrieval methods. These methods heavily rely on features derived from the limited number of 3D shapes, resulting in poor generalization ability across diverse scenarios. To address this challenge, we introduce SCA3D, a novel 3D shape and caption online data augmentation method for cross-modal 3D retrieval. Our approach uses the LLaVA model to create a component library, captioning each segmented part of every 3D shape within the dataset. Notably, it facilitates the generation of extensive new 3D-text pairs containing new semantic features. We employ both inter and intra distances to align various components into a new 3D shape, ensuring that the components do not overlap and are closely fitted. Further, text templates are utilized to process the captions of each component and generate new text descriptions. Besides, we use unimodal encoders to extract embeddings for 3D shapes and texts based on the enriched dataset. We then calculate fine-grained cross-modal similarity using Earth Mover's Distance (EMD) and enhance cross-modal matching with contrastive learning, enabling bidirectional retrieval between texts and 3D shapes. Extensive experiments show our SCA3D outperforms previous works on the Text2Shape dataset, raising the Shape-to-Text RR@1 score from 20.03 to 27.22 and the Text-to-Shape RR@1 score from 13.12 to 16.67. Codes can be found in https://github.com/3DAgentWorld/SCA3D.",
        "arxiv_id": "2502.19128",
        "ARXIVID": "2502.19128",
        "COMMENT": "Matches criterion 1 and 3 closely as it introduces a novel data augmentation method for cross-modal 3D retrieval, which enhances spatial understanding and builds a new dataset for embodied AI applications.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2502.18530": {
        "authors": [
            "Eric Xue",
            "Zeyi Huang",
            "Yuyang Ji",
            "Haohan Wang"
        ],
        "title": "IMPROVE: Iterative Model Pipeline Refinement and Optimization Leveraging LLM Agents",
        "abstract": "arXiv:2502.18530v1 Announce Type: new  Abstract: Computer vision is a critical component in a wide range of real-world applications, including plant monitoring in agriculture and handwriting classification in digital systems. However, developing high-performance computer vision models traditionally demands both machine learning (ML) expertise and domain-specific knowledge, making the process costly, labor-intensive, and inaccessible to many. Large language model (LLM) agents have emerged as a promising solution to automate this workflow, but most existing methods share a common limitation: they attempt to optimize entire pipelines in a single step before evaluation, making it difficult to attribute improvements to specific changes. This lack of granularity leads to unstable optimization and slower convergence, limiting their effectiveness. To address this, we introduce Iterative Refinement, a novel strategy for LLM-driven ML pipeline design inspired by how human ML experts iteratively refine models, focusing on one component at a time rather than making sweeping changes all at once. By systematically updating individual components based on real training feedback, Iterative Refinement improves stability, interpretability, and overall model performance. We implement this strategy in IMPROVE, an end-to-end LLM agent framework for automating and optimizing object classification pipelines. Through extensive evaluations across datasets of varying sizes and domains, including standard benchmarks and Kaggle competition datasets, we demonstrate that Iterative Refinement enables IMPROVE to consistently achieve better performance over existing zero-shot LLM-based approaches. These findings establish Iterative Refinement as an effective new strategy for LLM-driven ML automation and position IMPROVE as an accessible solution for building high-quality computer vision models without requiring ML expertise.",
        "arxiv_id": "2502.18530",
        "ARXIVID": "2502.18530",
        "COMMENT": "Matches criterion 2 as it discusses the use of LLM agents for iterative refinement in ML pipeline design, which could be extended to vision-related tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2502.18864": {
        "authors": [
            "Juraj Gottweis",
            "Wei-Hung Weng",
            "Alexander Daryin",
            "Tao Tu",
            "Anil Palepu",
            "Petar Sirkovic",
            "Artiom Myaskovsky",
            "Felix Weissenberger",
            "Keran Rong",
            "Ryutaro Tanno",
            "Khaled Saab",
            "Dan Popovici",
            "Jacob Blum",
            "Fan Zhang",
            "Katherine Chou",
            "Avinatan Hassidim",
            "Burak Gokturk",
            "Amin Vahdat",
            "Pushmeet Kohli",
            "Yossi Matias",
            "Andrew Carroll",
            "Kavita Kulkarni",
            "Nenad Tomasev",
            "Yuan Guan",
            "Vikram Dhillon",
            "Eeshit Dhaval Vaishnav",
            "Byron Lee",
            "Tiago R D Costa",
            "Jos\\'e R Penad\\'es",
            "Gary Peltz",
            "Yunhan Xu",
            "Annalisa Pawlosky",
            "Alan Karthikesalingam",
            "Vivek Natarajan"
        ],
        "title": "Towards an AI co-scientist",
        "abstract": "arXiv:2502.18864v1 Announce Type: new  Abstract: Scientific discovery relies on scientists generating novel hypotheses that undergo rigorous experimental validation. To augment this process, we introduce an AI co-scientist, a multi-agent system built on Gemini 2.0. The AI co-scientist is intended to help uncover new, original knowledge and to formulate demonstrably novel research hypotheses and proposals, building upon prior evidence and aligned to scientist-provided research objectives and guidance. The system's design incorporates a generate, debate, and evolve approach to hypothesis generation, inspired by the scientific method and accelerated by scaling test-time compute. Key contributions include: (1) a multi-agent architecture with an asynchronous task execution framework for flexible compute scaling; (2) a tournament evolution process for self-improving hypotheses generation. Automated evaluations show continued benefits of test-time compute, improving hypothesis quality. While general purpose, we focus development and validation in three biomedical areas: drug repurposing, novel target discovery, and explaining mechanisms of bacterial evolution and anti-microbial resistance. For drug repurposing, the system proposes candidates with promising validation findings, including candidates for acute myeloid leukemia that show tumor inhibition in vitro at clinically applicable concentrations. For novel target discovery, the AI co-scientist proposed new epigenetic targets for liver fibrosis, validated by anti-fibrotic activity and liver cell regeneration in human hepatic organoids. Finally, the AI co-scientist recapitulated unpublished experimental results via a parallel in silico discovery of a novel gene transfer mechanism in bacterial evolution. These results, detailed in separate, co-timed reports, demonstrate the potential to augment biomedical and scientific discovery and usher an era of AI empowered scientists.",
        "arxiv_id": "2502.18864",
        "ARXIVID": "2502.18864",
        "COMMENT": "Matches criterion 4 as it discusses a vision foundation model applied to biomedical discovery, showcasing its potential in scientific research.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2502.18512": {
        "authors": [
            "Jianjian Li",
            "Junquan Fan",
            "Feng Tang",
            "Gang Huang",
            "Shitao Zhu",
            "Songlin Liu",
            "Nian Xie",
            "Wulong Liu",
            "Yong Liao"
        ],
        "title": "FCoT-VL:Advancing Text-oriented Large Vision-Language Models with Efficient Visual Token Compression",
        "abstract": "arXiv:2502.18512v1 Announce Type: new  Abstract: The rapid success of Vision Large Language Models (VLLMs) often depends on the high-resolution images with abundant visual tokens, which hinders training and deployment efficiency. Current training-free visual token compression methods exhibit serious performance degradation in tasks involving high-resolution, text-oriented image understanding and reasoning. In this paper, we propose an efficient visual token compression framework for text-oriented VLLMs in high-resolution scenarios. In particular, we employ a light-weight self-distillation pre-training stage to compress the visual tokens, requiring a limited numbers of image-text pairs and minimal learnable parameters. Afterwards, to mitigate potential performance degradation of token-compressed models, we construct a high-quality post-train stage. To validate the effectiveness of our method, we apply it to an advanced VLLMs, InternVL2. Experimental results show that our approach significantly reduces computational overhead while outperforming the baselines across a range of text-oriented benchmarks. We will release the models and code soon.",
        "arxiv_id": "2502.18512",
        "ARXIVID": "2502.18512",
        "COMMENT": "Matches criterion 2 as it proposes advancements in text-oriented Vision Large Language Models (VLLMs).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.19313": {
        "authors": [
            "Zhe Wang",
            "Shaocong Xu",
            "Xucai Zhuang",
            "Tongda Xu",
            "Yan Wang",
            "Jingjing Liu",
            "Yilun Chen",
            "Ya-Qin Zhang"
        ],
        "title": "CoopDETR: A Unified Cooperative Perception Framework for 3D Detection via Object Query",
        "abstract": "arXiv:2502.19313v1 Announce Type: new  Abstract: Cooperative perception enhances the individual perception capabilities of autonomous vehicles (AVs) by providing a comprehensive view of the environment. However, balancing perception performance and transmission costs remains a significant challenge. Current approaches that transmit region-level features across agents are limited in interpretability and demand substantial bandwidth, making them unsuitable for practical applications. In this work, we propose CoopDETR, a novel cooperative perception framework that introduces object-level feature cooperation via object query. Our framework consists of two key modules: single-agent query generation, which efficiently encodes raw sensor data into object queries, reducing transmission cost while preserving essential information for detection; and cross-agent query fusion, which includes Spatial Query Matching (SQM) and Object Query Aggregation (OQA) to enable effective interaction between queries. Our experiments on the OPV2V and V2XSet datasets demonstrate that CoopDETR achieves state-of-the-art performance and significantly reduces transmission costs to 1/782 of previous methods.",
        "arxiv_id": "2502.19313",
        "ARXIVID": "2502.19313",
        "COMMENT": "Matches criterion 3 as it proposes a novel cooperative perception framework for autonomous vehicles, which could be considered a new method in embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.18712": {
        "authors": [
            "Chenlu Ju",
            "Jiaxin Liu",
            "Shobhit Sinha",
            "Hao Xue",
            "Flora Salim"
        ],
        "title": "TrajLLM: A Modular LLM-Enhanced Agent-Based Framework for Realistic Human Trajectory Simulation",
        "abstract": "arXiv:2502.18712v1 Announce Type: new  Abstract: This work leverages Large Language Models (LLMs) to simulate human mobility, addressing challenges like high costs and privacy concerns in traditional models. Our hierarchical framework integrates persona generation, activity selection, and destination prediction, using real-world demographic and psychological data to create realistic movement patterns. Both physical models and language models are employed to explore and demonstrate different methodologies for human mobility simulation. By structuring data with summarization and weighted density metrics, the system ensures scalable memory management while retaining actionable insights. Preliminary results indicate that LLM-driven simulations align with observed real-world patterns, offering scalable, interpretable insights for social problems such as urban planning, traffic management, and public health. The framework's ability to dynamically generate personas and activities enables it to provide adaptable and realistic daily routines. This study demonstrates the transformative potential of LLMs in advancing mobility modeling for societal and urban applications. The source code and interactive demo for our framework are available at https://github.com/cju0/TrajLLM.",
        "arxiv_id": "2502.18712",
        "ARXIVID": "2502.18712",
        "COMMENT": "Matches criterion 3 as it introduces a new framework for human trajectory simulation using LLMs, which could be considered a novel benchmark for embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.18748": {
        "authors": [
            "Shaheer Mohamed",
            "Tharindu Fernando",
            "Sridha Sridharan",
            "Peyman Moghadam",
            "Clinton Fookes"
        ],
        "title": "Spectral-Enhanced Transformers: Leveraging Large-Scale Pretrained Models for Hyperspectral Object Tracking",
        "abstract": "arXiv:2502.18748v1 Announce Type: new  Abstract: Hyperspectral object tracking using snapshot mosaic cameras is emerging as it provides enhanced spectral information alongside spatial data, contributing to a more comprehensive understanding of material properties. Using transformers, which have consistently outperformed convolutional neural networks (CNNs) in learning better feature representations, would be expected to be effective for Hyperspectral object tracking. However, training large transformers necessitates extensive datasets and prolonged training periods. This is particularly critical for complex tasks like object tracking, and the scarcity of large datasets in the hyperspectral domain acts as a bottleneck in achieving the full potential of powerful transformer models. This paper proposes an effective methodology that adapts large pretrained transformer-based foundation models for hyperspectral object tracking. We propose an adaptive, learnable spatial-spectral token fusion module that can be extended to any transformer-based backbone for learning inherent spatial-spectral features in hyperspectral data. Furthermore, our model incorporates a cross-modality training pipeline that facilitates effective learning across hyperspectral datasets collected with different sensor modalities. This enables the extraction of complementary knowledge from additional modalities, whether or not they are present during testing. Our proposed model also achieves superior performance with minimal training iterations.",
        "arxiv_id": "2502.18748",
        "ARXIVID": "2502.18748",
        "COMMENT": "Matches criterion 4 as it discusses leveraging large-scale pretrained models (foundation models) for hyperspectral object tracking.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.18725": {
        "authors": [
            "Xin Liu",
            "Ziyue Zhang",
            "Jingxin Nie"
        ],
        "title": "Talking to the brain: Using Large Language Models as Proxies to Model Brain Semantic Representation",
        "abstract": "arXiv:2502.18725v1 Announce Type: new  Abstract: Traditional psychological experiments utilizing naturalistic stimuli face challenges in manual annotation and ecological validity. To address this, we introduce a novel paradigm leveraging multimodal large language models (LLMs) as proxies to extract rich semantic information from naturalistic images through a Visual Question Answering (VQA) strategy for analyzing human visual semantic representation. LLM-derived representations successfully predict established neural activity patterns measured by fMRI (e.g., faces, buildings), validating its feasibility and revealing hierarchical semantic organization across cortical regions. A brain semantic network constructed from LLM-derived representations identifies meaningful clusters reflecting functional and contextual associations. This innovative methodology offers a powerful solution for investigating brain semantic organization with naturalistic stimuli, overcoming limitations of traditional annotation methods and paving the way for more ecologically valid explorations of human cognition.",
        "arxiv_id": "2502.18725",
        "ARXIVID": "2502.18725",
        "COMMENT": "Matches criterion 2 as it uses multimodal LLMs for brain semantic representation, which is a novel application of VLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.19285": {
        "authors": [
            "Ruben T. Lucassen",
            "Tijn van de Luijtgaarden",
            "Sander P. J. Moonemans",
            "Gerben E. Breimer",
            "Willeke A. M. Blokx",
            "Mitko Veta"
        ],
        "title": "On the Importance of Text Preprocessing for Multimodal Representation Learning and Pathology Report Generation",
        "abstract": "arXiv:2502.19285v1 Announce Type: new  Abstract: Vision-language models in pathology enable multimodal case retrieval and automated report generation. Many of the models developed so far, however, have been trained on pathology reports that include information which cannot be inferred from paired whole slide images (e.g., patient history), potentially leading to hallucinated sentences in generated reports. To this end, we investigate how the selection of information from pathology reports for vision-language modeling affects the quality of the multimodal representations and generated reports. More concretely, we compare a model trained on full reports against a model trained on preprocessed reports that only include sentences describing the cell and tissue appearances based on the H&E-stained slides. For the experiments, we built upon the BLIP-2 framework and used a cutaneous melanocytic lesion dataset of 42,433 H&E-stained whole slide images and 19,636 corresponding pathology reports. Model performance was assessed using image-to-text and text-to-image retrieval, as well as qualitative evaluation of the generated reports by an expert pathologist. Our results demonstrate that text preprocessing prevents hallucination in report generation. Despite the improvement in the quality of the generated reports, training the vision-language model on full reports showed better cross-modal retrieval performance.",
        "arxiv_id": "2502.19285",
        "ARXIVID": "2502.19285",
        "COMMENT": "Matches criterion 4 as it focuses on vision-language models and their application in pathology, addressing hallucination in generated reports.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.18928": {
        "authors": [
            "Achmad Anggawirya Alimin",
            "Dominik P. Goldstein",
            "Lukas Schulze Balhorn",
            "Artur M. Schweidtmann"
        ],
        "title": "Talking like Piping and Instrumentation Diagrams (P&IDs)",
        "abstract": "arXiv:2502.18928v1 Announce Type: new  Abstract: We propose a methodology that allows communication with Piping and Instrumentation Diagrams (P&IDs) using natural language. In particular, we represent P&IDs through the DEXPI data model as labeled property graphs and integrate them with Large Language Models (LLMs). The approach consists of three main parts: 1) P&IDs are cast into a graph representation from the DEXPI format using our pyDEXPI Python package. 2) A tool for generating P&ID knowledge graphs from pyDEXPI. 3) Integration of the P&ID knowledge graph to LLMs using graph-based retrieval augmented generation (graph-RAG). This approach allows users to communicate with P&IDs using natural language. It extends LLM's ability to retrieve contextual data from P&IDs and mitigate hallucinations. Leveraging the LLM's large corpus, the model is also able to interpret process information in PIDs, which could help engineers in their daily tasks. In the future, this work will also open up opportunities in the context of other generative Artificial Intelligence (genAI) solutions on P&IDs, and AI-assisted HAZOP studies.",
        "arxiv_id": "2502.18928",
        "ARXIVID": "2502.18928",
        "COMMENT": "Matches criterion 2 as it integrates LLMs with graph-based retrieval for P&ID diagrams, which is a novel application of VLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.19295": {
        "authors": [
            "Hongyi Ling",
            "Shubham Parashar",
            "Sambhav Khurana",
            "Blake Olson",
            "Anwesha Basu",
            "Gaurangi Sinha",
            "Zhengzhong Tu",
            "James Caverlee",
            "Shuiwang Ji"
        ],
        "title": "Complex LLM Planning via Automated Heuristics Discovery",
        "abstract": "arXiv:2502.19295v1 Announce Type: new  Abstract: We consider enhancing large language models (LLMs) for complex planning tasks. While existing methods allow LLMs to explore intermediate steps to make plans, they either depend on unreliable self-verification or external verifiers to evaluate these steps, which demand significant data and computations. Here, we propose automated heuristics discovery (AutoHD), a novel approach that enables LLMs to explicitly generate heuristic functions to guide inference-time search, allowing accurate evaluation of intermediate states. These heuristic functions are further refined through a heuristic evolution process, improving their robustness and effectiveness. Our proposed method requires no additional model training or fine-tuning, and the explicit definition of heuristic functions generated by the LLMs provides interpretability and insights into the reasoning process. Extensive experiments across diverse benchmarks demonstrate significant gains over multiple baselines, including nearly twice the accuracy on some datasets, establishing our approach as a reliable and interpretable solution for complex planning tasks.",
        "arxiv_id": "2502.19295",
        "ARXIVID": "2502.19295",
        "COMMENT": "Matches criterion 1 as it proposes a novel heuristic discovery method for improving planning tasks in LLMs, which could be relevant to spatial intelligence in embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.19269": {
        "authors": [
            "Jiawei Kong",
            "Hao Fang",
            "Sihang Guo",
            "Chenxi Qing",
            "Bin Chen",
            "Bin Wang",
            "Shu-Tao Xia"
        ],
        "title": "Neural Antidote: Class-Wise Prompt Tuning for Purifying Backdoors in Pre-trained Vision-Language Models",
        "abstract": "arXiv:2502.19269v1 Announce Type: new  Abstract: While pre-trained Vision-Language Models (VLMs) such as CLIP exhibit excellent representational capabilities for multimodal data, recent studies have shown that they are vulnerable to backdoor attacks. To alleviate the threat, existing defense strategies primarily focus on fine-tuning the entire suspicious model, yet offer only marginal resistance to state-of-the-art attacks and often result in a decrease in clean accuracy, particularly in data-limited scenarios. Their failure may be attributed to the mismatch between insufficient fine-tuning data and massive parameters in VLMs. To address this challenge, we propose Class-wise Backdoor Prompt Tuning (CBPT) defense, an efficient and effective method that operates on the text prompts to indirectly purify the poisoned VLMs. Specifically, we first employ the advanced contrastive learning via our carefully crafted positive and negative samples, to effectively invert the backdoor triggers that are potentially adopted by the attacker. Once the dummy trigger is established, we utilize the efficient prompt tuning technique to optimize these class-wise text prompts for modifying the model's decision boundary to further reclassify the feature regions of backdoor triggers. Extensive experiments demonstrate that CBPT significantly mitigates backdoor threats while preserving model utility, e.g. an average Clean Accuracy (CA) of 58.86\\% and an Attack Success Rate (ASR) of 0.39\\% across seven mainstream backdoor attacks. These results underscore the superiority of our prompt purifying design to strengthen model robustness against backdoor attacks.",
        "arxiv_id": "2502.19269",
        "ARXIVID": "2502.19269",
        "COMMENT": "Matches criterion 2 as it focuses on purifying backdoors in pre-trained Vision-Language Models (VLMs), which is a novel application of multimodal learning.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2502.19293": {
        "authors": [
            "Ruben T. Lucassen",
            "Sander P. J. Moonemans",
            "Tijn van de Luijtgaarden",
            "Gerben E. Breimer",
            "Willeke A. M. Blokx",
            "Mitko Veta"
        ],
        "title": "Pathology Report Generation and Multimodal Representation Learning for Cutaneous Melanocytic Lesions",
        "abstract": "arXiv:2502.19293v1 Announce Type: new  Abstract: Millions of melanocytic skin lesions are examined by pathologists each year, the majority of which concern common nevi (i.e., ordinary moles). While most of these lesions can be diagnosed in seconds, writing the corresponding pathology report is much more time-consuming. Automating part of the report writing could, therefore, alleviate the increasing workload of pathologists. In this work, we develop a vision-language model specifically for the pathology domain of cutaneous melanocytic lesions. The model follows the Contrastive Captioner framework and was trained and evaluated using a melanocytic lesion dataset of 42,512 H&E-stained whole slide images and 19,645 corresponding pathology reports. Our results show that the quality scores of model-generated reports were on par with pathologist-written reports for common nevi, assessed by an expert pathologist in a reader study. While report generation revealed to be more difficult for rare melanocytic lesion subtypes, the cross-modal retrieval performance for these cases was considerably better.",
        "arxiv_id": "2502.19293",
        "ARXIVID": "2502.19293",
        "COMMENT": "Matches criterion 2 as it develops a vision-language model for pathology report generation, which is a multimodal application.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2502.18873": {
        "authors": [
            "Sen Yang",
            "Yafu Li",
            "Wai Lam",
            "Yu Cheng"
        ],
        "title": "Multi-LLM Collaborative Search for Complex Problem Solving",
        "abstract": "arXiv:2502.18873v1 Announce Type: new  Abstract: Large language models (LLMs) often struggle with complex reasoning tasks due to their limitations in addressing the vast reasoning space and inherent ambiguities of natural language. We propose the Mixture-of-Search-Agents (MoSA) paradigm, a novel approach leveraging the collective expertise of multiple LLMs to enhance search-based reasoning. MoSA integrates diverse reasoning pathways by combining independent exploration with iterative refinement among LLMs, mitigating the limitations of single-model approaches. Using Monte Carlo Tree Search (MCTS) as a backbone, MoSA enables multiple agents to propose and aggregate reasoning steps, resulting in improved accuracy. Our comprehensive evaluation across four reasoning benchmarks demonstrates MoSA's consistent performance improvements over single-agent and other multi-agent baselines, particularly in complex mathematical and commonsense reasoning tasks.",
        "arxiv_id": "2502.18873",
        "ARXIVID": "2502.18873",
        "COMMENT": "Does not match any specific criterion but is tangentially related to multi-agent reasoning and LLMs, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.19106": {
        "authors": [
            "Tianle Yang",
            "Luyao Chang",
            "Jiadong Yan",
            "Juntao Li",
            "Zhi Wang",
            "Ke Zhang"
        ],
        "title": "A Survey on Foundation-Model-Based Industrial Defect Detection",
        "abstract": "arXiv:2502.19106v1 Announce Type: new  Abstract: As industrial products become abundant and sophisticated, visual industrial defect detection receives much attention, including two-dimensional and three-dimensional visual feature modeling. Traditional methods use statistical analysis, abnormal data synthesis modeling, and generation-based models to separate product defect features and complete defect detection. Recently, the emergence of foundation models has brought visual and textual semantic prior knowledge. Many methods are based on foundation models (FM) to improve the accuracy of detection, but at the same time, increase model complexity and slow down inference speed. Some FM-based methods have begun to explore lightweight modeling ways, which have gradually attracted attention and deserve to be systematically analyzed. In this paper, we conduct a systematic survey with comparisons and discussions of foundation model methods from different aspects and briefly review non-foundation model (NFM) methods recently published. Furthermore, we discuss the differences between FM and NFM methods from training objectives, model structure and scale, model performance, and potential directions for future exploration. Through comparison, we find FM methods are more suitable for few-shot and zero-shot learning, which are more in line with actual industrial application scenarios and worthy of in-depth research.",
        "arxiv_id": "2502.19106",
        "ARXIVID": "2502.19106",
        "COMMENT": "Matches criterion 4 as it surveys foundation models and their applications in industrial defect detection.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2502.18858": {
        "authors": [
            "Jingtao Zhan",
            "Jiahao Zhao",
            "Jiayu Li",
            "Yiqun Liu",
            "Bo Zhang",
            "Qingyao Ai",
            "Jiaxin Mao",
            "Hongning Wang",
            "Min Zhang",
            "Shaoping Ma"
        ],
        "title": "Intelligence Test",
        "abstract": "arXiv:2502.18858v1 Announce Type: new  Abstract: How does intelligence emerge? We propose that intelligence is not a sudden gift or random occurrence, but rather a necessary trait for species to survive through Natural Selection. If a species passes the test of Natural Selection, it demonstrates the intelligence to survive in nature. Extending this perspective, we introduce Intelligence Test, a method to quantify the intelligence of any subject on any task. Like how species evolve by trial and error, Intelligence Test quantifies intelligence by the number of failed attempts before success. Fewer failures correspond to higher intelligence. When the expectation and variance of failure counts are both finite, it signals the achievement of an autonomous level of intelligence. Using Intelligence Test, we comprehensively evaluate existing AI systems. Our results show that while AI systems achieve a level of autonomy in simple tasks, they are still far from autonomous in more complex tasks, such as vision, search, recommendation, and language. While scaling model size might help, this would come at an astronomical cost. Projections suggest that achieving general autonomy would require unimaginable $10^{26}$ parameters. Even if Moore's Law continuously holds, such a parameter scale would take $70$ years. This staggering cost highlights the complexity of human tasks and the inadequacies of current AI. To further understand this phenomenon, we conduct a theoretical analysis. Our simulations suggest that human tasks possess a criticality property. As a result, autonomy requires a deep understanding of the task's underlying mechanisms. Current AI, however, does not fully grasp these mechanisms and instead relies on superficial mimicry, making it difficult to reach an autonomous level. We believe Intelligence Test can not only guide the future development of AI but also offer profound insights into the intelligence of humans ourselves.",
        "arxiv_id": "2502.18858",
        "ARXIVID": "2502.18858",
        "COMMENT": "Does not match any specific criterion but provides an interesting perspective on intelligence and AI evaluation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.19200": {
        "authors": [
            "Zekang Weng",
            "Jinjin Shi",
            "Jinwei Wang",
            "Zeming Han"
        ],
        "title": "HDM: Hybrid Diffusion Model for Unified Image Anomaly Detection",
        "abstract": "arXiv:2502.19200v1 Announce Type: new  Abstract: Image anomaly detection plays a vital role in applications such as industrial quality inspection and medical imaging, where it directly contributes to improving product quality and system reliability. However, existing methods often struggle with complex and diverse anomaly patterns. In particular, the separation between generation and discrimination tasks limits the effective coordination between anomaly sample generation and anomaly region detection. To address these challenges, we propose a novel hybrid diffusion model (HDM) that integrates generation and discrimination into a unified framework. The model consists of three key modules: the Diffusion Anomaly Generation Module (DAGM), the Diffusion Discriminative Module (DDM), and the Probability Optimization Module (POM). DAGM generates realistic and diverse anomaly samples, improving their representativeness. DDM then applies a reverse diffusion process to capture the differences between generated and normal samples, enabling precise anomaly region detection and localization based on probability distributions. POM refines the probability distributions during both the generation and discrimination phases, ensuring high-quality samples are used for training. Extensive experiments on multiple industrial image datasets demonstrate that our method outperforms state-of-the-art approaches, significantly improving both image-level and pixel-level anomaly detection performance, as measured by AUROC.",
        "arxiv_id": "2502.19200",
        "ARXIVID": "2502.19200",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling for anomaly detection, which aligns with general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.18810": {
        "authors": [
            "Weipeng Jiang",
            "Juan Zhai",
            "Shiqing Ma",
            "Ziyan Lei",
            "Xiaofei Xie",
            "Yige Wang",
            "Chao Shen"
        ],
        "title": "Holistic Audit Dataset Generation for LLM Unlearning via Knowledge Graph Traversal and Redundancy Removal",
        "abstract": "arXiv:2502.18810v1 Announce Type: new  Abstract: In recent years, Large Language Models (LLMs) have faced increasing demands to selectively remove sensitive information, protect privacy, and comply with copyright regulations through unlearning, by Machine Unlearning. While evaluating unlearning effectiveness is crucial, existing benchmarks are limited in scale and comprehensiveness, typically containing only a few hundred test cases. We identify two critical challenges in generating holistic audit datasets: ensuring audit adequacy and handling knowledge redundancy between forget and retain dataset. To address these challenges, we propose HANKER, an automated framework for holistic audit dataset generation leveraging knowledge graphs to achieve fine-grained coverage and eliminate redundant knowledge. Applying HANKER to the popular MUSE benchmark, we successfully generated over 69,000 and 111,000 audit cases for the News and Books datasets respectively, identifying thousands of knowledge memorization instances that the previous benchmark failed to detect. Our empirical analysis uncovers how knowledge redundancy significantly skews unlearning effectiveness metrics, with redundant instances artificially inflating the observed memorization measurements ROUGE from 19.7% to 26.1% and Entailment Scores from 32.4% to 35.2%, highlighting the necessity of systematic deduplication for accurate assessment.",
        "arxiv_id": "2502.18810",
        "ARXIVID": "2502.18810",
        "COMMENT": "Does not match any specific criterion but is relevant to LLM unlearning and knowledge graph applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.18532": {
        "authors": [
            "Shuming Shi",
            "Ruobing Zuo",
            "Gaolei He",
            "Jianlin Wang",
            "Chenyang Xu",
            "Zhengfeng Yang"
        ],
        "title": "CuDIP: Enhancing Theorem Proving in LLMs via Curriculum Learning-based Direct Preference Optimization",
        "abstract": "arXiv:2502.18532v1 Announce Type: new  Abstract: Automated theorem proving (ATP) is one of the most challenging mathematical reasoning tasks for Large Language Models (LLMs). Most existing LLM-based ATP methods rely on supervised fine-tuning, which results in a limited alignment between the theorem proving process and human preferences. Direct Preference Optimization (DPO), which aligns LLMs with human preferences, has shown positive effects for certain tasks. However, the lack of high-quality preference data for theorem proving presents a significant challenge. In this paper, we innovatively apply DPO to formal automated theorem proving and introduces a Curriculum Learning-based DPO Iterative Theorem Proving (CuDIP) method. Specifically, we propose a method for constructing preference data which utilizes LLMs and existing theorem proving data to enhance the diversity of the preference data while reducing the reliance on human preference annotations. We then integrate this preference data construction method with curriculum learning to iteratively fine-tune the theorem proving model through DPO. Experimental results on the MiniF2F and ProofNet datasets demonstrate the effectiveness of the proposed method.",
        "arxiv_id": "2502.18532",
        "ARXIVID": "2502.18532",
        "COMMENT": "Does not match any specific criterion but is relevant to LLMs and optimization methods for theorem proving.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.19316": {
        "authors": [
            "Rui Li",
            "Qianfen Jiao",
            "Wenming Cao",
            "Hau-San Wong",
            "Si Wu"
        ],
        "title": "Model Adaptation: Unsupervised Domain Adaptation without Source Data",
        "abstract": "arXiv:2502.19316v1 Announce Type: new  Abstract: In this paper, we investigate a challenging unsupervised domain adaptation setting -- unsupervised model adaptation. We aim to explore how to rely only on unlabeled target data to improve performance of an existing source prediction model on the target domain, since labeled source data may not be available in some real-world scenarios due to data privacy issues. For this purpose, we propose a new framework, which is referred to as collaborative class conditional generative adversarial net to bypass the dependence on the source data. Specifically, the prediction model is to be improved through generated target-style data, which provides more accurate guidance for the generator. As a result, the generator and the prediction model can collaborate with each other without source data. Furthermore, due to the lack of supervision from source data, we propose a weight constraint that encourages similarity to the source model. A clustering-based regularization is also introduced to produce more discriminative features in the target domain. Compared to conventional domain adaptation methods, our model achieves superior performance on multiple adaptation tasks with only unlabeled target data, which verifies its effectiveness in this challenging setting.",
        "arxiv_id": "2502.19316",
        "ARXIVID": "2502.19316",
        "COMMENT": "Does not match any specific criterion but is relevant to domain adaptation and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.19334": {
        "authors": [
            "Qi Yu",
            "Zhichen Zeng",
            "Yuchen Yan",
            "Lei Ying",
            "R. Srikant",
            "Hanghang Tong"
        ],
        "title": "Joint Optimal Transport and Embedding for Network Alignment",
        "abstract": "arXiv:2502.19334v1 Announce Type: new  Abstract: Network alignment, which aims to find node correspondence across different networks, is the cornerstone of various downstream multi-network and Web mining tasks. Most of the embedding-based methods indirectly model cross-network node relationships by contrasting positive and negative node pairs sampled from hand-crafted strategies, which are vulnerable to graph noises and lead to potential misalignment of nodes. Another line of work based on the optimal transport (OT) theory directly models cross-network node relationships and generates noise-reduced alignments. However, OT methods heavily rely on fixed, pre-defined cost functions that prohibit end-to-end training and are hard to generalize. In this paper, we aim to unify the embedding and OT-based methods in a mutually beneficial manner and propose a joint optimal transport and embedding framework for network alignment named JOENA. For one thing (OT for embedding), through a simple yet effective transformation, the noise-reduced OT mapping serves as an adaptive sampling strategy directly modeling all cross-network node pairs for robust embedding learning.For another (embedding for OT), on top of the learned embeddings, the OT cost can be gradually trained in an end-to-end fashion, which further enhances the alignment quality. With a unified objective, the mutual benefits of both methods can be achieved by an alternating optimization schema with guaranteed convergence. Extensive experiments on real-world networks validate the effectiveness and scalability of JOENA, achieving up to 16% improvement in MRR and 20x speedup compared with the state-of-the-art alignment methods.",
        "arxiv_id": "2502.19334",
        "ARXIVID": "2502.19334",
        "COMMENT": "Does not match any specific criterion but is generally relevant to machine learning and optimization methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.18844": {
        "authors": [
            "Yunmei Huang",
            "Songlin Hou",
            "Zachary Nelson Horve",
            "Songlin Fei"
        ],
        "title": "BarkXAI: A Lightweight Post-Hoc Explainable Method for Tree Species Classification with Quantifiable Concepts",
        "abstract": "arXiv:2502.18844v1 Announce Type: new  Abstract: The precise identification of tree species is fundamental to forestry, conservation, and environmental monitoring. Though many studies have demonstrated that high accuracy can be achieved using bark-based species classification, these models often function as \"black boxes\", limiting interpretability, trust, and adoption in critical forestry applications. Attribution-based Explainable AI (XAI) methods have been used to address this issue in related works. However, XAI applications are often dependent on local features (such as a head shape or paw in animal applications) and cannot describe global visual features (such as ruggedness or smoothness) that are present in texture-dominant images such as tree bark. Concept-based XAI methods, on the other hand, offer explanations based on global visual features with concepts, but they tend to require large overhead in building external concept image datasets and the concepts can be vague and subjective without good means of precise quantification. To address these challenges, we propose a lightweight post-hoc method to interpret visual models for tree species classification using operators and quantifiable concepts. Our approach eliminates computational overhead, enables the quantification of complex concepts, and evaluates both concept importance and the model's reasoning process. To the best of our knowledge, our work is the first study to explain bark vision models in terms of global visual features with concepts. Using a human-annotated dataset as ground truth, our experiments demonstrate that our method significantly outperforms TCAV and Llama3.2 in concept importance ranking based on Kendall's Tau, highlighting its superior alignment with human perceptions.",
        "arxiv_id": "2502.18844",
        "ARXIVID": "2502.18844",
        "COMMENT": "Does not match any specific criterion but is related to explainable AI in computer vision, which aligns with general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.19217": {
        "authors": [
            "Nikita Shvetsov",
            "Thomas K. Kilvaer",
            "Masoud Tafavvoghi",
            "Anders Sildnes",
            "Kajsa M{\\o}llersen",
            "Lill-Tove Rasmussen Busund",
            "Lars Ailo Bongo"
        ],
        "title": "A Lightweight and Extensible Cell Segmentation and Classification Model for Whole Slide Images",
        "abstract": "arXiv:2502.19217v1 Announce Type: new  Abstract: Developing clinically useful cell-level analysis tools in digital pathology remains challenging due to limitations in dataset granularity, inconsistent annotations, high computational demands, and difficulties integrating new technologies into workflows. To address these issues, we propose a solution that enhances data quality, model performance, and usability by creating a lightweight, extensible cell segmentation and classification model. First, we update data labels through cross-relabeling to refine annotations of PanNuke and MoNuSAC, producing a unified dataset with seven distinct cell types. Second, we leverage the H-Optimus foundation model as a fixed encoder to improve feature representation for simultaneous segmentation and classification tasks. Third, to address foundation models' computational demands, we distill knowledge to reduce model size and complexity while maintaining comparable performance. Finally, we integrate the distilled model into QuPath, a widely used open-source digital pathology platform. Results demonstrate improved segmentation and classification performance using the H-Optimus-based model compared to a CNN-based model. Specifically, average $R^2$ improved from 0.575 to 0.871, and average $PQ$ score improved from 0.450 to 0.492, indicating better alignment with actual cell counts and enhanced segmentation quality. The distilled model maintains comparable performance while reducing parameter count by a factor of 48. By reducing computational complexity and integrating into workflows, this approach may significantly impact diagnostics, reduce pathologist workload, and improve outcomes. Although the method shows promise, extensive validation is necessary prior to clinical deployment.",
        "arxiv_id": "2502.19217",
        "ARXIVID": "2502.19217",
        "COMMENT": "Does not match any specific criterion but is related to computer vision applications in digital pathology.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}