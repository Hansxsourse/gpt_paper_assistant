{
    "2503.06235": {
        "authors": [
            "Yang LI",
            "Jinglu Wang",
            "Lei Chu",
            "Xiao Li",
            "Shiu-hong Kao",
            "Ying-Cong Chen",
            "Yan Lu"
        ],
        "title": "StreamGS: Online Generalizable Gaussian Splatting Reconstruction for Unposed Image Streams",
        "abstract": "arXiv:2503.06235v1 Announce Type: new  Abstract: The advent of 3D Gaussian Splatting (3DGS) has advanced 3D scene reconstruction and novel view synthesis. With the growing interest of interactive applications that need immediate feedback, online 3DGS reconstruction in real-time is in high demand. However, none of existing methods yet meet the demand due to three main challenges: the absence of predetermined camera parameters, the need for generalizable 3DGS optimization, and the necessity of reducing redundancy. We propose StreamGS, an online generalizable 3DGS reconstruction method for unposed image streams, which progressively transform image streams to 3D Gaussian streams by predicting and aggregating per-frame Gaussians. Our method overcomes the limitation of the initial point reconstruction \\cite{dust3r} in tackling out-of-domain (OOD) issues by introducing a content adaptive refinement. The refinement enhances cross-frame consistency by establishing reliable pixel correspondences between adjacent frames. Such correspondences further aid in merging redundant Gaussians through cross-frame feature aggregation. The density of Gaussians is thereby reduced, empowering online reconstruction by significantly lowering computational and memory costs. Extensive experiments on diverse datasets have demonstrated that StreamGS achieves quality on par with optimization-based approaches but does so 150 times faster, and exhibits superior generalizability in handling OOD scenes.",
        "arxiv_id": "2503.06235",
        "ARXIVID": "2503.06235",
        "COMMENT": "Matches criterion 3 as it introduces a novel online 3D Gaussian Splatting method for unposed image streams, relevant to embodied AI benchmarks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.06287": {
        "authors": [
            "Seil Kang",
            "Jinyeong Kim",
            "Junhyeok Kim",
            "Seong Jae Hwang"
        ],
        "title": "Your Large Vision-Language Model Only Needs A Few Attention Heads For Visual Grounding",
        "abstract": "arXiv:2503.06287v1 Announce Type: new  Abstract: Visual grounding seeks to localize the image region corresponding to a free-form text description. Recently, the strong multimodal capabilities of Large Vision-Language Models (LVLMs) have driven substantial improvements in visual grounding, though they inevitably require fine-tuning and additional model components to explicitly generate bounding boxes or segmentation masks. However, we discover that a few attention heads in frozen LVLMs demonstrate strong visual grounding capabilities. We refer to these heads, which consistently capture object locations related to text semantics, as localization heads. Using localization heads, we introduce a straightforward and effective training-free visual grounding framework that utilizes text-to-image attention maps from localization heads to identify the target objects. Surprisingly, only three out of thousands of attention heads are sufficient to achieve competitive localization performance compared to existing LVLM-based visual grounding methods that require fine-tuning. Our findings suggest that LVLMs can innately ground objects based on a deep comprehension of the text-image relationship, as they implicitly focus on relevant image regions to generate informative text outputs. All the source codes will be made available to the public.",
        "arxiv_id": "2503.06287",
        "ARXIVID": "2503.06287",
        "COMMENT": "Matches criterion 2 as it explores visual grounding capabilities of large vision-language models (LVLMs).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2503.06486": {
        "authors": [
            "Cong Chen",
            "Mingyu Liu",
            "Chenchen Jing",
            "Yizhou Zhou",
            "Fengyun Rao",
            "Hao Chen",
            "Bo Zhang",
            "Chunhua Shen"
        ],
        "title": "PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training",
        "abstract": "arXiv:2503.06486v1 Announce Type: new  Abstract: This paper aims to address the challenge of hallucinations in Multimodal Large Language Models (MLLMs) particularly for dense image captioning tasks. To tackle the challenge, we identify the current lack of a metric that finely measures the caption quality in concept level. We hereby introduce HalFscore, a novel metric built upon the language graph and is designed to evaluate both the accuracy and completeness of dense captions at a granular level. Additionally, we identify the root cause of hallucination as the model's over-reliance on its language prior. To address this, we propose PerturboLLaVA, which reduces the model's reliance on the language prior by incorporating adversarially perturbed text during training. This method enhances the model's focus on visual inputs, effectively reducing hallucinations and producing accurate, image-grounded descriptions without incurring additional computational overhead. PerturboLLaVA significantly improves the fidelity of generated captions, outperforming existing approaches in handling multimodal hallucinations and achieving improved performance across general multimodal benchmarks.",
        "arxiv_id": "2503.06486",
        "ARXIVID": "2503.06486",
        "COMMENT": "Matches criterion 2 as it addresses hallucinations in MLLMs and proposes a novel training method to improve multimodal grounding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.07334": {
        "authors": [
            "Xing Xie",
            "Jiawei Liu",
            "Ziyue Lin",
            "Huijie Fan",
            "Zhi Han",
            "Yandong Tang",
            "Liangqiong Qu"
        ],
        "title": "Unleashing the Potential of Large Language Models for Text-to-Image Generation through Autoregressive Representation Alignment",
        "abstract": "arXiv:2503.07334v1 Announce Type: new  Abstract: We present Autoregressive Representation Alignment (ARRA), a new training framework that unlocks global-coherent text-to-image generation in autoregressive LLMs without architectural changes. Unlike prior work that requires complex architectural redesigns, ARRA aligns LLM hidden states with visual representations from external visual foundational models via a global visual alignment loss and a hybrid token, . This token enforces dual constraints: local next-token prediction and global semantic distillation, enabling LLMs to implicitly learn spatial and contextual coherence while retaining their original autoregressive paradigm. Extensive experiments validate ARRA's plug-and-play versatility. When training from text-generation-only LLMs or random initialization, ARRA reduces FID by 25.5% (MIMIC-CXR), 8.8% (DeepEyeNet), and 7.5% (ImageNet) for advanced autoregressive LLMs like Chameleon and LlamaGen, all without framework modifications. For domain adaption, ARRA aligns general-purpose LLMs with specialized models (e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on medical imaging (MIMIC-CXR). By demonstrating that training objective redesign -- not just architectural innovation -- can resolve cross-modal global coherence challenges, ARRA offers a complementary paradigm for advancing autoregressive models. Code and models will be released to advance autoregressive image generation.",
        "arxiv_id": "2503.07334",
        "ARXIVID": "2503.07334",
        "COMMENT": "Matches criterion 2 as it introduces a new training framework for autoregressive LLMs to improve text-to-image generation, which is relevant to VLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.06469": {
        "authors": [
            "George Tang",
            "Aditya Agarwal",
            "Weiqiao Han",
            "Trevor Darrell",
            "Yutong Bai"
        ],
        "title": "Vector Quantized Feature Fields for Fast 3D Semantic Lifting",
        "abstract": "arXiv:2503.06469v1 Announce Type: new  Abstract: We generalize lifting to semantic lifting by incorporating per-view masks that indicate relevant pixels for lifting tasks. These masks are determined by querying corresponding multiscale pixel-aligned feature maps, which are derived from scene representations such as distilled feature fields and feature point clouds. However, storing per-view feature maps rendered from distilled feature fields is impractical, and feature point clouds are expensive to store and query. To enable lightweight on-demand retrieval of pixel-aligned relevance masks, we introduce the Vector-Quantized Feature Field. We demonstrate the effectiveness of the Vector-Quantized Feature Field on complex indoor and outdoor scenes. Semantic lifting, when paired with a Vector-Quantized Feature Field, can unlock a myriad of applications in scene representation and embodied intelligence. Specifically, we showcase how our method enables text-driven localized scene editing and significantly improves the efficiency of embodied question answering.",
        "arxiv_id": "2503.06469",
        "ARXIVID": "2503.06469",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for semantic lifting and its applications in embodied intelligence.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.05978": {
        "authors": [
            "Hongwei Yi",
            "Tian Ye",
            "Shitong Shao",
            "Xuancheng Yang",
            "Jiantong Zhao",
            "Hanzhong Guo",
            "Terrance Wang",
            "Qingyu Yin",
            "Zeke Xie",
            "Lei Zhu",
            "Wei Li",
            "Michael Lingelbach",
            "Daquan Zhou"
        ],
        "title": "MagicInfinite: Generating Infinite Talking Videos with Your Words and Voice",
        "abstract": "arXiv:2503.05978v1 Announce Type: new  Abstract: We present MagicInfinite, a novel diffusion Transformer (DiT) framework that overcomes traditional portrait animation limitations, delivering high-fidelity results across diverse character types-realistic humans, full-body figures, and stylized anime characters. It supports varied facial poses, including back-facing views, and animates single or multiple characters with input masks for precise speaker designation in multi-character scenes. Our approach tackles key challenges with three innovations: (1) 3D full-attention mechanisms with a sliding window denoising strategy, enabling infinite video generation with temporal coherence and visual quality across diverse character styles; (2) a two-stage curriculum learning scheme, integrating audio for lip sync, text for expressive dynamics, and reference images for identity preservation, enabling flexible multi-modal control over long sequences; and (3) region-specific masks with adaptive loss functions to balance global textual control and local audio guidance, supporting speaker-specific animations. Efficiency is enhanced via our innovative unified step and cfg distillation techniques, achieving a 20x inference speed boost over the basemodel: generating a 10 second 540x540p video in 10 seconds or 720x720p in 30 seconds on 8 H100 GPUs, without quality loss. Evaluations on our new benchmark demonstrate MagicInfinite's superiority in audio-lip synchronization, identity preservation, and motion naturalness across diverse scenarios. It is publicly available at https://www.hedra.com/, with examples at https://magicinfinite.github.io/.",
        "arxiv_id": "2503.05978",
        "ARXIVID": "2503.05978",
        "COMMENT": "Matches criterion 2 as it introduces a novel multi-modal framework for generating talking videos with text and audio inputs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.06260": {
        "authors": [
            "Muzhi Dai",
            "Jiashuo Sun",
            "Zhiyuan Zhao",
            "Shixuan Liu",
            "Rui Li",
            "Junyu Gao",
            "Xuelong Li"
        ],
        "title": "From Captions to Rewards (CAREVL): Leveraging Large Language Model Experts for Enhanced Reward Modeling in Large Vision-Language Models",
        "abstract": "arXiv:2503.06260v1 Announce Type: new  Abstract: Aligning large vision-language models (LVLMs) with human preferences is challenging due to the scarcity of fine-grained, high-quality, and multimodal preference data without human annotations. Existing methods relying on direct distillation often struggle with low-confidence data, leading to suboptimal performance. To address this, we propose CAREVL, a novel method for preference reward modeling by reliably using both high- and low-confidence data. First, a cluster of auxiliary expert models (textual reward models) innovatively leverages image captions as weak supervision signals to filter high-confidence data. The high-confidence data are then used to fine-tune the LVLM. Second, low-confidence data are used to generate diverse preference samples using the fine-tuned LVLM. These samples are then scored and selected to construct reliable chosen-rejected pairs for further training. CAREVL achieves performance improvements over traditional distillation-based methods on VL-RewardBench and MLLM-as-a-Judge benchmark, demonstrating its effectiveness. The code will be released soon.",
        "arxiv_id": "2503.06260",
        "ARXIVID": "2503.06260",
        "COMMENT": "Matches criterion 2 as it focuses on aligning large vision-language models (LVLMs) with human preferences using novel reward modeling techniques.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.07485": {
        "authors": [
            "Zongzheng Zhang",
            "Xinrun Li",
            "Sizhe Zou",
            "Guoxuan Chi",
            "Siqi Li",
            "Xuchong Qiu",
            "Guoliang Wang",
            "Guantian Zheng",
            "Leichen Wang",
            "Hang Zhao",
            "Hao Zhao"
        ],
        "title": "Chameleon: Fast-slow Neuro-symbolic Lane Topology Extraction",
        "abstract": "arXiv:2503.07485v1 Announce Type: new  Abstract: Lane topology extraction involves detecting lanes and traffic elements and determining their relationships, a key perception task for mapless autonomous driving. This task requires complex reasoning, such as determining whether it is possible to turn left into a specific lane. To address this challenge, we introduce neuro-symbolic methods powered by vision-language foundation models (VLMs). Existing approaches have notable limitations: (1) Dense visual prompting with VLMs can achieve strong performance but is costly in terms of both financial resources and carbon footprint, making it impractical for robotics applications. (2) Neuro-symbolic reasoning methods for 3D scene understanding fail to integrate visual inputs when synthesizing programs, making them ineffective in handling complex corner cases. To this end, we propose a fast-slow neuro-symbolic lane topology extraction algorithm, named Chameleon, which alternates between a fast system that directly reasons over detected instances using synthesized programs and a slow system that utilizes a VLM with a chain-of-thought design to handle corner cases. Chameleon leverages the strengths of both approaches, providing an affordable solution while maintaining high performance. We evaluate the method on the OpenLane-V2 dataset, showing consistent improvements across various baseline detectors. Our code, data, and models are publicly available at https://github.com/XR-Lee/neural-symbolic",
        "arxiv_id": "2503.07485",
        "ARXIVID": "2503.07485",
        "COMMENT": "Matches criterion 4 as it uses vision-language foundation models for neuro-symbolic reasoning in lane topology extraction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.06014": {
        "authors": [
            "Xiaohao Xu",
            "Feng Xue",
            "Xiang Li",
            "Haowei Li",
            "Shusheng Yang",
            "Tianyi Zhang",
            "Matthew Johnson-Roberson",
            "Xiaonan Huang"
        ],
        "title": "Towards Ambiguity-Free Spatial Foundation Model: Rethinking and Decoupling Depth Ambiguity",
        "abstract": "arXiv:2503.06014v1 Announce Type: new  Abstract: Depth ambiguity is a fundamental challenge in spatial scene understanding, especially in transparent scenes where single-depth estimates fail to capture full 3D structure. Existing models, limited to deterministic predictions, overlook real-world multi-layer depth. To address this, we introduce a paradigm shift from single-prediction to multi-hypothesis spatial foundation models. We first present \\texttt{MD-3k}, a benchmark exposing depth biases in expert and foundational models through multi-layer spatial relationship labels and new metrics. To resolve depth ambiguity, we propose Laplacian Visual Prompting (LVP), a training-free spectral prompting technique that extracts hidden depth from pre-trained models via Laplacian-transformed RGB inputs. By integrating LVP-inferred depth with standard RGB-based estimates, our approach elicits multi-layer depth without model retraining. Extensive experiments validate the effectiveness of LVP in zero-shot multi-layer depth estimation, unlocking more robust and comprehensive geometry-conditioned visual generation, 3D-grounded spatial reasoning, and temporally consistent video-level depth inference. Our benchmark and code will be available at https://github.com/Xiaohao-Xu/Ambiguity-in-Space.",
        "arxiv_id": "2503.06014",
        "ARXIVID": "2503.06014",
        "COMMENT": "Matches criterion 1 as it introduces a new method (Laplacian Visual Prompting) for addressing depth ambiguity in spatial understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.06312": {
        "authors": [
            "Zhitong Xiong",
            "Yi Wang",
            "Weikang Yu",
            "Adam J Stewart",
            "Jie Zhao",
            "Nils Lehmann",
            "Thomas Dujardin",
            "Zhenghang Yuan",
            "Pedram Ghamisi",
            "Xiao Xiang Zhu"
        ],
        "title": "GeoLangBind: Unifying Earth Observation with Agglomerative Vision-Language Foundation Models",
        "abstract": "arXiv:2503.06312v1 Announce Type: new  Abstract: Earth observation (EO) data, collected from diverse sensors with varying imaging principles, present significant challenges in creating unified analytical frameworks. We present GeoLangBind, a novel agglomerative vision--language foundation model that bridges the gap between heterogeneous EO data modalities using language as a unifying medium. Our approach aligns different EO data types into a shared language embedding space, enabling seamless integration and complementary feature learning from diverse sensor data. To achieve this, we construct a large-scale multimodal image--text dataset, GeoLangBind-2M, encompassing six data modalities. GeoLangBind leverages this dataset to develop a zero-shot foundation model capable of processing arbitrary numbers of EO data channels as input. Through our designed Modality-aware Knowledge Agglomeration (MaKA) module and progressive multimodal weight merging strategy, we create a powerful agglomerative foundation model that excels in both zero-shot vision--language comprehension and fine-grained visual understanding. Extensive evaluation across 23 datasets covering multiple tasks demonstrates GeoLangBind's superior performance and versatility in EO applications, offering a robust framework for various environmental monitoring and analysis tasks. The dataset and pretrained models will be publicly available.",
        "arxiv_id": "2503.06312",
        "ARXIVID": "2503.06312",
        "COMMENT": "Matches criterion 4 as it introduces a vision-language foundation model for Earth observation applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.06012": {
        "authors": [
            "Zhenrong Wang",
            "Qi Zheng",
            "Sihan Ma",
            "Maosheng Ye",
            "Yibing Zhan",
            "Dongjiang Li"
        ],
        "title": "End-to-End HOI Reconstruction Transformer with Graph-based Encoding",
        "abstract": "arXiv:2503.06012v1 Announce Type: new  Abstract: With the diversification of human-object interaction (HOI) applications and the success of capturing human meshes, HOI reconstruction has gained widespread attention. Existing mainstream HOI reconstruction methods often rely on explicitly modeling interactions between humans and objects. However, such a way leads to a natural conflict between 3D mesh reconstruction, which emphasizes global structure, and fine-grained contact reconstruction, which focuses on local details. To address the limitations of explicit modeling, we propose the End-to-End HOI Reconstruction Transformer with Graph-based Encoding (HOI-TG). It implicitly learns the interaction between humans and objects by leveraging self-attention mechanisms. Within the transformer architecture, we devise graph residual blocks to aggregate the topology among vertices of different spatial structures. This dual focus effectively balances global and local representations. Without bells and whistles, HOI-TG achieves state-of-the-art performance on BEHAVE and InterCap datasets. Particularly on the challenging InterCap dataset, our method improves the reconstruction results for human and object meshes by 8.9% and 8.6%, respectively.",
        "arxiv_id": "2503.06012",
        "ARXIVID": "2503.06012",
        "COMMENT": "Matches criterion 1 as it proposes a novel method for spatial understanding in human-object interaction reconstruction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.07416": {
        "authors": [
            "Shaobin Zhuang",
            "Yiwei Guo",
            "Yanbo Ding",
            "Kunchang Li",
            "Xinyuan Chen",
            "Yaohui Wang",
            "Fangyikang Wang",
            "Ying Zhang",
            "Chen Li",
            "Yali Wang"
        ],
        "title": "TimeStep Master: Asymmetrical Mixture of Timestep LoRA Experts for Versatile and Efficient Diffusion Models in Vision",
        "abstract": "arXiv:2503.07416v1 Announce Type: new  Abstract: Diffusion models have driven the advancement of vision generation over the past years. However, it is often difficult to apply these large models in downstream tasks, due to massive fine-tuning cost. Recently, Low-Rank Adaptation (LoRA) has been applied for efficient tuning of diffusion models. Unfortunately, the capabilities of LoRA-tuned diffusion models are limited, since the same LoRA is used for different timesteps of the diffusion process. To tackle this problem, we introduce a general and concise TimeStep Master (TSM) paradigm with two key fine-tuning stages. In the fostering stage (1-stage), we apply different LoRAs to fine-tune the diffusion model at different timestep intervals. This results in different TimeStep LoRA experts that can effectively capture different noise levels. In the assembling stage (2-stage), we design a novel asymmetrical mixture of TimeStep LoRA experts, via core-context collaboration of experts at multi-scale intervals. For each timestep, we leverage TimeStep LoRA expert within the smallest interval as the core expert without gating, and use experts within the bigger intervals as the context experts with time-dependent gating. Consequently, our TSM can effectively model the noise level via the expert in the finest interval, and adaptively integrate contexts from the experts of other scales, boosting the versatility of diffusion models. To show the effectiveness of our TSM paradigm, we conduct extensive experiments on three typical and popular LoRA-related tasks of diffusion models, including domain adaptation, post-pretraining, and model distillation. Our TSM achieves the state-of-the-art results on all these tasks, throughout various model structures (UNet, DiT and MM-DiT) and visual data modalities (Image, Video), showing its remarkable generalization capacity.",
        "arxiv_id": "2503.07416",
        "ARXIVID": "2503.07416",
        "COMMENT": "Matches criterion 4 as it proposes a novel method for fine-tuning diffusion models, which are foundational in vision generation tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.07148": {
        "authors": [
            "Ali Baheri",
            "Cecilia O. Alm"
        ],
        "title": "Hierarchical Neuro-Symbolic Decision Transformer",
        "abstract": "arXiv:2503.07148v1 Announce Type: new  Abstract: We present a hierarchical neuro-symbolic control framework that couples classical symbolic planning with transformer-based policies to address complex, long-horizon decision-making tasks. At the high level, a symbolic planner constructs an interpretable sequence of operators based on logical propositions, ensuring systematic adherence to global constraints and goals. At the low level, each symbolic operator is translated into a sub-goal token that conditions a decision transformer to generate a fine-grained sequence of actions in uncertain, high-dimensional environments. We provide theoretical analysis showing how approximation errors from both the symbolic planner and the neural execution layer accumulate. Empirical evaluations in grid-worlds with multiple keys, locked doors, and item-collection tasks show that our hierarchical approach outperforms purely end-to-end neural approach in success rates and policy efficiency.",
        "arxiv_id": "2503.07148",
        "ARXIVID": "2503.07148",
        "COMMENT": "Matches criterion 3 as it proposes a hierarchical neuro-symbolic framework for decision-making tasks, which is a novel method in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.06138": {
        "authors": [
            "Tadahiro Taniguchi",
            "Yasushi Hirai",
            "Masahiro Suzuki",
            "Shingo Murata",
            "Takato Horii",
            "Kazutoshi Tanaka"
        ],
        "title": "System 0/1/2/3: Quad-process theory for multi-timescale embodied collective cognitive systems",
        "abstract": "arXiv:2503.06138v1 Announce Type: new  Abstract: This paper introduces the System 0/1/2/3 framework as an extension of dual-process theory, employing a quad-process model of cognition. Expanding upon System 1 (fast, intuitive thinking) and System 2 (slow, deliberative thinking), we incorporate System 0, which represents pre-cognitive embodied processes, and System 3, which encompasses collective intelligence and symbol emergence. We contextualize this model within Bergson's philosophy by adopting multi-scale time theory to unify the diverse temporal dynamics of cognition. System 0 emphasizes morphological computation and passive dynamics, illustrating how physical embodiment enables adaptive behavior without explicit neural processing. Systems 1 and 2 are explained from a constructive perspective, incorporating neurodynamical and AI viewpoints. In System 3, we introduce collective predictive coding to explain how societal-level adaptation and symbol emergence operate over extended timescales. This comprehensive framework ranges from rapid embodied reactions to slow-evolving collective intelligence, offering a unified perspective on cognition across multiple timescales, levels of abstraction, and forms of human intelligence. The System 0/1/2/3 model provides a novel theoretical foundation for understanding the interplay between adaptive and cognitive processes, thereby opening new avenues for research in cognitive science, AI, robotics, and collective intelligence.",
        "arxiv_id": "2503.06138",
        "ARXIVID": "2503.06138",
        "COMMENT": "Matches criterion 3 as it proposes a novel theoretical framework for embodied cognitive systems, which could be relevant to embodied AI methods.",
        "RELEVANCE": 6,
        "NOVELTY": 8
    },
    "2503.06053": {
        "authors": [
            "Runze Zhang",
            "Guoguang Du",
            "Xiaochuan Li",
            "Qi Jia",
            "Liang Jin",
            "Lu Liu",
            "Jingjing Wang",
            "Cong Xu",
            "Zhenhua Guo",
            "Yaqian Zhao",
            "Xiaoli Gong",
            "Rengang Li",
            "Baoyu Fan"
        ],
        "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation",
        "abstract": "arXiv:2503.06053v1 Announce Type: new  Abstract: Spatio-temporal consistency is a critical research topic in video generation. A qualified generated video segment must ensure plot plausibility and coherence while maintaining visual consistency of objects and scenes across varying viewpoints. Prior research, especially in open-source projects, primarily focuses on either temporal or spatial consistency, or their basic combination, such as appending a description of a camera movement after a prompt without constraining the outcomes of this movement. However, camera movement may introduce new objects to the scene or eliminate existing ones, thereby overlaying and affecting the preceding narrative. Especially in videos with numerous camera movements, the interplay between multiple plots becomes increasingly complex. This paper introduces and examines integral spatio-temporal consistency, considering the synergy between plot progression and camera techniques, and the long-term impact of prior content on subsequent generation. Our research encompasses dataset construction through to the development of the model. Initially, we constructed a DropletVideo-10M dataset, which comprises 10 million videos featuring dynamic camera motion and object actions. Each video is annotated with an average caption of 206 words, detailing various camera movements and plot developments. Following this, we developed and trained the DropletVideo model, which excels in preserving spatio-temporal coherence during video generation. The DropletVideo dataset and model are accessible at https://dropletx.github.io.",
        "arxiv_id": "2503.06053",
        "ARXIVID": "2503.06053",
        "COMMENT": "Matches criterion 3 as it introduces a new dataset (DropletVideo-10M) and explores spatio-temporal consistency in video generation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.06553": {
        "authors": [
            "Jiaxin Ai",
            "Pengfei Zhou",
            "Zhaopan Xu",
            "Ming Li",
            "Fanrui Zhang",
            "Zizhen Li",
            "Jianwen Sun",
            "Yukang Feng",
            "Baojin Huang",
            "Zhongyuan Wang",
            "Kaipeng Zhang"
        ],
        "title": "ProJudge: A Multi-Modal Multi-Discipline Benchmark and Instruction-Tuning Dataset for MLLM-based Process Judges",
        "abstract": "arXiv:2503.06553v1 Announce Type: new  Abstract: As multi-modal large language models (MLLMs) frequently exhibit errors when solving scientific problems, evaluating the validity of their reasoning processes is critical for ensuring reliability and uncovering fine-grained model weaknesses. Since human evaluation is laborious and costly, prompting MLLMs as automated process judges has become a common practice. However, the reliability of these model-based judges remains uncertain. To address this, we introduce ProJudgeBench, the first comprehensive benchmark specifically designed for evaluating abilities of MLLM-based process judges. ProJudgeBench comprises 2,400 test cases and 50,118 step-level labels, spanning four scientific disciplines with diverse difficulty levels and multi-modal content. In ProJudgeBench, each step is meticulously annotated by human experts for correctness, error type, and explanation, enabling a systematic evaluation of judges' capabilities to detect, classify and diagnose errors. Evaluation on ProJudgeBench reveals a significant performance gap between open-source and proprietary models. To bridge this gap, we further propose ProJudge-173k, a large-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning strategy that encourages models to explicitly reason through problem-solving before assessing solutions. Both contributions significantly enhance the process evaluation capabilities of open-source models. All the resources will be released to foster future research of reliable multi-modal process evaluation.",
        "arxiv_id": "2503.06553",
        "ARXIVID": "2503.06553",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark and dataset for evaluating MLLM-based process judges.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.07125": {
        "authors": [
            "Sihao Lin",
            "Daqi Liu",
            "Ruochong Fu",
            "Dongrui Liu",
            "Andy Song",
            "Hongwei Xie",
            "Zhihui Li",
            "Bing Wang",
            "Xiaojun Chang"
        ],
        "title": "Learning A Zero-shot Occupancy Network from Vision Foundation Models via Self-supervised Adaptation",
        "abstract": "arXiv:2503.07125v1 Announce Type: new  Abstract: Estimating the 3D world from 2D monocular images is a fundamental yet challenging task due to the labour-intensive nature of 3D annotations. To simplify label acquisition, this work proposes a novel approach that bridges 2D vision foundation models (VFMs) with 3D tasks by decoupling 3D supervision into an ensemble of image-level primitives, e.g., semantic and geometric components. As a key motivator, we leverage the zero-shot capabilities of vision-language models for image semantics. However, due to the notorious ill-posed problem - multiple distinct 3D scenes can produce identical 2D projections, directly inferring metric depth from a monocular image in a zero-shot manner is unsuitable. In contrast, 2D VFMs provide promising sources of relative depth, which theoretically aligns with metric depth when properly scaled and offset. Thus, we adapt the relative depth derived from VFMs into metric depth by optimising the scale and offset using temporal consistency, also known as novel view synthesis, without access to ground-truth metric depth. Consequently, we project the semantics into 3D space using the reconstructed metric depth, thereby providing 3D supervision. Extensive experiments on nuScenes and SemanticKITTI demonstrate the effectiveness of our framework. For instance, the proposed method surpasses the current state-of-the-art by 3.34% mIoU on nuScenes for voxel occupancy prediction.",
        "arxiv_id": "2503.07125",
        "ARXIVID": "2503.07125",
        "COMMENT": "Matches criterion 4 as it leverages vision foundation models for 3D occupancy network estimation, showcasing an innovative application.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2503.06089": {
        "authors": [
            "David C. Jeong",
            "Aditya Puranik",
            "James Vong",
            "Vrushabh Abhijit Deogirikar",
            "Ryan Fell",
            "Julianna Dietrich",
            "Maria Kyrarini",
            "Christopher Kitts"
        ],
        "title": "Fish2Mesh Transformer: 3D Human Mesh Recovery from Egocentric Vision",
        "abstract": "arXiv:2503.06089v1 Announce Type: new  Abstract: Egocentric human body estimation allows for the inference of user body pose and shape from a wearable camera's first-person perspective. Although research has used pose estimation techniques to overcome self-occlusions and image distortions caused by head-mounted fisheye images, similar advances in 3D human mesh recovery (HMR) techniques have been limited. We introduce Fish2Mesh, a fisheye-aware transformer-based model designed for 3D egocentric human mesh recovery. We propose an egocentric position embedding block to generate an ego-specific position table for the Swin Transformer to reduce fisheye image distortion. Our model utilizes multi-task heads for SMPL parametric regression and camera translations, estimating 3D and 2D joints as auxiliary loss to support model training. To address the scarcity of egocentric camera data, we create a training dataset by employing the pre-trained 4D-Human model and third-person cameras for weak supervision. Our experiments demonstrate that Fish2Mesh outperforms previous state-of-the-art 3D HMR models.",
        "arxiv_id": "2503.06089",
        "ARXIVID": "2503.06089",
        "COMMENT": "Matches criterion 1 as it introduces a novel transformer-based model for spatial understanding in egocentric vision, which is relevant to embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.06515": {
        "authors": [
            "Jing Zhang",
            "Zhikai Li",
            "Qingyi Gu"
        ],
        "title": "SAQ-SAM: Semantically-Aligned Quantization for Segment Anything Model",
        "abstract": "arXiv:2503.06515v1 Announce Type: new  Abstract: Segment Anything Model (SAM) exhibits remarkable zero-shot segmentation capability; however, its prohibitive computational costs make edge deployment challenging. Although post-training quantization (PTQ) offers a promising compression solution, existing methods yield unsatisfactory results when applied to SAM, owing to its specialized model components and promptable workflow: (i) The mask decoder's attention exhibits extreme outliers, and we find that aggressive clipping (ranging down to even 100$\\times$), instead of smoothing or isolation, is effective in suppressing outliers while maintaining semantic capabilities. Unfortunately, traditional metrics (e.g., MSE) fail to provide such large-scale clipping. (ii) Existing reconstruction methods potentially neglect prompts' intention, resulting in distorted visual encodings during prompt interactions. To address the above issues, we propose SAQ-SAM in this paper, which boosts PTQ of SAM with semantic alignment. Specifically, we propose Perceptual-Consistency Clipping, which exploits attention focus overlap as clipping metric, to significantly suppress outliers. Furthermore, we propose Prompt-Aware Reconstruction, which incorporates visual-prompt interactions by leveraging cross-attention responses in mask decoder, thus facilitating alignment in both distribution and semantics. To ensure the interaction efficiency, we also introduce a layer-skipping strategy for visual tokens. Extensive experiments are conducted on different segmentation tasks and SAMs of various sizes, and the results show that the proposed SAQ-SAM consistently outperforms baselines. For example, when quantizing SAM-B to 4-bit, our method achieves 11.7% higher mAP than the baseline in instance segmentation task.",
        "arxiv_id": "2503.06515",
        "ARXIVID": "2503.06515",
        "COMMENT": "Matches criterion 4 as it focuses on improving the efficiency of the Segment Anything Model (SAM), a vision foundation model, through quantization techniques.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.07275": {
        "authors": [
            "Won-Sang You",
            "Tae-Gwan Ha",
            "Seo-Young Lee",
            "Kyung-Joong Kim"
        ],
        "title": "Automatic Curriculum Design for Zero-Shot Human-AI Coordination",
        "abstract": "arXiv:2503.07275v1 Announce Type: new  Abstract: Zero-shot human-AI coordination is the training of an ego-agent to coordinate with humans without using human data. Most studies on zero-shot human-AI coordination have focused on enhancing the ego-agent's coordination ability in a given environment without considering the issue of generalization to unseen environments. Real-world applications of zero-shot human-AI coordination should consider unpredictable environmental changes and the varying coordination ability of co-players depending on the environment. Previously, the multi-agent UED (Unsupervised Environment Design) approach has investigated these challenges by jointly considering environmental changes and co-player policy in competitive two-player AI-AI scenarios. In this paper, our study extends the multi-agent UED approach to a zero-shot human-AI coordination. We propose a utility function and co-player sampling for a zero-shot human-AI coordination setting that helps train the ego-agent to coordinate with humans more effectively than the previous multi-agent UED approach. The zero-shot human-AI coordination performance was evaluated in the Overcooked-AI environment, using human proxy agents and real humans. Our method outperforms other baseline models and achieves a high human-AI coordination performance in unseen environments.",
        "arxiv_id": "2503.07275",
        "ARXIVID": "2503.07275",
        "COMMENT": "Matches criterion 3 as it focuses on a novel approach to zero-shot human-AI coordination in unseen environments, which is relevant to embodied AI benchmarks and methods.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.07033": {
        "authors": [
            "Haolong Ma",
            "Hui Li",
            "Chunyang Cheng",
            "Zeyang Zhang",
            "Xiaoning Song",
            "Xiao-Jun Wu"
        ],
        "title": "Learning a Unified Degradation-aware Representation Model for Multi-modal Image Fusion",
        "abstract": "arXiv:2503.07033v1 Announce Type: new  Abstract: All-in-One Degradation-Aware Fusion Models (ADFMs), a class of multi-modal image fusion models, address complex scenes by mitigating degradations from source images and generating high-quality fused images. Mainstream ADFMs often rely on highly synthetic multi-modal multi-quality images for supervision, limiting their effectiveness in cross-modal and rare degradation scenarios. The inherent relationship among these multi-modal, multi-quality images of the same scene provides explicit supervision for training, but also raises above problems. To address these limitations, we present LURE, a Learning-driven Unified Representation model for infrared and visible Image Fusion, which is degradation-aware. LURE decouples multi-modal multi-quality data at the data level and recouples this relationship in a unified latent feature space (ULFS) by proposing a novel unified loss. This decoupling circumvents data-level limitations of prior models and allows leveraging real-world restoration datasets for training high-quality degradation-aware models, sidestepping above issues. To enhance text-image interaction, we refine image-text interaction and residual structures via Text-Guided Attention (TGA) and an inner residual structure. These enhances text's spatial perception of images and preserve more visual details. Experiments show our method outperforms state-of-the-art (SOTA) methods across general fusion, degradation-aware fusion, and downstream tasks. The code will be publicly available.",
        "arxiv_id": "2503.07033",
        "ARXIVID": "2503.07033",
        "COMMENT": "Matches criterion 2 as it focuses on multi-modal image fusion with a novel degradation-aware representation model.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.06821": {
        "authors": [
            "Siyu Li",
            "Yihong Cao",
            "Hao Shi",
            "Yongsheng Zang",
            "Xuan He",
            "Kailun Yang",
            "Zhiyong Li"
        ],
        "title": "HierDAMap: Towards Universal Domain Adaptive BEV Mapping via Hierarchical Perspective Priors",
        "abstract": "arXiv:2503.06821v1 Announce Type: new  Abstract: The exploration of Bird's-Eye View (BEV) mapping technology has driven significant innovation in visual perception technology for autonomous driving. BEV mapping models need to be applied to the unlabeled real world, making the study of unsupervised domain adaptation models an essential path. However, research on unsupervised domain adaptation for BEV mapping remains limited and cannot perfectly accommodate all BEV mapping tasks. To address this gap, this paper proposes HierDAMap, a universal and holistic BEV domain adaptation framework with hierarchical perspective priors. Unlike existing research that solely focuses on image-level learning using prior knowledge, this paper explores the guiding role of perspective prior knowledge across three distinct levels: global, sparse, and instance levels. With these priors, HierDA consists of three essential components, including Semantic-Guided Pseudo Supervision (SGPS), Dynamic-Aware Coherence Learning (DACL), and Cross-Domain Frustum Mixing (CDFM). SGPS constrains the cross-domain consistency of perspective feature distribution through pseudo labels generated by vision foundation models in 2D space. To mitigate feature distribution discrepancies caused by spatial variations, DACL employs uncertainty-aware predicted depth as an intermediary to derive dynamic BEV labels from perspective pseudo-labels, thereby constraining the coarse BEV features derived from corresponding perspective features. CDFM, on the other hand, leverages perspective masks of view frustum to mix multi-view perspective images from both domains, which guides cross-domain view transformation and encoding learning through mixed BEV labels. The proposed method is verified on multiple BEV mapping tasks, such as BEV semantic segmentation, high-definition semantic, and vectorized mapping. The source code will be made publicly available at https://github.com/lynn-yu/HierDAMap.",
        "arxiv_id": "2503.06821",
        "ARXIVID": "2503.06821",
        "COMMENT": "Matches criterion 4 as it explores BEV mapping with vision foundation models and domain adaptation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.07598": {
        "authors": [
            "Zeyinzi Jiang",
            "Zhen Han",
            "Chaojie Mao",
            "Jingfeng Zhang",
            "Yulin Pan",
            "Yu Liu"
        ],
        "title": "VACE: All-in-One Video Creation and Editing",
        "abstract": "arXiv:2503.07598v1 Announce Type: new  Abstract: Diffusion Transformer has demonstrated powerful capability and scalability in generating high-quality images and videos. Further pursuing the unification of generation and editing tasks has yielded significant progress in the domain of image content creation. However, due to the intrinsic demands for consistency across both temporal and spatial dynamics, achieving a unified approach for video synthesis remains challenging. We introduce VACE, which enables users to perform Video tasks within an All-in-one framework for Creation and Editing. These tasks include reference-to-video generation, video-to-video editing, and masked video-to-video editing. Specifically, we effectively integrate the requirements of various tasks by organizing video task inputs, such as editing, reference, and masking, into a unified interface referred to as the Video Condition Unit (VCU). Furthermore, by utilizing a Context Adapter structure, we inject different task concepts into the model using formalized representations of temporal and spatial dimensions, allowing it to handle arbitrary video synthesis tasks flexibly. Extensive experiments demonstrate that the unified model of VACE achieves performance on par with task-specific models across various subtasks. Simultaneously, it enables diverse applications through versatile task combinations. Project page: https://ali-vilab.github.io/VACE-Page/.",
        "arxiv_id": "2503.07598",
        "ARXIVID": "2503.07598",
        "COMMENT": "Matches criterion 2 as it discusses a unified framework for video creation and editing using diffusion transformers.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.06163": {
        "authors": [
            "Haotong Yang",
            "Qingyuan Zheng",
            "Yunjian Gao",
            "Yongkun Yang",
            "Yangbo He",
            "Zhouchen Lin",
            "Muhan Zhang"
        ],
        "title": "VACT: A Video Automatic Causal Testing System and a Benchmark",
        "abstract": "arXiv:2503.06163v1 Announce Type: new  Abstract: With the rapid advancement of text-conditioned Video Generation Models (VGMs), the quality of generated videos has significantly improved, bringing these models closer to functioning as ``*world simulators*'' and making real-world-level video generation more accessible and cost-effective. However, the generated videos often contain factual inaccuracies and lack understanding of fundamental physical laws. While some previous studies have highlighted this issue in limited domains through manual analysis, a comprehensive solution has not yet been established, primarily due to the absence of a generalized, automated approach for modeling and assessing the causal reasoning of these models across diverse scenarios. To address this gap, we propose VACT: an **automated** framework for modeling, evaluating, and measuring the causal understanding of VGMs in real-world scenarios. By combining causal analysis techniques with a carefully designed large language model assistant, our system can assess the causal behavior of models in various contexts without human annotation, which offers strong generalization and scalability. Additionally, we introduce multi-level causal evaluation metrics to provide a detailed analysis of the causal performance of VGMs. As a demonstration, we use our framework to benchmark several prevailing VGMs, offering insight into their causal reasoning capabilities. Our work lays the foundation for systematically addressing the causal understanding deficiencies in VGMs and contributes to advancing their reliability and real-world applicability.",
        "arxiv_id": "2503.06163",
        "ARXIVID": "2503.06163",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark and evaluation framework for video generation models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.06660": {
        "authors": [
            "Yang Zou",
            "Zhaoshuai Qi",
            "Yating Liu",
            "Zihao Xu",
            "Weipeng Sun",
            "Weiyi Liu",
            "Xingyuan Li",
            "Jiaqi Yang",
            "Yanning Zhang"
        ],
        "title": "AxisPose: Model-Free Matching-Free Single-Shot 6D Object Pose Estimation via Axis Generation",
        "abstract": "arXiv:2503.06660v1 Announce Type: new  Abstract: Object pose estimation, which plays a vital role in robotics, augmented reality, and autonomous driving, has been of great interest in computer vision. Existing studies either require multi-stage pose regression or rely on 2D-3D feature matching. Though these approaches have shown promising results, they rely heavily on appearance information, requiring complex input (i.e., multi-view reference input, depth, or CAD models) and intricate pipeline (i.e., feature extraction-SfM-2D to 3D matching-PnP). We propose AxisPose, a model-free, matching-free, single-shot solution for robust 6D pose estimation, which fundamentally diverges from the existing paradigm. Unlike existing methods that rely on 2D-3D or 2D-2D matching using 3D techniques, such as SfM and PnP, AxisPose directly infers a robust 6D pose from a single view by leveraging a diffusion model to learn the latent axis distribution of objects without reference views. Specifically, AxisPose constructs an Axis Generation Module (AGM) to capture the latent geometric distribution of object axes through a diffusion model. The diffusion process is guided by injecting the gradient of geometric consistency loss into the noise estimation to maintain the geometric consistency of the generated tri-axis. With the generated tri-axis projection, AxisPose further adopts a Triaxial Back-projection Module (TBM) to recover the 6D pose from the object tri-axis. The proposed AxisPose achieves robust performance at the cross-instance level (i.e., one model for N instances) using only a single view as input without reference images, with great potential for generalization to unseen-object level.",
        "arxiv_id": "2503.06660",
        "ARXIVID": "2503.06660",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for 6D object pose estimation, which is relevant to spatial understanding in embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.07365": {
        "authors": [
            "Fanqing Meng",
            "Lingxiao Du",
            "Zongkai Liu",
            "Zhixiang Zhou",
            "Quanfeng Lu",
            "Daocheng Fu",
            "Botian Shi",
            "Wenhai Wang",
            "Junjun He",
            "Kaipeng Zhang",
            "Ping Luo",
            "Yu Qiao",
            "Qiaosheng Zhang",
            "Wenqi Shao"
        ],
        "title": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning",
        "abstract": "arXiv:2503.07365v1 Announce Type: new  Abstract: We present MM-Eureka, a multimodal reasoning model that successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning. While rule-based RL has shown remarkable success in improving LLMs' reasoning abilities in text domains, its application to multimodal settings has remained challenging. Our work reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, including steady increases in accuracy reward and response length, and the emergence of reflection behaviors. We demonstrate that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at https://github.com/ModalMinds/MM-EUREKA",
        "arxiv_id": "2503.07365",
        "ARXIVID": "2503.07365",
        "COMMENT": "Matches criterion 2 as it extends large-scale reinforcement learning to multimodal reasoning, which involves vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.07587": {
        "authors": [
            "Dunant Cusipuma",
            "David Ortega",
            "Victor Flores-Benites",
            "Arturo Deza"
        ],
        "title": "Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution Autonomous Driving VQA from Peru",
        "abstract": "arXiv:2503.07587v1 Announce Type: new  Abstract: As multimodal foundational models start being deployed experimentally in Self-Driving cars, a reasonable question we ask ourselves is how similar to humans do these systems respond in certain driving situations -- especially those that are out-of-distribution? To study this, we create the Robusto-1 dataset that uses dashcam video data from Peru, a country with one of the worst (aggressive) drivers in the world, a high traffic index, and a high ratio of bizarre to non-bizarre street objects likely never seen in training. In particular, to preliminarly test at a cognitive level how well Foundational Visual Language Models (VLMs) compare to Humans in Driving, we move away from bounding boxes, segmentation maps, occupancy maps or trajectory estimation to multi-modal Visual Question Answering (VQA) comparing both humans and machines through a popular method in systems neuroscience known as Representational Similarity Analysis (RSA). Depending on the type of questions we ask and the answers these systems give, we will show in what cases do VLMs and Humans converge or diverge allowing us to probe on their cognitive alignment. We find that the degree of alignment varies significantly depending on the type of questions asked to each type of system (Humans vs VLMs), highlighting a gap in their alignment.",
        "arxiv_id": "2503.07587",
        "ARXIVID": "2503.07587",
        "COMMENT": "Matches criterion 2 as it evaluates foundational visual language models (VLMs) in a novel out-of-distribution dataset for autonomous driving.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.06955": {
        "authors": [
            "Zeyu Zhang",
            "Yiran Wang",
            "Wei Mao",
            "Danning Li",
            "Rui Zhao",
            "Biao Wu",
            "Zirui Song",
            "Bohan Zhuang",
            "Ian Reid",
            "Richard Hartley"
        ],
        "title": "Motion Anything: Any to Motion Generation",
        "abstract": "arXiv:2503.06955v1 Announce Type: new  Abstract: Conditional motion generation has been extensively studied in computer vision, yet two critical challenges remain. First, while masked autoregressive methods have recently outperformed diffusion-based approaches, existing masking models lack a mechanism to prioritize dynamic frames and body parts based on given conditions. Second, existing methods for different conditioning modalities often fail to integrate multiple modalities effectively, limiting control and coherence in generated motion. To address these challenges, we propose Motion Anything, a multimodal motion generation framework that introduces an Attention-based Mask Modeling approach, enabling fine-grained spatial and temporal control over key frames and actions. Our model adaptively encodes multimodal conditions, including text and music, improving controllability. Additionally, we introduce Text-Motion-Dance (TMD), a new motion dataset consisting of 2,153 pairs of text, music, and dance, making it twice the size of AIST++, thereby filling a critical gap in the community. Extensive experiments demonstrate that Motion Anything surpasses state-of-the-art methods across multiple benchmarks, achieving a 15% improvement in FID on HumanML3D and showing consistent performance gains on AIST++ and TMD. See our project website https://steve-zeyu-zhang.github.io/MotionAnything",
        "arxiv_id": "2503.06955",
        "ARXIVID": "2503.06955",
        "COMMENT": "Matches criterion 2 as it introduces a multimodal motion generation framework with novel mechanisms for text and music conditioning.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.06252": {
        "authors": [
            "Kun Xiang",
            "Zhili Liu",
            "Zihao Jiang",
            "Yunshuang Nie",
            "Kaixin Cai",
            "Yiyang Yin",
            "Runhui Huang",
            "Haoxiang Fan",
            "Hanhui Li",
            "Weiran Huang",
            "Yihan Zeng",
            "Yu-Jie Yuan",
            "Jianhua Han",
            "Lanqing Hong",
            "Hang Xu",
            "Xiaodan Liang"
        ],
        "title": "Can Atomic Step Decomposition Enhance the Self-structured Reasoning of Multimodal Large Models?",
        "abstract": "arXiv:2503.06252v1 Announce Type: new  Abstract: In this paper, we address the challenging task of multimodal mathematical reasoning by incorporating the ability of \"slow thinking\" into multimodal large language models (MLLMs). Our core idea is that different levels of reasoning abilities can be combined dynamically to tackle questions with different complexity. To this end, we propose a paradigm of Self-structured Chain of Thought (SCoT), which is composed of minimal semantic atomic steps. Different from existing methods that rely on structured templates or free-form paradigms, our method can not only generate cognitive CoT structures for various complex tasks but also mitigates the phenomenon of overthinking. To introduce structured reasoning capabilities into visual understanding models, we further design a novel AtomThink framework with four key modules, including (i) a data engine to generate high-quality multimodal reasoning paths; (ii) a supervised fine-tuning process with serialized inference data; (iii) a policy-guided multi-turn inference method; and (iv) an atomic capability metric to evaluate the single step utilization rate. We conduct extensive experiments to show that the proposed AtomThink significantly improves the performance of baseline MLLMs, achieving more than 10\\% average accuracy gains on MathVista and MathVerse. Compared to state-of-the-art structured CoT approaches, our method not only achieves higher accuracy but also improves data utilization by 5 times and boosts inference efficiency by 85.3\\%. Our code is now public available in https://github.com/Quinn777/AtomThink.",
        "arxiv_id": "2503.06252",
        "ARXIVID": "2503.06252",
        "COMMENT": "Matches criterion 2 as it introduces a novel multimodal large language model (MLLM) framework for reasoning tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.06071": {
        "authors": [
            "Hangyu Du",
            "Chee-Meng Chew"
        ],
        "title": "TransParking: A Dual-Decoder Transformer Framework with Soft Localization for End-to-End Automatic Parking",
        "abstract": "arXiv:2503.06071v1 Announce Type: new  Abstract: In recent years, fully differentiable end-to-end autonomous driving systems have become a research hotspot in the field of intelligent transportation. Among various research directions, automatic parking is particularly critical as it aims to enable precise vehicle parking in complex environments. In this paper, we present a purely vision-based transformer model for end-to-end automatic parking, trained using expert trajectories. Given camera-captured data as input, the proposed model directly outputs future trajectory coordinates. Experimental results demonstrate that the various errors of our model have decreased by approximately 50% in comparison with the current state-of-the-art end-to-end trajectory prediction algorithm of the same type. Our approach thus provides an effective solution for fully differentiable automatic parking.",
        "arxiv_id": "2503.06071",
        "ARXIVID": "2503.06071",
        "COMMENT": "Matches criterion 3 as it focuses on a novel method for embodied AI in the context of automatic parking.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.07597": {
        "authors": [
            "Yuhong Zhang",
            "Guanlin Wu",
            "Ling-Hao Chen",
            "Zhuokai Zhao",
            "Jing Lin",
            "Xiaoke Jiang",
            "Jiamin Wu",
            "Zhuoheng Li",
            "Hao Frank Yang",
            "Haoqian Wang",
            "Lei Zhang"
        ],
        "title": "HumanMM: Global Human Motion Recovery from Multi-shot Videos",
        "abstract": "arXiv:2503.07597v1 Announce Type: new  Abstract: In this paper, we present a novel framework designed to reconstruct long-sequence 3D human motion in the world coordinates from in-the-wild videos with multiple shot transitions. Such long-sequence in-the-wild motions are highly valuable to applications such as motion generation and motion understanding, but are of great challenge to be recovered due to abrupt shot transitions, partial occlusions, and dynamic backgrounds presented in such videos. Existing methods primarily focus on single-shot videos, where continuity is maintained within a single camera view, or simplify multi-shot alignment in camera space only. In this work, we tackle the challenges by integrating an enhanced camera pose estimation with Human Motion Recovery (HMR) by incorporating a shot transition detector and a robust alignment module for accurate pose and orientation continuity across shots. By leveraging a custom motion integrator, we effectively mitigate the problem of foot sliding and ensure temporal consistency in human pose. Extensive evaluations on our created multi-shot dataset from public 3D human datasets demonstrate the robustness of our method in reconstructing realistic human motion in world coordinates.",
        "arxiv_id": "2503.07597",
        "ARXIVID": "2503.07597",
        "COMMENT": "Matches criterion 1 as it focuses on spatial understanding and motion recovery in embodied agents.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.06974": {
        "authors": [
            "Yang Liu",
            "Mengyuan Liu",
            "Shudong Huang",
            "Jiancheng Lv"
        ],
        "title": "Asymmetric Visual Semantic Embedding Framework for Efficient Vision-Language Alignment",
        "abstract": "arXiv:2503.06974v1 Announce Type: new  Abstract: Learning visual semantic similarity is a critical challenge in bridging the gap between images and texts. However, there exist inherent variations between vision and language data, such as information density, i.e., images can contain textual information from multiple different views, which makes it difficult to compute the similarity between these two modalities accurately and efficiently. In this paper, we propose a novel framework called Asymmetric Visual Semantic Embedding (AVSE) to dynamically select features from various regions of images tailored to different textual inputs for similarity calculation. To capture information from different views in the image, we design a radial bias sampling module to sample image patches and obtain image features from various views, Furthermore, AVSE introduces a novel module for efficient computation of visual semantic similarity between asymmetric image and text embeddings. Central to this module is the presumption of foundational semantic units within the embeddings, denoted as ``meta-semantic embeddings.\" It segments all embeddings into meta-semantic embeddings with the same dimension and calculates visual semantic similarity by finding the optimal match of meta-semantic embeddings of two modalities. Our proposed AVSE model is extensively evaluated on the large-scale MS-COCO and Flickr30K datasets, demonstrating its superiority over recent state-of-the-art methods.",
        "arxiv_id": "2503.06974",
        "ARXIVID": "2503.06974",
        "COMMENT": "Matches criterion 2 as it proposes a novel framework for vision-language alignment, which is relevant to multi-modal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.06117": {
        "authors": [
            "Hongjia Zhai",
            "Boming Zhao",
            "Hai Li",
            "Xiaokun Pan",
            "Yijia He",
            "Zhaopeng Cui",
            "Hujun Bao",
            "Guofeng Zhang"
        ],
        "title": "NeuraLoc: Visual Localization in Neural Implicit Map with Dual Complementary Features",
        "abstract": "arXiv:2503.06117v1 Announce Type: new  Abstract: Recently, neural radiance fields (NeRF) have gained significant attention in the field of visual localization. However, existing NeRF-based approaches either lack geometric constraints or require extensive storage for feature matching, limiting their practical applications. To address these challenges, we propose an efficient and novel visual localization approach based on the neural implicit map with complementary features. Specifically, to enforce geometric constraints and reduce storage requirements, we implicitly learn a 3D keypoint descriptor field, avoiding the need to explicitly store point-wise features. To further address the semantic ambiguity of descriptors, we introduce additional semantic contextual feature fields, which enhance the quality and reliability of 2D-3D correspondences. Besides, we propose descriptor similarity distribution alignment to minimize the domain gap between 2D and 3D feature spaces during matching. Finally, we construct the matching graph using both complementary descriptors and contextual features to establish accurate 2D-3D correspondences for 6-DoF pose estimation. Compared with the recent NeRF-based approaches, our method achieves a 3$\\times$ faster training speed and a 45$\\times$ reduction in model storage. Extensive experiments on two widely used datasets demonstrate that our approach outperforms or is highly competitive with other state-of-the-art NeRF-based visual localization methods. Project page: \\href{https://zju3dv.github.io/neuraloc}{https://zju3dv.github.io/neuraloc}",
        "arxiv_id": "2503.06117",
        "ARXIVID": "2503.06117",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for visual localization in neural implicit maps, focusing on efficiency and complementary features.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.06885": {
        "authors": [
            "Yan Yang",
            "Dongxu Li",
            "Haoning Wu",
            "Bei Chen",
            "Liu Liu",
            "Liyuan Pan",
            "Junnan Li"
        ],
        "title": "ProBench: Judging Multimodal Foundation Models on Open-ended Multi-domain Expert Tasks",
        "abstract": "arXiv:2503.06885v1 Announce Type: new  Abstract: Solving expert-level multimodal tasks is a key milestone towards general intelligence. As the capabilities of multimodal large language models (MLLMs) continue to improve, evaluation of such advanced multimodal intelligence becomes necessary yet challenging. In this work, we introduce ProBench, a benchmark of open-ended user queries that require professional expertise and advanced reasoning. ProBench consists of 4,000 high-quality samples independently submitted by professionals based on their daily productivity demands. It spans across 10 fields and 56 sub-fields, including science, arts, humanities, coding, mathematics, and creative writing. Experimentally, we evaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal that although the best open-source models rival the proprietary ones, ProBench presents significant challenges in visual perception, textual understanding, domain knowledge and advanced reasoning, thus providing valuable directions for future multimodal AI research efforts.",
        "arxiv_id": "2503.06885",
        "ARXIVID": "2503.06885",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (ProBench) for evaluating multimodal foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.07591": {
        "authors": [
            "Bardia Safaei",
            "Faizan Siddiqui",
            "Jiacong Xu",
            "Vishal M. Patel",
            "Shao-Yuan Lo"
        ],
        "title": "Filter Images First, Generate Instructions Later: Pre-Instruction Data Selection for Visual Instruction Tuning",
        "abstract": "arXiv:2503.07591v1 Announce Type: new  Abstract: Visual instruction tuning (VIT) for large vision-language models (LVLMs) requires training on expansive datasets of image-instruction pairs, which can be costly. Recent efforts in VIT data selection aim to select a small subset of high-quality image-instruction pairs, reducing VIT runtime while maintaining performance comparable to full-scale training. However, a major challenge often overlooked is that generating instructions from unlabeled images for VIT is highly expensive. Most existing VIT datasets rely heavily on human annotations or paid services like the GPT API, which limits users with constrained resources from creating VIT datasets for custom applications. To address this, we introduce Pre-Instruction Data Selection (PreSel), a more practical data selection paradigm that directly selects the most beneficial unlabeled images and generates instructions only for the selected images. PreSel first estimates the relative importance of each vision task within VIT datasets to derive task-wise sampling budgets. It then clusters image features within each task, selecting the most representative images with the budget. This approach reduces computational overhead for both instruction generation during VIT data formation and LVLM fine-tuning. By generating instructions for only 15% of the images, PreSel achieves performance comparable to full-data VIT on the LLaVA-1.5 and Vision-Flan datasets. The link to our project page: https://bardisafa.github.io/PreSel",
        "arxiv_id": "2503.07591",
        "ARXIVID": "2503.07591",
        "COMMENT": "Matches criterion 2 as it discusses visual instruction tuning for large vision-language models (LVLMs).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.07098": {
        "authors": [
            "Ding Zhong",
            "Xu Zheng",
            "Chenfei Liao",
            "Yuanhuiyi Lyu",
            "Jialei Chen",
            "Shengyang Wu",
            "Linfeng Zhang",
            "Xuming Hu"
        ],
        "title": "OmniSAM: Omnidirectional Segment Anything Model for UDA in Panoramic Semantic Segmentation",
        "abstract": "arXiv:2503.07098v1 Announce Type: new  Abstract: Segment Anything Model 2 (SAM2) has emerged as a strong base model in various pinhole imaging segmentation tasks. However, when applying it to $360^\\circ$ domain, the significant field-of-view (FoV) gap between pinhole ($70^\\circ \\times 70^\\circ$) and panoramic images ($180^\\circ \\times 360^\\circ$) poses unique challenges. Two major concerns for this application includes 1) inevitable distortion and object deformation brought by the large FoV disparity between domains; 2) the lack of pixel-level semantic understanding that the original SAM2 cannot provide. To address these issues, we propose a novel OmniSAM framework, which makes the first attempt to apply SAM2 for panoramic semantic segmentation. Specifically, to bridge the first gap, OmniSAM first divides the panorama into sequences of patches. These patches are then treated as image sequences in similar manners as in video segmentation tasks. We then leverage the SAM2's memory mechanism to extract cross-patch correspondences that embeds the cross-FoV dependencies, improving feature continuity and the prediction consistency along mask boundaries. For the second gap, OmniSAM fine-tunes the pretrained image encoder and reutilize the mask decoder for semantic prediction. An FoV-based prototypical adaptation module with dynamic pseudo label update mechanism is also introduced to facilitate the alignment of memory and backbone features, thereby improving model generalization ability across different sizes of source models. Extensive experimental results demonstrate that OmniSAM outperforms the state-of-the-art methods by large margins, e.g., 79.06% (+10.22%) on SPin8-to-SPan8, 62.46% (+6.58%) on CS13-to-DP13.",
        "arxiv_id": "2503.07098",
        "ARXIVID": "2503.07098",
        "COMMENT": "Matches criterion 4 as it extends the Segment Anything Model (SAM) for panoramic semantic segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.07588": {
        "authors": [
            "Junwei Luo",
            "Yingying Zhang",
            "Xue Yang",
            "Kang Wu",
            "Qi Zhu",
            "Lei Liang",
            "Jingdong Chen",
            "Yansheng Li"
        ],
        "title": "When Large Vision-Language Model Meets Large Remote Sensing Imagery: Coarse-to-Fine Text-Guided Token Pruning",
        "abstract": "arXiv:2503.07588v1 Announce Type: new  Abstract: Efficient vision-language understanding of large Remote Sensing Images (RSIs) is meaningful but challenging. Current Large Vision-Language Models (LVLMs) typically employ limited pre-defined grids to process images, leading to information loss when handling gigapixel RSIs. Conversely, using unlimited grids significantly increases computational costs. To preserve image details while reducing computational complexity, we propose a text-guided token pruning method with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i) a Region Focus Module (RFM) that leverages text-aware region localization capability to identify critical vision tokens, and (ii) a coarse-to-fine image tile selection and vision token pruning strategy based on DIP, which is guided by RFM outputs and avoids directly processing the entire large imagery. Additionally, existing benchmarks for evaluating LVLMs' perception ability on large RSI suffer from limited question diversity and constrained image sizes. We construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs across 8 categories, with image length up to 27,328 pixels. Our method outperforms existing high-resolution strategies on four datasets using the same data. Moreover, compared to existing token reduction methods, our approach demonstrates higher efficiency under high-resolution settings. Dataset and code are in https://github.com/VisionXLab/LRS-VQA.",
        "arxiv_id": "2503.07588",
        "ARXIVID": "2503.07588",
        "COMMENT": "Matches criterion 2 as it discusses a new method for improving large vision-language models (LVLMs) for remote sensing imagery.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.07204": {
        "authors": [
            "Mona Sheikh Zeinoddin",
            "Mobarakol Islam",
            "Zafer Tandogdu",
            "Greg Shaw",
            "Mathew J. Clarkson",
            "Evangelos Mazomenos",
            "Danail Stoyanov"
        ],
        "title": "Endo-FASt3r: Endoscopic Foundation model Adaptation for Structure from motion",
        "abstract": "arXiv:2503.07204v1 Announce Type: new  Abstract: Accurate depth and camera pose estimation is essential for achieving high-quality 3D visualisations in robotic-assisted surgery. Despite recent advancements in foundation model adaptation to monocular depth estimation of endoscopic scenes via self-supervised learning (SSL), no prior work has explored their use for pose estimation. These methods rely on low rank-based adaptation approaches, which constrain model updates to a low-rank space. We propose Endo-FASt3r, the first monocular SSL depth and pose estimation framework that uses foundation models for both tasks. We extend the Reloc3r relative pose estimation foundation model by designing Reloc3rX, introducing modifications necessary for convergence in SSL. We also present DoMoRA, a novel adaptation technique that enables higher-rank updates and faster convergence. Experiments on the SCARED dataset show that Endo-FASt3r achieves a substantial $10\\%$ improvement in pose estimation and a $2\\%$ improvement in depth estimation over prior work. Similar performance gains on the Hamlyn and StereoMIS datasets reinforce the generalisability of Endo-FASt3r across different datasets.",
        "arxiv_id": "2503.07204",
        "ARXIVID": "2503.07204",
        "COMMENT": "Matches criterion 4 as it discusses a foundation model adaptation for depth and pose estimation in endoscopic scenes.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.07603": {
        "authors": [
            "Sedrick Keh",
            "Jean Mercat",
            "Samir Yitzhak Gadre",
            "Kushal Arora",
            "Igor Vasiljevic",
            "Benjamin Burchfiel",
            "Shuran Song",
            "Russ Tedrake",
            "Thomas Kollar",
            "Ludwig Schmidt",
            "Achal Dave"
        ],
        "title": "Should VLMs be Pre-trained with Image Data?",
        "abstract": "arXiv:2503.07603v1 Announce Type: new  Abstract: Pre-trained LLMs that are further trained with image data perform well on vision-language tasks. While adding images during a second training phase effectively unlocks this capability, it is unclear how much of a gain or loss this two-step pipeline gives over VLMs which integrate images earlier into the training process. To investigate this, we train models spanning various datasets, scales, image-text ratios, and amount of pre-training done before introducing vision tokens. We then fine-tune these models and evaluate their downstream performance on a suite of vision-language and text-only tasks. We find that pre-training with a mixture of image and text data allows models to perform better on vision-language tasks while maintaining strong performance on text-only evaluations. On an average of 6 diverse tasks, we find that for a 1B model, introducing visual tokens 80% of the way through pre-training results in a 2% average improvement over introducing visual tokens to a fully pre-trained model.",
        "arxiv_id": "2503.07603",
        "ARXIVID": "2503.07603",
        "COMMENT": "Matches criterion 2 as it investigates pre-training strategies for vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.06820": {
        "authors": [
            "Wei Dai",
            "Alan Luo",
            "Zane Durante",
            "Debadutta Dash",
            "Arnold Milstein",
            "Kevin Schulman",
            "Ehsan Adeli",
            "Li Fei-Fei"
        ],
        "title": "Towards Fine-Grained Video Question Answering",
        "abstract": "arXiv:2503.06820v1 Announce Type: new  Abstract: In the rapidly evolving domain of video understanding, Video Question Answering (VideoQA) remains a focal point. However, existing datasets exhibit gaps in temporal and spatial granularity, which consequently limits the capabilities of existing VideoQA methods. This paper introduces the Multi-Object Multi-Actor Question Answering (MOMA-QA) dataset, which is designed to address these shortcomings by emphasizing temporal localization, spatial relationship reasoning, and entity-centric queries. With ground truth scene graphs and temporal interval annotations, MOMA-QA is ideal for developing models for fine-grained video understanding. Furthermore, we present a novel video-language model, SGVLM, which incorporates a scene graph predictor, an efficient frame retriever, and a pre-trained large language model for temporal localization and fine-grained relationship understanding. Evaluations on MOMA-QA and other public datasets demonstrate the superior performance of our model, setting new benchmarks for VideoQA.",
        "arxiv_id": "2503.06820",
        "ARXIVID": "2503.06820",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (MOMA-QA) for fine-grained video question answering.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.07234": {
        "authors": [
            "Haicheng Liao",
            "Hanlin Kong",
            "Bonan Wang",
            "Chengyue Wang",
            "Wang Ye",
            "Zhengbing He",
            "Chengzhong Xu",
            "Zhenning Li"
        ],
        "title": "CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs and Chain-of-Thought Prompting",
        "abstract": "arXiv:2503.07234v1 Announce Type: new  Abstract: Accurate motion forecasting is crucial for safe autonomous driving (AD). This study proposes CoT-Drive, a novel approach that enhances motion forecasting by leveraging large language models (LLMs) and a chain-of-thought (CoT) prompting method. We introduce a teacher-student knowledge distillation strategy to effectively transfer LLMs' advanced scene understanding capabilities to lightweight language models (LMs), ensuring that CoT-Drive operates in real-time on edge devices while maintaining comprehensive scene understanding and generalization capabilities. By leveraging CoT prompting techniques for LLMs without additional training, CoT-Drive generates semantic annotations that significantly improve the understanding of complex traffic environments, thereby boosting the accuracy and robustness of predictions. Additionally, we present two new scene description datasets, Highway-Text and Urban-Text, designed for fine-tuning lightweight LMs to generate context-specific semantic annotations. Comprehensive evaluations of five real-world datasets demonstrate that CoT-Drive outperforms existing models, highlighting its effectiveness and efficiency in handling complex traffic scenarios. Overall, this study is the first to consider the practical application of LLMs in this field. It pioneers the training and use of a lightweight LLM surrogate for motion forecasting, setting a new benchmark and showcasing the potential of integrating LLMs into AD systems.",
        "arxiv_id": "2503.07234",
        "ARXIVID": "2503.07234",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark and method for motion forecasting in autonomous driving.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.06542": {
        "authors": [
            "Jianwen Sun",
            "Yukang Feng",
            "Chuanhao Li",
            "Fanrui Zhang",
            "Zizhen Li",
            "Jiaxin Ai",
            "Sizhuo Zhou",
            "Yu Dai",
            "Shenglin Zhang",
            "Kaipeng Zhang"
        ],
        "title": "ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model with Interleaved Multimodal Generation via Asymmetric Synergy",
        "abstract": "arXiv:2503.06542v1 Announce Type: new  Abstract: Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, a resource-efficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large language models (MLLMs). Specifically, ARMOR extends existing MLLMs from three perspectives: (1) For model architecture, an asymmetric encoder-decoder architecture with a forward-switching mechanism is introduced to unify embedding space integrating textual and visual modalities for enabling natural text-image interleaved generation with minimal computational overhead. (2) For training data, a meticulously curated, high-quality interleaved dataset is collected for fine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how to generate\" algorithm to empower existing MLLMs with multimodal generation capabilities while preserving their multimodal understanding capabilities, through three progressive training stages based on the collected dataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources. Our code will be released soon at https://armor.github.io.",
        "arxiv_id": "2503.06542",
        "ARXIVID": "2503.06542",
        "COMMENT": "Matches criterion 2 as it introduces a new multimodal large language model (MLLM) framework for understanding and generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.06903": {
        "authors": [
            "Hanqing Liu",
            "Shouwei Ruan",
            "Yao Huang",
            "Shiji Zhao",
            "Xingxing Wei"
        ],
        "title": "When Lighting Deceives: Exposing Vision-Language Models' Illumination Vulnerability Through Illumination Transformation Attack",
        "abstract": "arXiv:2503.06903v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have achieved remarkable success in various tasks, yet their robustness to real-world illumination variations remains largely unexplored. To bridge this gap, we propose \\textbf{I}llumination \\textbf{T}ransformation \\textbf{A}ttack (\\textbf{ITA}), the first framework to systematically assess VLMs' robustness against illumination changes. However, there still exist two key challenges: (1) how to model global illumination with fine-grained control to achieve diverse lighting conditions and (2) how to ensure adversarial effectiveness while maintaining naturalness. To address the first challenge, we innovatively decompose global illumination into multiple parameterized point light sources based on the illumination rendering equation. This design enables us to model more diverse lighting variations that previous methods could not capture. Then, by integrating these parameterized lighting variations with physics-based lighting reconstruction techniques, we could precisely render such light interactions in the original scenes, finally meeting the goal of fine-grained lighting control. For the second challenge, by controlling illumination through the lighting reconstrution model's latent space rather than direct pixel manipulation, we inherently preserve physical lighting priors. Furthermore, to prevent potential reconstruction artifacts, we design additional perceptual constraints for maintaining visual consistency with original images and diversity constraints for avoiding light source convergence.   Extensive experiments demonstrate that our ITA could significantly reduce the performance of advanced VLMs, e.g., LLaVA-1.6, while possessing competitive naturalness, exposing VLMS' critical illuminiation vulnerabilities.",
        "arxiv_id": "2503.06903",
        "ARXIVID": "2503.06903",
        "COMMENT": "Matches criterion 2 as it explores vulnerabilities in vision-language models (VLMs) under illumination changes.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.06435": {
        "authors": [
            "Adrian Chow",
            "Evelien Riddell",
            "Yimu Wang",
            "Sean Sedwards",
            "Krzysztof Czarnecki"
        ],
        "title": "OV-SCAN: Semantically Consistent Alignment for Novel Object Discovery in Open-Vocabulary 3D Object Detection",
        "abstract": "arXiv:2503.06435v1 Announce Type: new  Abstract: Open-vocabulary 3D object detection for autonomous driving aims to detect novel objects beyond the predefined training label sets in point cloud scenes. Existing approaches achieve this by connecting traditional 3D object detectors with vision-language models (VLMs) to regress 3D bounding boxes for novel objects and perform open-vocabulary classification through cross-modal alignment between 3D and 2D features. However, achieving robust cross-modal alignment remains a challenge due to semantic inconsistencies when generating corresponding 3D and 2D feature pairs. To overcome this challenge, we present OV-SCAN, an Open-Vocabulary 3D framework that enforces Semantically Consistent Alignment for Novel object discovery. OV-SCAN employs two core strategies: discovering precise 3D annotations and filtering out low-quality or corrupted alignment pairs (arising from 3D annotation, occlusion-induced, or resolution-induced noise). Extensive experiments on the nuScenes dataset demonstrate that OV-SCAN achieves state-of-the-art performance.",
        "arxiv_id": "2503.06435",
        "ARXIVID": "2503.06435",
        "COMMENT": "Matches criterion 2 as it discusses vision-language models (VLMs) and their application in open-vocabulary 3D object detection.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.07465": {
        "authors": [
            "Ao Wang",
            "Lihao Liu",
            "Hui Chen",
            "Zijia Lin",
            "Jungong Han",
            "Guiguang Ding"
        ],
        "title": "YOLOE: Real-Time Seeing Anything",
        "abstract": "arXiv:2503.07465v1 Announce Type: new  Abstract: Object detection and segmentation are widely employed in computer vision applications, yet conventional models like YOLO series, while efficient and accurate, are limited by predefined categories, hindering adaptability in open scenarios. Recent open-set methods leverage text prompts, visual cues, or prompt-free paradigm to overcome this, but often compromise between performance and efficiency due to high computational demands or deployment complexity. In this work, we introduce YOLOE, which integrates detection and segmentation across diverse open prompt mechanisms within a single highly efficient model, achieving real-time seeing anything. For text prompts, we propose Re-parameterizable Region-Text Alignment (RepRTA) strategy. It refines pretrained textual embeddings via a re-parameterizable lightweight auxiliary network and enhances visual-textual alignment with zero inference and transferring overhead. For visual prompts, we present Semantic-Activated Visual Prompt Encoder (SAVPE). It employs decoupled semantic and activation branches to bring improved visual embedding and accuracy with minimal complexity. For prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy. It utilizes a built-in large vocabulary and specialized embedding to identify all objects, avoiding costly language model dependency. Extensive experiments show YOLOE's exceptional zero-shot performance and transferability with high inference efficiency and low training cost. Notably, on LVIS, with 3$\\times$ less training cost and 1.4$\\times$ inference speedup, YOLOE-v8-S surpasses YOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6 AP$^b$ and 0.4 AP$^m$ gains over closed-set YOLOv8-L with nearly 4$\\times$ less training time. Code and models are available at https://github.com/THU-MIG/yoloe.",
        "arxiv_id": "2503.07465",
        "ARXIVID": "2503.07465",
        "COMMENT": "Matches criterion 4 as it introduces YOLOE, a real-time model for open-set object detection and segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.06446": {
        "authors": [
            "Mingxiang Cao",
            "Weiying Xie",
            "Xin Zhang",
            "Jiaqing Zhang",
            "Kai Jiang",
            "Jie Lei",
            "Yunsong Li"
        ],
        "title": "M$^3$amba: CLIP-driven Mamba Model for Multi-modal Remote Sensing Classification",
        "abstract": "arXiv:2503.06446v1 Announce Type: new  Abstract: Multi-modal fusion holds great promise for integrating information from different modalities. However, due to a lack of consideration for modal consistency, existing multi-modal fusion methods in the field of remote sensing still face challenges of incomplete semantic information and low computational efficiency in their fusion designs. Inspired by the observation that the visual language pre-training model CLIP can effectively extract strong semantic information from visual features, we propose M$^3$amba, a novel end-to-end CLIP-driven Mamba model for multi-modal fusion to address these challenges. Specifically, we introduce CLIP-driven modality-specific adapters in the fusion architecture to avoid the bias of understanding specific domains caused by direct inference, making the original CLIP encoder modality-specific perception. This unified framework enables minimal training to achieve a comprehensive semantic understanding of different modalities, thereby guiding cross-modal feature fusion. To further enhance the consistent association between modality mappings, a multi-modal Mamba fusion architecture with linear complexity and a cross-attention module Cross-SS2D are designed, which fully considers effective and efficient information interaction to achieve complete fusion. Extensive experiments have shown that M$^3$amba has an average performance improvement of at least 5.98\\% compared with the state-of-the-art methods in multi-modal hyperspectral image classification tasks in the remote sensing field, while also demonstrating excellent training efficiency, achieving a double improvement in accuracy and efficiency. The code is released at https://github.com/kaka-Cao/M3amba.",
        "arxiv_id": "2503.06446",
        "ARXIVID": "2503.06446",
        "COMMENT": "Matches criterion 4 as it proposes a CLIP-driven model for multi-modal remote sensing classification.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.06261": {
        "authors": [
            "Wei-En Tai",
            "Yu-Lin Shih",
            "Cheng Sun",
            "Yu-Chiang Frank Wang",
            "Hwann-Tzong Chen"
        ],
        "title": "Segment Anything, Even Occluded",
        "abstract": "arXiv:2503.06261v1 Announce Type: new  Abstract: Amodal instance segmentation, which aims to detect and segment both visible and invisible parts of objects in images, plays a crucial role in various applications including autonomous driving, robotic manipulation, and scene understanding. While existing methods require training both front-end detectors and mask decoders jointly, this approach lacks flexibility and fails to leverage the strengths of pre-existing modal detectors. To address this limitation, we propose SAMEO, a novel framework that adapts the Segment Anything Model (SAM) as a versatile mask decoder capable of interfacing with various front-end detectors to enable mask prediction even for partially occluded objects. Acknowledging the constraints of limited amodal segmentation datasets, we introduce Amodal-LVIS, a large-scale synthetic dataset comprising 300K images derived from the modal LVIS and LVVIS datasets. This dataset significantly expands the training data available for amodal segmentation research. Our experimental results demonstrate that our approach, when trained on the newly extended dataset, including Amodal-LVIS, achieves remarkable zero-shot performance on both COCOA-cls and D2SA benchmarks, highlighting its potential for generalization to unseen scenarios.",
        "arxiv_id": "2503.06261",
        "ARXIVID": "2503.06261",
        "COMMENT": "Matches criterion 4 as it adapts the Segment Anything Model (SAM) for amodal instance segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.07266": {
        "authors": [
            "Fu Rong",
            "Meng Lan",
            "Qian Zhang",
            "Lefei Zhang"
        ],
        "title": "Customized SAM 2 for Referring Remote Sensing Image Segmentation",
        "abstract": "arXiv:2503.07266v1 Announce Type: new  Abstract: Referring Remote Sensing Image Segmentation (RRSIS) aims to segment target objects in remote sensing (RS) images based on textual descriptions. Although Segment Anything Model 2 (SAM 2) has shown remarkable performance in various segmentation tasks, its application to RRSIS presents several challenges, including understanding the text-described RS scenes and generating effective prompts from text descriptions. To address these issues, we propose RS2-SAM 2, a novel framework that adapts SAM 2 to RRSIS by aligning the adapted RS features and textual features, providing pseudo-mask-based dense prompts, and enforcing boundary constraints. Specifically, we first employ a union encoder to jointly encode the visual and textual inputs, generating aligned visual and text embeddings as well as multimodal class tokens. Then, we design a bidirectional hierarchical fusion module to adapt SAM 2 to RS scenes and align adapted visual features with the visually enhanced text embeddings, improving the model's interpretation of text-described RS scenes. Additionally, a mask prompt generator is introduced to take the visual embeddings and class tokens as input and produce a pseudo-mask as the dense prompt of SAM 2. To further refine segmentation, we introduce a text-guided boundary loss to optimize segmentation boundaries by computing text-weighted gradient differences. Experimental results on several RRSIS benchmarks demonstrate that RS2-SAM 2 achieves state-of-the-art performance.",
        "arxiv_id": "2503.07266",
        "ARXIVID": "2503.07266",
        "COMMENT": "Matches criterion 4 as it adapts a vision foundation model (SAM 2) for remote sensing image segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.07503": {
        "authors": [
            "Shiu-hong Kao",
            "Yu-Wing Tai",
            "Chi-Keung Tang"
        ],
        "title": "Think Before You Segment: High-Quality Reasoning Segmentation with GPT Chain of Thoughts",
        "abstract": "arXiv:2503.07503v1 Announce Type: new  Abstract: Reasoning segmentation is a challenging vision-language task that aims to output the segmentation mask with respect to a complex, implicit, and even non-visual query text. Previous works incorporated multimodal Large Language Models (MLLMs) with segmentation models to approach the difficult problem. However, their segmentation quality often falls short in complex cases, particularly when dealing with out-of-domain objects with intricate structures, blurry boundaries, occlusions, or high similarity with surroundings. In this paper, we introduce ThinkFirst, a training-free reasoning segmentation framework that leverages GPT's chain of thought to address these challenging cases. Our approach allows GPT-4o or other powerful MLLMs to generate a detailed, chain-of-thought description of an image. This summarized description is then passed to a language-instructed segmentation assistant to aid the segmentation process. Our framework allows users to easily interact with the segmentation agent using multimodal inputs, such as easy text and image scribbles, for successive refinement or communication. We evaluate the performance of ThinkFirst on diverse objects. Extensive experiments show that, this zero-shot-CoT approach significantly improves the vanilla reasoning segmentation agent, both qualitatively and quantitatively, while being less sensitive or critical to user-supplied prompts after Thinking First.",
        "arxiv_id": "2503.07503",
        "ARXIVID": "2503.07503",
        "COMMENT": "Matches criterion 2 as it leverages GPT's chain of thought for reasoning segmentation, which is relevant to multi-modal large language models (MLLMs).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.06161": {
        "authors": [
            "Kai Li",
            "Junhao Wang",
            "William Han",
            "Ding Zhao"
        ],
        "title": "Feature-EndoGaussian: Feature Distilled Gaussian Splatting in Surgical Deformable Scene Reconstruction",
        "abstract": "arXiv:2503.06161v1 Announce Type: new  Abstract: Minimally invasive surgery (MIS) has transformed clinical practice by reducing recovery times, minimizing complications, and enhancing precision. Nonetheless, MIS inherently relies on indirect visualization and precise instrument control, posing unique challenges. Recent advances in artificial intelligence have enabled real-time surgical scene understanding through techniques such as image classification, object detection, and segmentation, with scene reconstruction emerging as a key element for enhanced intraoperative guidance. Although neural radiance fields (NeRFs) have been explored for this purpose, their substantial data requirements and slow rendering inhibit real-time performance. In contrast, 3D Gaussian Splatting (3DGS) offers a more efficient alternative, achieving state-of-the-art performance in dynamic surgical scene reconstruction. In this work, we introduce Feature-EndoGaussian (FEG), an extension of 3DGS that integrates 2D segmentation cues into 3D rendering to enable real-time semantic and scene reconstruction. By leveraging pretrained segmentation foundation models, FEG incorporates semantic feature distillation within the Gaussian deformation framework, thereby enhancing both reconstruction fidelity and segmentation accuracy. On the EndoNeRF dataset, FEG achieves superior performance (SSIM of 0.97, PSNR of 39.08, and LPIPS of 0.03) compared to leading methods. Additionally, on the EndoVis18 dataset, FEG demonstrates competitive class-wise segmentation metrics while balancing model size and real-time performance.",
        "arxiv_id": "2503.06161",
        "ARXIVID": "2503.06161",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for surgical scene reconstruction, which involves building new methods for embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.07265": {
        "authors": [
            "Yuwei Niu",
            "Munan Ning",
            "Mengren Zheng",
            "Bin Lin",
            "Peng Jin",
            "Jiaqi Liao",
            "Kunpeng Ning",
            "Bin Zhu",
            "Li Yuan"
        ],
        "title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation",
        "abstract": "arXiv:2503.07265v1 Announce Type: new  Abstract: Text-to-Image (T2I) models are capable of generating high-quality artistic creations and visual content. However, existing research and evaluation standards predominantly focus on image realism and shallow text-image alignment, lacking a comprehensive assessment of complex semantic understanding and world knowledge integration in text to image generation. To address this challenge, we propose $\\textbf{WISE}$, the first benchmark specifically designed for $\\textbf{W}$orld Knowledge-$\\textbf{I}$nformed $\\textbf{S}$emantic $\\textbf{E}$valuation. WISE moves beyond simple word-pixel mapping by challenging models with 1000 meticulously crafted prompts across 25 sub-domains in cultural common sense, spatio-temporal reasoning, and natural science. To overcome the limitations of traditional CLIP metric, we introduce $\\textbf{WiScore}$, a novel quantitative metric for assessing knowledge-image alignment. Through comprehensive testing of 20 models (10 dedicated T2I models and 10 unified multimodal models) using 1,000 structured prompts spanning 25 subdomains, our findings reveal significant limitations in their ability to effectively integrate and apply world knowledge during image generation, highlighting critical pathways for enhancing knowledge incorporation and application in next-generation T2I models. Code and data are available at https://github.com/PKU-YuanGroup/WISE.",
        "arxiv_id": "2503.07265",
        "ARXIVID": "2503.07265",
        "COMMENT": "Matches criterion 4 as it introduces a benchmark for evaluating text-to-image models with world knowledge, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.06456": {
        "authors": [
            "Chengxuan Qian",
            "Kai Han",
            "Jingchao Wang",
            "Zhenlong Yuan",
            "Rui Qian",
            "Chongwen Lyu",
            "Jun Chen",
            "Zhe Liu"
        ],
        "title": "DynCIM: Dynamic Curriculum for Imbalanced Multimodal Learning",
        "abstract": "arXiv:2503.06456v1 Announce Type: new  Abstract: Multimodal learning integrates complementary information from diverse modalities to enhance the decision-making process. However, the potential of multimodal collaboration remains under-exploited due to disparities in data quality and modality representation capabilities. To address this, we introduce DynCIM, a novel dynamic curriculum learning framework designed to quantify the inherent imbalances from both sample and modality perspectives. DynCIM employs a sample-level curriculum to dynamically assess each sample's difficulty according to prediction deviation, consistency, and stability, while a modality-level curriculum measures modality contributions from global and local. Furthermore, a gating-based dynamic fusion mechanism is introduced to adaptively adjust modality contributions, minimizing redundancy and optimizing fusion effectiveness. Extensive experiments on six multimodal benchmarking datasets, spanning both bimodal and trimodal scenarios, demonstrate that DynCIM consistently outperforms state-of-the-art methods. Our approach effectively mitigates modality and sample imbalances while enhancing adaptability and robustness in multimodal learning tasks. Our code is available at https://github.com/Raymond-Qiancx/DynCIM.",
        "arxiv_id": "2503.06456",
        "ARXIVID": "2503.06456",
        "COMMENT": "Matches criterion 2 as it introduces a novel dynamic curriculum learning framework for multimodal learning, which could be relevant to visual large language models (VLLMs) or multi-modal large language models (MLLMs).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.06960": {
        "authors": [
            "Xin Wen",
            "Bingchen Zhao",
            "Yilun Chen",
            "Jiangmiao Pang",
            "Xiaojuan Qi"
        ],
        "title": "A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning",
        "abstract": "arXiv:2503.06960v1 Announce Type: new  Abstract: Pre-trained vision models (PVMs) are fundamental to modern robotics, yet their optimal configuration remains unclear. Through systematic evaluation, we find that while DINO and iBOT outperform MAE across visuomotor control and perception tasks, they struggle when trained on non-(single-)object-centric (NOC) data--a limitation strongly correlated with their diminished ability to learn object-centric representations. This investigation indicates that the ability to form object-centric representations from the non-object-centric robotics dataset is the key to success for PVMs. Motivated by this discovery, we designed SlotMIM, a method that induces object-centric representations by introducing a semantic bottleneck to reduce the number of prototypes to encourage the emergence of objectness as well as cross-view consistency regularization for encouraging multiview invariance. Our experiments encompass pre-training on object-centric, scene-centric, web-crawled, and ego-centric data. Across all settings, our approach learns transferrable representations and achieves significant improvements over prior work in image recognition, scene understanding, and robot learning evaluations. When scaled up with million-scale datasets, our method also demonstrates superior data efficiency and scalability. Our code and models are publicly available at https://github.com/CVMI-Lab/SlotMIM.",
        "arxiv_id": "2503.06960",
        "ARXIVID": "2503.06960",
        "COMMENT": "Matches criterion 4 as it discusses vision foundation models (PVMs) and their applications in robot learning, with a focus on object-centric representations.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.06800": {
        "authors": [
            "Hritik Bansal",
            "Clark Peng",
            "Yonatan Bitton",
            "Roman Goldenberg",
            "Aditya Grover",
            "Kai-Wei Chang"
        ],
        "title": "VideoPhy-2: A Challenging Action-Centric Physical Commonsense Evaluation in Video Generation",
        "abstract": "arXiv:2503.06800v1 Announce Type: new  Abstract: Large-scale video generative models, capable of creating realistic videos of diverse visual concepts, are strong candidates for general-purpose physical world simulators. However, their adherence to physical commonsense across real-world actions remains unclear (e.g., playing tennis, backflip). Existing benchmarks suffer from limitations such as limited size, lack of human evaluation, sim-to-real gaps, and absence of fine-grained physical rule analysis. To address this, we introduce VideoPhy-2, an action-centric dataset for evaluating physical commonsense in generated videos. We curate 200 diverse actions and detailed prompts for video synthesis from modern generative models. We perform human evaluation that assesses semantic adherence, physical commonsense, and grounding of physical rules in the generated videos. Our findings reveal major shortcomings, with even the best model achieving only 22% joint performance (i.e., high semantic and physical commonsense adherence) on the hard subset of VideoPhy-2. We find that the models particularly struggle with conservation laws like mass and momentum. Finally, we also train VideoPhy-AutoEval, an automatic evaluator for fast, reliable assessment on our dataset. Overall, VideoPhy-2 serves as a rigorous benchmark, exposing critical gaps in video generative models and guiding future research in physically-grounded video generation. The data and code is available at https://videophy2.github.io/.",
        "arxiv_id": "2503.06800",
        "ARXIVID": "2503.06800",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (VideoPhy-2) for evaluating physical commonsense in video generation, which is relevant to embodied AI and simulators.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.06684": {
        "authors": [
            "Yanjie Pan",
            "Qingdong He",
            "Zhengkai Jiang",
            "Pengcheng Xu",
            "Chaoyi Wang",
            "Jinlong Peng",
            "Haoxuan Wang",
            "Yun Cao",
            "Zhenye Gan",
            "Mingmin Chi",
            "Bo Peng",
            "Yabiao Wang"
        ],
        "title": "PixelPonder: Dynamic Patch Adaptation for Enhanced Multi-Conditional Text-to-Image Generation",
        "abstract": "arXiv:2503.06684v1 Announce Type: new  Abstract: Recent advances in diffusion-based text-to-image generation have demonstrated promising results through visual condition control. However, existing ControlNet-like methods struggle with compositional visual conditioning - simultaneously preserving semantic fidelity across multiple heterogeneous control signals while maintaining high visual quality, where they employ separate control branches that often introduce conflicting guidance during the denoising process, leading to structural distortions and artifacts in generated images. To address this issue, we present PixelPonder, a novel unified control framework, which allows for effective control of multiple visual conditions under a single control structure. Specifically, we design a patch-level adaptive condition selection mechanism that dynamically prioritizes spatially relevant control signals at the sub-region level, enabling precise local guidance without global interference. Additionally, a time-aware control injection scheme is deployed to modulate condition influence according to denoising timesteps, progressively transitioning from structural preservation to texture refinement and fully utilizing the control information from different categories to promote more harmonious image generation. Extensive experiments demonstrate that PixelPonder surpasses previous methods across different benchmark datasets, showing superior improvement in spatial alignment accuracy while maintaining high textual semantic consistency.",
        "arxiv_id": "2503.06684",
        "ARXIVID": "2503.06684",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models and their applications in text-to-image generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.06134": {
        "authors": [
            "Jian Ma",
            "Qirong Peng",
            "Xu Guo",
            "Chen Chen",
            "Haonan Lu",
            "Zhenyu Yang"
        ],
        "title": "X2I: Seamless Integration of Multimodal Understanding into Diffusion Transformer via Attention Distillation",
        "abstract": "arXiv:2503.06134v1 Announce Type: new  Abstract: Text-to-image (T2I) models are well known for their ability to produce highly realistic images, while multimodal large language models (MLLMs) are renowned for their proficiency in understanding and integrating multiple modalities. However, currently there is no straightforward and efficient framework to transfer the multimodal comprehension abilities of MLLMs to T2I models to enable them to understand multimodal inputs. In this paper, we propose the X2I framework, which endows Diffusion Transformer (DiT) models with the capability to comprehend various modalities, including multilingual text, screenshot documents, images, videos, and audio. X2I is trained using merely 100K English corpus with 160 GPU hours. Building on the DiT teacher model, we adopt an innovative distillation method to extract the inference capabilities of the teacher model and design a lightweight AlignNet structure to serve as an intermediate bridge. Compared to the teacher model, X2I shows a decrease in performance degradation of less than 1\\% while gaining various multimodal understanding abilities, including multilingual to image, image to image, image-text to image, video to image, audio to image, and utilizing creative fusion to enhance imagery. Furthermore, it is applicable for LoRA training in the context of image-text to image generation, filling a void in the industry in this area. We further design a simple LightControl to enhance the fidelity of instructional image editing. Finally, extensive experiments demonstrate the effectiveness, efficiency, multifunctionality, and transferability of our X2I. The open-source code and checkpoints for X2I can be found at the following link: https://github.com/OPPO-Mente-Lab/X2I.",
        "arxiv_id": "2503.06134",
        "ARXIVID": "2503.06134",
        "COMMENT": "Matches criterion 2 as it introduces a framework for integrating multimodal understanding into diffusion transformers, showcasing a new MLLM application.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.06271": {
        "authors": [
            "Anh Thai",
            "Songyou Peng",
            "Kyle Genova",
            "Leonidas Guibas",
            "Thomas Funkhouser"
        ],
        "title": "SplatTalk: 3D VQA with Gaussian Splatting",
        "abstract": "arXiv:2503.06271v1 Announce Type: new  Abstract: Language-guided 3D scene understanding is important for advancing applications in robotics, AR/VR, and human-computer interaction, enabling models to comprehend and interact with 3D environments through natural language. While 2D vision-language models (VLMs) have achieved remarkable success in 2D VQA tasks, progress in the 3D domain has been significantly slower due to the complexity of 3D data and the high cost of manual annotations. In this work, we introduce SplatTalk, a novel method that uses a generalizable 3D Gaussian Splatting (3DGS) framework to produce 3D tokens suitable for direct input into a pretrained LLM, enabling effective zero-shot 3D visual question answering (3D VQA) for scenes with only posed images. During experiments on multiple benchmarks, our approach outperforms both 3D models trained specifically for the task and previous 2D-LMM-based models utilizing only images (our setting), while achieving competitive performance with state-of-the-art 3D LMMs that additionally utilize 3D inputs.",
        "arxiv_id": "2503.06271",
        "ARXIVID": "2503.06271",
        "COMMENT": "Matches criterion 2 as it introduces a novel method for 3D VQA using a visual language model, which is a new VLLM application.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.05808": {
        "authors": [
            "Shenyu Zhang",
            "Jiaguo Tian",
            "Zhengbang Zhu",
            "Shan Huang",
            "Jucheng Yang",
            "Weinan Zhang"
        ],
        "title": "DriveGen: Towards Infinite Diverse Traffic Scenarios with Large Models",
        "abstract": "arXiv:2503.05808v1 Announce Type: new  Abstract: Microscopic traffic simulation has become an important tool for autonomous driving training and testing. Although recent data-driven approaches advance realistic behavior generation, their learning still relies primarily on a single real-world dataset, which limits their diversity and thereby hinders downstream algorithm optimization. In this paper, we propose DriveGen, a novel traffic simulation framework with large models for more diverse traffic generation that supports further customized designs. DriveGen consists of two internal stages: the initialization stage uses large language model and retrieval technique to generate map and vehicle assets; the rollout stage outputs trajectories with selected waypoint goals from visual language model and a specific designed diffusion planner. Through this two-staged process, DriveGen fully utilizes large models' high-level cognition and reasoning of driving behavior, obtaining greater diversity beyond datasets while maintaining high realism. To support effective downstream optimization, we additionally develop DriveGen-CS, an automatic corner case generation pipeline that uses failures of the driving algorithm as additional prompt knowledge for large models without the need for retraining or fine-tuning. Experiments show that our generated scenarios and corner cases have a superior performance compared to state-of-the-art baselines. Downstream experiments further verify that the synthesized traffic of DriveGen provides better optimization of the performance of typical driving algorithms, demonstrating the effectiveness of our framework.",
        "arxiv_id": "2503.05808",
        "ARXIVID": "2503.05808",
        "COMMENT": "Matches criterion 3 as it proposes a novel traffic simulation framework with large models, focusing on diversity and corner case generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.06744": {
        "authors": [
            "Rui Song",
            "Chenwei Liang",
            "Yan Xia",
            "Walter Zimmer",
            "Hu Cao",
            "Holger Caesar",
            "Andreas Festag",
            "Alois Knoll"
        ],
        "title": "CoDa-4DGS: Dynamic Gaussian Splatting with Context and Deformation Awareness for Autonomous Driving",
        "abstract": "arXiv:2503.06744v1 Announce Type: new  Abstract: Dynamic scene rendering opens new avenues in autonomous driving by enabling closed-loop simulations with photorealistic data, which is crucial for validating end-to-end algorithms. However, the complex and highly dynamic nature of traffic environments presents significant challenges in accurately rendering these scenes. In this paper, we introduce a novel 4D Gaussian Splatting (4DGS) approach, which incorporates context and temporal deformation awareness to improve dynamic scene rendering. Specifically, we employ a 2D semantic segmentation foundation model to self-supervise the 4D semantic features of Gaussians, ensuring meaningful contextual embedding. Simultaneously, we track the temporal deformation of each Gaussian across adjacent frames. By aggregating and encoding both semantic and temporal deformation features, each Gaussian is equipped with cues for potential deformation compensation within 3D space, facilitating a more precise representation of dynamic scenes. Experimental results show that our method improves 4DGS's ability to capture fine details in dynamic scene rendering for autonomous driving and outperforms other self-supervised methods in 4D reconstruction and novel view synthesis. Furthermore, CoDa-4DGS deforms semantic features with each Gaussian, enabling broader applications.",
        "arxiv_id": "2503.06744",
        "ARXIVID": "2503.06744",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for dynamic scene rendering in autonomous driving, which is a new angle for embodied AI benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.07197": {
        "authors": [
            "Zebin You",
            "Jingyang Ou",
            "Xiaolu Zhang",
            "Jun Hu",
            "Jun Zhou",
            "Chongxuan Li"
        ],
        "title": "Effective and Efficient Masked Image Generation Models",
        "abstract": "arXiv:2503.07197v1 Announce Type: new  Abstract: Although masked image generation models and masked diffusion models are designed with different motivations and objectives, we observe that they can be unified within a single framework. Building upon this insight, we carefully explore the design space of training and sampling, identifying key factors that contribute to both performance and efficiency. Based on the improvements observed during this exploration, we develop our model, referred to as eMIGM. Empirically, eMIGM demonstrates strong performance on ImageNet generation, as measured by Fr\\'echet Inception Distance (FID). In particular, on ImageNet 256x256, with similar number of function evaluations (NFEs) and model parameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model parameters increase, eMIGM achieves performance comparable to the state-of-the-art continuous diffusion models while requiring less than 40% of the NFE. Additionally, on ImageNet 512x512, with only about 60% of the NFE, eMIGM outperforms the state-of-the-art continuous diffusion models.",
        "arxiv_id": "2503.07197",
        "ARXIVID": "2503.07197",
        "COMMENT": "Partially relevant to criterion 4 as it discusses masked image generation models, which are related to generative modeling but not directly to vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2503.07050": {
        "authors": [
            "Victor Shea-Jay Huang",
            "Le Zhuo",
            "Yi Xin",
            "Zhaokai Wang",
            "Peng Gao",
            "Hongsheng Li"
        ],
        "title": "TIDE : Temporal-Aware Sparse Autoencoders for Interpretable Diffusion Transformers in Image Generation",
        "abstract": "arXiv:2503.07050v1 Announce Type: new  Abstract: Diffusion Transformers (DiTs) are a powerful yet underexplored class of generative models compared to U-Net-based diffusion models. To bridge this gap, we introduce TIDE (Temporal-aware Sparse Autoencoders for Interpretable Diffusion transformErs), a novel framework that enhances temporal reconstruction within DiT activation layers across denoising steps. TIDE employs Sparse Autoencoders (SAEs) with a sparse bottleneck layer to extract interpretable and hierarchical features, revealing that diffusion models inherently learn hierarchical features at multiple levels (e.g., 3D, semantic, class) during generative pre-training. Our approach achieves state-of-the-art reconstruction performance, with a mean squared error (MSE) of 1e-3 and a cosine similarity of 0.97, demonstrating superior accuracy in capturing activation dynamics along the denoising trajectory. Beyond interpretability, we showcase TIDE's potential in downstream applications such as sparse activation-guided image editing and style transfer, enabling improved controllability for generative systems. By providing a comprehensive training and evaluation protocol tailored for DiTs, TIDE contributes to developing more interpretable, transparent, and trustworthy generative models.",
        "arxiv_id": "2503.07050",
        "ARXIVID": "2503.07050",
        "COMMENT": "Partially relevant to criterion 4 as it discusses interpretability and applications of diffusion transformers, which are related to generative modeling but not directly to vision foundation models.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2503.06063": {
        "authors": [
            "Junyan Lin",
            "Haoran Chen",
            "Yue Fan",
            "Yingqi Fan",
            "Xin Jin",
            "Hui Su",
            "Jinlan Fu",
            "Xiaoyu Shen"
        ],
        "title": "Multi-Layer Visual Feature Fusion in Multimodal LLMs: Methods, Analysis, and Best Practices",
        "abstract": "arXiv:2503.06063v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have made significant advancements in recent years, with visual features playing an increasingly critical role in enhancing model performance. However, the integration of multi-layer visual features in MLLMs remains underexplored, particularly with regard to optimal layer selection and fusion strategies. Existing methods often rely on arbitrary design choices, leading to suboptimal outcomes. In this paper, we systematically investigate two core aspects of multi-layer visual feature fusion: (1) selecting the most effective visual layers and (2) identifying the best fusion approach with the language model. Our experiments reveal that while combining visual features from multiple stages improves generalization, incorporating additional features from the same stage typically leads to diminished performance. Furthermore, we find that direct fusion of multi-layer visual features at the input stage consistently yields superior and more stable performance across various configurations. We make all our code publicly available: https://github.com/EIT-NLP/Layer_Select_Fuse_for_MLLM.",
        "arxiv_id": "2503.06063",
        "ARXIVID": "2503.06063",
        "COMMENT": "Matches criterion 2 as it explores multi-layer visual feature fusion in multimodal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2503.06625": {
        "authors": [
            "Chaocan Xue",
            "Bineng Zhong",
            "Qihua Liang",
            "Yaozong Zheng",
            "Ning Li",
            "Yuanliang Xue",
            "Shuxiang Song"
        ],
        "title": "Similarity-Guided Layer-Adaptive Vision Transformer for UAV Tracking",
        "abstract": "arXiv:2503.06625v1 Announce Type: new  Abstract: Vision transformers (ViTs) have emerged as a popular backbone for visual tracking. However, complete ViT architectures are too cumbersome to deploy for unmanned aerial vehicle (UAV) tracking which extremely emphasizes efficiency. In this study, we discover that many layers within lightweight ViT-based trackers tend to learn relatively redundant and repetitive target representations. Based on this observation, we propose a similarity-guided layer adaptation approach to optimize the structure of ViTs. Our approach dynamically disables a large number of representation-similar layers and selectively retains only a single optimal layer among them, aiming to achieve a better accuracy-speed trade-off. By incorporating this approach into existing ViTs, we tailor previously complete ViT architectures into an efficient similarity-guided layer-adaptive framework, namely SGLATrack, for real-time UAV tracking. Extensive experiments on six tracking benchmarks verify the effectiveness of the proposed approach, and show that our SGLATrack achieves a state-of-the-art real-time speed while maintaining competitive tracking precision. Codes and models are available at https://github.com/GXNU-ZhongLab/SGLATrack.",
        "arxiv_id": "2503.06625",
        "ARXIVID": "2503.06625",
        "COMMENT": "Matches criterion 4 as it focuses on optimizing vision transformers for UAV tracking, which is related to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2503.06840": {
        "authors": [
            "Somayeh Hussaini",
            "Tobias Fischer",
            "Michael Milford"
        ],
        "title": "Improving Visual Place Recognition with Sequence-Matching Receptiveness Prediction",
        "abstract": "arXiv:2503.06840v1 Announce Type: new  Abstract: In visual place recognition (VPR), filtering and sequence-based matching approaches can improve performance by integrating temporal information across image sequences, especially in challenging conditions. While these methods are commonly applied, their effects on system behavior can be unpredictable and can actually make performance worse in certain situations. In this work, we present a new supervised learning approach that learns to predict the per-frame sequence matching receptiveness (SMR) of VPR techniques, enabling the system to selectively decide when to trust the output of a sequence matching system. The approach is agnostic to the underlying VPR technique. Our approach predicts SMR-and hence significantly improves VPR performance-across a large range of state-of-the-art and classical VPR techniques (namely CosPlace, MixVPR, EigenPlaces, SALAD, AP-GeM, NetVLAD and SAD), and across three benchmark VPR datasets (Nordland, Oxford RobotCar, and SFU-Mountain). We also provide insights into a complementary approach that uses the predictor to replace discarded matches, as well as ablation studies, including an analysis of the interactions between our SMR predictor and the selected sequence length. We will release our code upon acceptance.",
        "arxiv_id": "2503.06840",
        "ARXIVID": "2503.06840",
        "COMMENT": "Matches criterion 1 as it proposes a new supervised learning approach for visual place recognition, which involves spatial understanding and intelligence.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2503.06497": {
        "authors": [
            "Enming Zhang",
            "Peizhe Gong",
            "Xingyuan Dai",
            "Yisheng Lv",
            "Qinghai Miao"
        ],
        "title": "Evaluation of Safety Cognition Capability in Vision-Language Models for Autonomous Driving",
        "abstract": "arXiv:2503.06497v1 Announce Type: new  Abstract: Assessing the safety of vision-language models (VLMs) in autonomous driving is particularly important; however, existing work mainly focuses on traditional benchmark evaluations. As interactive components within autonomous driving systems, VLMs must maintain strong safety cognition during interactions. From this perspective, we propose a novel evaluation method: Safety Cognitive Driving Benchmark (SCD-Bench) . To address the large-scale annotation challenge for SCD-Bench, we develop the Autonomous Driving Image-Text Annotation System (ADA) . Additionally, to ensure data quality in SCD-Bench, our dataset undergoes manual refinement by experts with professional knowledge in autonomous driving. We further develop an automated evaluation method based on large language models (LLMs). To verify its effectiveness, we compare its evaluation results with those of expert human evaluations, achieving a consistency rate of 99.74%. Preliminary experimental results indicate that existing open-source models still lack sufficient safety cognition, showing a significant gap compared to GPT-4o. Notably, lightweight models (1B-4B) demonstrate minimal safety cognition. However, since lightweight models are crucial for autonomous driving systems, this presents a significant challenge for integrating VLMs into the field.",
        "arxiv_id": "2503.06497",
        "ARXIVID": "2503.06497",
        "COMMENT": "Matches criterion 2 as it evaluates vision-language models (VLMs) in the context of safety cognition for autonomous driving.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2503.07575": {
        "authors": [
            "Jen-tse Huang",
            "Jiantong Qin",
            "Jianping Zhang",
            "Youliang Yuan",
            "Wenxuan Wang",
            "Jieyu Zhao"
        ],
        "title": "VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models",
        "abstract": "arXiv:2503.07575v1 Announce Type: new  Abstract: This research investigates both explicit and implicit social biases exhibited by Vision-Language Models (VLMs). The key distinction between these bias types lies in the level of awareness: explicit bias refers to conscious, intentional biases, while implicit bias operates subconsciously. To analyze explicit bias, we directly pose questions to VLMs related to gender and racial differences: (1) Multiple-choice questions based on a given image (e.g., \"What is the education level of the person in the image?\") (2) Yes-No comparisons using two images (e.g., \"Is the person in the first image more educated than the person in the second image?\") For implicit bias, we design tasks where VLMs assist users but reveal biases through their responses: (1) Image description tasks: Models are asked to describe individuals in images, and we analyze disparities in textual cues across demographic groups. (2) Form completion tasks: Models draft a personal information collection form with 20 attributes, and we examine correlations among selected attributes for potential biases. We evaluate Gemini-1.5, GPT-4V, GPT-4o, LLaMA-3.2-Vision and LLaVA-v1.6. Our code and data are publicly available at https://github.com/uscnlp-lime/VisBias.",
        "arxiv_id": "2503.07575",
        "ARXIVID": "2503.07575",
        "COMMENT": "Matches criterion 2 as it investigates biases in vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2503.06900": {
        "authors": [
            "Xiaoliang Ju",
            "Hongsheng Li"
        ],
        "title": "DirectTriGS: Triplane-based Gaussian Splatting Field Representation for 3D Generation",
        "abstract": "arXiv:2503.06900v1 Announce Type: new  Abstract: We present DirectTriGS, a novel framework designed for 3D object generation with Gaussian Splatting (GS). GS-based rendering for 3D content has gained considerable attention recently. However, there has been limited exploration in directly generating 3D Gaussians compared to traditional generative modeling approaches. The main challenge lies in the complex data structure of GS represented by discrete point clouds with multiple channels. To overcome this challenge, we propose employing the triplane representation, which allows us to represent Gaussian Splatting as an image-like continuous field. This representation effectively encodes both the geometry and texture information, enabling smooth transformation back to Gaussian point clouds and rendering into images by a TriRenderer, with only 2D supervisions. The proposed TriRenderer is fully differentiable, so that the rendering loss can supervise both texture and geometry encoding. Furthermore, the triplane representation can be compressed using a Variational Autoencoder (VAE), which can subsequently be utilized in latent diffusion to generate 3D objects. The experiments demonstrate that the proposed generation framework can produce high-quality 3D object geometry and rendering results in the text-to-3D task.",
        "arxiv_id": "2503.06900",
        "ARXIVID": "2503.06900",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in 3D, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2503.07375": {
        "authors": [
            "R. Spencer Hallyburton",
            "David Hunt",
            "Yiwei He",
            "Judy He",
            "Miroslav Pajic"
        ],
        "title": "Probabilistic Segmentation for Robust Field of View Estimation",
        "abstract": "arXiv:2503.07375v1 Announce Type: new  Abstract: Attacks on sensing and perception threaten the safe deployment of autonomous vehicles (AVs). Security-aware sensor fusion helps mitigate threats but requires accurate field of view (FOV) estimation which has not been evaluated autonomy. To address this gap, we adapt classical computer graphics algorithms to develop the first autonomy-relevant FOV estimators and create the first datasets with ground truth FOV labels. Unfortunately, we find that these approaches are themselves highly vulnerable to attacks on sensing. To improve robustness of FOV estimation against attacks, we propose a learning-based segmentation model that captures FOV features, integrates Monte Carlo dropout (MCD) for uncertainty quantification, and performs anomaly detection on confidence maps. We illustrate through comprehensive evaluations attack resistance and strong generalization across environments. Architecture trade studies demonstrate the model is feasible for real-time deployment in multiple applications.",
        "arxiv_id": "2503.07375",
        "ARXIVID": "2503.07375",
        "COMMENT": "Partially relevant to criterion 3 as it discusses a novel method for robust field of view estimation, which could be useful in embodied AI but does not directly focus on benchmarks or simulators.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2503.06954": {
        "authors": [
            "Xingye Fan (Rex)",
            "Zhongwen (Rex)",
            "Zhang",
            "Yuri Boykov"
        ],
        "title": "Approximate Size Targets Are Sufficient for Accurate Semantic Segmentation",
        "abstract": "arXiv:2503.06954v1 Announce Type: new  Abstract: This paper demonstrates a surprising result for segmentation with image-level targets: extending binary class tags to approximate relative object-size distributions allows off-the-shelf architectures to solve the segmentation problem. A straightforward zero-avoiding KL-divergence loss for average predictions produces segmentation accuracy comparable to the standard pixel-precise supervision with full ground truth masks. In contrast, current results based on class tags typically require complex non-reproducible architectural modifications and specialized multi-stage training procedures. Our ideas are validated on PASCAL VOC using our new human annotations of approximate object sizes. We also show the results on COCO and medical data using synthetically corrupted size targets. All standard networks demonstrate robustness to the size targets' errors. For some classes, the validation accuracy is significantly better than the pixel-level supervision; the latter is not robust to errors in the masks. Our work provides new ideas and insights on image-level supervision in segmentation and may encourage other simple general solutions to the problem.",
        "arxiv_id": "2503.06954",
        "ARXIVID": "2503.06954",
        "COMMENT": "Does not match any specific criteria but provides surprising empirical results in segmentation with approximate size targets.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.07076": {
        "authors": [
            "Zhihao Huang",
            "Xi Qiu",
            "Yukuo Ma",
            "Yifu Zhou",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "title": "NFIG: Autoregressive Image Generation with Next-Frequency Prediction",
        "abstract": "arXiv:2503.07076v1 Announce Type: new  Abstract: Autoregressive models have achieved promising results in natural language processing. However, for image generation tasks, they encounter substantial challenges in effectively capturing long-range dependencies, managing computational costs, and most crucially, defining meaningful autoregressive sequences that reflect natural image hierarchies. To address these issues, we present \\textbf{N}ext-\\textbf{F}requency \\textbf{I}mage \\textbf{G}eneration (\\textbf{NFIG}), a novel framework that decomposes the image generation process into multiple frequency-guided stages. Our approach first generates low-frequency components to establish global structure with fewer tokens, then progressively adds higher-frequency details, following the natural spectral hierarchy of images. This principled autoregressive sequence not only improves the quality of generated images by better capturing true causal relationships between image components, but also significantly reduces computational overhead during inference. Extensive experiments demonstrate that NFIG achieves state-of-the-art performance with fewer steps, offering a more efficient solution for image generation, with 1.25$\\times$ speedup compared to VAR-d20 while achieving better performance (FID: 2.81) on the ImageNet-256 benchmark. We hope that our insight of incorporating frequency-domain knowledge to guide autoregressive sequence design will shed light on future research. We will make our code publicly available upon acceptance of the paper.",
        "arxiv_id": "2503.07076",
        "ARXIVID": "2503.07076",
        "COMMENT": "Does not match any specific criterion but is tangentially related to generative modeling in image generation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.06369": {
        "authors": [
            "Sahar Dastani",
            "Ali Bahri",
            "Moslem Yazdanpanah",
            "Mehrdad Noori",
            "David Osowiechi",
            "Gustavo Adolfo Vargas Hakim",
            "Farzad Beizaee",
            "Milad Cheraghalikhani",
            "Arnab Kumar Mondal",
            "Herve Lombaert",
            "Christian Desrosiers"
        ],
        "title": "Spectral State Space Model for Rotation-Invariant~Visual~Representation~Learning",
        "abstract": "arXiv:2503.06369v1 Announce Type: new  Abstract: State Space Models (SSMs) have recently emerged as an alternative to Vision Transformers (ViTs) due to their unique ability of modeling global relationships with linear complexity. SSMs are specifically designed to capture spatially proximate relationships of image patches. However, they fail to identify relationships between conceptually related yet not adjacent patches. This limitation arises from the non-causal nature of image data, which lacks inherent directional relationships. Additionally, current vision-based SSMs are highly sensitive to transformations such as rotation. Their predefined scanning directions depend on the original image orientation, which can cause the model to produce inconsistent patch-processing sequences after rotation. To address these limitations, we introduce Spectral VMamba, a novel approach that effectively captures the global structure within an image by leveraging spectral information derived from the graph Laplacian of image patches. Through spectral decomposition, our approach encodes patch relationships independently of image orientation, achieving rotation invariance with the aid of our Rotational Feature Normalizer (RFN) module. Our experiments on classification tasks show that Spectral VMamba outperforms the leading SSM models in vision, such as VMamba, while maintaining invariance to rotations and a providing a similar runtime efficiency.",
        "arxiv_id": "2503.06369",
        "ARXIVID": "2503.06369",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision with a focus on rotation-invariant visual representation learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.06677": {
        "authors": [
            "Di Wu",
            "Liu Liu",
            "Zhou Linli",
            "Anran Huang",
            "Liangtu Song",
            "Qiaojun Yu",
            "Qi Wu",
            "Cewu Lu"
        ],
        "title": "REArtGS: Reconstructing and Generating Articulated Objects via 3D Gaussian Splatting with Geometric and Motion Constraints",
        "abstract": "arXiv:2503.06677v1 Announce Type: new  Abstract: Articulated objects, as prevalent entities in human life, their 3D representations play crucial roles across various applications. However, achieving both high-fidelity textured surface reconstruction and dynamic generation for articulated objects remains challenging for existing methods. In this paper, we present REArtGS, a novel framework that introduces additional geometric and motion constraints to 3D Gaussian primitives, enabling high-quality textured surface reconstruction and generation for articulated objects. Specifically, given multi-view RGB images of arbitrary two states of articulated objects, we first introduce an unbiased Signed Distance Field (SDF) guidance to regularize Gaussian opacity fields, enhancing geometry constraints and improving surface reconstruction quality. Then we establish deformable fields for 3D Gaussians constrained by the kinematic structures of articulated objects, achieving unsupervised generation of surface meshes in unseen states. Extensive experiments on both synthetic and real datasets demonstrate our approach achieves high-quality textured surface reconstruction for given states, and enables high-fidelity surface generation for unseen states. Codes will be released within the next four months.",
        "arxiv_id": "2503.06677",
        "ARXIVID": "2503.06677",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and generative modeling in 3D reconstruction.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.07392": {
        "authors": [
            "Ouxiang Li",
            "Yuan Wang",
            "Xinting Hu",
            "Houcheng Jiang",
            "Tao Liang",
            "Yanbin Hao",
            "Guojun Ma",
            "Fuli Feng"
        ],
        "title": "SPEED: Scalable, Precise, and Efficient Concept Erasure for Diffusion Models",
        "abstract": "arXiv:2503.07392v1 Announce Type: new  Abstract: Erasing concepts from large-scale text-to-image (T2I) diffusion models has become increasingly crucial due to the growing concerns over copyright infringement, offensive content, and privacy violations. However, existing methods either require costly fine-tuning or degrade image quality for non-target concepts (i.e., prior) due to inherent optimization limitations. In this paper, we introduce SPEED, a model editing-based concept erasure approach that leverages null-space constraints for scalable, precise, and efficient erasure. Specifically, SPEED incorporates Influence-based Prior Filtering (IPF) to retain the most affected non-target concepts during erasing, Directed Prior Augmentation (DPA) to expand prior coverage while maintaining semantic consistency, and Invariant Equality Constraints (IEC) to regularize model editing by explicitly preserving key invariants during the T2I generation process. Extensive evaluations across multiple concept erasure tasks demonstrate that SPEED consistently outperforms existing methods in prior preservation while achieving efficient and high-fidelity concept erasure, successfully removing 100 concepts within just 5 seconds. Our code and models are available at: https://github.com/Ouxiang-Li/SPEED.",
        "arxiv_id": "2503.07392",
        "ARXIVID": "2503.07392",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling in text-to-image diffusion models, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.06094": {
        "authors": [
            "Yong He",
            "Hongshan Yu",
            "Mingtao Feng",
            "Tongjia Chen",
            "Zechuan Li",
            "Anwaar Ulhaq",
            "Saeed Anwar",
            "Ajmal Saeed Mian"
        ],
        "title": "PointDiffuse: A Dual-Conditional Diffusion Model for Enhanced Point Cloud Semantic Segmentation",
        "abstract": "arXiv:2503.06094v1 Announce Type: new  Abstract: Diffusion probabilistic models are traditionally used to generate colors at fixed pixel positions in 2D images. Building on this, we extend diffusion models to point cloud semantic segmentation, where point positions also remain fixed, and the diffusion model generates point labels instead of colors. To accelerate the denoising process in reverse diffusion, we introduce a noisy label embedding mechanism. This approach integrates semantic information into the noisy label, providing an initial semantic reference that improves the reverse diffusion efficiency. Additionally, we propose a point frequency transformer that enhances the adjustment of high-level context in point clouds. To reduce computational complexity, we introduce the position condition into MLP and propose denoising PointNet to process the high-resolution point cloud without sacrificing geometric details. Finally, we integrate the proposed noisy label embedding, point frequency transformer and denoising PointNet in our proposed dual conditional diffusion model-based network (PointDiffuse) to perform large-scale point cloud semantic segmentation. Extensive experiments on five benchmarks demonstrate the superiority of PointDiffuse, achieving the state-of-the-art mIoU of 74.2\\% on S3DIS Area 5, 81.2\\% on S3DIS 6-fold and 64.8\\% on SWAN dataset.",
        "arxiv_id": "2503.06094",
        "ARXIVID": "2503.06094",
        "COMMENT": "Does not match any specific criterion but is relevant to point cloud semantic segmentation and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.06790": {
        "authors": [
            "Yan Wang",
            "Shijie Zhao",
            "Kai Chen",
            "Kexin Zhang",
            "Junlin Li",
            "Li Zhang"
        ],
        "title": "GenDR: Lightning Generative Detail Restorator",
        "abstract": "arXiv:2503.06790v1 Announce Type: new  Abstract: Recent research applying text-to-image (T2I) diffusion models to real-world super-resolution (SR) has achieved remarkable success. However, fundamental misalignments between T2I and SR targets result in a dilemma between inference speed and detail fidelity. Specifically, T2I tasks prioritize multi-step inversion to synthesize coherent outputs aligned with textual prompts and shrink the latent space to reduce generating complexity. Contrariwise, SR tasks preserve most information from low-resolution input while solely restoring high-frequency details, thus necessitating sufficient latent space and fewer inference steps. To bridge the gap, we present a one-step diffusion model for generative detail restoration, GenDR, distilled from a tailored diffusion model with larger latent space. In detail, we train a new SD2.1-VAE16 (0.9B) via representation alignment to expand latent space without enlarging the model size. Regarding step-distillation, we propose consistent score identity distillation (CiD) that incorporates SR task-specific loss into score distillation to leverage more SR priors and align the training target. Furthermore, we extend CiD with adversarial learning and representation alignment (CiDA) to enhance perceptual quality and accelerate training. We also polish the pipeline to achieve a more efficient inference. Experimental results demonstrate that GenDR achieves state-of-the-art performance in both quantitative metrics and visual fidelity.",
        "arxiv_id": "2503.06790",
        "ARXIVID": "2503.06790",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.06748": {
        "authors": [
            "Hantao Zhang",
            "Yuhe Liu",
            "Jiancheng Yang",
            "Weidong Guo",
            "Xinyuan Wang",
            "Pascal Fua"
        ],
        "title": "DiffAtlas: GenAI-fying Atlas Segmentation via Image-Mask Diffusion",
        "abstract": "arXiv:2503.06748v1 Announce Type: new  Abstract: Accurate medical image segmentation is crucial for precise anatomical delineation. Deep learning models like U-Net have shown great success but depend heavily on large datasets and struggle with domain shifts, complex structures, and limited training samples. Recent studies have explored diffusion models for segmentation by iteratively refining masks. However, these methods still retain the conventional image-to-mask mapping, making them highly sensitive to input data, which hampers stability and generalization. In contrast, we introduce DiffAtlas, a novel generative framework that models both images and masks through diffusion during training, effectively ``GenAI-fying'' atlas-based segmentation. During testing, the model is guided to generate a specific target image-mask pair, from which the corresponding mask is obtained. DiffAtlas retains the robustness of the atlas paradigm while overcoming its scalability and domain-specific limitations. Extensive experiments on CT and MRI across same-domain, cross-modality, varying-domain, and different data-scale settings using the MMWHS and TotalSegmentator datasets demonstrate that our approach outperforms existing methods, particularly in limited-data and zero-shot modality segmentation. Code is available at https://github.com/M3DV/DiffAtlas.",
        "arxiv_id": "2503.06748",
        "ARXIVID": "2503.06748",
        "COMMENT": "Does not match any specific criteria. Focuses on generative segmentation for medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.06564": {
        "authors": [
            "Yihua Shao",
            "Deyang Lin",
            "Fanhu Zeng",
            "Minxi Yan",
            "Muyang Zhang",
            "Siyu Chen",
            "Yuxuan Fan",
            "Ziyang Yan",
            "Haozhe Wang",
            "Jingcai Guo",
            "Yan Wang",
            "Haotong Qin",
            "Hao Tang"
        ],
        "title": "TR-DQ: Time-Rotation Diffusion Quantization",
        "abstract": "arXiv:2503.06564v1 Announce Type: new  Abstract: Diffusion models have been widely adopted in image and video generation. However, their complex network architecture leads to high inference overhead for its generation process. Existing diffusion quantization methods primarily focus on the quantization of the model structure while ignoring the impact of time-steps variation during sampling. At the same time, most current approaches fail to account for significant activations that cannot be eliminated, resulting in substantial performance degradation after quantization. To address these issues, we propose Time-Rotation Diffusion Quantization (TR-DQ), a novel quantization method incorporating time-step and rotation-based optimization. TR-DQ first divides the sampling process based on time-steps and applies a rotation matrix to smooth activations and weights dynamically. For different time-steps, a dedicated hyperparameter is introduced for adaptive timing modeling, which enables dynamic quantization across different time steps. Additionally, we also explore the compression potential of Classifier-Free Guidance (CFG-wise) to establish a foundation for subsequent work. TR-DQ achieves state-of-the-art (SOTA) performance on image generation and video generation tasks and a 1.38-1.89x speedup and 1.97-2.58x memory reduction in inference compared to existing quantization methods.",
        "arxiv_id": "2503.06564",
        "ARXIVID": "2503.06564",
        "COMMENT": "Does not match any specific criteria. Focuses on quantization methods for diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.06364": {
        "authors": [
            "Chen Liu",
            "Tobias Ritschel"
        ],
        "title": "Generative Video Bi-flow",
        "abstract": "arXiv:2503.06364v1 Announce Type: new  Abstract: We propose a novel generative video model by robustly learning temporal change as a neural Ordinary Differential Equation (ODE) flow with a bilinear objective of combining two aspects: The first is to map from the past into future video frames directly. Previous work has mapped the noise to new frames, a more computationally expensive process. Unfortunately, starting from the previous frame, instead of noise, is more prone to drifting errors. Hence, second, we additionally learn how to remove the accumulated errors as the joint objective by adding noise during training. We demonstrate unconditional video generation in a streaming manner for various video datasets, all at competitive quality compared to a baseline conditional diffusion but with higher speed, i.e., fewer ODE solver steps.",
        "arxiv_id": "2503.06364",
        "ARXIVID": "2503.06364",
        "COMMENT": "Does not match any specific criteria. Focuses on generative video modeling with neural ODEs.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.06242": {
        "authors": [
            "{\\L}ukasz Struski",
            "Micha{\\l} B. Bednarczyk",
            "Igor T. Podolak",
            "Jacek Tabor"
        ],
        "title": "LapSum -- One Method to Differentiate Them All: Ranking, Sorting and Top-k Selection",
        "abstract": "arXiv:2503.06242v1 Announce Type: new  Abstract: We present a novel technique for constructing differentiable order-type operations, including soft ranking, soft top-k selection, and soft permutations. Our approach leverages an efficient closed-form formula for the inverse of the function LapSum, defined as the sum of Laplace distributions. This formulation ensures low computational and memory complexity in selecting the highest activations, enabling losses and gradients to be computed in $O(n\\log{}n)$ time. Through extensive experiments, we demonstrate that our method outperforms state-of-the-art techniques for high-dimensional vectors and large $k$ values. Furthermore, we provide efficient implementations for both CPU and CUDA environments, underscoring the practicality and scalability of our method for large-scale ranking and differentiable ordering problems.",
        "arxiv_id": "2503.06242",
        "ARXIVID": "2503.06242",
        "COMMENT": "Does not match any specific criteria. Focuses on differentiable ranking and sorting techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.06378": {
        "authors": [
            "Lexin Zhou",
            "Lorenzo Pacchiardi",
            "Fernando Mart\\'inez-Plumed",
            "Katherine M. Collins",
            "Yael Moros-Daval",
            "Seraphina Zhang",
            "Qinlin Zhao",
            "Yitian Huang",
            "Luning Sun",
            "Jonathan E. Prunty",
            "Zongqian Li",
            "Pablo S\\'anchez-Garc\\'ia",
            "Kexin Jiang Chen",
            "Pablo A. M. Casares",
            "Jiyun Zu",
            "John Burden",
            "Behzad Mehrbakhsh",
            "David Stillwell",
            "Manuel Cebrian",
            "Jindong Wang",
            "Peter Henderson",
            "Sherry Tongshuang Wu",
            "Patrick C. Kyllonen",
            "Lucy Cheke",
            "Xing Xie",
            "Jos\\'e Hern\\'andez-Orallo"
        ],
        "title": "General Scales Unlock AI Evaluation with Explanatory and Predictive Power",
        "abstract": "arXiv:2503.06378v1 Announce Type: new  Abstract: Ensuring safe and effective use of AI requires understanding and anticipating its performance on novel tasks, from advanced scientific challenges to transformed workplace activities. So far, benchmarking has guided progress in AI, but it has offered limited explanatory and predictive power for general-purpose AI systems, given the low transferability across diverse tasks. In this paper, we introduce general scales for AI evaluation that can explain what common AI benchmarks really measure, extract ability profiles of AI systems, and predict their performance for new task instances, in- and out-of-distribution. Our fully-automated methodology builds on 18 newly-crafted rubrics that place instance demands on general scales that do not saturate. Illustrated for 15 large language models and 63 tasks, high explanatory power is unleashed from inspecting the demand and ability profiles, bringing insights on the sensitivity and specificity exhibited by different benchmarks, and how knowledge, metacognition and reasoning are affected by model size, chain-of-thought and distillation. Surprisingly, high predictive power at the instance level becomes possible using these demand levels, providing superior estimates over black-box baseline predictors based on embeddings or finetuning, especially in out-of-distribution settings (new tasks and new benchmarks). The scales, rubrics, battery, techniques and results presented here represent a major step for AI evaluation, underpinning the reliable deployment of AI in the years ahead.",
        "arxiv_id": "2503.06378",
        "ARXIVID": "2503.06378",
        "COMMENT": "Does not match any specific criterion but is relevant to AI evaluation and general-purpose AI systems, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.06641": {
        "authors": [
            "Shipeng Liu",
            "Liang Zhao",
            "Dengfeng Chen"
        ],
        "title": "CLICv2: Image Complexity Representation via Content Invariance Contrastive Learning",
        "abstract": "arXiv:2503.06641v1 Announce Type: new  Abstract: Unsupervised image complexity representation often suffers from bias in positive sample selection and sensitivity to image content. We propose CLICv2, a contrastive learning framework that enforces content invariance for complexity representation. Unlike CLIC, which generates positive samples via cropping-introducing positive pairs bias-our shifted patchify method applies randomized directional shifts to image patches before contrastive learning. Patches at corresponding positions serve as positive pairs, ensuring content-invariant learning. Additionally, we propose patch-wise contrastive loss, which enhances local complexity representation while mitigating content interference. In order to further suppress the interference of image content, we introduce Masked Image Modeling as an auxiliary task, but we set its modeling objective as the entropy of masked patches, which recovers the entropy of the overall image by using the information of the unmasked patches, and then obtains the global complexity perception ability. Extensive experiments on IC9600 demonstrate that CLICv2 significantly outperforms existing unsupervised methods in PCC and SRCC, achieving content-invariant complexity representation without introducing positive pairs bias.",
        "arxiv_id": "2503.06641",
        "ARXIVID": "2503.06641",
        "COMMENT": "Does not match any specific criteria. Focuses on unsupervised image complexity representation, which is not directly related to spatial understanding, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06930": {
        "authors": [
            "Ning Ding",
            "Jing Han",
            "Yuchuan Tian",
            "Chao Xu",
            "Kai Han",
            "Yehui Tang"
        ],
        "title": "Post-Training Quantization for Diffusion Transformer via Hierarchical Timestep Grouping",
        "abstract": "arXiv:2503.06930v1 Announce Type: new  Abstract: Diffusion Transformer (DiT) has now become the preferred choice for building image generation models due to its great generation capability. Unlike previous convolution-based UNet models, DiT is purely composed of a stack of transformer blocks, which renders DiT excellent in scalability like large language models. However, the growing model size and multi-step sampling paradigm bring about considerable pressure on deployment and inference. In this work, we propose a post-training quantization framework tailored for Diffusion Transforms to tackle these challenges. We firstly locate that the quantization difficulty of DiT mainly originates from the time-dependent channel-specific outliers. We propose a timestep-aware shift-and-scale strategy to smooth the activation distribution to reduce the quantization error. Secondly, based on the observation that activations of adjacent timesteps have similar distributions, we utilize a hierarchical clustering scheme to divide the denoising timesteps into multiple groups. We further design a re-parameterization scheme which absorbs the quantization parameters into nearby module to avoid redundant computations. Comprehensive experiments demonstrate that out PTQ method successfully quantize the Diffusion Transformer into 8-bit weight and 8-bit activation (W8A8) with state-of-the-art FiD score. And our method can further quantize DiT model into 4-bit weight and 8-bit activation (W4A8) without sacrificing generation quality.",
        "arxiv_id": "2503.06930",
        "ARXIVID": "2503.06930",
        "COMMENT": "Does not match any specific criteria. Focuses on post-training quantization for diffusion transformers, which is not directly related to spatial understanding, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06901": {
        "authors": [
            "Chikai Shang",
            "Mengke Li",
            "Yiqun Zhang",
            "Zhen Chen",
            "Jinlin Wu",
            "Fangqing Gu",
            "Yang Lu",
            "Yiu-ming Cheung"
        ],
        "title": "Iterative Prompt Relocation for Distribution-Adaptive Visual Prompt Tuning",
        "abstract": "arXiv:2503.06901v1 Announce Type: new  Abstract: Visual prompt tuning (VPT) provides an efficient and effective solution for adapting pre-trained models to various downstream tasks by incorporating learnable prompts. However, most prior art indiscriminately applies a fixed prompt distribution across different tasks, neglecting the importance of each block differing depending on the task. In this paper, we investigate adaptive distribution optimization (ADO) by addressing two key questions: (1) How to appropriately and formally define ADO, and (2) How to design an adaptive distribution strategy guided by this definition? Through in-depth analysis, we provide an affirmative answer that properly adjusting the distribution significantly improves VPT performance, and further uncover a key insight that a nested relationship exists between ADO and VPT. Based on these findings, we propose a new VPT framework, termed PRO-VPT (iterative Prompt RelOcation-based VPT), which adaptively adjusts the distribution building upon a nested optimization formulation. Specifically, we develop a prompt relocation strategy for ADO derived from this formulation, comprising two optimization steps: identifying and pruning idle prompts, followed by determining the optimal blocks for their relocation. By iteratively performing prompt relocation and VPT, our proposal adaptively learns the optimal prompt distribution, thereby unlocking the full potential of VPT. Extensive experiments demonstrate that our proposal significantly outperforms state-of-the-art VPT methods, e.g., PRO-VPT surpasses VPT by 1.6% average accuracy, leading prompt-based methods to state-of-the-art performance on the VTAB-1k benchmark. The code is available at https://github.com/ckshang/PRO-VPT.",
        "arxiv_id": "2503.06901",
        "ARXIVID": "2503.06901",
        "COMMENT": "Does not match any specific criteria but involves visual prompt tuning and optimization, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06517": {
        "authors": [
            "Shinnosuke Matsuo",
            "Riku Togashi",
            "Ryoma Bise",
            "Seiichi Uchida",
            "Masahiro Nomura"
        ],
        "title": "Instance-wise Supervision-level Optimization in Active Learning",
        "abstract": "arXiv:2503.06517v1 Announce Type: new  Abstract: Active learning (AL) is a label-efficient machine learning paradigm that focuses on selectively annotating high-value instances to maximize learning efficiency. Its effectiveness can be further enhanced by incorporating weak supervision, which uses rough yet cost-effective annotations instead of exact (i.e., full) but expensive annotations. We introduce a novel AL framework, Instance-wise Supervision-Level Optimization (ISO), which not only selects the instances to annotate but also determines their optimal annotation level within a fixed annotation budget. Its optimization criterion leverages the value-to-cost ratio (VCR) of each instance while ensuring diversity among the selected instances. In classification experiments, ISO consistently outperforms traditional AL methods and surpasses a state-of-the-art AL approach that combines full and weak supervision, achieving higher accuracy at a lower overall cost. This code is available at https://github.com/matsuo-shinnosuke/ISOAL.",
        "arxiv_id": "2503.06517",
        "ARXIVID": "2503.06517",
        "COMMENT": "Does not match any specific criteria but involves active learning and optimization, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06462": {
        "authors": [
            "Zexu Huang",
            "Min Xu",
            "Stuart Perry"
        ],
        "title": "StructGS: Adaptive Spherical Harmonics and Rendering Enhancements for Superior 3D Gaussian Splatting",
        "abstract": "arXiv:2503.06462v1 Announce Type: new  Abstract: Recent advancements in 3D reconstruction coupled with neural rendering techniques have greatly improved the creation of photo-realistic 3D scenes, influencing both academic research and industry applications. The technique of 3D Gaussian Splatting and its variants incorporate the strengths of both primitive-based and volumetric representations, achieving superior rendering quality. While 3D Geometric Scattering (3DGS) and its variants have advanced the field of 3D representation, they fall short in capturing the stochastic properties of non-local structural information during the training process. Additionally, the initialisation of spherical functions in 3DGS-based methods often fails to engage higher-order terms in early training rounds, leading to unnecessary computational overhead as training progresses. Furthermore, current 3DGS-based approaches require training on higher resolution images to render higher resolution outputs, significantly increasing memory demands and prolonging training durations. We introduce StructGS, a framework that enhances 3D Gaussian Splatting (3DGS) for improved novel-view synthesis in 3D reconstruction. StructGS innovatively incorporates a patch-based SSIM loss, dynamic spherical harmonics initialisation and a Multi-scale Residual Network (MSRN) to address the above-mentioned limitations, respectively. Our framework significantly reduces computational redundancy, enhances detail capture and supports high-resolution rendering from low-resolution inputs. Experimentally, StructGS demonstrates superior performance over state-of-the-art (SOTA) models, achieving higher quality and more detailed renderings with fewer artifacts.",
        "arxiv_id": "2503.06462",
        "ARXIVID": "2503.06462",
        "COMMENT": "Does not match any specific criteria but focuses on 3D reconstruction and rendering, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06437": {
        "authors": [
            "Juhyeon Park",
            "Peter Yongho Kim",
            "Jiook Cha",
            "Shinjae Yoo",
            "Taesup Moon"
        ],
        "title": "SEED: Towards More Accurate Semantic Evaluation for Visual Brain Decoding",
        "abstract": "arXiv:2503.06437v1 Announce Type: new  Abstract: We present SEED (\\textbf{Se}mantic \\textbf{E}valuation for Visual Brain \\textbf{D}ecoding), a novel metric for evaluating the semantic decoding performance of visual brain decoding models. It integrates three complementary metrics, each capturing a different aspect of semantic similarity between images. Using carefully crowd-sourced human judgment data, we demonstrate that SEED achieves the highest alignment with human evaluations, outperforming other widely used metrics. Through the evaluation of existing visual brain decoding models, we further reveal that crucial information is often lost in translation, even in state-of-the-art models that achieve near-perfect scores on existing metrics. To facilitate further research, we open-source the human judgment data, encouraging the development of more advanced evaluation methods for brain decoding models. Additionally, we propose a novel loss function designed to enhance semantic decoding performance by leveraging the order of pairwise cosine similarity in CLIP image embeddings. This loss function is compatible with various existing methods and has been shown to consistently improve their semantic decoding performances when used for training, with respect to both existing metrics and SEED.",
        "arxiv_id": "2503.06437",
        "ARXIVID": "2503.06437",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to vision-language models and evaluation metrics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.07417": {
        "authors": [
            "Minwen Liao",
            "Hao Bo Dong",
            "Xinyi Wang",
            "Ziyang Yan",
            "Yihua Shao"
        ],
        "title": "GM-MoE: Low-Light Enhancement with Gated-Mechanism Mixture-of-Experts",
        "abstract": "arXiv:2503.07417v1 Announce Type: new  Abstract: Low-light enhancement has wide applications in autonomous driving, 3D reconstruction, remote sensing, surveillance, and so on, which can significantly improve information utilization. However, most existing methods lack generalization and are limited to specific tasks such as image recovery. To address these issues, we propose \\textbf{Gated-Mechanism Mixture-of-Experts (GM-MoE)}, the first framework to introduce a mixture-of-experts network for low-light image enhancement. GM-MoE comprises a dynamic gated weight conditioning network and three sub-expert networks, each specializing in a distinct enhancement task. Combining a self-designed gated mechanism that dynamically adjusts the weights of the sub-expert networks for different data domains. Additionally, we integrate local and global feature fusion within sub-expert networks to enhance image quality by capturing multi-scale features. Experimental results demonstrate that the GM-MoE achieves superior generalization with respect to 25 compared approaches, reaching state-of-the-art performance on PSNR on 5 benchmarks and SSIM on 4 benchmarks, respectively.",
        "arxiv_id": "2503.07417",
        "ARXIVID": "2503.07417",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06965": {
        "authors": [
            "Shining Wang",
            "Yunlong Wang",
            "Ruiqi Wu",
            "Bingliang Jiao",
            "Wenxuan Wang",
            "Peng Wang"
        ],
        "title": "SeCap: Self-Calibrating and Adaptive Prompts for Cross-view Person Re-Identification in Aerial-Ground Networks",
        "abstract": "arXiv:2503.06965v1 Announce Type: new  Abstract: When discussing the Aerial-Ground Person Re-identification (AGPReID) task, we face the main challenge of the significant appearance variations caused by different viewpoints, making identity matching difficult. To address this issue, previous methods attempt to reduce the differences between viewpoints by critical attributes and decoupling the viewpoints. While these methods can mitigate viewpoint differences to some extent, they still face two main issues: (1) difficulty in handling viewpoint diversity and (2) neglect of the contribution of local features. To effectively address these challenges, we design and implement the Self-Calibrating and Adaptive Prompt (SeCap) method for the AGPReID task. The core of this framework relies on the Prompt Re-calibration Module (PRM), which adaptively re-calibrates prompts based on the input. Combined with the Local Feature Refinement Module (LFRM), SeCap can extract view-invariant features from local features for AGPReID. Meanwhile, given the current scarcity of datasets in the AGPReID field, we further contribute two real-world Large-scale Aerial-Ground Person Re-Identification datasets, LAGPeR and G2APS-ReID. The former is collected and annotated by us independently, covering $4,231$ unique identities and containing $63,841$ high-quality images; the latter is reconstructed from the person search dataset G2APS. Through extensive experiments on AGPReID datasets, we demonstrate that SeCap is a feasible and effective solution for the AGPReID task. The datasets and source code available on https://github.com/wangshining681/SeCap-AGPReID.",
        "arxiv_id": "2503.06965",
        "ARXIVID": "2503.06965",
        "COMMENT": "Does not match any specific criterion but is tangentially related to multi-modal learning and person re-identification.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.07077": {
        "authors": [
            "Zhaoqi Dong",
            "Zhinan Wang",
            "Quanqi Zheng",
            "Bin Xu",
            "Lei Chen",
            "Jinhu Lv"
        ],
        "title": "Rule-Based Conflict-Free Decision Framework in Swarm Confrontation",
        "abstract": "arXiv:2503.07077v1 Announce Type: new  Abstract: Traditional rule--based decision--making methods with interpretable advantage, such as finite state machine, suffer from the jitter or deadlock(JoD) problems in extremely dynamic scenarios. To realize agent swarm confrontation, decision conflicts causing many JoD problems are a key issue to be solved. Here, we propose a novel decision--making framework that integrates probabilistic finite state machine, deep convolutional networks, and reinforcement learning to implement interpretable intelligence into agents. Our framework overcomes state machine instability and JoD problems, ensuring reliable and adaptable decisions in swarm confrontation. The proposed approach demonstrates effective performance via enhanced human--like cooperation and competitive strategies in the rigorous evaluation of real experiments, outperforming other methods.",
        "arxiv_id": "2503.07077",
        "ARXIVID": "2503.07077",
        "COMMENT": "Does not match any specific criterion but is tangentially related to decision-making in embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.07082": {
        "authors": [
            "Spyros Kondylatos",
            "Nikolaos Ioannis Bountos",
            "Dimitrios Michail",
            "Xiao Xiang Zhu",
            "Gustau Camps-Valls",
            "Ioannis Papoutsis"
        ],
        "title": "On the Generalization of Representation Uncertainty in Earth Observation",
        "abstract": "arXiv:2503.07082v1 Announce Type: new  Abstract: Recent advances in Computer Vision have introduced the concept of pretrained representation uncertainty, enabling zero-shot uncertainty estimation. This holds significant potential for Earth Observation (EO), where trustworthiness is critical, yet the complexity of EO data poses challenges to uncertainty-aware methods. In this work, we investigate the generalization of representation uncertainty in EO, considering the domain's unique semantic characteristics. We pretrain uncertainties on large EO datasets and propose an evaluation framework to assess their zero-shot performance in multi-label classification and segmentation EO tasks. Our findings reveal that, unlike uncertainties pretrained on natural images, EO-pretraining exhibits strong generalization across unseen EO domains, geographic locations, and target granularities, while maintaining sensitivity to variations in ground sampling distance. We demonstrate the practical utility of pretrained uncertainties showcasing their alignment with task-specific uncertainties in downstream tasks, their sensitivity to real-world EO image noise, and their ability to generate spatial uncertainty estimates out-of-the-box. Initiating the discussion on representation uncertainty in EO, our study provides insights into its strengths and limitations, paving the way for future research in the field. Code and weights are available at: https://github.com/Orion-AI-Lab/EOUncertaintyGeneralization.",
        "arxiv_id": "2503.07082",
        "ARXIVID": "2503.07082",
        "COMMENT": "Does not match any specific criterion but is tangentially related to computer vision and uncertainty modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06860": {
        "authors": [
            "Cagri Gungor",
            "Derek Eppinger",
            "Adriana Kovashka"
        ],
        "title": "Towards Generalization of Tactile Image Generation: Reference-Free Evaluation in a Leakage-Free Setting",
        "abstract": "arXiv:2503.06860v1 Announce Type: new  Abstract: Tactile sensing, which relies on direct physical contact, is critical for human perception and underpins applications in computer vision, robotics, and multimodal learning. Because tactile data is often scarce and costly to acquire, generating synthetic tactile images provides a scalable solution to augment real-world measurements. However, ensuring robust generalization in synthesizing tactile images-capturing subtle, material-specific contact features-remains challenging. We demonstrate that overlapping training and test samples in commonly used datasets inflate performance metrics, obscuring the true generalizability of tactile models. To address this, we propose a leakage-free evaluation protocol coupled with novel, reference-free metrics-TMMD, I-TMMD, CI-TMMD, and D-TMMD-tailored for tactile generation. Moreover, we propose a vision-to-touch generation method that leverages text as an intermediate modality by incorporating concise, material-specific descriptions during training to better capture essential tactile features. Experiments on two popular visuo-tactile datasets, Touch and Go and HCT, show that our approach achieves superior performance and enhanced generalization in a leakage-free setting.",
        "arxiv_id": "2503.06860",
        "ARXIVID": "2503.06860",
        "COMMENT": "Does not match any specific criterion but is tangentially related to multi-modal learning and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06951": {
        "authors": [
            "Zhao Xinjie",
            "Fan Gao",
            "Rui Yang",
            "Yingjian Chen",
            "Yuyang Wang",
            "Ying Zhu",
            "Jiacheng Tang",
            "Irene Li"
        ],
        "title": "ReAgent: Reversible Multi-Agent Reasoning for Knowledge-Enhanced Multi-Hop QA",
        "abstract": "arXiv:2503.06951v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) have significantly improved multi-hop question answering (QA) through direct Chain-of-Thought (CoT) reasoning. However, the irreversible nature of CoT leads to error accumulation, making it challenging to correct mistakes in multi-hop reasoning. This paper introduces ReAgent: a Reversible multi-Agent collaborative framework augmented with explicit backtracking mechanisms, enabling reversible multi-hop reasoning. By incorporating text-based retrieval, information aggregation and validation, our system can detect and correct errors mid-reasoning, leading to more robust and interpretable QA outcomes. The framework and experiments serve as a foundation for future work on error-tolerant QA systems. Empirical evaluations across three benchmarks indicate ReAgent's efficacy, yielding average about 6\\% improvements against baseline models.",
        "arxiv_id": "2503.06951",
        "ARXIVID": "2503.06951",
        "COMMENT": "Does not match any specific criteria but is related to multi-hop reasoning and QA systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.07029": {
        "authors": [
            "Dong-Hee Paek",
            "Seung-Hyun Kong"
        ],
        "title": "Availability-aware Sensor Fusion via Unified Canonical Space for 4D Radar, LiDAR, and Camera",
        "abstract": "arXiv:2503.07029v1 Announce Type: new  Abstract: Sensor fusion of camera, LiDAR, and 4-dimensional (4D) Radar has brought a significant performance improvement in autonomous driving (AD). However, there still exist fundamental challenges: deeply coupled fusion methods assume continuous sensor availability, making them vulnerable to sensor degradation and failure, whereas sensor-wise cross-attention fusion methods struggle with computational cost and unified feature representation. This paper presents availability-aware sensor fusion (ASF), a novel method that employs unified canonical projection (UCP) to enable consistency in all sensor features for fusion and cross-attention across sensors along patches (CASAP) to enhance robustness of sensor fusion against sensor degradation and failure. As a result, the proposed ASF shows a superior object detection performance to the existing state-of-the-art fusion methods under various weather and sensor degradation (or failure) conditions; Extensive experiments on the K-Radar dataset demonstrate that ASF achieves improvements of 9.7% in AP BEV (87.2%) and 20.1% in AP 3D (73.6%) in object detection at IoU=0.5, while requiring a low computational cost. The code will be available at https://github.com/kaist-avelab/K-Radar.",
        "arxiv_id": "2503.07029",
        "ARXIVID": "2503.07029",
        "COMMENT": "Does not match any specific criteria but is related to sensor fusion for autonomous driving.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06485": {
        "authors": [
            "Jiajie Fan",
            "Amal Trigui",
            "Andrea Bonfanti",
            "Felix Dietrich",
            "Thomas B\\\"ack",
            "Hao Wang"
        ],
        "title": "A Mesh Is Worth 512 Numbers: Spectral-domain Diffusion Modeling for High-dimension Shape Generation",
        "abstract": "arXiv:2503.06485v1 Announce Type: new  Abstract: Recent advancements in learning latent codes derived from high-dimensional shapes have demonstrated impressive outcomes in 3D generative modeling. Traditionally, these approaches employ a trained autoencoder to acquire a continuous implicit representation of source shapes, which can be computationally expensive. This paper introduces a novel framework, spectral-domain diffusion for high-quality shape generation SpoDify, that utilizes singular value decomposition (SVD) for shape encoding. The resulting eigenvectors can be stored for subsequent decoding, while generative modeling is performed on the eigenfeatures. This approach efficiently encodes complex meshes into continuous implicit representations, such as encoding a 15k-vertex mesh to a 512-dimensional latent code without learning. Our method exhibits significant advantages in scenarios with limited samples or GPU resources. In mesh generation tasks, our approach produces high-quality shapes that are comparable to state-of-the-art methods.",
        "arxiv_id": "2503.06485",
        "ARXIVID": "2503.06485",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling for 3D shapes.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.07363": {
        "authors": [
            "Jiang Lin",
            "Zili Yi"
        ],
        "title": "Inversion-Free Video Style Transfer with Trajectory Reset Attention Control and Content-Style Bridging",
        "abstract": "arXiv:2503.07363v1 Announce Type: new  Abstract: Video style transfer aims to alter the style of a video while preserving its content. Previous methods often struggle with content leakage and style misalignment, particularly when using image-driven approaches that aim to transfer precise styles. In this work, we introduce Trajectory Reset Attention Control (TRAC), a novel method that allows for high-quality style transfer while preserving content integrity. TRAC operates by resetting the denoising trajectory and enforcing attention control, thus enhancing content consistency while significantly reducing the computational costs against inversion-based methods. Additionally, a concept termed Style Medium is introduced to bridge the gap between content and style, enabling a more precise and harmonious transfer of stylistic elements. Building upon these concepts, we present a tuning-free framework that offers a stable, flexible, and efficient solution for both image and video style transfer. Experimental results demonstrate that our proposed framework accommodates a wide range of stylized outputs, from precise content preservation to the production of visually striking results with vibrant and expressive styles.",
        "arxiv_id": "2503.07363",
        "ARXIVID": "2503.07363",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling in video style transfer.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06545": {
        "authors": [
            "Junyi Wu",
            "Zhiteng Li",
            "Zheng Hui",
            "Yulun Zhang",
            "Linghe Kong",
            "Xiaokang Yang"
        ],
        "title": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical Latent and Layer Caching for Video Generation",
        "abstract": "arXiv:2503.06545v1 Announce Type: new  Abstract: Recently, Diffusion Transformers (DiTs) have emerged as a dominant architecture in video generation, surpassing U-Net-based models in terms of performance. However, the enhanced capabilities of DiTs come with significant drawbacks, including increased computational and memory costs, which hinder their deployment on resource-constrained devices. Current acceleration techniques, such as quantization and cache mechanism, offer limited speedup and are often applied in isolation, failing to fully address the complexities of DiT architectures. In this paper, we propose QuantCache, a novel training-free inference acceleration framework that jointly optimizes hierarchical latent caching, adaptive importance-guided quantization, and structural redundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of 6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive experiments across multiple video generation benchmarks demonstrate the effectiveness of our method, setting a new standard for efficient DiT inference. The code and models will be available at https://github.com/JunyiWuCode/QuantCache.",
        "arxiv_id": "2503.06545",
        "ARXIVID": "2503.06545",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in video generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.07249": {
        "authors": [
            "Feng Huang",
            "Shuyuan Zheng",
            "Zhaobing Qiu",
            "Huanxian Liu",
            "Huanxin Bai",
            "Liqiong Chen"
        ],
        "title": "Text-IRSTD: Leveraging Semantic Text to Promote Infrared Small Target Detection in Complex Scenes",
        "abstract": "arXiv:2503.07249v1 Announce Type: new  Abstract: Infrared small target detection is currently a hot and challenging task in computer vision. Existing methods usually focus on mining visual features of targets, which struggles to cope with complex and diverse detection scenarios. The main reason is that infrared small targets have limited image information on their own, thus relying only on visual features fails to discriminate targets and interferences, leading to lower detection performance. To address this issue, we introduce a novel approach leveraging semantic text to guide infrared small target detection, called Text-IRSTD. It innovatively expands classical IRSTD to text-guided IRSTD, providing a new research idea. On the one hand, we devise a novel fuzzy semantic text prompt to accommodate ambiguous target categories. On the other hand, we propose a progressive cross-modal semantic interaction decoder (PCSID) to facilitate information fusion between texts and images. In addition, we construct a new benchmark consisting of 2,755 infrared images of different scenarios with fuzzy semantic textual annotations, called FZDT. Extensive experimental results demonstrate that our method achieves better detection performance and target contour recovery than the state-of-the-art methods. Moreover, proposed Text-IRSTD shows strong generalization and wide application prospects in unseen detection scenarios. The dataset and code will be publicly released after acceptance of this paper.",
        "arxiv_id": "2503.07249",
        "ARXIVID": "2503.07249",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and multi-modal learning with text-guided infrared detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06746": {
        "authors": [
            "Ka Chun Shum",
            "Binh-Son Hua",
            "Duc Thanh Nguyen",
            "Sai-Kit Yeung"
        ],
        "title": "Color Alignment in Diffusion",
        "abstract": "arXiv:2503.06746v1 Announce Type: new  Abstract: Diffusion models have shown great promise in synthesizing visually appealing images. However, it remains challenging to condition the synthesis at a fine-grained level, for instance, synthesizing image pixels following some generic color pattern. Existing image synthesis methods often produce contents that fall outside the desired pixel conditions. To address this, we introduce a novel color alignment algorithm that confines the generative process in diffusion models within a given color pattern. Specifically, we project diffusion terms, either imagery samples or latent representations, into a conditional color space to align with the input color distribution. This strategy simplifies the prediction in diffusion models within a color manifold while still allowing plausible structures in generated contents, thus enabling the generation of diverse contents that comply with the target color pattern. Experimental results demonstrate our state-of-the-art performance in conditioning and controlling of color pixels, while maintaining on-par generation quality and diversity in comparison with regular diffusion models.",
        "arxiv_id": "2503.06746",
        "ARXIVID": "2503.06746",
        "COMMENT": "Does not match any specific criteria. Focuses on color alignment in diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.07047": {
        "authors": [
            "Yongle Zhang",
            "Yimin Liu",
            "Qiang Wu"
        ],
        "title": "Recovering Partially Corrupted Major Objects through Tri-modality Based Image Completion",
        "abstract": "arXiv:2503.07047v1 Announce Type: new  Abstract: Diffusion models have become widely adopted in image completion tasks, with text prompts commonly employed to ensure semantic coherence by providing high-level guidance. However, a persistent challenge arises when an object is partially obscured in the damaged region, yet its remaining parts are still visible in the background. While text prompts offer semantic direction, they often fail to precisely recover fine-grained structural details, such as the object's overall posture, ensuring alignment with the visible object information in the background. This limitation stems from the inability of text prompts to provide pixel-level specificity. To address this, we propose supplementing text-based guidance with a novel visual aid: a casual sketch, which can be roughly drawn by anyone based on visible object parts. This sketch supplies critical structural cues, enabling the generative model to produce an object structure that seamlessly integrates with the existing background. We introduce the Visual Sketch Self-Aware (VSSA) model, which integrates the casual sketch into each iterative step of the diffusion process, offering distinct advantages for partially corrupted scenarios. By blending sketch-derived features with those of the corrupted image, and leveraging text prompt guidance, the VSSA assists the diffusion model in generating images that preserve both the intended object semantics and structural consistency across the restored objects and original regions. To support this research, we created two datasets, CUB-sketch and MSCOCO-sketch, each combining images, sketches, and text. Extensive qualitative and quantitative experiments demonstrate that our approach outperforms several state-of-the-art methods.",
        "arxiv_id": "2503.07047",
        "ARXIVID": "2503.07047",
        "COMMENT": "Does not match any specific criteria. Focuses on image completion with tri-modality guidance.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.07429": {
        "authors": [
            "Jaewook Lee",
            "Jeongah Lee",
            "Wanyong Feng",
            "Andrew Lan"
        ],
        "title": "From Text to Visuals: Using LLMs to Generate Math Diagrams with Vector Graphics",
        "abstract": "arXiv:2503.07429v1 Announce Type: new  Abstract: Advances in large language models (LLMs) offer new possibilities for enhancing math education by automating support for both teachers and students. While prior work has focused on generating math problems and high-quality distractors, the role of visualization in math learning remains under-explored. Diagrams are essential for mathematical thinking and problem-solving, yet manually creating them is time-consuming and requires domain-specific expertise, limiting scalability. Recent research on using LLMs to generate Scalable Vector Graphics (SVG) presents a promising approach to automating diagram creation. Unlike pixel-based images, SVGs represent geometric figures using XML, allowing seamless scaling and adaptability. Educational platforms such as Khan Academy and IXL already use SVGs to display math problems and hints. In this paper, we explore the use of LLMs to generate math-related diagrams that accompany textual hints via intermediate SVG representations. We address three research questions: (1) how to automatically generate math diagrams in problem-solving hints and evaluate their quality, (2) whether SVG is an effective intermediate representation for math diagrams, and (3) what prompting strategies and formats are required for LLMs to generate accurate SVG-based diagrams. Our contributions include defining the task of automatically generating SVG-based diagrams for math hints, developing an LLM prompting-based pipeline, and identifying key strategies for improving diagram generation. Additionally, we introduce a Visual Question Answering-based evaluation setup and conduct ablation studies to assess different pipeline variations. By automating the math diagram creation, we aim to provide students and teachers with accurate, conceptually relevant visual aids that enhance problem-solving and learning experiences.",
        "arxiv_id": "2503.07429",
        "ARXIVID": "2503.07429",
        "COMMENT": "Does not match any specific criteria. Focuses on generating math diagrams using LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06201": {
        "authors": [
            "Yixin Wu",
            "Feiran Zhang",
            "Tianyuan Shi",
            "Ruicheng Yin",
            "Zhenghua Wang",
            "Zhenliang Gan",
            "Xiaohua Wang",
            "Changze Lv",
            "Xiaoqing Zheng",
            "Xuanjing Huang"
        ],
        "title": "Explainable Synthetic Image Detection through Diffusion Timestep Ensembling",
        "abstract": "arXiv:2503.06201v1 Announce Type: new  Abstract: Recent advances in diffusion models have enabled the creation of deceptively real images, posing significant security risks when misused. In this study, we reveal that natural and synthetic images exhibit distinct differences in the high-frequency domains of their Fourier power spectra after undergoing iterative noise perturbations through an inverse multi-step denoising process, suggesting that such noise can provide additional discriminative information for identifying synthetic images. Based on this observation, we propose a novel detection method that amplifies these differences by progressively adding noise to the original images across multiple timesteps, and train an ensemble of classifiers on these noised images. To enhance human comprehension, we introduce an explanation generation and refinement module to identify flaws located in AI-generated images. Additionally, we construct two new datasets, GenHard and GenExplain, derived from the GenImage benchmark, providing detection samples of greater difficulty and high-quality rationales for fake images. Extensive experiments show that our method achieves state-of-the-art performance with 98.91% and 95.89% detection accuracy on regular and harder samples, increasing a minimal of 2.51% and 3.46% compared to baselines. Furthermore, our method also generalizes effectively to images generated by other diffusion models. Our code and datasets will be made publicly available.",
        "arxiv_id": "2503.06201",
        "ARXIVID": "2503.06201",
        "COMMENT": "Does not match any specific criteria. Focuses on synthetic image detection using diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.07026": {
        "authors": [
            "Yi Liu",
            "Hao Zhou",
            "Wenxiang Shang",
            "Ran Lin",
            "Benlei Cui"
        ],
        "title": "Erase Diffusion: Empowering Object Removal Through Calibrating Diffusion Pathways",
        "abstract": "arXiv:2503.07026v1 Announce Type: new  Abstract: Erase inpainting, or object removal, aims to precisely remove target objects within masked regions while preserving the overall consistency of the surrounding content. Despite diffusion-based methods have made significant strides in the field of image inpainting, challenges remain regarding the emergence of unexpected objects or artifacts. We assert that the inexact diffusion pathways established by existing standard optimization paradigms constrain the efficacy of object removal. To tackle these challenges, we propose a novel Erase Diffusion, termed EraDiff, aimed at unleashing the potential power of standard diffusion in the context of object removal. In contrast to standard diffusion, the EraDiff adapts both the optimization paradigm and the network to improve the coherence and elimination of the erasure results. We first introduce a Chain-Rectifying Optimization (CRO) paradigm, a sophisticated diffusion process specifically designed to align with the objectives of erasure. This paradigm establishes innovative diffusion transition pathways that simulate the gradual elimination of objects during optimization, allowing the model to accurately capture the intent of object removal. Furthermore, to mitigate deviations caused by artifacts during the sampling pathways, we develop a simple yet effective Self-Rectifying Attention (SRA) mechanism. The SRA calibrates the sampling pathways by altering self-attention activation, allowing the model to effectively bypass artifacts while further enhancing the coherence of the generated content. With this design, our proposed EraDiff achieves state-of-the-art performance on the OpenImages V5 dataset and demonstrates significant superiority in real-world scenarios.",
        "arxiv_id": "2503.07026",
        "ARXIVID": "2503.07026",
        "COMMENT": "Does not match any specific criteria but is related to diffusion-based object removal, which is tangentially relevant to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06559": {
        "authors": [
            "Yuzheng Wang",
            "Zhaoyu Chen",
            "Dingkang Yang",
            "Yuanhang Wang",
            "Lizhe Qi"
        ],
        "title": "MMARD: Improving the Min-Max Optimization Process in Adversarial Robustness Distillation",
        "abstract": "arXiv:2503.06559v1 Announce Type: new  Abstract: Adversarial Robustness Distillation (ARD) is a promising task to boost the robustness of small-capacity models with the guidance of the pre-trained robust teacher. The ARD can be summarized as a min-max optimization process, i.e., synthesizing adversarial examples (inner) & training the student (outer). Although competitive robustness performance, existing ARD methods still have issues. In the inner process, the synthetic training examples are far from the teacher's decision boundary leading to important robust information missing. In the outer process, the student model is decoupled from learning natural and robust scenarios, leading to the robustness saturation, i.e., student performance is highly susceptible to customized teacher selection. To tackle these issues, this paper proposes a general Min-Max optimization Adversarial Robustness Distillation (MMARD) method. For the inner process, we introduce the teacher's robust predictions, which drive the training examples closer to the teacher's decision boundary to explore more robust knowledge. For the outer process, we propose a structured information modeling method based on triangular relationships to measure the mutual information of the model in natural and robust scenarios and enhance the model's ability to understand multi-scenario mapping relationships. Experiments show our MMARD achieves state-of-the-art performance on multiple benchmarks. Besides, MMARD is plug-and-play and convenient to combine with existing methods.",
        "arxiv_id": "2503.06559",
        "ARXIVID": "2503.06559",
        "COMMENT": "Does not match any specific criteria but is related to adversarial robustness distillation, which is tangentially relevant to machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.07232": {
        "authors": [
            "Chenglu Pan",
            "Xiaogang Xu",
            "Ganggui Ding",
            "Yunke Zhang",
            "Wenbo Li",
            "Jiarong Xu",
            "Qingbiao Wu"
        ],
        "title": "Boosting Diffusion-Based Text Image Super-Resolution Model Towards Generalized Real-World Scenarios",
        "abstract": "arXiv:2503.07232v1 Announce Type: new  Abstract: Restoring low-resolution text images presents a significant challenge, as it requires maintaining both the fidelity and stylistic realism of the text in restored images. Existing text image restoration methods often fall short in hard situations, as the traditional super-resolution models cannot guarantee clarity, while diffusion-based methods fail to maintain fidelity. In this paper, we introduce a novel framework aimed at improving the generalization ability of diffusion models for text image super-resolution (SR), especially promoting fidelity. First, we propose a progressive data sampling strategy that incorporates diverse image types at different stages of training, stabilizing the convergence and improving the generalization. For the network architecture, we leverage a pre-trained SR prior to provide robust spatial reasoning capabilities, enhancing the model's ability to preserve textual information. Additionally, we employ a cross-attention mechanism to better integrate textual priors. To further reduce errors in textual priors, we utilize confidence scores to dynamically adjust the importance of textual features during training. Extensive experiments on real-world datasets demonstrate that our approach not only produces text images with more realistic visual appearances but also improves the accuracy of text structure.",
        "arxiv_id": "2503.07232",
        "ARXIVID": "2503.07232",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and spatial reasoning in text image super-resolution.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06617": {
        "authors": [
            "Long Peng",
            "Anran Wu",
            "Wenbo Li",
            "Peizhe Xia",
            "Xueyuan Dai",
            "Xinjie Zhang",
            "Xin Di",
            "Haoze Sun",
            "Renjing Pei",
            "Yang Wang",
            "Yang Cao",
            "Zheng-Jun Zha"
        ],
        "title": "Pixel to Gaussian: Ultra-Fast Continuous Super-Resolution with 2D Gaussian Modeling",
        "abstract": "arXiv:2503.06617v1 Announce Type: new  Abstract: Arbitrary-scale super-resolution (ASSR) aims to reconstruct high-resolution (HR) images from low-resolution (LR) inputs with arbitrary upsampling factors using a single model, addressing the limitations of traditional SR methods constrained to fixed-scale factors (\\textit{e.g.}, $\\times$ 2). Recent advances leveraging implicit neural representation (INR) have achieved great progress by modeling coordinate-to-pixel mappings. However, the efficiency of these methods may suffer from repeated upsampling and decoding, while their reconstruction fidelity and quality are constrained by the intrinsic representational limitations of coordinate-based functions. To address these challenges, we propose a novel ContinuousSR framework with a Pixel-to-Gaussian paradigm, which explicitly reconstructs 2D continuous HR signals from LR images using Gaussian Splatting. This approach eliminates the need for time-consuming upsampling and decoding, enabling extremely fast arbitrary-scale super-resolution. Once the Gaussian field is built in a single pass, ContinuousSR can perform arbitrary-scale rendering in just 1ms per scale. Our method introduces several key innovations. Through statistical ana",
        "arxiv_id": "2503.06617",
        "ARXIVID": "2503.06617",
        "COMMENT": "Does not match any specific criterion but is related to super-resolution and efficient modeling, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06397": {
        "authors": [
            "Yanyu Zhu",
            "Licheng Bai",
            "Jintao Xu",
            "Jiwei Tang",
            "Hai-tao Zheng"
        ],
        "title": "Removing Averaging: Personalized Lip-Sync Driven Characters Based on Identity Adapter",
        "abstract": "arXiv:2503.06397v1 Announce Type: new  Abstract: Recent advances in diffusion-based lip-syncing generative models have demonstrated their ability to produce highly synchronized talking face videos for visual dubbing. Although these models excel at lip synchronization, they often struggle to maintain fine-grained control over facial details in generated images. In this work, we identify \"lip averaging\" phenomenon where the model fails to preserve subtle facial details when dubbing unseen in-the-wild videos. This issue arises because the commonly used UNet backbone primarily integrates audio features into visual representations in the latent space via cross-attention mechanisms and multi-scale fusion, but it struggles to retain fine-grained lip details in the generated faces. To address this issue, we propose UnAvgLip, which extracts identity embeddings from reference videos to generate highly faithful facial sequences while maintaining accurate lip synchronization. Specifically, our method comprises two primary components: (1) an Identity Perceiver module that encodes facial embeddings to align with conditioned audio features; and (2) an ID-CrossAttn module that injects facial embeddings into the generation process, enhancing model's capability of identity retention. Extensive experiments demonstrate that, at a modest training and inference cost, UnAvgLip effectively mitigates the \"averaging\" phenomenon in lip inpainting, significantly preserving unique facial characteristics while maintaining precise lip synchronization. Compared with the original approach, our method demonstrates significant improvements of 5% on the identity consistency metric and 2% on the SSIM metric across two benchmark datasets (HDTF and LRW).",
        "arxiv_id": "2503.06397",
        "ARXIVID": "2503.06397",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling in lip-syncing, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06202": {
        "authors": [
            "Wei Liu",
            "Zhiying Deng",
            "Zhongyu Niu",
            "Jun Wang",
            "Haozhao Wang",
            "Zhigang Zeng",
            "Ruixuan Li"
        ],
        "title": "Breaking Free from MMI: A New Frontier in Rationalization by Probing Input Utilization",
        "abstract": "arXiv:2503.06202v1 Announce Type: new  Abstract: Extracting a small subset of crucial rationales from the full input is a key problem in explainability research. The most widely used fundamental criterion for rationale extraction is the maximum mutual information (MMI) criterion. In this paper, we first demonstrate that MMI suffers from diminishing marginal returns. Once part of the rationale has been identified, finding the remaining portions contributes only marginally to increasing the mutual information, making it difficult to use MMI to locate the rest. In contrast to MMI that aims to reproduce the prediction, we seek to identify the parts of the input that the network can actually utilize.   This is achieved by comparing how different rationale candidates match the capability space of the weight matrix. The weight matrix of a neural network is typically low-rank, meaning that the linear combinations of its column vectors can only cover part of the directions in a high-dimensional space (high-dimension: the dimensions of an input vector). If an input is fully utilized by the network, {it generally matches these directions (e.g., a portion of a hypersphere), resulting in a representation with a high norm. Conversely, if an input primarily falls outside (orthogonal to) these directions}, its representation norm will approach zero, behaving like noise that the network cannot effectively utilize. Building on this, we propose using the norms of rationale candidates as an alternative objective to MMI. Through experiments on four text classification datasets and one graph classification dataset using three network architectures (GRUs, BERT, and GCN), we show that our method outperforms MMI and its improved variants in identifying better rationales. We also compare our method with a representative LLM (llama-3.1-8b-instruct) and find that our simple method gets comparable results to it and can sometimes even outperform it.",
        "arxiv_id": "2503.06202",
        "ARXIVID": "2503.06202",
        "COMMENT": "Does not match any specific criteria. Focuses on rationale extraction and explainability in neural networks, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.07602": {
        "authors": [
            "Yujie Wei",
            "Shiwei Zhang",
            "Hangjie Yuan",
            "Biao Gong",
            "Longxiang Tang",
            "Xiang Wang",
            "Haonan Qiu",
            "Hengjia Li",
            "Shuai Tan",
            "Yingya Zhang",
            "Hongming Shan"
        ],
        "title": "DreamRelation: Relation-Centric Video Customization",
        "abstract": "arXiv:2503.07602v1 Announce Type: new  Abstract: Relational video customization refers to the creation of personalized videos that depict user-specified relations between two subjects, a crucial task for comprehending real-world visual content. While existing methods can personalize subject appearances and motions, they still struggle with complex relational video customization, where precise relational modeling and high generalization across subject categories are essential. The primary challenge arises from the intricate spatial arrangements, layout variations, and nuanced temporal dynamics inherent in relations; consequently, current models tend to overemphasize irrelevant visual details rather than capturing meaningful interactions. To address these challenges, we propose DreamRelation, a novel approach that personalizes relations through a small set of exemplar videos, leveraging two key components: Relational Decoupling Learning and Relational Dynamics Enhancement. First, in Relational Decoupling Learning, we disentangle relations from subject appearances using relation LoRA triplet and hybrid mask training strategy, ensuring better generalization across diverse relationships. Furthermore, we determine the optimal design of relation LoRA triplet by analyzing the distinct roles of the query, key, and value features within MM-DiT's attention mechanism, making DreamRelation the first relational video generation framework with explainable components. Second, in Relational Dynamics Enhancement, we introduce space-time relational contrastive loss, which prioritizes relational dynamics while minimizing the reliance on detailed subject appearances. Extensive experiments demonstrate that DreamRelation outperforms state-of-the-art methods in relational video customization. Code and models will be made publicly available.",
        "arxiv_id": "2503.07602",
        "ARXIVID": "2503.07602",
        "COMMENT": "Does not match any specific criteria. Focuses on relational video customization, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.07389": {
        "authors": [
            "Ruidong Chen",
            "Honglin Guo",
            "Lanjun Wang",
            "Chenyu Zhang",
            "Weizhi Nie",
            "An-An Liu"
        ],
        "title": "TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image Diffusion Models",
        "abstract": "arXiv:2503.07389v1 Announce Type: new  Abstract: Recent advances in text-to-image diffusion models enable photorealistic image generation, but they also risk producing malicious content, such as NSFW images. To mitigate risk, concept erasure methods are studied to facilitate the model to unlearn specific concepts. However, current studies struggle to fully erase malicious concepts implicitly embedded in prompts (e.g., metaphorical expressions or adversarial prompts) while preserving the model's normal generation capability. To address this challenge, our study proposes TRCE, using a two-stage concept erasure strategy to achieve an effective trade-off between reliable erasure and knowledge preservation. Firstly, TRCE starts by erasing the malicious semantics implicitly embedded in textual prompts. By identifying a critical mapping objective(i.e., the [EoT] embedding), we optimize the cross-attention layers to map malicious prompts to contextually similar prompts but with safe concepts. This step prevents the model from being overly influenced by malicious semantics during the denoising process. Following this, considering the deterministic properties of the sampling trajectory of the diffusion model, TRCE further steers the early denoising prediction toward the safe direction and away from the unsafe one through contrastive learning, thus further avoiding the generation of malicious content. Finally, we conduct comprehensive evaluations of TRCE on multiple malicious concept erasure benchmarks, and the results demonstrate its effectiveness in erasing malicious concepts while better preserving the model's original generation ability. The code is available at: http://github.com/ddgoodgood/TRCE. CAUTION: This paper includes model-generated content that may contain offensive material.",
        "arxiv_id": "2503.07389",
        "ARXIVID": "2503.07389",
        "COMMENT": "Does not match any specific criteria. Focuses on malicious concept erasure in text-to-image diffusion models, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06277": {
        "authors": [
            "Siyi Du",
            "Xinzhe Luo",
            "Declan P. O'Regan",
            "Chen Qin"
        ],
        "title": "STiL: Semi-supervised Tabular-Image Learning for Comprehensive Task-Relevant Information Exploration in Multimodal Classification",
        "abstract": "arXiv:2503.06277v1 Announce Type: new  Abstract: Multimodal image-tabular learning is gaining attention, yet it faces challenges due to limited labeled data. While earlier work has applied self-supervised learning (SSL) to unlabeled data, its task-agnostic nature often results in learning suboptimal features for downstream tasks. Semi-supervised learning (SemiSL), which combines labeled and unlabeled data, offers a promising solution. However, existing multimodal SemiSL methods typically focus on unimodal or modality-shared features, ignoring valuable task-relevant modality-specific information, leading to a Modality Information Gap. In this paper, we propose STiL, a novel SemiSL tabular-image framework that addresses this gap by comprehensively exploring task-relevant information. STiL features a new disentangled contrastive consistency module to learn cross-modal invariant representations of shared information while retaining modality-specific information via disentanglement. We also propose a novel consensus-guided pseudo-labeling strategy to generate reliable pseudo-labels based on classifier consensus, along with a new prototype-guided label smoothing technique to refine pseudo-label quality with prototype embeddings, thereby enhancing task-relevant information learning in unlabeled data. Experiments on natural and medical image datasets show that STiL outperforms the state-of-the-art supervised/SSL/SemiSL image/multimodal approaches. Our code is publicly available.",
        "arxiv_id": "2503.06277",
        "ARXIVID": "2503.06277",
        "COMMENT": "Does not match any specific criteria. Focuses on semi-supervised learning for multimodal tabular-image data, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06442": {
        "authors": [
            "Yu Liu",
            "Hao Tang",
            "Haiqi Zhang",
            "Jing Qin",
            "Zechao Li"
        ],
        "title": "OT-DETECTOR: Delving into Optimal Transport for Zero-shot Out-of-Distribution Detection",
        "abstract": "arXiv:2503.06442v1 Announce Type: new  Abstract: Out-of-distribution (OOD) detection is crucial for ensuring the reliability and safety of machine learning models in real-world applications. While zero-shot OOD detection, which requires no training on in-distribution (ID) data, has become feasible with the emergence of vision-language models like CLIP, existing methods primarily focus on semantic matching and fail to fully capture distributional discrepancies. To address these limitations, we propose OT-DETECTOR, a novel framework that employs Optimal Transport (OT) to quantify both semantic and distributional discrepancies between test samples and ID labels. Specifically, we introduce cross-modal transport mass and transport cost as semantic-wise and distribution-wise OOD scores, respectively, enabling more robust detection of OOD samples. Additionally, we present a semantic-aware content refinement (SaCR) module, which utilizes semantic cues from ID labels to amplify the distributional discrepancy between ID and hard OOD samples. Extensive experiments on several benchmarks demonstrate that OT-DETECTOR achieves state-of-the-art performance across various OOD detection tasks, particularly in challenging hard-OOD scenarios.",
        "arxiv_id": "2503.06442",
        "ARXIVID": "2503.06442",
        "COMMENT": "Does not match any specific criteria. Focuses on OOD detection using vision-language models, which is tangentially related to vision-language models but not directly to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06410": {
        "authors": [
            "Alex Casella",
            "Wayne Wang"
        ],
        "title": "Performant LLM Agentic Framework for Conversational AI",
        "abstract": "arXiv:2503.06410v1 Announce Type: new  Abstract: The rise of Agentic applications and automation in the Voice AI industry has led to an increased reliance on Large Language Models (LLMs) to navigate graph-based logic workflows composed of nodes and edges. However, existing methods face challenges such as alignment errors in complex workflows and hallucinations caused by excessive context size. To address these limitations, we introduce the Performant Agentic Framework (PAF), a novel system that assists LLMs in selecting appropriate nodes and executing actions in order when traversing complex graphs. PAF combines LLM-based reasoning with a mathematically grounded vector scoring mechanism, achieving both higher accuracy and reduced latency. Our approach dynamically balances strict adherence to predefined paths with flexible node jumps to handle various user inputs efficiently. Experiments demonstrate that PAF significantly outperforms baseline methods, paving the way for scalable, real-time Conversational AI systems in complex business environments.",
        "arxiv_id": "2503.06410",
        "ARXIVID": "2503.06410",
        "COMMENT": "Does not match any specific criterion but is relevant to conversational AI and LLM frameworks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.07168": {
        "authors": [
            "Jing Yang",
            "Sen Yang",
            "Xiao Tan",
            "Hanli Wang"
        ],
        "title": "HisTrackMap: Global Vectorized High-Definition Map Construction via History Map Tracking",
        "abstract": "arXiv:2503.07168v1 Announce Type: new  Abstract: As an essential component of autonomous driving systems, high-definition (HD) maps provide rich and precise environmental information for auto-driving scenarios; however, existing methods, which primarily rely on query-based detection frameworks to directly model map elements or implicitly propagate queries over time, often struggle to maintain consistent temporal perception outcomes. These inconsistencies pose significant challenges to the stability and reliability of real-world autonomous driving and map data collection systems. To address this limitation, we propose a novel end-to-end tracking framework for global map construction by temporally tracking map elements' historical trajectories. Firstly, instance-level historical rasterization map representation is designed to explicitly store previous perception results, which can control and maintain different global instances' history information in a fine-grained way. Secondly, we introduce a Map-Trajectory Prior Fusion module within this tracking framework, leveraging historical priors for tracked instances to improve temporal smoothness and continuity. Thirdly, we propose a global perspective metric to evaluate the quality of temporal geometry construction in HD maps, filling the gap in current metrics for assessing global geometric perception results. Substantial experiments on the nuScenes and Argoverse2 datasets demonstrate that the proposed method outperforms state-of-the-art (SOTA) methods in both single-frame and temporal metrics. our project page: $\\href{https://yj772881654.github.io/HisTrackMap/}{https://yj772881654.github.io/HisTrackMap.}$",
        "arxiv_id": "2503.07168",
        "ARXIVID": "2503.07168",
        "COMMENT": "Does not match any specific criterion but is relevant to autonomous driving and HD map construction.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06526": {
        "authors": [
            "Chen-Lin Zhang",
            "Lin Sui",
            "Shuming Liu",
            "Fangzhou Mu",
            "Zhangcheng Wang",
            "Bernard Ghanem"
        ],
        "title": "TimeLoc: A Unified End-to-End Framework for Precise Timestamp Localization in Long Videos",
        "abstract": "arXiv:2503.06526v1 Announce Type: new  Abstract: Temporal localization in untrimmed videos, which aims to identify specific timestamps, is crucial for video understanding but remains challenging. This task encompasses several subtasks, including temporal action localization, temporal video grounding, moment retrieval, and generic event boundary detection. Existing methods in each subfield are typically designed for specific tasks and lack generalizability across domains. In this paper, we propose TimeLoc, a unified end-to-end framework for timestamp localization that can handle multiple tasks. First, our approach employs a simple yet effective one-stage localization model that supports text queries as input and multiple actions as output. Second, we jointly train the video encoder and localization model in an end-to-end manner. To efficiently process long videos, we introduce temporal chunking, enabling the handling of videos with over 30k frames. Third, we find that fine-tuning pre-trained text encoders with a multi-stage training strategy further enhances text-conditioned localization. TimeLoc achieves state-of-the-art results across multiple benchmarks: +1.3% and +1.9% mAP over previous best methods on THUMOS14 and EPIC-Kitchens-100, +1.1% on Kinetics-GEBD, +2.94% mAP on QVHighlights, and significant improvements in temporal video grounding (+11.5% on TACoS and +6.7% on Charades-STA under R1@0.5). Our code and checkpoints will be released at https://github.com/sming256/TimeLoc.",
        "arxiv_id": "2503.06526",
        "ARXIVID": "2503.06526",
        "COMMENT": "Does not match any specific criterion but is relevant to video understanding and temporal localization.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06966": {
        "authors": [
            "Guanghao Li",
            "Mingzhi Chen",
            "Hao Yu",
            "Shuting Dong",
            "Wenhao Jiang",
            "Ming Tang",
            "Chun Yuan"
        ],
        "title": "MIGA: Mutual Information-Guided Attack on Denoising Models for Semantic Manipulation",
        "abstract": "arXiv:2503.06966v1 Announce Type: new  Abstract: Deep learning-based denoising models have been widely employed in vision tasks, functioning as filters to eliminate noise while retaining crucial semantic information. Additionally, they play a vital role in defending against adversarial perturbations that threaten downstream tasks. However, these models can be intrinsically susceptible to adversarial attacks due to their dependence on specific noise assumptions. Existing attacks on denoising models mainly aim at deteriorating visual clarity while neglecting semantic manipulation, rendering them either easily detectable or limited in effectiveness. In this paper, we propose Mutual Information-Guided Attack (MIGA), the first method designed to directly attack deep denoising models by strategically disrupting their ability to preserve semantic content via adversarial perturbations. By minimizing the mutual information between the original and denoised images, a measure of semantic similarity. MIGA forces the denoiser to produce perceptually clean yet semantically altered outputs. While these images appear visually plausible, they encode systematically distorted semantics, revealing a fundamental vulnerability in denoising models. These distortions persist in denoised outputs and can be quantitatively assessed through downstream task performance. We propose new evaluation metrics and systematically assess MIGA on four denoising models across five datasets, demonstrating its consistent effectiveness in disrupting semantic fidelity. Our findings suggest that denoising models are not always robust and can introduce security risks in real-world applications. Code is available in the Supplementary Material.",
        "arxiv_id": "2503.06966",
        "ARXIVID": "2503.06966",
        "COMMENT": "Does not match any specific criteria. Focuses on adversarial attacks on denoising models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.07294": {
        "authors": [
            "Thomas Boucher",
            "Evangelos B. Mazomenos"
        ],
        "title": "Distilling Knowledge into Quantum Vision Transformers for Biomedical Image Classification",
        "abstract": "arXiv:2503.07294v1 Announce Type: new  Abstract: Quantum vision transformers (QViTs) build on vision transformers (ViTs) by replacing linear layers within the self-attention mechanism with parameterised quantum neural networks (QNNs), harnessing quantum mechanical properties to improve feature representation. This hybrid approach aims to achieve superior performance, with significantly reduced model complexity as a result of the enriched feature representation, requiring fewer parameters. This paper proposes a novel QViT model for biomedical image classification and investigates its performance against comparable ViTs across eight diverse datasets, encompassing various modalities and classification tasks. We assess models trained from scratch and those pre-trained using knowledge distillation (KD) from high-quality teacher models. Our findings demonstrate that QViTs outperform comparable ViTs with average ROC AUC (0.863 vs 0.846) and accuracy (0.710 vs 0.687) when trained from scratch, and even compete with state-of-the-art classical models in multiple tasks, whilst being significantly more efficient (89% reduction in GFLOPs and 99.99% in parameter number). Additionally, we find that QViTs and ViTs respond equally well to KD, with QViT pre-training performance scaling with model complexity. This is the first investigation into the efficacy of deploying QViTs with KD for computer-aided diagnosis. Our results highlight the enormous potential of quantum machine learning (QML) in biomedical image analysis.",
        "arxiv_id": "2503.07294",
        "ARXIVID": "2503.07294",
        "COMMENT": "Does not match any specific criteria. Focuses on quantum vision transformers for biomedical image classification.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06458": {
        "authors": [
            "Guanyu Cao",
            "Takuya Maekawa",
            "Kazuya Ohara",
            "Yasue Kishino"
        ],
        "title": "Reconstructing Depth Images of Moving Objects from Wi-Fi CSI Data",
        "abstract": "arXiv:2503.06458v1 Announce Type: new  Abstract: This study proposes a new deep learning method for reconstructing depth images of moving objects within a specific area using Wi-Fi channel state information (CSI). The Wi-Fi-based depth imaging technique has novel applications in domains such as security and elder care. However, reconstructing depth images from CSI is challenging because learning the mapping function between CSI and depth images, both of which are high-dimensional data, is particularly difficult. To address the challenge, we propose a new approach called Wi-Depth. The main idea behind the design of Wi-Depth is that a depth image of a moving object can be decomposed into three core components: the shape, depth, and position of the target. Therefore, in the depth-image reconstruction task, Wi-Depth simultaneously estimates the three core pieces of information as auxiliary tasks in our proposed VAE-based teacher-student architecture, enabling it to output images with the consistency of a correct shape, depth, and position. In addition, the design of Wi-Depth is based on our idea that this decomposition efficiently takes advantage of the fact that shape, depth, and position relate to primitive information inferred from CSI such as angle-of-arrival, time-of-flight, and Doppler frequency shift.",
        "arxiv_id": "2503.06458",
        "ARXIVID": "2503.06458",
        "COMMENT": "Does not match any specific criterion but is relevant to depth imaging and Wi-Fi-based sensing, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.07520": {
        "authors": [
            "Zhongwei Chen",
            "Zhao-Xu Yang",
            "Hai-Jun Rong",
            "Jiawei Lang"
        ],
        "title": "From Limited Labels to Open Domains: An Efficient Learning Paradigm for UAV-view Geo-Localization",
        "abstract": "arXiv:2503.07520v1 Announce Type: new  Abstract: Traditional UAV-view Geo-Localization (UVGL) supervised paradigms are constrained by the strict reliance on paired data for positive sample selection, which limits their ability to learn cross-view domain-invariant representations from unpaired data. Moreover, it is necessary to reconstruct the pairing relationship with expensive re-labeling costs for scenario-specific training when deploying in a new domain, which fails to meet the practical demands of open-environment applications. To address this issue, we propose a novel cross-domain invariance knowledge transfer network (CDIKTNet), which comprises a cross-domain invariance sub-network and a cross-domain transfer sub-network to realize a closed-loop framework of invariance feature learning and knowledge transfer. The cross-domain invariance sub-network is utilized to construct an essentially shared feature space across domains by learning structural invariance and spatial invariance in cross-view features. Meanwhile, the cross-domain transfer sub-network uses these invariant features as anchors and employs a dual-path contrastive memory learning mechanism to mine latent cross-domain correlation patterns in unpaired data. Extensive experiments demonstrate that our method achieves state-of-the-art performance under fully supervised conditions. More importantly, with merely 2\\% paired data, our method exhibits performance comparable to existing supervised paradigms and possesses the ability to transfer directly to qualify for applications in the other scenarios completely without any prior pairing relationship.",
        "arxiv_id": "2503.07520",
        "ARXIVID": "2503.07520",
        "COMMENT": "Does not match any specific criterion but is relevant to UAV-view geo-localization, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06470": {
        "authors": [
            "Fei Tang",
            "Yongliang Shen",
            "Hang Zhang",
            "Siqi Chen",
            "Guiyang Hou",
            "Wenqi Zhang",
            "Wenqiao Zhang",
            "Kaitao Song",
            "Weiming Lu",
            "Yueting Zhuang"
        ],
        "title": "Think Twice, Click Once: Enhancing GUI Grounding via Fast and Slow Systems",
        "abstract": "arXiv:2503.06470v1 Announce Type: new  Abstract: Humans can flexibly switch between different modes of thinking based on task complexity: from rapid intuitive judgments to in-depth analytical understanding. However, current Graphical User Interface (GUI) grounding systems which locate interface elements based on natural language instructions rely solely on immediate prediction without reasoning, struggling to understand complex interface layouts with nested structures and hierarchical relationships, limiting their effectiveness on complex interfaces. Inspired by human dual-system cognition, we present Focus, a novel GUI grounding framework that combines fast prediction with systematic analysis. The framework dynamically switches between rapid and deliberate processing through an adaptive system switching based on task complexity, optimizing both efficiency and accuracy. Focus decomposes grounding into progressive stages: interface summarization, visual focused analysis, and precise coordinate prediction. This structured decomposition enables systematic understanding of both interface layouts and visual relationships. Extensive experiments show that Focus achieves state-of-the-art performance using only 300K of the training data with a 2B parameter model compared to existing approaches. Focus demonstrates superior performance particularly in complex GUI scenarios, achieving 77.4% average accuracy on ScreenSpot and 13.3% on the more challenging ScreenSpot-Pro. Our analysis reveals the effectiveness of this dual-system approach while demonstrating its potential for improving complex GUI interaction scenarios.",
        "arxiv_id": "2503.06470",
        "ARXIVID": "2503.06470",
        "COMMENT": "Does not match any specific criterion but is relevant to GUI grounding and systematic reasoning, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06847": {
        "authors": [
            "Xiangyan Qu",
            "Jing Yu",
            "Jiamin Zhuang",
            "Gaopeng Gou",
            "Gang Xiong",
            "Qi Wu"
        ],
        "title": "MADS: Multi-Attribute Document Supervision for Zero-Shot Image Classification",
        "abstract": "arXiv:2503.06847v1 Announce Type: new  Abstract: Zero-shot learning (ZSL) aims to train a model on seen classes and recognize unseen classes by knowledge transfer through shared auxiliary information. Recent studies reveal that documents from encyclopedias provide helpful auxiliary information. However, existing methods align noisy documents, entangled in visual and non-visual descriptions, with image regions, yet solely depend on implicit learning. These models fail to filter non-visual noise reliably and incorrectly align non-visual words to image regions, which is harmful to knowledge transfer. In this work, we propose a novel multi-attribute document supervision framework to remove noises at both document collection and model learning stages. With the help of large language models, we introduce a novel prompt algorithm that automatically removes non-visual descriptions and enriches less-described documents in multiple attribute views. Our proposed model, MADS, extracts multi-view transferable knowledge with information decoupling and semantic interactions for semantic alignment at local and global levels. Besides, we introduce a model-agnostic focus loss to explicitly enhance attention to visually discriminative information during training, also improving existing methods without additional parameters. With comparable computation costs, MADS consistently outperforms the SOTA by 7.2% and 8.2% on average in three benchmarks for document-based ZSL and GZSL settings, respectively. Moreover, we qualitatively offer interpretable predictions from multiple attribute views.",
        "arxiv_id": "2503.06847",
        "ARXIVID": "2503.06847",
        "COMMENT": "Does not match any specific criterion but is relevant to vision-language models and zero-shot learning, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.07418": {
        "authors": [
            "Mingzhen Sun",
            "Weining Wang",
            "Gen Li",
            "Jiawei Liu",
            "Jiahui Sun",
            "Wanquan Feng",
            "Shanshan Lao",
            "SiYu Zhou",
            "Qian He",
            "Jing Liu"
        ],
        "title": "AR-Diffusion: Asynchronous Video Generation with Auto-Regressive Diffusion",
        "abstract": "arXiv:2503.07418v1 Announce Type: new  Abstract: The task of video generation requires synthesizing visually realistic and temporally coherent video frames. Existing methods primarily use asynchronous auto-regressive models or synchronous diffusion models to address this challenge. However, asynchronous auto-regressive models often suffer from inconsistencies between training and inference, leading to issues such as error accumulation, while synchronous diffusion models are limited by their reliance on rigid sequence length. To address these issues, we introduce Auto-Regressive Diffusion (AR-Diffusion), a novel model that combines the strengths of auto-regressive and diffusion models for flexible, asynchronous video generation. Specifically, our approach leverages diffusion to gradually corrupt video frames in both training and inference, reducing the discrepancy between these phases. Inspired by auto-regressive generation, we incorporate a non-decreasing constraint on the corruption timesteps of individual frames, ensuring that earlier frames remain clearer than subsequent ones. This setup, together with temporal causal attention, enables flexible generation of videos with varying lengths while preserving temporal coherence. In addition, we design two specialized timestep schedulers: the FoPP scheduler for balanced timestep sampling during training, and the AD scheduler for flexible timestep differences during inference, supporting both synchronous and asynchronous generation. Extensive experiments demonstrate the superiority of our proposed method, which achieves competitive and state-of-the-art results across four challenging benchmarks.",
        "arxiv_id": "2503.07418",
        "ARXIVID": "2503.07418",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling and video generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06623": {
        "authors": [
            "Sijie Zhao",
            "Feng Liu",
            "Xueliang Zhang",
            "Hao Chen",
            "Tao Han",
            "Junchao Gong",
            "Ran Tao",
            "Pengfeng Xiao",
            "Lei Bai",
            "Wanli Ouyang"
        ],
        "title": "Transforming Weather Data from Pixel to Latent Space",
        "abstract": "arXiv:2503.06623v1 Announce Type: new  Abstract: The increasing impact of climate change and extreme weather events has spurred growing interest in deep learning for weather research. However, existing studies often rely on weather data in pixel space, which presents several challenges such as smooth outputs in model outputs, limited applicability to a single pressure-variable subset (PVS), and high data storage and computational costs. To address these challenges, we propose a novel Weather Latent Autoencoder (WLA) that transforms weather data from pixel space to latent space, enabling efficient weather task modeling. By decoupling weather reconstruction from downstream tasks, WLA improves the accuracy and sharpness of weather task model results. The incorporated Pressure-Variable Unified Module transforms multiple PVS into a unified representation, enhancing the adaptability of the model in multiple weather scenarios. Furthermore, weather tasks can be performed in a low-storage latent space of WLA rather than a high-storage pixel space, thus significantly reducing data storage and computational costs. Through extensive experimentation, we demonstrate its superior compression and reconstruction performance, enabling the creation of the ERA5-latent dataset with unified representations of multiple PVS from ERA5 data. The compressed full PVS in the ERA5-latent dataset reduces the original 244.34 TB of data to 0.43 TB. The downstream task further demonstrates that task models can apply to multiple PVS with low data costs in latent space and achieve superior performance compared to models in pixel space. Code, ERA5-latent data, and pre-trained models are available at https://anonymous.4open.science/r/Weather-Latent-Autoencoder-8467.",
        "arxiv_id": "2503.06623",
        "ARXIVID": "2503.06623",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of computer vision and efficient modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.06632": {
        "authors": [
            "Mingxiao Li",
            "Tingyu Qu",
            "Tinne Tuytelaars",
            "Marie-Francine Moens"
        ],
        "title": "Towards More Accurate Personalized Image Generation: Addressing Overfitting and Evaluation Bias",
        "abstract": "arXiv:2503.06632v1 Announce Type: new  Abstract: Personalized image generation via text prompts has great potential to improve daily life and professional work by facilitating the creation of customized visual content. The aim of image personalization is to create images based on a user-provided subject while maintaining both consistency of the subject and flexibility to accommodate various textual descriptions of that subject. However, current methods face challenges in ensuring fidelity to the text prompt while not overfitting to the training data. In this work, we introduce a novel training pipeline that incorporates an attractor to filter out distractions in training images, allowing the model to focus on learning an effective representation of the personalized subject. Moreover, current evaluation methods struggle due to the lack of a dedicated test set. The evaluation set-up typically relies on the training data of the personalization task to compute text-image and image-image similarity scores, which, while useful, tend to overestimate performance. Although human evaluations are commonly used as an alternative, they often suffer from bias and inconsistency. To address these issues, we curate a diverse and high-quality test set with well-designed prompts. With this new benchmark, automatic evaluation metrics can reliably assess model performance",
        "arxiv_id": "2503.06632",
        "ARXIVID": "2503.06632",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.07091": {
        "authors": [
            "Shuhe Wang",
            "Xiaoya Li",
            "Jiwei Li",
            "Guoyin Wang",
            "Xiaofei Sun",
            "Bob Zhu",
            "Han Qiu",
            "Mo Yu",
            "Shengjie Shen",
            "Eduard Hovy"
        ],
        "title": "FaceID-6M: A Large-Scale, Open-Source FaceID Customization Dataset",
        "abstract": "arXiv:2503.07091v1 Announce Type: new  Abstract: Due to the data-driven nature of current face identity (FaceID) customization methods, all state-of-the-art models rely on large-scale datasets containing millions of high-quality text-image pairs for training. However, none of these datasets are publicly available, which restricts transparency and hinders further advancements in the field.   To address this issue, in this paper, we collect and release FaceID-6M, the first large-scale, open-source FaceID dataset containing 6 million high-quality text-image pairs. Filtered from LAION-5B \\cite{schuhmann2022laion}, FaceID-6M undergoes a rigorous image and text filtering steps to ensure dataset quality, including resolution filtering to maintain high-quality images and faces, face filtering to remove images that lack human faces, and keyword-based strategy to retain descriptions containing human-related terms (e.g., nationality, professions and names). Through these cleaning processes, FaceID-6M provides a high-quality dataset optimized for training powerful FaceID customization models, facilitating advancements in the field by offering an open resource for research and development.   We conduct extensive experiments to show the effectiveness of our FaceID-6M, demonstrating that models trained on our FaceID-6M dataset achieve performance that is comparable to, and slightly better than currently available industrial models. Additionally, to support and advance research in the FaceID customization community, we make our code, datasets, and models fully publicly available. Our codes, models, and datasets are available at: https://github.com/ShuheSH/FaceID-6M.",
        "arxiv_id": "2503.07091",
        "ARXIVID": "2503.07091",
        "COMMENT": "Does not match any specific criteria. Focuses on a dataset for FaceID customization, which is not directly related to spatial understanding, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.07517": {
        "authors": [
            "Takeru Inoue",
            "Ryusuke Miyamoto"
        ],
        "title": "FastInstShadow: A Simple Query-Based Model for Instance Shadow Detection",
        "abstract": "arXiv:2503.07517v1 Announce Type: new  Abstract: Instance shadow detection is the task of detecting pairs of shadows and objects, where existing methods first detect shadows and objects independently, then associate them. This paper introduces FastInstShadow, a method that enhances detection accuracy through a query-based architecture featuring an association transformer decoder with two dual-path transformer decoders to assess relationships between shadows and objects during detection. Experimental results using the SOBA dataset showed that the proposed method outperforms all existing methods across all criteria. This method makes real-time processing feasible for moderate-resolution images with better accuracy than SSISv2, the most accurate existing method. Our code is available at https://github.com/wlotkr/FastInstShadow.",
        "arxiv_id": "2503.07517",
        "ARXIVID": "2503.07517",
        "COMMENT": "Does not match any specific criteria. Focuses on instance shadow detection, which is not directly related to spatial understanding, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.06839": {
        "authors": [
            "Zhuowen Zheng",
            "Yain-Whar Si",
            "Xiaochen Yuan",
            "Junwei Duan",
            "Ke Wang",
            "Xiaofan Li",
            "Xinyuan Zhang",
            "Xueyuan Gong"
        ],
        "title": "AttFC: Attention Fully-Connected Layer for Large-Scale Face Recognition with One GPU",
        "abstract": "arXiv:2503.06839v1 Announce Type: new  Abstract: Nowadays, with the advancement of deep neural networks (DNNs) and the availability of large-scale datasets, the face recognition (FR) model has achieved exceptional performance. However, since the parameter magnitude of the fully connected (FC) layer directly depends on the number of identities in the dataset. If training the FR model on large-scale datasets, the size of the model parameter will be excessively huge, leading to substantial demand for computational resources, such as time and memory. This paper proposes the attention fully connected (AttFC) layer, which could significantly reduce computational resources. AttFC employs an attention loader to generate the generative class center (GCC), and dynamically store the class center with Dynamic Class Container (DCC). DCC only stores a small subset of all class centers in FC, thus its parameter count is substantially less than the FC layer. Also, training face recognition models on large-scale datasets with one GPU often encounter out-of-memory (OOM) issues. AttFC overcomes this and achieves comparable performance to state-of-the-art methods.",
        "arxiv_id": "2503.06839",
        "ARXIVID": "2503.06839",
        "COMMENT": "Does not match any specific criteria but involves efficient training for face recognition, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.07230": {
        "authors": [
            "Luigi Russo",
            "Antonietta Sorriso",
            "Silvia Liberata Ullo",
            "Paolo Gamba"
        ],
        "title": "A Deep Learning Architecture for Land Cover Mapping Using Spatio-Temporal Sentinel-1 Features",
        "abstract": "arXiv:2503.07230v1 Announce Type: new  Abstract: Land Cover (LC) mapping using satellite imagery is critical for environmental monitoring and management. Deep Learning (DL), particularly Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), have revolutionized this field by enhancing the accuracy of classification tasks. In this work, a novel approach combining a transformer-based Swin-Unet architecture with seasonal synthesized spatio-temporal images has been employed to classify LC types using spatio-temporal features extracted from Sentinel-1 (S1) Synthetic Aperture Radar (SAR) data, organized into seasonal clusters. The study focuses on three distinct regions - Amazonia, Africa, and Siberia - and evaluates the model performance across diverse ecoregions within these areas. By utilizing seasonal feature sequences instead of dense temporal sequences, notable performance improvements have been achieved, especially in regions with temporal data gaps like Siberia, where S1 data distribution is uneven and non-uniform. The results demonstrate the effectiveness and the generalization capabilities of the proposed methodology in achieving high overall accuracy (O.A.) values, even in regions with limited training data.",
        "arxiv_id": "2503.07230",
        "ARXIVID": "2503.07230",
        "COMMENT": "Does not match any specific criteria but involves spatio-temporal features and transformers, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.06100": {
        "authors": [
            "Xianjie Liu",
            "Keren Fu",
            "Qijun Zhao"
        ],
        "title": "Patch-Depth Fusion: Dichotomous Image Segmentation via Fine-Grained Patch Strategy and Depth Integrity-Prior",
        "abstract": "arXiv:2503.06100v1 Announce Type: new  Abstract: Dichotomous Image Segmentation (DIS) is a high-precision object segmentation task for high-resolution natural images. The current mainstream methods focus on the optimization of local details but overlook the fundamental challenge of modeling the integrity of objects. We have found that the depth integrity-prior implicit in the the pseudo-depth maps generated by Depth Anything Model v2 and the local detail features of image patches can jointly address the above dilemmas. Based on the above findings, we have designed a novel Patch-Depth Fusion Network (PDFNet) for high-precision dichotomous image segmentation. The core of PDFNet consists of three aspects. Firstly, the object perception is enhanced through multi-modal input fusion. By utilizing the patch fine-grained strategy, coupled with patch selection and enhancement, the sensitivity to details is improved. Secondly, by leveraging the depth integrity-prior distributed in the depth maps, we propose an integrity-prior loss to enhance the uniformity of the segmentation results in the depth maps. Finally, we utilize the features of the shared encoder and, through a simple depth refinement decoder, improve the ability of the shared encoder to capture subtle depth-related information in the images. Experiments on the DIS-5K dataset show that PDFNet significantly outperforms state-of-the-art non-diffusion methods. Due to the incorporation of the depth integrity-prior, PDFNet achieves or even surpassing the performance of the latest diffusion-based methods while using less than 11% of the parameters of diffusion-based methods. The source code at https://github.com/Tennine2077/PDFNet.",
        "arxiv_id": "2503.06100",
        "ARXIVID": "2503.06100",
        "COMMENT": "Does not match any specific criteria but involves multi-modal input fusion and depth-related segmentation, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.07235": {
        "authors": [
            "Haowen Bai",
            "Jiangshe Zhang",
            "Zixiang Zhao",
            "Lilun Deng",
            "Yukun Cui",
            "Shuang Xu"
        ],
        "title": "Retinex-MEF: Retinex-based Glare Effects Aware Unsupervised Multi-Exposure Image Fusion",
        "abstract": "arXiv:2503.07235v1 Announce Type: new  Abstract: Multi-exposure image fusion consolidates multiple low dynamic range images of the same scene into a singular high dynamic range image. Retinex theory, which separates image illumination from scene reflectance, is naturally adopted to ensure consistent scene representation and effective information fusion across varied exposure levels. However, the conventional pixel-wise multiplication of illumination and reflectance inadequately models the glare effect induced by overexposure. To better adapt this theory for multi-exposure image fusion, we introduce an unsupervised and controllable method termed~\\textbf{(Retinex-MEF)}. Specifically, our method decomposes multi-exposure images into separate illumination components and a shared reflectance component, and effectively modeling the glare induced by overexposure. Employing a bidirectional loss constraint to learn the common reflectance component, our approach effectively mitigates the glare effect. Furthermore, we establish a controllable exposure fusion criterion, enabling global exposure adjustments while preserving contrast, thus overcoming the constraints of fixed-level fusion. A series of experiments across multiple datasets, including underexposure-overexposure fusion, exposure control fusion, and homogeneous extreme exposure fusion, demonstrate the effective decomposition and flexible fusion capability of our model.",
        "arxiv_id": "2503.07235",
        "ARXIVID": "2503.07235",
        "COMMENT": "Does not match any specific criteria but is related to image fusion and glare effect modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.06064": {
        "authors": [
            "Wenzhuo Du",
            "Gerun Wang",
            "Guancheng Chen",
            "Hang Zhao",
            "Xin Li",
            "Jian Gao"
        ],
        "title": "A Novel Trustworthy Video Summarization Algorithm Through a Mixture of LoRA Experts",
        "abstract": "arXiv:2503.06064v1 Announce Type: new  Abstract: With the exponential growth of user-generated content on video-sharing platforms, the challenge of facilitating efficient searching and browsing of videos has garnered significant attention. To enhance users' ability to swiftly locate and review pertinent videos, the creation of concise and informative video summaries has become increasingly important. Video-llama is an effective tool for generating video summarization, but it cannot effectively unify and optimize the modeling of temporal and spatial features and requires a lot of computational resources and time. Therefore, we propose MiLoRA-ViSum to more efficiently capture complex temporal dynamics and spatial relationships inherent in video data and to control the number of parameters for training. By extending traditional Low-Rank Adaptation (LoRA) into a sophisticated mixture-of-experts paradigm, MiLoRA-ViSum incorporates a dual temporal-spatial adaptation mechanism tailored specifically for video summarization tasks. This approach dynamically integrates specialized LoRA experts, each fine-tuned to address distinct temporal or spatial dimensions. Extensive evaluations of the VideoXum and ActivityNet datasets demonstrate that MiLoRA-ViSum achieves the best summarization performance compared to state-of-the-art models, while maintaining significantly lower computational costs. The proposed mixture-of-experts strategy, combined with the dual adaptation mechanism, highlights the model's potential to enhance video summarization capabilities, particularly in large-scale applications requiring both efficiency and precision.",
        "arxiv_id": "2503.06064",
        "ARXIVID": "2503.06064",
        "COMMENT": "Does not match any specific criteria but is related to video summarization and efficient modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.06996": {
        "authors": [
            "Stylianos Zindros",
            "Christos Chronis",
            "Panagiotis Radoglou-Grammatikis",
            "Vasileios Argyriou",
            "Panagiotis Sarigiannidis",
            "Iraklis Varlamis",
            "Georgios Th. Papadopoulos"
        ],
        "title": "Public space security management using digital twin technologies",
        "abstract": "arXiv:2503.06996v1 Announce Type: new  Abstract: As the security of public spaces remains a critical issue in today's world, Digital Twin technologies have emerged in recent years as a promising solution for detecting and predicting potential future threats. The applied methodology leverages a Digital Twin of a metro station in Athens, Greece, using the FlexSim simulation software. The model encompasses points of interest and passenger flows, and sets their corresponding parameters. These elements influence and allow the model to provide reasonable predictions on the security management of the station under various scenarios. Experimental tests are conducted with different configurations of surveillance cameras and optimizations of camera angles to evaluate the effectiveness of the space surveillance setup. The results show that the strategic positioning of surveillance cameras and the adjustment of their angles significantly improves the detection of suspicious behaviors and with the use of the DT it is possible to evaluate different scenarios and find the optimal camera setup for each case. In summary, this study highlights the value of Digital Twins in real-time simulation and data-driven security management. The proposed approach contributes to the ongoing development of smart security solutions for public spaces and provides an innovative framework for threat detection and prevention.",
        "arxiv_id": "2503.06996",
        "ARXIVID": "2503.06996",
        "COMMENT": "Does not match any specific criteria. Focuses on digital twin technologies for public space security.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.07371": {
        "authors": [
            "Qizhi Zheng",
            "Zhongze Luo",
            "Meiyan Guo",
            "Xinzhu Wang",
            "Renqimuge Wu",
            "Qiu Meng",
            "Guanghui Dong"
        ],
        "title": "HGO-YOLO: Advancing Anomaly Behavior Detection with Hierarchical Features and Lightweight Optimized Detection",
        "abstract": "arXiv:2503.07371v1 Announce Type: new  Abstract: Accurate and real-time object detection is crucial for anomaly behavior detection, especially in scenarios constrained by hardware limitations, where balancing accuracy and speed is essential for enhancing detection performance. This study proposes a model called HGO-YOLO, which integrates the HGNetv2 architecture into YOLOv8. This combination expands the receptive field and captures a wider range of features while simplifying model complexity through GhostConv. We introduced a lightweight detection head, OptiConvDetect, which utilizes parameter sharing to construct the detection head effectively. Evaluation results show that the proposed algorithm achieves a mAP@0.5 of 87.4% and a recall rate of 81.1%, with a model size of only 4.6 MB and a frame rate of 56 FPS on the CPU. HGO-YOLO not only improves accuracy by 3.0% but also reduces computational load by 51.69% (from 8.9 GFLOPs to 4.3 GFLOPs), while increasing the frame rate by a factor of 1.7. Additionally, real-time tests were conducted on Raspberry Pi4 and NVIDIA platforms. These results indicate that the HGO-YOLO model demonstrates superior performance in anomaly behavior detection.",
        "arxiv_id": "2503.07371",
        "ARXIVID": "2503.07371",
        "COMMENT": "Does not match any specific criteria. Focuses on anomaly detection and lightweight object detection.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.06399": {
        "authors": [
            "Haisheng Fu",
            "Jie Liang",
            "Zhenman Fang",
            "Jingning Han"
        ],
        "title": "FEDS: Feature and Entropy-Based Distillation Strategy for Efficient Learned Image Compression",
        "abstract": "arXiv:2503.06399v1 Announce Type: new  Abstract: Learned image compression (LIC) methods have recently outperformed traditional codecs such as VVC in rate-distortion performance. However, their large models and high computational costs have limited their practical adoption. In this paper, we first construct a high-capacity teacher model by integrating Swin-Transformer V2-based attention modules, additional residual blocks, and expanded latent channels, thus achieving enhanced compression performance. Building on this foundation, we propose a \\underline{F}eature and \\underline{E}ntropy-based \\underline{D}istillation \\underline{S}trategy (\\textbf{FEDS}) that transfers key knowledge from the teacher to a lightweight student model. Specifically, we align intermediate feature representations and emphasize the most informative latent channels through an entropy-based loss. A staged training scheme refines this transfer in three phases: feature alignment, channel-level distillation, and final fine-tuning. Our student model nearly matches the teacher across Kodak (1.24\\% BD-Rate increase), Tecnick (1.17\\%), and CLIC (0.55\\%) while cutting parameters by about 63\\% and accelerating encoding/decoding by around 73\\%. Moreover, ablation studies indicate that FEDS generalizes effectively to transformer-based networks. The experimental results demonstrate our approach strikes a compelling balance among compression performance, speed, and model parameters, making it well-suited for real-time or resource-limited scenarios.",
        "arxiv_id": "2503.06399",
        "ARXIVID": "2503.06399",
        "COMMENT": "Does not match any specific criteria but is related to efficient learned image compression, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.06678": {
        "authors": [
            "Hantao Zhou",
            "Rui Yang",
            "Longxiang Tang",
            "Guanyi Qin",
            "Yan Zhang",
            "Runze Hu",
            "Xiu Li"
        ],
        "title": "Gamma: Toward Generic Image Assessment with Mixture of Assessment Experts",
        "abstract": "arXiv:2503.06678v1 Announce Type: new  Abstract: Image assessment aims to evaluate the quality and aesthetics of images and has been applied across various scenarios, such as natural and AIGC scenes. Existing methods mostly address these sub-tasks or scenes individually. While some works attempt to develop unified image assessment models, they have struggled to achieve satisfactory performance or cover a broad spectrum of assessment scenarios. In this paper, we present \\textbf{Gamma}, a \\textbf{G}eneric im\\textbf{A}ge assess\\textbf{M}ent model using \\textbf{M}ixture of \\textbf{A}ssessment Experts, which can effectively assess images from diverse scenes through mixed-dataset training. Achieving unified training in image assessment presents significant challenges due to annotation biases across different datasets. To address this issue, we first propose a Mixture of Assessment Experts (MoAE) module, which employs shared and adaptive experts to dynamically learn common and specific knowledge for different datasets, respectively. In addition, we introduce a Scene-based Differential Prompt (SDP) strategy, which uses scene-specific prompts to provide prior knowledge and guidance during the learning process, further boosting adaptation for various scenes. Our Gamma model is trained and evaluated on 12 datasets spanning 6 image assessment scenarios. Extensive experiments show that our unified Gamma outperforms other state-of-the-art mixed-training methods by significant margins while covering more scenes. Code: https://github.com/zht8506/Gamma.",
        "arxiv_id": "2503.06678",
        "ARXIVID": "2503.06678",
        "COMMENT": "Does not match any specific criterion but is related to image assessment and quality evaluation, which is tangentially relevant to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.06237": {
        "authors": [
            "Yifan Chang",
            "Junjie Huang",
            "Xiaofeng Wang",
            "Yun Ye",
            "Zhujin Liang",
            "Yi Shan",
            "Dalong Du",
            "Xingang Wang"
        ],
        "title": "Rethinking Lanes and Points in Complex Scenarios for Monocular 3D Lane Detection",
        "abstract": "arXiv:2503.06237v1 Announce Type: new  Abstract: Monocular 3D lane detection is a fundamental task in autonomous driving. Although sparse-point methods lower computational load and maintain high accuracy in complex lane geometries, current methods fail to fully leverage the geometric structure of lanes in both lane geometry representations and model design. In lane geometry representations, we present a theoretical analysis alongside experimental validation to verify that current sparse lane representation methods contain inherent flaws, resulting in potential errors of up to 20 m, which raise significant safety concerns for driving. To address this issue, we propose a novel patching strategy to completely represent the full lane structure. To enable existing models to match this strategy, we introduce the EndPoint head (EP-head), which adds a patching distance to endpoints. The EP-head enables the model to predict more complete lane representations even with fewer preset points, effectively addressing existing limitations and paving the way for models that are faster and require fewer parameters in the future. In model design, to enhance the model's perception of lane structures, we propose the PointLane attention (PL-attention), which incorporates prior geometric knowledge into the attention mechanism. Extensive experiments demonstrate the effectiveness of the proposed methods on various state-of-the-art models. For instance, in terms of the overall F1-score, our methods improve Persformer by 4.4 points, Anchor3DLane by 3.2 points, and LATR by 2.8 points. The code will be available soon.",
        "arxiv_id": "2503.06237",
        "ARXIVID": "2503.06237",
        "COMMENT": "Does not match any specific criteria. Focuses on monocular 3D lane detection for autonomous driving, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.07607": {
        "authors": [
            "Ying Xu",
            "Marius Pedersen",
            "Kiran Raja"
        ],
        "title": "VoD: Learning Volume of Differences for Video-Based Deepfake Detection",
        "abstract": "arXiv:2503.07607v1 Announce Type: new  Abstract: The rapid development of deep learning and generative AI technologies has profoundly transformed the digital contact landscape, creating realistic Deepfake that poses substantial challenges to public trust and digital media integrity. This paper introduces a novel Deepfake detention framework, Volume of Differences (VoD), designed to enhance detection accuracy by exploiting temporal and spatial inconsistencies between consecutive video frames. VoD employs a progressive learning approach that captures differences across multiple axes through the use of consecutive frame differences (CFD) and a network with stepwise expansions. We evaluate our approach with intra-dataset and cross-dataset testing scenarios on various well-known Deepfake datasets. Our findings demonstrate that VoD excels with the data it has been trained on and shows strong adaptability to novel, unseen data. Additionally, comprehensive ablation studies examine various configurations of segment length, sampling steps, and intervals, offering valuable insights for optimizing the framework. The code for our VoD framework is available at https://github.com/xuyingzhongguo/VoD.",
        "arxiv_id": "2503.07607",
        "ARXIVID": "2503.07607",
        "COMMENT": "Does not match any specific criteria. Focuses on Deepfake detection, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.06537": {
        "authors": [
            "Xiaoyang Liu",
            "Yuquan Wang",
            "Zheng Chen",
            "Jiezhang Cao",
            "He Zhang",
            "Yulun Zhang",
            "Xiaokang Yang"
        ],
        "title": "One-Step Diffusion Model for Image Motion-Deblurring",
        "abstract": "arXiv:2503.06537v1 Announce Type: new  Abstract: Currently, methods for single-image deblurring based on CNNs and transformers have demonstrated promising performance. However, these methods often suffer from perceptual limitations, poor generalization ability, and struggle with heavy or complex blur. While diffusion-based methods can partially address these shortcomings, their multi-step denoising process limits their practical usage. In this paper, we conduct an in-depth exploration of diffusion models in deblurring and propose a one-step diffusion model for deblurring (OSDD), a novel framework that reduces the denoising process to a single step, significantly improving inference efficiency while maintaining high fidelity. To tackle fidelity loss in diffusion models, we introduce an enhanced variational autoencoder (eVAE), which improves structural restoration. Additionally, we construct a high-quality synthetic deblurring dataset to mitigate perceptual collapse and design a dynamic dual-adapter (DDA) to enhance perceptual quality while preserving fidelity. Extensive experiments demonstrate that our method achieves strong performance on both full and no-reference metrics. Our code and pre-trained model will be publicly available at https://github.com/xyLiu339/OSDD.",
        "arxiv_id": "2503.06537",
        "ARXIVID": "2503.06537",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.06805": {
        "authors": [
            "Aref Farhadipour",
            "Hossein Ranjbar",
            "Masoumeh Chapariniya",
            "Teodora Vukovic",
            "Sarah Ebling",
            "Volker Dellwo"
        ],
        "title": "Multimodal Emotion Recognition and Sentiment Analysis in Multi-Party Conversation Contexts",
        "abstract": "arXiv:2503.06805v1 Announce Type: new  Abstract: Emotion recognition and sentiment analysis are pivotal tasks in speech and language processing, particularly in real-world scenarios involving multi-party, conversational data. This paper presents a multimodal approach to tackle these challenges on a well-known dataset. We propose a system that integrates four key modalities/channels using pre-trained models: RoBERTa for text, Wav2Vec2 for speech, a proposed FacialNet for facial expressions, and a CNN+Transformer architecture trained from scratch for video analysis. Feature embeddings from each modality are concatenated to form a multimodal vector, which is then used to predict emotion and sentiment labels. The multimodal system demonstrates superior performance compared to unimodal approaches, achieving an accuracy of 66.36% for emotion recognition and 72.15% for sentiment analysis.",
        "arxiv_id": "2503.06805",
        "ARXIVID": "2503.06805",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.07535": {
        "authors": [
            "Cl\\'ement Chadebec",
            "Onur Tasar",
            "Sanjeev Sreetharan",
            "Benjamin Aubin"
        ],
        "title": "LBM: Latent Bridge Matching for Fast Image-to-Image Translation",
        "abstract": "arXiv:2503.07535v1 Announce Type: new  Abstract: In this paper, we introduce Latent Bridge Matching (LBM), a new, versatile and scalable method that relies on Bridge Matching in a latent space to achieve fast image-to-image translation. We show that the method can reach state-of-the-art results for various image-to-image tasks using only a single inference step. In addition to its efficiency, we also demonstrate the versatility of the method across different image translation tasks such as object removal, normal and depth estimation, and object relighting. We also derive a conditional framework of LBM and demonstrate its effectiveness by tackling the tasks of controllable image relighting and shadow generation. We provide an open-source implementation of the method at https://github.com/gojasper/LBM.",
        "arxiv_id": "2503.07535",
        "ARXIVID": "2503.07535",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}