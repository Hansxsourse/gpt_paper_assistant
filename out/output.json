{
    "2502.06034": {
        "authors": [
            "Mozes Jacobs",
            "Roberto C. Budzinski",
            "Lyle Muller",
            "Demba Ba",
            "T. Anderson Keller"
        ],
        "title": "Traveling Waves Integrate Spatial Information Into Spectral Representations",
        "abstract": "arXiv:2502.06034v1 Announce Type: new  Abstract: Traveling waves are widely observed in the brain, but their precise computational function remains unclear. One prominent hypothesis is that they enable the transfer and integration of spatial information across neural populations. However, few computational models have explored how traveling waves might be harnessed to perform such integrative processing. Drawing inspiration from the famous ``Can one hear the shape of a drum?'' problem -- which highlights how spectral modes encode geometric information -- we introduce a set of convolutional recurrent neural networks that learn to produce traveling waves in their hidden states in response to visual stimuli. By applying a spectral decomposition to these wave-like activations, we obtain a powerful new representational space that outperforms equivalently local feed-forward networks on tasks requiring global spatial context. In particular, we observe that traveling waves effectively expand the receptive field of locally connected neurons, supporting long-range encoding and communication of information. We demonstrate that models equipped with this mechanism and spectral readouts solve visual semantic segmentation tasks demanding global integration, where local feed-forward models fail. As a first step toward traveling-wave-based representations in artificial networks, our findings suggest potential efficiency benefits and offer a new framework for connecting to biological recordings of neural activity.",
        "arxiv_id": "2502.06034",
        "ARXIVID": "2502.06034",
        "COMMENT": "Matches criterion 1. Introduces a method for integrating spatial information into spectral representations using traveling waves, which relates to spatial understanding in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.06682": {
        "authors": [
            "Tai-Yu Pan",
            "Sooyoung Jeon",
            "Mengdi Fan",
            "Jinsu Yoo",
            "Zhenyang Feng",
            "Mark Campbell",
            "Kilian Q. Weinberger",
            "Bharath Hariharan",
            "Wei-Lun Chao"
        ],
        "title": "Transfer Your Perspective: Controllable 3D Generation from Any Viewpoint in a Driving Scene",
        "abstract": "arXiv:2502.06682v1 Announce Type: new  Abstract: Self-driving cars relying solely on ego-centric perception face limitations in sensing, often failing to detect occluded, faraway objects. Collaborative autonomous driving (CAV) seems like a promising direction, but collecting data for development is non-trivial. It requires placing multiple sensor-equipped agents in a real-world driving scene, simultaneously! As such, existing datasets are limited in locations and agents. We introduce a novel surrogate to the rescue, which is to generate realistic perception from different viewpoints in a driving scene, conditioned on a real-world sample - the ego-car's sensory data. This surrogate has huge potential: it could potentially turn any ego-car dataset into a collaborative driving one to scale up the development of CAV. We present the very first solution, using a combination of simulated collaborative data and real ego-car data. Our method, Transfer Your Perspective (TYP), learns a conditioned diffusion model whose output samples are not only realistic but also consistent in both semantics and layouts with the given ego-car data. Empirical results demonstrate TYP's effectiveness in aiding in a CAV setting. In particular, TYP enables us to (pre-)train collaborative perception algorithms like early and late fusion with little or no real-world collaborative data, greatly facilitating downstream CAV applications.",
        "arxiv_id": "2502.06682",
        "ARXIVID": "2502.06682",
        "COMMENT": "Matches criterion 3 as it introduces a novel surrogate for collaborative autonomous driving and a new method for generating realistic perception from different viewpoints.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.05857": {
        "authors": [
            "Lu Chen",
            "Yizhou Wang",
            "Shixiang Tang",
            "Qianhong Ma",
            "Tong He",
            "Wanli Ouyang",
            "Xiaowei Zhou",
            "Hujun Bao",
            "Sida Peng"
        ],
        "title": "Acquisition through My Eyes and Steps: A Joint Predictive Agent Model in Egocentric Worlds",
        "abstract": "arXiv:2502.05857v1 Announce Type: new  Abstract: This paper addresses the task of learning an agent model behaving like humans, which can jointly perceive, predict, and act in egocentric worlds. Previous methods usually train separate models for these three abilities, leading to information silos among them, which prevents these abilities from learning from each other and collaborating effectively. In this paper, we propose a joint predictive agent model, named EgoAgent, that simultaneously learns to represent the world, predict future states, and take reasonable actions with a single transformer. EgoAgent unifies the representational spaces of the three abilities by mapping them all into a sequence of continuous tokens. Learnable query tokens are appended to obtain current states, future states, and next actions. With joint supervision, our agent model establishes the internal relationship among these three abilities and effectively mimics the human inference and learning processes. Comprehensive evaluations of EgoAgent covering image classification, egocentric future state prediction, and 3D human motion prediction tasks demonstrate the superiority of our method. The code and trained model will be released for reproducibility.",
        "arxiv_id": "2502.05857",
        "ARXIVID": "2502.05857",
        "COMMENT": "This paper matches criterion 3 as it proposes a novel embodied AI agent model (EgoAgent) that unifies perception, prediction, and action in egocentric worlds, addressing a previously unexplored angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.06219": {
        "authors": [
            "Sicen Guo",
            "Tianyou Wen",
            "Chuang-Wei Liu",
            "Qijun Chen",
            "Rui Fan"
        ],
        "title": "Fully Exploiting Vision Foundation Model's Profound Prior Knowledge for Generalizable RGB-Depth Driving Scene Parsing",
        "abstract": "arXiv:2502.06219v1 Announce Type: new  Abstract: Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks. Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored. In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing. Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT). This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs. Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps. Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets. We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing. Our source code is publicly available at https://mias.group/HFIT.",
        "arxiv_id": "2502.06219",
        "ARXIVID": "2502.06219",
        "COMMENT": "Matches criterion 4. Explores vision foundation models (VFMs) and their application to RGB-depth driving scene parsing.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2502.05574": {
        "authors": [
            "Shiao Wang",
            "Xiao Wang",
            "Chao Wang",
            "Liye Jin",
            "Lin Zhu",
            "Bo Jiang",
            "Yonghong Tian",
            "Jin Tang"
        ],
        "title": "Event Stream-based Visual Object Tracking: HDETrack V2 and A High-Definition Benchmark",
        "abstract": "arXiv:2502.05574v1 Announce Type: new  Abstract: We then introduce a novel hierarchical knowledge distillation strategy that incorporates the similarity matrix, feature representation, and response map-based distillation to guide the learning of the student Transformer network. We also enhance the model's ability to capture temporal dependencies by applying the temporal Fourier transform to establish temporal relationships between video frames. We adapt the network model to specific target objects during testing via a newly proposed test-time tuning strategy to achieve high performance and flexibility in target tracking. Recognizing the limitations of existing event-based tracking datasets, which are predominantly low-resolution, we propose EventVOT, the first large-scale high-resolution event-based tracking dataset. It comprises 1141 videos spanning diverse categories such as pedestrians, vehicles, UAVs, ping pong, etc. Extensive experiments on both low-resolution (FE240hz, VisEvent, FELT), and our newly proposed high-resolution EventVOT dataset fully validated the effectiveness of our proposed method. Both the benchmark dataset and source code have been released on https://github.com/Event-AHU/EventVOT_Benchmark",
        "arxiv_id": "2502.05574",
        "ARXIVID": "2502.05574",
        "COMMENT": "This paper matches criterion 3 as it introduces a new high-resolution event-based tracking benchmark (EventVOT) and proposes a novel method for event-based visual object tracking.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2502.06019": {
        "authors": [
            "Raza Imam",
            "Asif Hanif",
            "Jian Zhang",
            "Khaled Waleed Dawoud",
            "Yova Kementchedjhieva",
            "Mohammad Yaqub"
        ],
        "title": "Noise is an Efficient Learner for Zero-Shot Vision-Language Models",
        "abstract": "arXiv:2502.06019v1 Announce Type: new  Abstract: Recently, test-time adaptation has garnered attention as a method for tuning models without labeled data. The conventional modus operandi for adapting pre-trained vision-language models (VLMs) during test-time primarily focuses on tuning learnable prompts; however, this approach overlooks potential distribution shifts in the visual representations themselves. In this work, we address this limitation by introducing Test-Time Noise Tuning (TNT), a novel method for handling unpredictable shifts in the visual space. TNT leverages, for the first time, a noise adaptation strategy that optimizes learnable noise directly in the visual input space, enabling adaptive feature learning from a single test sample. We further introduce a novel approach for inter-view representation alignment by explicitly enforcing coherence in embedding distances, ensuring consistent feature representations across views. Combined with scaled logits and confident view selection at inference, TNT substantially enhances VLM generalization and calibration, achieving average gains of +7.38% on natural distributions benchmark and +0.80% on cross-dataset evaluations over zero-shot CLIP. These improvements lay a strong foundation for adaptive out-of-distribution handling.",
        "arxiv_id": "2502.06019",
        "ARXIVID": "2502.06019",
        "COMMENT": "Matches criterion 2. Proposes a novel test-time adaptation method for vision-language models (VLMs), which aligns with the interest in VLLMs or MLLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.05979": {
        "authors": [
            "Xinyu Liu",
            "Ailing Zeng",
            "Wei Xue",
            "Harry Yang",
            "Wenhan Luo",
            "Qifeng Liu",
            "Yike Guo"
        ],
        "title": "VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer",
        "abstract": "arXiv:2502.05979v1 Announce Type: new  Abstract: Crafting magic and illusions is one of the most thrilling aspects of filmmaking, with visual effects (VFX) serving as the powerhouse behind unforgettable cinematic experiences. While recent advances in generative artificial intelligence have driven progress in generic image and video synthesis, the domain of controllable VFX generation remains relatively underexplored. In this work, we propose a novel paradigm for animated VFX generation as image animation, where dynamic effects are generated from user-friendly textual descriptions and static reference images.   Our work makes two primary contributions: (i) Open-VFX, the first high-quality VFX video dataset spanning 15 diverse effect categories, annotated with textual descriptions, instance segmentation masks for spatial conditioning, and start-end timestamps for temporal control. (ii) VFX Creator, a simple yet effective controllable VFX generation framework based on a Video Diffusion Transformer. The model incorporates a spatial and temporal controllable LoRA adapter, requiring minimal training videos. Specifically, a plug-and-play mask control module enables instance-level spatial manipulation, while tokenized start-end motion timestamps embedded in the diffusion process, alongside the text encoder, allow precise temporal control over effect timing and pace.   Extensive experiments on the Open-VFX test set demonstrate the superiority of the proposed system in generating realistic and dynamic effects, achieving state-of-the-art performance and generalization ability in both spatial and temporal controllability. Furthermore, we introduce a specialized metric to evaluate the precision of temporal control. By bridging traditional VFX techniques with generative approaches, VFX Creator unlocks new possibilities for efficient and high-quality video effect generation, making advanced VFX accessible to a broader audience.",
        "arxiv_id": "2502.05979",
        "ARXIVID": "2502.05979",
        "COMMENT": "Matches criterion 2 as it proposes a controllable diffusion transformer for animated visual effect generation, which is a novel application of generative modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.06527": {
        "authors": [
            "D. She",
            "Mushui Liu",
            "Jingxuan Pang",
            "Jin Wang",
            "Zhen Yang",
            "Wanggui He",
            "Guanghao Zhang",
            "Yi Wang",
            "Qihan Huang",
            "Haobin Tang",
            "Yunlong Yu",
            "Siming Fu"
        ],
        "title": "CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers",
        "abstract": "arXiv:2502.06527v1 Announce Type: new  Abstract: Customized generation has achieved significant progress in image synthesis, yet personalized video generation remains challenging due to temporal inconsistencies and quality degradation. In this paper, we introduce CustomVideoX, an innovative framework leveraging the video diffusion transformer for personalized video generation from a reference image. CustomVideoX capitalizes on pre-trained video networks by exclusively training the LoRA parameters to extract reference features, ensuring both efficiency and adaptability. To facilitate seamless interaction between the reference image and video content, we propose 3D Reference Attention, which enables direct and simultaneous engagement of reference image features with all video frames across spatial and temporal dimensions. To mitigate the excessive influence of reference image features and textual guidance on generated video content during inference, we implement the Time-Aware Reference Attention Bias (TAB) strategy, dynamically modulating reference bias over different time steps. Additionally, we introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly activated regions of key entity tokens with reference feature injection by adjusting attention bias. To thoroughly evaluate personalized video generation, we establish a new benchmark, VideoBench, comprising over 50 objects and 100 prompts for extensive assessment. Experimental results show that CustomVideoX significantly outperforms existing methods in terms of video consistency and quality.",
        "arxiv_id": "2502.06527",
        "ARXIVID": "2502.06527",
        "COMMENT": "Matches criterion 2 as it introduces a novel framework for personalized video generation using video diffusion transformers.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.06787": {
        "authors": [
            "Damiano Marsili",
            "Rohun Agrawal",
            "Yisong Yue",
            "Georgia Gkioxari"
        ],
        "title": "Visual Agentic AI for Spatial Reasoning with a Dynamic API",
        "abstract": "arXiv:2502.06787v1 Announce Type: new  Abstract: Visual reasoning -- the ability to interpret the visual world -- is crucial for embodied agents that operate within three-dimensional scenes. Progress in AI has led to vision and language models capable of answering questions from images. However, their performance declines when tasked with 3D spatial reasoning. To tackle the complexity of such reasoning problems, we introduce an agentic program synthesis approach where LLM agents collaboratively generate a Pythonic API with new functions to solve common subproblems. Our method overcomes limitations of prior approaches that rely on a static, human-defined API, allowing it to handle a wider range of queries. To assess AI capabilities for 3D understanding, we introduce a new benchmark of queries involving multiple steps of grounding and inference. We show that our method outperforms prior zero-shot models for visual reasoning in 3D and empirically validate the effectiveness of our agentic framework for 3D spatial reasoning tasks. Project website: https://glab-caltech.github.io/vadar/",
        "arxiv_id": "2502.06787",
        "ARXIVID": "2502.06787",
        "COMMENT": "Matches criterion 1 as it focuses on spatial reasoning for embodied agents using a dynamic API.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2502.06583": {
        "authors": [
            "Xiantao Hu",
            "Bineng Zhong",
            "Qihua Liang",
            "Zhiyi Mo",
            "Liangtao Shi",
            "Ying Tai",
            "Jian Yang"
        ],
        "title": "Adaptive Perception for Unified Visual Multi-modal Object Tracking",
        "abstract": "arXiv:2502.06583v1 Announce Type: new  Abstract: Recently, many multi-modal trackers prioritize RGB as the dominant modality, treating other modalities as auxiliary, and fine-tuning separately various multi-modal tasks. This imbalance in modality dependence limits the ability of methods to dynamically utilize complementary information from each modality in complex scenarios, making it challenging to fully perceive the advantages of multi-modal. As a result, a unified parameter model often underperforms in various multi-modal tracking tasks. To address this issue, we propose APTrack, a novel unified tracker designed for multi-modal adaptive perception. Unlike previous methods, APTrack explores a unified representation through an equal modeling strategy. This strategy allows the model to dynamically adapt to various modalities and tasks without requiring additional fine-tuning between different tasks. Moreover, our tracker integrates an adaptive modality interaction (AMI) module that efficiently bridges cross-modality interactions by generating learnable tokens. Experiments conducted on five diverse multi-modal datasets (RGBT234, LasHeR, VisEvent, DepthTrack, and VOT-RGBD2022) demonstrate that APTrack not only surpasses existing state-of-the-art unified multi-modal trackers but also outperforms trackers designed for specific multi-modal tasks.",
        "arxiv_id": "2502.06583",
        "ARXIVID": "2502.06583",
        "COMMENT": "This paper aligns with criterion 2 as it proposes a unified multi-modal tracker (APTrack) that dynamically adapts to various modalities and tasks, showcasing advancements in multi-modal learning.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2502.05902": {
        "authors": [
            "Xuelin Shen",
            "Yitong Wang",
            "Silin Zheng",
            "Kang Xiao",
            "Wenhan Yang",
            "Xu Wang"
        ],
        "title": "Fast Omni-Directional Image Super-Resolution: Adapting the Implicit Image Function with Pixel and Semantic-Wise Spherical Geometric Priors",
        "abstract": "arXiv:2502.05902v1 Announce Type: new  Abstract: In the context of Omni-Directional Image (ODI) Super-Resolution (SR), the unique challenge arises from the non-uniform oversampling characteristics caused by EquiRectangular Projection (ERP). Considerable efforts in designing complex spherical convolutions or polyhedron reprojection offer significant performance improvements but at the expense of cumbersome processing procedures and slower inference speeds. Under these circumstances, this paper proposes a new ODI-SR model characterized by its capacity to perform Fast and Arbitrary-scale ODI-SR processes, denoted as FAOR. The key innovation lies in adapting the implicit image function from the planar image domain to the ERP image domain by incorporating spherical geometric priors at both the latent representation and image reconstruction stages, in a low-overhead manner. Specifically, at the latent representation stage, we adopt a pair of pixel-wise and semantic-wise sphere-to-planar distortion maps to perform affine transformations on the latent representation, thereby incorporating it with spherical properties. Moreover, during the image reconstruction stage, we introduce a geodesic-based resampling strategy, aligning the implicit image function with spherical geometrics without introducing additional parameters. As a result, the proposed FAOR outperforms the state-of-the-art ODI-SR models with a much faster inference speed. Extensive experimental results and ablation studies have demonstrated the effectiveness of our design.",
        "arxiv_id": "2502.05902",
        "ARXIVID": "2502.05902",
        "COMMENT": "Matches criterion 1 as it proposes a novel method for Omni-Directional Image Super-Resolution with spherical geometric priors, which is relevant to spatial intelligence.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.06194": {
        "authors": [
            "You Zhou",
            "Jiangshan Zhao",
            "Deyu Zeng",
            "Zuo Zuo",
            "Weixiang Liu",
            "Zongze Wu"
        ],
        "title": "Multimodal Task Representation Memory Bank vs. Catastrophic Forgetting in Anomaly Detection",
        "abstract": "arXiv:2502.06194v1 Announce Type: new  Abstract: Unsupervised Continuous Anomaly Detection (UCAD) faces significant challenges in multi-task representation learning, with existing methods suffering from incomplete representation and catastrophic forgetting. Unlike supervised models, unsupervised scenarios lack prior information, making it difficult to effectively distinguish redundant and complementary multimodal features. To address this, we propose the Multimodal Task Representation Memory Bank (MTRMB) method through two key technical innovations: A Key-Prompt-Multimodal Knowledge (KPMK) mechanism that uses concise key prompts to guide cross-modal feature interaction between BERT and ViT. Refined Structure-based Contrastive Learning (RSCL) leveraging Grounding DINO and SAM to generate precise segmentation masks, pulling features of the same structural region closer while pushing different structural regions apart. Experiments on MVtec AD and VisA datasets demonstrate MTRMB's superiority, achieving an average detection accuracy of 0.921 at the lowest forgetting rate, significantly outperforming state-of-the-art methods. We plan to open source on GitHub.",
        "arxiv_id": "2502.06194",
        "ARXIVID": "2502.06194",
        "COMMENT": "Matches criterion 2 as it discusses multimodal task representation and anomaly detection, which aligns with multi-modal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.05859": {
        "authors": [
            "Qingsong Yan",
            "Qiang Wang",
            "Kaiyong Zhao",
            "Jie Chen",
            "Bo Li",
            "Xiaowen Chu",
            "Fei Deng"
        ],
        "title": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion",
        "abstract": "arXiv:2502.05859v1 Announce Type: new  Abstract: Due to the rapid development of panorama cameras, the task of estimating panorama depth has attracted significant attention from the computer vision community, especially in applications such as robot sensing and autonomous driving. However, existing methods relying on different projection formats often encounter challenges, either struggling with distortion and discontinuity in the case of equirectangular, cubemap, and tangent projections, or experiencing a loss of texture details with the spherical projection. To tackle these concerns, we present SphereFusion, an end-to-end framework that combines the strengths of various projection methods. Specifically, SphereFusion initially employs 2D image convolution and mesh operations to extract two distinct types of features from the panorama image in both equirectangular and spherical projection domains. These features are then projected onto the spherical domain, where a gate fusion module selects the most reliable features for fusion. Finally, SphereFusion estimates panorama depth within the spherical domain. Meanwhile, SphereFusion employs a cache strategy to improve the efficiency of mesh operation. Extensive experiments on three public panorama datasets demonstrate that SphereFusion achieves competitive results with other state-of-the-art methods, while presenting the fastest inference speed at only 17 ms on a 512$\\times$1024 panorama image.",
        "arxiv_id": "2502.05859",
        "ARXIVID": "2502.05859",
        "COMMENT": "Matches criterion 1 as it proposes a novel method for panorama depth estimation, which is relevant to spatial understanding in embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.05503": {
        "authors": [
            "Yongfan Chen",
            "Xiuwen Zhu",
            "Tianyu Li",
            "Hao Chen",
            "Chunhua Shen"
        ],
        "title": "A Physical Coherence Benchmark for Evaluating Video Generation Models via Optical Flow-guided Frame Prediction",
        "abstract": "arXiv:2502.05503v1 Announce Type: new  Abstract: Recent advances in video generation models demonstrate their potential as world simulators, but they often struggle with videos deviating from physical laws, a key concern overlooked by most text-to-video benchmarks. We introduce a benchmark designed specifically to assess the Physical Coherence of generated videos, PhyCoBench. Our benchmark includes 120 prompts covering 7 categories of physical principles, capturing key physical laws observable in video content. We evaluated four state-of-the-art (SoTA) T2V models on PhyCoBench and conducted manual assessments. Additionally, we propose an automated evaluation model: PhyCoPredictor, a diffusion model that generates optical flow and video frames in a cascade manner. Through a consistency evaluation comparing automated and manual sorting, the experimental results show that PhyCoPredictor currently aligns most closely with human evaluation. Therefore, it can effectively evaluate the physical coherence of videos, providing insights for future model optimization. Our benchmark, which includes physical coherence prompts, automatic evaluation tool PhyCoPredictor, and generated video dataset, will all be released on GitHub shortly.",
        "arxiv_id": "2502.05503",
        "ARXIVID": "2502.05503",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (PhyCoBench) for evaluating physical coherence in video generation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.05772": {
        "authors": [
            "Yijun Yang",
            "Lichao Wang",
            "Xiao Yang",
            "Lanqing Hong",
            "Jun Zhu"
        ],
        "title": "Effective Black-Box Multi-Faceted Attacks Breach Vision Large Language Model Guardrails",
        "abstract": "arXiv:2502.05772v1 Announce Type: new  Abstract: Vision Large Language Models (VLLMs) integrate visual data processing, expanding their real-world applications, but also increasing the risk of generating unsafe responses. In response, leading companies have implemented Multi-Layered safety defenses, including alignment training, safety system prompts, and content moderation. However, their effectiveness against sophisticated adversarial attacks remains largely unexplored. In this paper, we propose MultiFaceted Attack, a novel attack framework designed to systematically bypass Multi-Layered Defenses in VLLMs. It comprises three complementary attack facets: Visual Attack that exploits the multimodal nature of VLLMs to inject toxic system prompts through images; Alignment Breaking Attack that manipulates the model's alignment mechanism to prioritize the generation of contrasting responses; and Adversarial Signature that deceives content moderators by strategically placing misleading information at the end of the response. Extensive evaluations on eight commercial VLLMs in a black-box setting demonstrate that MultiFaceted Attack achieves a 61.56% attack success rate, surpassing state-of-the-art methods by at least 42.18%.",
        "arxiv_id": "2502.05772",
        "ARXIVID": "2502.05772",
        "COMMENT": "Matches criterion 2 as it discusses Vision Large Language Models (VLLMs) and their vulnerabilities to adversarial attacks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.05928": {
        "authors": [
            "Hongyu Ge",
            "Longkun Hao",
            "Zihui Xu",
            "Zhenxin Lin",
            "Bin Li",
            "Shoujun Zhou",
            "Hongjin Zhao",
            "Yihang Liu"
        ],
        "title": "ClinKD: Cross-Modal Clinic Knowledge Distiller For Multi-Task Medical Images",
        "abstract": "arXiv:2502.05928v1 Announce Type: new  Abstract: Med-VQA (Medical Visual Question Answering) is a crucial subtask within the broader VQA (Visual Question Answering) domain. This task requires a visual question answering system to analyze the provided image and corresponding question,offering reasonable analysis and suggestions to assist medical professionals in making pathological diagnoses, or ideally, enabling the system to independently provide correct diagnoses. Furthermore, more advanced Med-VQA tasks involve Referring and Grounding, which not only require the system to accurately comprehend medical images but also to pinpoint specific biological locations within those images. While many large pre-trained models have demonstrated substantial VQA capabilities,challenges persist in the medical imaging domain. The intricacy of biological features in medical images and the scarcity of high-quality medical image datasets, combined with the fact that current models are not tailored for the medical field in terms of architecture and training paradigms, hinder the full exploitation of model generalization. This results in issues such as hallucination in Visual Grounding. In this paper, we introduce the ClinKD model, which incorporates modifications to model position encoding and a diversified training process. Initially, we enhance the model's ability to perceive image and modality variations by using Med-CLIP Guided Rotary Position Embedding. Subsequently, we leverage distillation to provide prior knowledge to the model before using complete training data. Additionally, the feedback-based training process during the formal training phase further enhances data utilization. Notably, under unchanged evaluation protocols, we achieve a new state-of-the-art performance on the Med-GRIT-270k dataset, and the Med-CLIP Guided Rotary Position Embedding approach presents potential for generalizing to universal model position encoding.",
        "arxiv_id": "2502.05928",
        "ARXIVID": "2502.05928",
        "COMMENT": "Matches criterion 2 as it introduces a new model (ClinKD) for Med-VQA tasks, focusing on multi-modal learning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.06708": {
        "authors": [
            "Muhammad Bilal",
            "Mahmood Alam",
            "Deepa Bapu",
            "Stephan Korsgen",
            "Neeraj Lal",
            "Simon Bach",
            "Amir M Hajivanand",
            "Muhammed Ali",
            "Kamran Soomro",
            "Iqbal Qasim",
            "Pawe{\\l} Capik",
            "Aslam Khan",
            "Zaheer Khan",
            "Hunaid Vohra",
            "Massimo Caputo",
            "Andrew Beggs",
            "Adnan Qayyum",
            "Junaid Qadir",
            "Shazad Ashraf"
        ],
        "title": "TEMSET-24K: Densely Annotated Dataset for Indexing Multipart Endoscopic Videos using Surgical Timeline Segmentation",
        "abstract": "arXiv:2502.06708v1 Announce Type: new  Abstract: Indexing endoscopic surgical videos is vital in surgical data science, forming the basis for systematic retrospective analysis and clinical performance evaluation. Despite its significance, current video analytics rely on manual indexing, a time-consuming process. Advances in computer vision, particularly deep learning, offer automation potential, yet progress is limited by the lack of publicly available, densely annotated surgical datasets. To address this, we present TEMSET-24K, an open-source dataset comprising 24,306 trans-anal endoscopic microsurgery (TEMS) video micro-clips. Each clip is meticulously annotated by clinical experts using a novel hierarchical labeling taxonomy encompassing phase, task, and action triplets, capturing intricate surgical workflows. To validate this dataset, we benchmarked deep learning models, including transformer-based architectures. Our in silico evaluation demonstrates high accuracy (up to 0.99) and F1 scores (up to 0.99) for key phases like Setup and Suturing. The STALNet model, tested with ConvNeXt, ViT, and SWIN V2 encoders, consistently segmented well-represented phases. TEMSET-24K provides a critical benchmark, propelling state-of-the-art solutions in surgical data science.",
        "arxiv_id": "2502.06708",
        "ARXIVID": "2502.06708",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset (TEMSET-24K) for surgical video analysis.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.06428": {
        "authors": [
            "Jian Hu",
            "Zixu Cheng",
            "Chenyang Si",
            "Wei Li",
            "Shaogang Gong"
        ],
        "title": "CoS: Chain-of-Shot Prompting for Long Video Understanding",
        "abstract": "arXiv:2502.06428v1 Announce Type: new  Abstract: Multi-modal Large Language Models (MLLMs) struggle with long videos due to the need for excessive visual tokens. These tokens exceed massively the context length of MLLMs, resulting in filled by redundant task-irrelevant shots. How to select shots is an unsolved critical problem: sparse sampling risks missing key details, while exhaustive sampling overwhelms the model with irrelevant content, leading to video misunderstanding. To solve this problem, we propose Chain-of-Shot prompting (CoS). The key idea is to frame shot selection as test-time visual prompt optimisation, choosing shots adaptive to video understanding semantic task by optimising shots-task alignment. CoS has two key parts: (1) a binary video summary mechanism that performs pseudo temporal grounding, discovering a binary coding to identify task-relevant shots, and (2) a video co-reasoning module that deploys the binary coding to pair (learning to align) task-relevant positive shots with irrelevant negative shots. It embeds the optimised shot selections into the original video, facilitating a focus on relevant context to optimize long video understanding. Experiments across three baselines and five datasets demonstrate the effectiveness and adaptability of CoS. Code given in https://lwpyh.github.io/CoS.",
        "arxiv_id": "2502.06428",
        "ARXIVID": "2502.06428",
        "COMMENT": "Matches criterion 2 as it discusses a new method (Chain-of-Shot prompting) for improving MLLMs' understanding of long videos.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.06756": {
        "authors": [
            "Yuqi Lin",
            "Hengjia Li",
            "Wenqi Shao",
            "Zheng Yang",
            "Jun Zhao",
            "Xiaofei He",
            "Ping Luo",
            "Kaipeng Zhang"
        ],
        "title": "SAMRefiner: Taming Segment Anything Model for Universal Mask Refinement",
        "abstract": "arXiv:2502.06756v1 Announce Type: new  Abstract: In this paper, we explore a principal way to enhance the quality of widely pre-existing coarse masks, enabling them to serve as reliable training data for segmentation models to reduce the annotation cost. In contrast to prior refinement techniques that are tailored to specific models or tasks in a close-world manner, we propose SAMRefiner, a universal and efficient approach by adapting SAM to the mask refinement task. The core technique of our model is the noise-tolerant prompting scheme. Specifically, we introduce a multi-prompt excavation strategy to mine diverse input prompts for SAM (i.e., distance-guided points, context-aware elastic bounding boxes, and Gaussian-style masks) from initial coarse masks. These prompts can collaborate with each other to mitigate the effect of defects in coarse masks. In particular, considering the difficulty of SAM to handle the multi-object case in semantic segmentation, we introduce a split-then-merge (STM) pipeline. Additionally, we extend our method to SAMRefiner++ by introducing an additional IoU adaption step to further boost the performance of the generic SAMRefiner on the target dataset. This step is self-boosted and requires no additional annotation. The proposed framework is versatile and can flexibly cooperate with existing segmentation methods. We evaluate our mask framework on a wide range of benchmarks under different settings, demonstrating better accuracy and efficiency. SAMRefiner holds significant potential to expedite the evolution of refinement tools. Our code is available at https://github.com/linyq2117/SAMRefiner.",
        "arxiv_id": "2502.06756",
        "ARXIVID": "2502.06756",
        "COMMENT": "This paper aligns with criterion 4 as it explores the refinement of masks using the Segment Anything Model (SAM), which is related to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.06337": {
        "authors": [
            "Taosi Xu",
            "Yinlong Liu",
            "Xianbo Wang",
            "Zhi-Xin Yang"
        ],
        "title": "Accelerating Outlier-robust Rotation Estimation by Stereographic Projection",
        "abstract": "arXiv:2502.06337v1 Announce Type: new  Abstract: Rotation estimation plays a fundamental role in many computer vision and robot tasks. However, efficiently estimating rotation in large inputs containing numerous outliers (i.e., mismatches) and noise is a recognized challenge. Many robust rotation estimation methods have been designed to address this challenge. Unfortunately, existing methods are often inapplicable due to their long computation time and the risk of local optima. In this paper, we propose an efficient and robust rotation estimation method. Specifically, our method first investigates geometric constraints involving only the rotation axis. Then, it uses stereographic projection and spatial voting techniques to identify the rotation axis and angle. Furthermore, our method efficiently obtains the optimal rotation estimation and can estimate multiple rotations simultaneously. To verify the feasibility of our method, we conduct comparative experiments using both synthetic and real-world data. The results show that, with GPU assistance, our method can solve large-scale ($10^6$ points) and severely corrupted (90\\% outlier rate) rotation estimation problems within 0.07 seconds, with an angular error of only 0.01 degrees, which is superior to existing methods in terms of accuracy and efficiency.",
        "arxiv_id": "2502.06337",
        "ARXIVID": "2502.06337",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for robust rotation estimation, which is relevant to embodied AI and novel methods.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.06782": {
        "authors": [
            "Dongyang Liu",
            "Shicheng Li",
            "Yutong Liu",
            "Zhen Li",
            "Kai Wang",
            "Xinyue Li",
            "Qi Qin",
            "Yufei Liu",
            "Yi Xin",
            "Zhongyu Li",
            "Bin Fu",
            "Chenyang Si",
            "Yuewen Cao",
            "Conghui He",
            "Ziwei Liu",
            "Yu Qiao",
            "Qibin Hou",
            "Hongsheng Li",
            "Peng Gao"
        ],
        "title": "Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT",
        "abstract": "arXiv:2502.06782v1 Announce Type: new  Abstract: Recent advancements have established Diffusion Transformers (DiTs) as a dominant framework in generative modeling. Building on this success, Lumina-Next achieves exceptional performance in the generation of photorealistic images with Next-DiT. However, its potential for video generation remains largely untapped, with significant challenges in modeling the spatiotemporal complexity inherent to video data. To address this, we introduce Lumina-Video, a framework that leverages the strengths of Next-DiT while introducing tailored solutions for video synthesis. Lumina-Video incorporates a Multi-scale Next-DiT architecture, which jointly learns multiple patchifications to enhance both efficiency and flexibility. By incorporating the motion score as an explicit condition, Lumina-Video also enables direct control of generated videos' dynamic degree. Combined with a progressive training scheme with increasingly higher resolution and FPS, and a multi-source training scheme with mixed natural and synthetic data, Lumina-Video achieves remarkable aesthetic quality and motion smoothness at high training and inference efficiency. We additionally propose Lumina-V2A, a video-to-audio model based on Next-DiT, to create synchronized sounds for generated videos. Codes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.",
        "arxiv_id": "2502.06782",
        "ARXIVID": "2502.06782",
        "COMMENT": "Matches criterion 4 as it introduces a video generation framework based on Diffusion Transformers, which is related to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.05869": {
        "authors": [
            "Yue Li",
            "Haoxuan Qu",
            "Mengyuan Liu",
            "Jun Liu",
            "Yujun Cai"
        ],
        "title": "HyLiFormer: Hyperbolic Linear Attention for Skeleton-based Human Action Recognition",
        "abstract": "arXiv:2502.05869v1 Announce Type: new  Abstract: Transformers have demonstrated remarkable performance in skeleton-based human action recognition, yet their quadratic computational complexity remains a bottleneck for real-world applications. To mitigate this, linear attention mechanisms have been explored but struggle to capture the hierarchical structure of skeleton data. Meanwhile, the Poincar\\'e model, as a typical hyperbolic geometry, offers a powerful framework for modeling hierarchical structures but lacks well-defined operations for existing mainstream linear attention. In this paper, we propose HyLiFormer, a novel hyperbolic linear attention Transformer tailored for skeleton-based action recognition. Our approach incorporates a Hyperbolic Transformation with Curvatures (HTC) module to map skeleton data into hyperbolic space and a Hyperbolic Linear Attention (HLA) module for efficient long-range dependency modeling. Theoretical analysis and extensive experiments on NTU RGB+D and NTU RGB+D 120 datasets demonstrate that HyLiFormer significantly reduces computational complexity while preserving model accuracy, making it a promising solution for efficiency-critical applications.",
        "arxiv_id": "2502.05869",
        "ARXIVID": "2502.05869",
        "COMMENT": "Matches criterion 1 as it proposes a hyperbolic linear attention Transformer for skeleton-based human action recognition, which involves spatial understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.06114": {
        "authors": [
            "Seung-Hyun Song",
            "Dong-Hee Paek",
            "Minh-Quan Dao",
            "Ezio Malis",
            "Seung-Hyun Kong"
        ],
        "title": "A Novel Multi-Teacher Knowledge Distillation for Real-Time Object Detection using 4D Radar",
        "abstract": "arXiv:2502.06114v1 Announce Type: new  Abstract: Accurate 3D object detection is crucial for safe autonomous navigation, requiring reliable performance across diverse weather conditions. While LiDAR performance deteriorates in challenging weather, Radar systems maintain their reliability. Traditional Radars have limitations due to their lack of elevation data, but the recent 4D Radars overcome this by measuring elevation alongside range, azimuth, and Doppler velocity, making them invaluable for autonomous vehicles. The primary challenge in utilizing 4D Radars is the sparsity of their point clouds. Previous works address this by developing architectures that better capture semantics and context in sparse point cloud, largely drawing from LiDAR-based approaches. However, these methods often overlook a unique advantage of 4D Radars: the dense Radar tensor, which encapsulates power measurements across three spatial dimensions and the Doppler dimension. Our paper leverages this tensor to tackle the sparsity issue. We introduce a novel knowledge distillation framework that enables a student model to densify its sparse input in the latent space by emulating an ensemble of teacher models. Our experiments demonstrate a 25% performance improvement over the state-of-the-art RTNH model on the K-Radar dataset. Notably, this improvement is achieved while still maintaining a real-time inference speed.",
        "arxiv_id": "2502.06114",
        "ARXIVID": "2502.06114",
        "COMMENT": "Matches criterion 3 as it introduces a novel knowledge distillation framework for 4D Radar-based object detection, which is relevant to embodied AI and novel methods.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.05534": {
        "authors": [
            "Yin Wang",
            "Mu Li",
            "Jiapeng Liu",
            "Zhiying Leng",
            "Frederick W. B. Li",
            "Ziyao Zhang",
            "Xiaohui Liang"
        ],
        "title": "Fg-T2M++: LLMs-Augmented Fine-Grained Text Driven Human Motion Generation",
        "abstract": "arXiv:2502.05534v1 Announce Type: new  Abstract: We address the challenging problem of fine-grained text-driven human motion generation. Existing works generate imprecise motions that fail to accurately capture relationships specified in text due to: (1) lack of effective text parsing for detailed semantic cues regarding body parts, (2) not fully modeling linguistic structures between words to comprehend text comprehensively. To tackle these limitations, we propose a novel fine-grained framework Fg-T2M++ that consists of: (1) an LLMs semantic parsing module to extract body part descriptions and semantics from text, (2) a hyperbolic text representation module to encode relational information between text units by embedding the syntactic dependency graph into hyperbolic space, and (3) a multi-modal fusion module to hierarchically fuse text and motion features. Extensive experiments on HumanML3D and KIT-ML datasets demonstrate that Fg-T2M++ outperforms SOTA methods, validating its ability to accurately generate motions adhering to comprehensive text semantics.",
        "arxiv_id": "2502.05534",
        "ARXIVID": "2502.05534",
        "COMMENT": "Matches criterion 1 as it proposes a novel framework for fine-grained text-driven human motion generation, which involves spatial understanding and intelligence in embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.06734": {
        "authors": [
            "Bojia Zi",
            "Penghui Ruan",
            "Marco Chen",
            "Xianbiao Qi",
            "Shaozhe Hao",
            "Shihao Zhao",
            "Youze Huang",
            "Bin Liang",
            "Rong Xiao",
            "Kam-Fai Wong"
        ],
        "title": "Se\\~norita-2M: A High-Quality Instruction-based Dataset for General Video Editing by Video Specialists",
        "abstract": "arXiv:2502.06734v1 Announce Type: new  Abstract: Recent advancements in video generation have spurred the development of video editing techniques, which can be divided into inversion-based and end-to-end methods. However, current video editing methods still suffer from several challenges. Inversion-based methods, though training-free and flexible, are time-consuming during inference, struggle with fine-grained editing instructions, and produce artifacts and jitter. On the other hand, end-to-end methods, which rely on edited video pairs for training, offer faster inference speeds but often produce poor editing results due to a lack of high-quality training video pairs. In this paper, to close the gap in end-to-end methods, we introduce Se\\~norita-2M, a high-quality video editing dataset. Se\\~norita-2M consists of approximately 2 millions of video editing pairs. It is built by crafting four high-quality, specialized video editing models, each crafted and trained by our team to achieve state-of-the-art editing results. We also propose a filtering pipeline to eliminate poorly edited video pairs. Furthermore, we explore common video editing architectures to identify the most effective structure based on current pre-trained generative model. Extensive experiments show that our dataset can help to yield remarkably high-quality video editing results. More details are available at https://senorita.github.io.",
        "arxiv_id": "2502.06734",
        "ARXIVID": "2502.06734",
        "COMMENT": "Matches criterion 4 as it introduces a high-quality dataset for video editing, which is an application of vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.05378": {
        "authors": [
            "Shiyao Li",
            "Antoine Gu\\'edon",
            "Cl\\'ementin Boittiaux",
            "Shizhe Chen",
            "Vincent Lepetit"
        ],
        "title": "NextBestPath: Efficient 3D Mapping of Unseen Environments",
        "abstract": "arXiv:2502.05378v1 Announce Type: new  Abstract: This work addresses the problem of active 3D mapping, where an agent must find an efficient trajectory to exhaustively reconstruct a new scene. Previous approaches mainly predict the next best view near the agent's location, which is prone to getting stuck in local areas. Additionally, existing indoor datasets are insufficient due to limited geometric complexity and inaccurate ground truth meshes. To overcome these limitations, we introduce a novel dataset AiMDoom with a map generator for the Doom video game, enabling to better benchmark active 3D mapping in diverse indoor environments. Moreover, we propose a new method we call next-best-path (NBP), which predicts long-term goals rather than focusing solely on short-sighted views. The model jointly predicts accumulated surface coverage gains for long-term goals and obstacle maps, allowing it to efficiently plan optimal paths with a unified model. By leveraging online data collection, data augmentation and curriculum learning, NBP significantly outperforms state-of-the-art methods on both the existing MP3D dataset and our AiMDoom dataset, achieving more efficient mapping in indoor environments of varying complexity.",
        "arxiv_id": "2502.05378",
        "ARXIVID": "2502.05378",
        "COMMENT": "Matches criterion 3 as it introduces a new dataset and method for active 3D mapping, which is relevant to embodied AI benchmarks and methods.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.06020": {
        "authors": [
            "Xingjian Diao",
            "Chunhui Zhang",
            "Weiyi Wu",
            "Zhongyu Ouyang",
            "Peijun Qing",
            "Ming Cheng",
            "Soroush Vosoughi",
            "Jiang Gui"
        ],
        "title": "Temporal Working Memory: Query-Guided Segment Refinement for Enhanced Multimodal Understanding",
        "abstract": "arXiv:2502.06020v1 Announce Type: new  Abstract: Multimodal foundation models (MFMs) have demonstrated significant success in tasks such as visual captioning, question answering, and image-text retrieval. However, these models face inherent limitations due to their finite internal capacity, which restricts their ability to process extended temporal sequences, a crucial requirement for comprehensive video and audio analysis. To overcome these challenges, we introduce a specialized cognitive module, temporal working memory (TWM), which aims to enhance the temporal modeling capabilities of MFMs. It selectively retains task-relevant information across temporal dimensions, ensuring that critical details are preserved throughout the processing of video and audio content. The TWM uses a query-guided attention approach to focus on the most informative multimodal segments within temporal sequences. By retaining only the most relevant content, TWM optimizes the use of the model's limited capacity, enhancing its temporal modeling ability. This plug-and-play module can be easily integrated into existing MFMs. With our TWM, nine state-of-the-art models exhibit significant performance improvements across tasks such as video captioning, question answering, and video-text retrieval. By enhancing temporal modeling, TWM extends the capability of MFMs to handle complex, time-sensitive data effectively. Our code is available at https://github.com/xid32/NAACL_2025_TWM.",
        "arxiv_id": "2502.06020",
        "ARXIVID": "2502.06020",
        "COMMENT": "Matches criterion 2 as it introduces a temporal working memory module to enhance multi-modal foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.05415": {
        "authors": [
            "Chenkai Xu",
            "Xu Wang",
            "Zhenyi Liao",
            "Yishun Li",
            "Tianqi Hou",
            "Zhijie Deng"
        ],
        "title": "Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation",
        "abstract": "arXiv:2502.05415v1 Announce Type: new  Abstract: There has been increasing research interest in building unified multimodal understanding and generation models, among which Show-o stands as a notable representative, demonstrating great promise for both text-to-image and image-to-text generation. The inference of Show-o involves progressively denoising image tokens and autoregressively decoding text tokens, and hence, unfortunately, suffers from inefficiency issues from both sides. This paper introduces Show-o Turbo to bridge the gap. We first identify a unified denoising perspective for the generation of images and text in Show-o based on the parallel decoding of text tokens. We then propose to extend consistency distillation (CD), a qualified approach for shortening the denoising process of diffusion models, to the multimodal denoising trajectories of Show-o. We introduce a trajectory segmentation strategy and a curriculum learning procedure to improve the training convergence. Empirically, in text-to-image generation, Show-o Turbo displays a GenEval score of 0.625 at 4 sampling steps without using classifier-free guidance (CFG), outperforming that of the original Show-o with 8 steps and CFG; in image-to-text generation, Show-o Turbo exhibits a 1.5x speedup without significantly sacrificing performance. The code is available at https://github.com/zhijie-group/Show-o-Turbo.",
        "arxiv_id": "2502.05415",
        "ARXIVID": "2502.05415",
        "COMMENT": "Matches criterion 2 as it introduces improvements to a multi-modal large language model (Show-o Turbo) for unified multimodal understanding and generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.05660": {
        "authors": [
            "Sree Bhattacharyya",
            "James Z. Wang"
        ],
        "title": "Evaluating Vision-Language Models for Emotion Recognition",
        "abstract": "arXiv:2502.05660v1 Announce Type: new  Abstract: Large Vision-Language Models (VLMs) have achieved unprecedented success in several objective multimodal reasoning tasks. However, to further enhance their capabilities of empathetic and effective communication with humans, improving how VLMs process and understand emotions is crucial. Despite significant research attention on improving affective understanding, there is a lack of detailed evaluations of VLMs for emotion-related tasks, which can potentially help inform downstream fine-tuning efforts. In this work, we present the first comprehensive evaluation of VLMs for recognizing evoked emotions from images. We create a benchmark for the task of evoked emotion recognition and study the performance of VLMs for this task, from perspectives of correctness and robustness. Through several experiments, we demonstrate important factors that emotion recognition performance depends on, and also characterize the various errors made by VLMs in the process. Finally, we pinpoint potential causes for errors through a human evaluation study. We use our experimental results to inform recommendations for the future of emotion research in the context of VLMs.",
        "arxiv_id": "2502.05660",
        "ARXIVID": "2502.05660",
        "COMMENT": "Matches criterion 2 as it evaluates vision-language models (VLMs) for emotion recognition, which is a novel application of VLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2502.06608": {
        "authors": [
            "Yangguang Li",
            "Zi-Xin Zou",
            "Zexiang Liu",
            "Dehu Wang",
            "Yuan Liang",
            "Zhipeng Yu",
            "Xingchao Liu",
            "Yuan-Chen Guo",
            "Ding Liang",
            "Wanli Ouyang",
            "Yan-Pei Cao"
        ],
        "title": "TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models",
        "abstract": "arXiv:2502.06608v1 Announce Type: new  Abstract: Recent advancements in diffusion techniques have propelled image and video generation to unprece- dented levels of quality, significantly accelerating the deployment and application of generative AI. However, 3D shape generation technology has so far lagged behind, constrained by limitations in 3D data scale, complexity of 3D data process- ing, and insufficient exploration of advanced tech- niques in the 3D domain. Current approaches to 3D shape generation face substantial challenges in terms of output quality, generalization capa- bility, and alignment with input conditions. We present TripoSG, a new streamlined shape diffu- sion paradigm capable of generating high-fidelity 3D meshes with precise correspondence to input images. Specifically, we propose: 1) A large-scale rectified flow transformer for 3D shape generation, achieving state-of-the-art fidelity through training on extensive, high-quality data. 2) A hybrid supervised training strategy combining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality 3D reconstruction performance. 3) A data processing pipeline to generate 2 million high- quality 3D samples, highlighting the crucial rules for data quality and quantity in training 3D gen- erative models. Through comprehensive experi- ments, we have validated the effectiveness of each component in our new framework. The seamless integration of these parts has enabled TripoSG to achieve state-of-the-art performance in 3D shape generation. The resulting 3D shapes exhibit en- hanced detail due to high-resolution capabilities and demonstrate exceptional fidelity to input im- ages. Moreover, TripoSG demonstrates improved versatility in generating 3D models from diverse image styles and contents, showcasing strong gen- eralization capabilities. To foster progress and innovation in the field of 3D generation, we will make our model publicly available.",
        "arxiv_id": "2502.06608",
        "ARXIVID": "2502.06608",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling and 3D shape synthesis.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2502.06750": {
        "authors": [
            "Andrew Zhang",
            "Guillaume Jaume",
            "Anurag Vaidya",
            "Tong Ding",
            "Faisal Mahmood"
        ],
        "title": "Accelerating Data Processing and Benchmarking of AI Models for Pathology",
        "abstract": "arXiv:2502.06750v1 Announce Type: new  Abstract: Advances in foundation modeling have reshaped computational pathology. However, the increasing number of available models and lack of standardized benchmarks make it increasingly complex to assess their strengths, limitations, and potential for further development. To address these challenges, we introduce a new suite of software tools for whole-slide image processing, foundation model benchmarking, and curated publicly available tasks. We anticipate that these resources will promote transparency, reproducibility, and continued progress in the field.",
        "arxiv_id": "2502.06750",
        "ARXIVID": "2502.06750",
        "COMMENT": "Matches criterion 3 as it introduces new benchmarking tools for foundation models in pathology, which could be relevant to embodied AI benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2502.06773": {
        "authors": [
            "Guanghao Ye",
            "Khiem Duc Pham",
            "Xinzhi Zhang",
            "Sivakanth Gopi",
            "Baolin Peng",
            "Beibin Li",
            "Janardhan Kulkarni",
            "Huseyin A. Inan"
        ],
        "title": "On the Emergence of Thinking in LLMs I: Searching for the Right Intuition",
        "abstract": "arXiv:2502.06773v1 Announce Type: new  Abstract: Recent AI advancements, such as OpenAI's new models, are transforming LLMs into LRMs (Large Reasoning Models) that perform reasoning during inference, taking extra time and compute for higher-quality outputs. We aim to uncover the algorithmic framework for training LRMs. Methods like self-consistency, PRM, and AlphaZero suggest reasoning as guided search. We ask: what is the simplest, most scalable way to enable search in LLMs?   We propose a post-training framework called Reinforcement Learning via Self-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with human or synthetic demonstrations of the reasoning process, (2) using an exploration reward signal to encourage diverse and efficient reasoning behaviors, and (3) RL training with an outcome verifier to ensure correctness while preventing reward hacking. Our key innovation is to decouple exploration and correctness signals during PPO training, carefully balancing them to improve performance and efficiency.   Empirical studies in the math domain show that RLSP improves reasoning. On the Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500 test set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due to RLSP. However, a more important finding of this work is that the models trained using RLSP, even with the simplest exploration reward that encourages the model to take more intermediate steps, showed several emergent behaviors such as backtracking, exploration of ideas, and verification. These findings demonstrate that RLSP framework might be enough to enable emergence of complex reasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why RLSP search strategy is more suitable for LLMs inspired by a remarkable result that says CoT provably increases computational power of LLMs, which grows as the number of steps in CoT \\cite{li2024chain,merrill2023expresssive}.",
        "arxiv_id": "2502.06773",
        "ARXIVID": "2502.06773",
        "COMMENT": "Does not match any specific criteria but is related to reasoning in LLMs, which is tangentially relevant to your friend's interest in multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.05749": {
        "authors": [
            "Kaizhen Zhu",
            "Mokai Pan",
            "Yuexin Ma",
            "Yanwei Fu",
            "Jingyi Yu",
            "Jingya Wang",
            "Ye Shi"
        ],
        "title": "UniDB: A Unified Diffusion Bridge Framework via Stochastic Optimal Control",
        "abstract": "arXiv:2502.05749v1 Announce Type: new  Abstract: Recent advances in diffusion bridge models leverage Doob's $h$-transform to establish fixed endpoints between distributions, demonstrating promising results in image translation and restoration tasks. However, these approaches frequently produce blurred or excessively smoothed image details and lack a comprehensive theoretical foundation to explain these shortcomings. To address these limitations, we propose UniDB, a unified framework for diffusion bridges based on Stochastic Optimal Control (SOC). UniDB formulates the problem through an SOC-based optimization and derives a closed-form solution for the optimal controller, thereby unifying and generalizing existing diffusion bridge models. We demonstrate that existing diffusion bridges employing Doob's $h$-transform constitute a special case of our framework, emerging when the terminal penalty coefficient in the SOC cost function tends to infinity. By incorporating a tunable terminal penalty coefficient, UniDB achieves an optimal balance between control costs and terminal penalties, substantially improving detail preservation and output quality. Notably, UniDB seamlessly integrates with existing diffusion bridge models, requiring only minimal code modifications. Extensive experiments across diverse image restoration tasks validate the superiority and adaptability of the proposed framework. Our code is available at https://github.com/UniDB-SOC/UniDB/.",
        "arxiv_id": "2502.05749",
        "ARXIVID": "2502.05749",
        "COMMENT": "Does not match any specific criterion but discusses a unified framework for diffusion bridges, relevant to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.06390": {
        "authors": [
            "Aobotao Dai",
            "Xinyu Ma",
            "Lei Chen",
            "Songze Li",
            "Lin Wang"
        ],
        "title": "When Data Manipulation Meets Attack Goals: An In-depth Survey of Attacks for VLMs",
        "abstract": "arXiv:2502.06390v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have gained considerable prominence in recent years due to their remarkable capability to effectively integrate and process both textual and visual information. This integration has significantly enhanced performance across a diverse spectrum of applications, such as scene perception and robotics. However, the deployment of VLMs has also given rise to critical safety and security concerns, necessitating extensive research to assess the potential vulnerabilities these VLM systems may harbor. In this work, we present an in-depth survey of the attack strategies tailored for VLMs. We categorize these attacks based on their underlying objectives - namely jailbreak, camouflage, and exploitation - while also detailing the various methodologies employed for data manipulation of VLMs. Meanwhile, we outline corresponding defense mechanisms that have been proposed to mitigate these vulnerabilities. By discerning key connections and distinctions among the diverse types of attacks, we propose a compelling taxonomy for VLM attacks. Moreover, we summarize the evaluation metrics that comprehensively describe the characteristics and impact of different attacks on VLMs. Finally, we conclude with a discussion of promising future research directions that could further enhance the robustness and safety of VLMs, emphasizing the importance of ongoing exploration in this critical area of study. To facilitate community engagement, we maintain an up-to-date project page, accessible at: https://github.com/AobtDai/VLM_Attack_Paper_List.",
        "arxiv_id": "2502.06390",
        "ARXIVID": "2502.06390",
        "COMMENT": "Matches criterion 2 as it surveys vulnerabilities and attacks on vision-language models, which is relevant to understanding VLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2502.06243": {
        "authors": [
            "Jiacheng Hu",
            "Yanlin Xiang",
            "Yang Lin",
            "Junliang Du",
            "Hanchao Zhang",
            "Houze Liu"
        ],
        "title": "Multi-Scale Transformer Architecture for Accurate Medical Image Classification",
        "abstract": "arXiv:2502.06243v1 Announce Type: new  Abstract: This study introduces an AI-driven skin lesion classification algorithm built on an enhanced Transformer architecture, addressing the challenges of accuracy and robustness in medical image analysis. By integrating a multi-scale feature fusion mechanism and refining the self-attention process, the model effectively extracts both global and local features, enhancing its ability to detect lesions with ambiguous boundaries and intricate structures. Performance evaluation on the ISIC 2017 dataset demonstrates that the improved Transformer surpasses established AI models, including ResNet50, VGG19, ResNext, and Vision Transformer, across key metrics such as accuracy, AUC, F1-Score, and Precision. Grad-CAM visualizations further highlight the interpretability of the model, showcasing strong alignment between the algorithm's focus areas and actual lesion sites. This research underscores the transformative potential of advanced AI models in medical imaging, paving the way for more accurate and reliable diagnostic tools. Future work will explore the scalability of this approach to broader medical imaging tasks and investigate the integration of multimodal data to enhance AI-driven diagnostic frameworks for intelligent healthcare.",
        "arxiv_id": "2502.06243",
        "ARXIVID": "2502.06243",
        "COMMENT": "Focuses on medical image classification using an enhanced Transformer architecture, which is not directly related to any specific criteria but is tangentially relevant to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.06288": {
        "authors": [
            "Matteo Mule",
            "Matteo Pannacci",
            "Ali Ghasemi Goudarzi",
            "Francesco Pro",
            "Lorenzo Papa",
            "Luca Maiano",
            "Irene Amerini"
        ],
        "title": "Enhancing Ground-to-Aerial Image Matching for Visual Misinformation Detection Using Semantic Segmentation",
        "abstract": "arXiv:2502.06288v1 Announce Type: new  Abstract: The recent advancements in generative AI techniques, which have significantly increased the online dissemination of altered images and videos, have raised serious concerns about the credibility of digital media available on the Internet and distributed through information channels and social networks. This issue particularly affects domains that rely heavily on trustworthy data, such as journalism, forensic analysis, and Earth observation. To address these concerns, the ability to geolocate a non-geo-tagged ground-view image without external information, such as GPS coordinates, has become increasingly critical. This study tackles the challenge of linking a ground-view image, potentially exhibiting varying fields of view (FoV), to its corresponding satellite image without the aid of GPS data. To achieve this, we propose a novel four-stream Siamese-like architecture, the Quadruple Semantic Align Net (SAN-QUAD), which extends previous state-of-the-art (SOTA) approaches by leveraging semantic segmentation applied to both ground and satellite imagery. Experimental results on a subset of the CVUSA dataset demonstrate significant improvements of up to 9.8\\% over prior methods across various FoV settings.",
        "arxiv_id": "2502.06288",
        "ARXIVID": "2502.06288",
        "COMMENT": "Does not match any specific criteria. Focuses on ground-to-aerial image matching for misinformation detection, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.06434": {
        "authors": [
            "Lingao Xiao",
            "Songhua Liu",
            "Yang He",
            "Xinchao Wang"
        ],
        "title": "Rethinking Large-scale Dataset Compression: Shifting Focus From Labels to Images",
        "abstract": "arXiv:2502.06434v1 Announce Type: new  Abstract: Dataset distillation and dataset pruning are two prominent techniques for compressing datasets to improve computational and storage efficiency. Despite their overlapping objectives, these approaches are rarely compared directly. Even within each field, the evaluation protocols are inconsistent across various methods, which complicates fair comparisons and hinders reproducibility. Considering these limitations, we introduce in this paper a benchmark that equitably evaluates methodologies across both distillation and pruning literatures. Notably, our benchmark reveals that in the mainstream dataset distillation setting for large-scale datasets, which heavily rely on soft labels from pre-trained models, even randomly selected subsets can achieve surprisingly competitive performance. This finding suggests that an overemphasis on soft labels may be diverting attention from the intrinsic value of the image data, while also imposing additional burdens in terms of generation, storage, and application. To address these issues, we propose a new framework for dataset compression, termed Prune, Combine, and Augment (PCA), which focuses on leveraging image data exclusively, relies solely on hard labels for evaluation, and achieves state-of-the-art performance in this setup. By shifting the emphasis back to the images, our benchmark and PCA framework pave the way for more balanced and accessible techniques in dataset compression research. Our code is available at: https://github.com/ArmandXiao/Rethinking-Dataset-Compression",
        "arxiv_id": "2502.06434",
        "ARXIVID": "2502.06434",
        "COMMENT": "Does not match any specific criteria. Focuses on dataset compression techniques, which are not directly related to spatial understanding, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.06427": {
        "authors": [
            "Muhammad Ahmad",
            "Muhammad Hassaan Farooq Butt",
            "Muhammad Usama",
            "Manuel Mazzara",
            "Salvatore Distefano",
            "Adil Mehmood Khan",
            "Danfeng Hong"
        ],
        "title": "Hybrid State-Space and GRU-based Graph Tokenization Mamba for Hyperspectral Image Classification",
        "abstract": "arXiv:2502.06427v1 Announce Type: new  Abstract: Hyperspectral image (HSI) classification plays a pivotal role in domains such as environmental monitoring, agriculture, and urban planning. However, it faces significant challenges due to the high-dimensional nature of the data and the complex spectral-spatial relationships inherent in HSI. Traditional methods, including conventional machine learning and convolutional neural networks (CNNs), often struggle to effectively capture these intricate spectral-spatial features and global contextual information. Transformer-based models, while powerful in capturing long-range dependencies, often demand substantial computational resources, posing challenges in scenarios where labeled datasets are limited, as is commonly seen in HSI applications. To overcome these challenges, this work proposes GraphMamba, a hybrid model that combines spectral-spatial token generation, graph-based token prioritization, and cross-attention mechanisms. The model introduces a novel hybridization of state-space modeling and Gated Recurrent Units (GRU), capturing both linear and nonlinear spatial-spectral dynamics. GraphMamba enhances the ability to model complex spatial-spectral relationships while maintaining scalability and computational efficiency across diverse HSI datasets. Through comprehensive experiments, we demonstrate that GraphMamba outperforms existing state-of-the-art models, offering a scalable and robust solution for complex HSI classification tasks.",
        "arxiv_id": "2502.06427",
        "ARXIVID": "2502.06427",
        "COMMENT": "Does not match any specific criteria. Focuses on hyperspectral image classification using a hybrid model, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.06354": {
        "authors": [
            "Tatsuhiro Eguchi",
            "Shumpei Takezaki",
            "Mihoko Shimano",
            "Takayuki Yagi",
            "Ryoma Bise"
        ],
        "title": "Guidance-base Diffusion Models for Improving Photoacoustic Image Quality",
        "abstract": "arXiv:2502.06354v1 Announce Type: new  Abstract: Photoacoustic(PA) imaging is a non-destructive and non-invasive technology for visualizing minute blood vessel structures in the body using ultrasonic sensors. In PA imaging, the image quality of a single-shot image is poor, and it is necessary to improve the image quality by averaging many single-shot images. Therefore, imaging the entire subject requires high imaging costs. In our study, we propose a method to improve the quality of PA images using diffusion models. In our method, we improve the reverse diffusion process using sensor information of PA imaging and introduce a guidance method using imaging condition information to generate high-quality images.",
        "arxiv_id": "2502.06354",
        "ARXIVID": "2502.06354",
        "COMMENT": "Does not match any specific criteria but is related to improving image quality using diffusion models, which is tangentially relevant to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.05229": {
        "authors": [
            "Vandan Gorade",
            "Sparsh Mittal",
            "Neethi Dasu",
            "Rekha Singhal",
            "KC Santosh",
            "Debesh Jha"
        ],
        "title": "L2GNet: Optimal Local-to-Global Representation of Anatomical Structures for Generalized Medical Image Segmentation",
        "abstract": "arXiv:2502.05229v1 Announce Type: new  Abstract: Continuous Latent Space (CLS) and Discrete Latent Space (DLS) models, like AttnUNet and VQUNet, have excelled in medical image segmentation. In contrast, Synergistic Continuous and Discrete Latent Space (CDLS) models show promise in handling fine and coarse-grained information. However, they struggle with modeling long-range dependencies. CLS or CDLS-based models, such as TransUNet or SynergyNet are adept at capturing long-range dependencies. Since they rely heavily on feature pooling or aggregation using self-attention, they may capture dependencies among redundant regions. This hinders comprehension of anatomical structure content, poses challenges in modeling intra-class and inter-class dependencies, increases false negatives and compromises generalization. Addressing these issues, we propose L2GNet, which learns global dependencies by relating discrete codes obtained from DLS using optimal transport and aligning codes on a trainable reference. L2GNet achieves discriminative on-the-fly representation learning without an additional weight matrix in self-attention models, making it computationally efficient for medical applications. Extensive experiments on multi-organ segmentation and cardiac datasets demonstrate L2GNet's superiority over state-of-the-art methods, including the CDLS method SynergyNet, offering an novel approach to enhance deep learning models' performance in medical image analysis.",
        "arxiv_id": "2502.05229",
        "ARXIVID": "2502.05229",
        "COMMENT": "Does not match any specific criteria but is related to medical image segmentation, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.06779": {
        "authors": [
            "Yue Zhu",
            "Haiwen Diao",
            "Shang Gao",
            "Long Chen",
            "Huchuan Lu"
        ],
        "title": "KARST: Multi-Kernel Kronecker Adaptation with Re-Scaling Transmission for Visual Classification",
        "abstract": "arXiv:2502.06779v1 Announce Type: new  Abstract: Fine-tuning pre-trained vision models for specific tasks is a common practice in computer vision. However, this process becomes more expensive as models grow larger. Recently, parameter-efficient fine-tuning (PEFT) methods have emerged as a popular solution to improve training efficiency and reduce storage needs by tuning additional low-rank modules within pre-trained backbones. Despite their advantages, they struggle with limited representation capabilities and misalignment with pre-trained intermediate features. To address these issues, we introduce an innovative Multi-Kernel Kronecker Adaptation with Re-Scaling Transmission (KARST) for various recognition tasks. Specifically, its multi-kernel design extends Kronecker projections horizontally and separates adaptation matrices into multiple complementary spaces, reducing parameter dependency and creating more compact subspaces. Besides, it incorporates extra learnable re-scaling factors to better align with pre-trained feature distributions, allowing for more flexible and balanced feature aggregation. Extensive experiments validate that our KARST outperforms other PEFT counterparts with a negligible inference cost due to its re-parameterization characteristics. Code is publicly available at: https://github.com/Lucenova/KARST.",
        "arxiv_id": "2502.06779",
        "ARXIVID": "2502.06779",
        "COMMENT": "Does not match any specific criteria but is related to fine-tuning vision models, which is tangentially relevant to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.06181": {
        "authors": [
            "Lv Tang",
            "Jun Zhu",
            "Xinfeng Zhang",
            "Li Zhang",
            "Siwei Ma",
            "Qingming Huang"
        ],
        "title": "CANeRV: Content Adaptive Neural Representation for Video Compression",
        "abstract": "arXiv:2502.06181v1 Announce Type: new  Abstract: Recent advances in video compression introduce implicit neural representation (INR) based methods, which effectively capture global dependencies and characteristics of entire video sequences. Unlike traditional and deep learning based approaches, INR-based methods optimize network parameters from a global perspective, resulting in superior compression potential. However, most current INR methods utilize a fixed and uniform network architecture across all frames, limiting their adaptability to dynamic variations within and between video sequences. This often leads to suboptimal compression outcomes as these methods struggle to capture the distinct nuances and transitions in video content. To overcome these challenges, we propose Content Adaptive Neural Representation for Video Compression (CANeRV), an innovative INR-based video compression network that adaptively conducts structure optimisation based on the specific content of each video sequence. To better capture dynamic information across video sequences, we propose a dynamic sequence-level adjustment (DSA). Furthermore, to enhance the capture of dynamics between frames within a sequence, we implement a dynamic frame-level adjustment (DFA). {Finally, to effectively capture spatial structural information within video frames, thereby enhancing the detail restoration capabilities of CANeRV, we devise a structure level hierarchical structural adaptation (HSA).} Experimental results demonstrate that CANeRV can outperform both H.266/VVC and state-of-the-art INR-based video compression techniques across diverse video datasets.",
        "arxiv_id": "2502.06181",
        "ARXIVID": "2502.06181",
        "COMMENT": "Does not match any specific criteria but is related to video compression using neural representations, which is tangentially relevant to your friend's interest in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.06460": {
        "authors": [
            "Qingxin Zhang",
            "Haoyan Wei",
            "Yang Qian"
        ],
        "title": "Group-CLIP Uncertainty Modeling for Group Re-Identification",
        "abstract": "arXiv:2502.06460v1 Announce Type: new  Abstract: Group Re-Identification (Group ReID) aims matching groups of pedestrians across non-overlapping cameras. Unlike single-person ReID, Group ReID focuses more on the changes in group structure, emphasizing the number of members and their spatial arrangement. However, most methods rely on certainty-based models, which consider only the specific group structures in the group images, often failing to match unseen group configurations. To this end, we propose a novel Group-CLIP UncertaintyModeling (GCUM) approach that adapts group text descriptions to undetermined accommodate member and layout variations. Specifically, we design a Member Variant Simulation (MVS)module that simulates member exclusions using a Bernoulli distribution and a Group Layout Adaptation (GLA) module that generates uncertain group text descriptions with identity-specific tokens. In addition, we design a Group RelationshipConstruction Encoder (GRCE) that uses group features to refine individual features, and employ cross-modal contrastive loss to obtain generalizable knowledge from group text descriptions. It is worth noting that we are the first to employ CLIP to GroupReID, and extensive experiments show that GCUM significantly outperforms state-of-the-art Group ReID methods.",
        "arxiv_id": "2502.06460",
        "ARXIVID": "2502.06460",
        "COMMENT": "Does not match any specific criteria but is related to group re-identification using CLIP, which is tangentially relevant to vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.06741": {
        "authors": [
            "Ehsan Zeraatkar",
            "Salah Faroughi",
            "Jelena Tesic"
        ],
        "title": "ViSIR: Vision Transformer Single Image Reconstruction Method for Earth System Models",
        "abstract": "arXiv:2502.06741v1 Announce Type: new  Abstract: Purpose: Earth system models (ESMs) integrate the interactions of the atmosphere, ocean, land, ice, and biosphere to estimate the state of regional and global climate under a wide variety of conditions. The ESMs are highly complex, and thus, deep neural network architectures are used to model the complexity and store the down-sampled data. In this paper, we propose the Vision Transformer Sinusoidal Representation Networks (ViSIR) to improve the single image SR (SR) reconstruction task for the ESM data.   Methods: ViSIR combines the SR capability of Vision Transformers (ViT) with the high-frequency detail preservation of the Sinusoidal Representation Network (SIREN) to address the spectral bias observed in SR tasks.   Results: The ViSIR outperforms ViT by 4.1 dB, SIREN by 7.5 dB, and SR-Generative Adversarial (SR-GANs) by 7.1dB PSNR on average for three different measurements.   Conclusion: The proposed ViSIR is evaluated and compared with state-of-the-art methods. The results show that the proposed algorithm is outperforming other methods in terms of Mean Square Error(MSE), Peak-Signal-to-Noise-Ratio(PSNR), and Structural Similarity Index Measure(SSIM).",
        "arxiv_id": "2502.06741",
        "ARXIVID": "2502.06741",
        "COMMENT": "Does not match any specific criteria but is related to vision transformers, which is tangentially relevant to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.06523": {
        "authors": [
            "Merlijn Krale",
            "Wietze Koops",
            "Sebastian Junges",
            "Thiago D. Sim\\~ao",
            "Nils Jansen"
        ],
        "title": "Tighter Value-Function Approximations for POMDPs",
        "abstract": "arXiv:2502.06523v1 Announce Type: new  Abstract: Solving partially observable Markov decision processes (POMDPs) typically requires reasoning about the values of exponentially many state beliefs. Towards practical performance, state-of-the-art solvers use value bounds to guide this reasoning. However, sound upper value bounds are often computationally expensive to compute, and there is a tradeoff between the tightness of such bounds and their computational cost. This paper introduces new and provably tighter upper value bounds than the commonly used fast informed bound. Our empirical evaluation shows that, despite their additional computational overhead, the new upper bounds accelerate state-of-the-art POMDP solvers on a wide range of benchmarks.",
        "arxiv_id": "2502.06523",
        "ARXIVID": "2502.06523",
        "COMMENT": "Does not match any specific criterion but is relevant to POMDPs and tighter value-function approximations.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.06100": {
        "authors": [
            "Chenyu Liu",
            "Jinshui Hu",
            "Baocai Yin",
            "Jia Pan",
            "Bing Yin",
            "Jun Du",
            "Qingfeng Liu"
        ],
        "title": "Col-OLHTR: A Novel Framework for Multimodal Online Handwritten Text Recognition",
        "abstract": "arXiv:2502.06100v1 Announce Type: new  Abstract: Online Handwritten Text Recognition (OLHTR) has gained considerable attention for its diverse range of applications. Current approaches usually treat OLHTR as a sequence recognition task, employing either a single trajectory or image encoder, or multi-stream encoders, combined with a CTC or attention-based recognition decoder. However, these approaches face several drawbacks: 1) single encoders typically focus on either local trajectories or visual regions, lacking the ability to dynamically capture relevant global features in challenging cases; 2) multi-stream encoders, while more comprehensive, suffer from complex structures and increased inference costs. To tackle this, we propose a Collaborative learning-based OLHTR framework, called Col-OLHTR, that learns multimodal features during training while maintaining a single-stream inference process. Col-OLHTR consists of a trajectory encoder, a Point-to-Spatial Alignment (P2SA) module, and an attention-based decoder. The P2SA module is designed to learn image-level spatial features through trajectory-encoded features and 2D rotary position embeddings. During training, an additional image-stream encoder-decoder is collaboratively trained to provide supervision for P2SA features. At inference, the extra streams are discarded, and only the P2SA module is used and merged before the decoder, simplifying the process while preserving high performance. Extensive experimental results on several OLHTR benchmarks demonstrate the state-of-the-art (SOTA) performance, proving the effectiveness and robustness of our design.",
        "arxiv_id": "2502.06100",
        "ARXIVID": "2502.06100",
        "COMMENT": "Does not match any specific criterion but is relevant to multi-modal learning and efficient OLHTR methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.05606": {
        "authors": [
            "Yufan Zhou",
            "Haoyu Shen",
            "Huan Wang"
        ],
        "title": "FreeBlend: Advancing Concept Blending with Staged Feedback-Driven Interpolation Diffusion",
        "abstract": "arXiv:2502.05606v1 Announce Type: new  Abstract: Concept blending is a promising yet underexplored area in generative models. While recent approaches, such as embedding mixing and latent modification based on structural sketches, have been proposed, they often suffer from incompatible semantic information and discrepancies in shape and appearance. In this work, we introduce FreeBlend, an effective, training-free framework designed to address these challenges. To mitigate cross-modal loss and enhance feature detail, we leverage transferred image embeddings as conditional inputs. The framework employs a stepwise increasing interpolation strategy between latents, progressively adjusting the blending ratio to seamlessly integrate auxiliary features. Additionally, we introduce a feedback-driven mechanism that updates the auxiliary latents in reverse order, facilitating global blending and preventing rigid or unnatural outputs. Extensive experiments demonstrate that our method significantly improves both the semantic coherence and visual quality of blended images, yielding compelling and coherent results.",
        "arxiv_id": "2502.05606",
        "ARXIVID": "2502.05606",
        "COMMENT": "This paper does not match any specific criteria. It focuses on concept blending in generative models, which is tangentially related to multi-modal generative modeling but not directly aligned with the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.05482": {
        "authors": [
            "Mingze Ma",
            "Qingtian Zhu",
            "Yifan Zhan",
            "Zhengwei Yin",
            "Hongjun Wang",
            "Yinqiang Zheng"
        ],
        "title": "Robustifying Fourier Features Embeddings for Implicit Neural Representations",
        "abstract": "arXiv:2502.05482v1 Announce Type: new  Abstract: Implicit Neural Representations (INRs) employ neural networks to represent continuous functions by mapping coordinates to the corresponding values of the target function, with applications e.g., inverse graphics. However, INRs face a challenge known as spectral bias when dealing with scenes containing varying frequencies. To overcome spectral bias, the most common approach is the Fourier features-based methods such as positional encoding. However, Fourier features-based methods will introduce noise to output, which degrades their performances when applied to downstream tasks. In response, this paper initially hypothesizes that combining multi-layer perceptrons (MLPs) with Fourier feature embeddings mutually enhances their strengths, yet simultaneously introduces limitations inherent in Fourier feature embeddings. By presenting a simple theorem, we validate our hypothesis, which serves as a foundation for the design of our solution. Leveraging these insights, we propose the use of multi-layer perceptrons (MLPs) without additive",
        "arxiv_id": "2502.05482",
        "ARXIVID": "2502.05482",
        "COMMENT": "This paper does not match any specific criteria. It focuses on improving Fourier feature embeddings for implicit neural representations, which is not directly related to the specified interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.05673": {
        "authors": [
            "Ping Liu",
            "Jiawei Du"
        ],
        "title": "The Evolution of Dataset Distillation: Toward Scalable and Generalizable Solutions",
        "abstract": "arXiv:2502.05673v1 Announce Type: new  Abstract: Dataset distillation, which condenses large-scale datasets into compact synthetic representations, has emerged as a critical solution for training modern deep learning models efficiently. While prior surveys focus on developments before 2023, this work comprehensively reviews recent advances, emphasizing scalability to large-scale datasets such as ImageNet-1K and ImageNet-21K. We categorize progress into a few key methodologies: trajectory matching, gradient matching, distribution matching, scalable generative approaches, and decoupling optimization mechanisms. As a comprehensive examination of recent dataset distillation advances, this survey highlights breakthrough innovations: the SRe2L framework for efficient and effective condensation, soft label strategies that significantly enhance model accuracy, and lossless distillation techniques that maximize compression while maintaining performance. Beyond these methodological advancements, we address critical challenges, including robustness against adversarial and backdoor attacks, effective handling of non-IID data distributions. Additionally, we explore emerging applications in video and audio processing, multi-modal learning, medical imaging, and scientific computing, highlighting its domain versatility. By offering extensive performance comparisons and actionable research directions, this survey equips researchers and practitioners with practical insights to advance efficient and generalizable dataset distillation, paving the way for future innovations.",
        "arxiv_id": "2502.05673",
        "ARXIVID": "2502.05673",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of dataset distillation and efficient training.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.06189": {
        "authors": [
            "Yaoxin Yang",
            "Peng Ye",
            "Weihao Lin",
            "Kangcong Li",
            "Yan Wen",
            "Jia Hao",
            "Tao Chen"
        ],
        "title": "Multi-Level Decoupled Relational Distillation for Heterogeneous Architectures",
        "abstract": "arXiv:2502.06189v1 Announce Type: new  Abstract: Heterogeneous distillation is an effective way to transfer knowledge from cross-architecture teacher models to student models. However, existing heterogeneous distillation methods do not take full advantage of the dark knowledge hidden in the teacher's output, limiting their performance.To this end, we propose a novel framework named Multi-Level Decoupled Relational Knowledge Distillation (MLDR-KD) to unleash the potential of relational distillation in heterogeneous distillation. Concretely, we first introduce Decoupled Finegrained Relation Alignment (DFRA) in both logit and feature levels to balance the trade-off between distilled dark knowledge and the confidence in the correct category of the heterogeneous teacher model. Then, Multi-Scale Dynamic Fusion (MSDF) module is applied to dynamically fuse the projected logits of multiscale features at different stages in student model, further improving performance of our method in feature level. We verify our method on four architectures (CNNs, Transformers, MLPs and Mambas), two datasets (CIFAR-100 and Tiny-ImageNet). Compared with the best available method, our MLDR-KD improves student model performance with gains of up to 4.86% on CIFAR-100 and 2.78% on Tiny-ImageNet datasets respectively, showing robustness and generality in heterogeneous distillation. Code will be released soon.",
        "arxiv_id": "2502.06189",
        "ARXIVID": "2502.06189",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of knowledge distillation and heterogeneous architectures.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.05539": {
        "authors": [
            "Yixian Shen",
            "Qi Bi",
            "Jia-Hong Huang",
            "Hongyi Zhu",
            "Andy D. Pimentel",
            "Anuj Pathania"
        ],
        "title": "SSH: Sparse Spectrum Adaptation via Discrete Hartley Transformation",
        "abstract": "arXiv:2502.05539v1 Announce Type: new  Abstract: Low-rank adaptation (LoRA) has been demonstrated effective in reducing the trainable parameter number when fine-tuning a large foundation model (LLM). However, it still encounters computational and memory challenges when scaling to larger models or addressing more complex task adaptation.   In this work, we introduce Sparse Spectrum Adaptation via Discrete Hartley Transformation (SSH), a novel approach that significantly reduces the number of trainable parameters while enhancing model performance. It selects the most informative spectral components across all layers, under the guidance of the initial weights after a discrete Hartley transformation (DHT). The lightweight inverse DHT then projects the spectrum back into the spatial domain for updates.   Extensive experiments across both single-modality tasks such as language understanding and generation and multi-modality tasks such as video-text understanding demonstrate that SSH outperforms existing parameter-efficient fine-tuning (PEFT) methods while achieving substantial reductions in computational cost and memory requirements.",
        "arxiv_id": "2502.05539",
        "ARXIVID": "2502.05539",
        "COMMENT": "Does not match any specific criteria but involves parameter-efficient fine-tuning, which is tangentially related to foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.05843": {
        "authors": [
            "Yuhui Zeng",
            "Haoxiang Wu",
            "Wenjie Nie",
            "Guangyao Chen",
            "Xiawu Zheng",
            "Yunhang Shen",
            "Guilin Li",
            "Yixiong Zou",
            "Yonghong Tian",
            "Rongrong Ji"
        ],
        "title": "Training-free Anomaly Event Detection via LLM-guided Symbolic Pattern Discovery",
        "abstract": "arXiv:2502.05843v1 Announce Type: new  Abstract: Anomaly event detection plays a crucial role in various real-world applications. However, current approaches predominantly rely on supervised learning, which faces significant challenges: the requirement for extensive labeled training data and lack of interpretability in decision-making processes. To address these limitations, we present a training-free framework that integrates open-set object detection with symbolic regression, powered by Large Language Models (LLMs) for efficient symbolic pattern discovery. The LLMs guide the symbolic reasoning process, establishing logical relationships between detected entities. Through extensive experiments across multiple domains, our framework demonstrates several key advantages: (1) achieving superior detection accuracy through direct reasoning without any training process; (2) providing highly interpretable logical expressions that are readily comprehensible to humans; and (3) requiring minimal annotation effort - approximately 1% of the data needed by traditional training-based methods.To facilitate comprehensive evaluation and future research, we introduce two datasets: a large-scale private dataset containing over 110,000 annotated images covering various anomaly scenarios including construction site safety violations, illegal fishing activities, and industrial hazards, along with a public benchmark dataset of 5,000 samples with detailed anomaly event annotations. Code is available at here.",
        "arxiv_id": "2502.05843",
        "ARXIVID": "2502.05843",
        "COMMENT": "Does not match any specific criteria but involves LLMs in anomaly detection, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.06559": {
        "authors": [
            "Maria Eriksson",
            "Erasmo Purificato",
            "Arman Noroozian",
            "Joao Vinagre",
            "Guillaume Chaslot",
            "Emilia Gomez",
            "David Fernandez-Llorca"
        ],
        "title": "Can We Trust AI Benchmarks? An Interdisciplinary Review of Current Issues in AI Evaluation",
        "abstract": "arXiv:2502.06559v1 Announce Type: new  Abstract: Quantitative Artificial Intelligence (AI) Benchmarks have emerged as fundamental tools for evaluating the performance, capability, and safety of AI models and systems. Currently, they shape the direction of AI development and are playing an increasingly prominent role in regulatory frameworks. As their influence grows, however, so too does concerns about how and with what effects they evaluate highly sensitive topics such as capabilities, including high-impact capabilities, safety and systemic risks. This paper presents an interdisciplinary meta-review of about 100 studies that discuss shortcomings in quantitative benchmarking practices, published in the last 10 years. It brings together many fine-grained issues in the design and application of benchmarks (such as biases in dataset creation, inadequate documentation, data contamination, and failures to distinguish signal from noise) with broader sociotechnical issues (such as an over-focus on evaluating text-based AI models according to one-time testing logic that fails to account for how AI models are increasingly multimodal and interact with humans and other technical systems). Our review also highlights a series of systemic flaws in current benchmarking practices, such as misaligned incentives, construct validity issues, unknown unknowns, and problems with the gaming of benchmark results. Furthermore, it underscores how benchmark practices are fundamentally shaped by cultural, commercial and competitive dynamics that often prioritise state-of-the-art performance at the expense of broader societal concerns. By providing an overview of risks associated with existing benchmarking procedures, we problematise disproportionate trust placed in benchmarks and contribute to ongoing efforts to improve the accountability and relevance of quantitative AI benchmarks within the complexities of real-world scenarios.",
        "arxiv_id": "2502.06559",
        "ARXIVID": "2502.06559",
        "COMMENT": "Discusses AI benchmarks and their limitations, which is tangentially related to criterion 3 but does not propose a new benchmark or simulator.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.05275": {
        "authors": [
            "Kien X. Nguyen",
            "Tang Li",
            "Xi Peng"
        ],
        "title": "Interpretable Failure Detection with Human-Level Concepts",
        "abstract": "arXiv:2502.05275v1 Announce Type: new  Abstract: Reliable failure detection holds paramount importance in safety-critical applications. Yet, neural networks are known to produce overconfident predictions for misclassified samples. As a result, it remains a problematic matter as existing confidence score functions rely on category-level signals, the logits, to detect failures. This research introduces an innovative strategy, leveraging human-level concepts for a dual purpose: to reliably detect when a model fails and to transparently interpret why. By integrating a nuanced array of signals for each category, our method enables a finer-grained assessment of the model's confidence. We present a simple yet highly effective approach based on the ordinal ranking of concept activation to the input image. Without bells and whistles, our method significantly reduce the false positive rate across diverse real-world image classification benchmarks, specifically by 3.7% on ImageNet and 9% on EuroSAT.",
        "arxiv_id": "2502.05275",
        "ARXIVID": "2502.05275",
        "COMMENT": "Does not match any specific criteria but discusses interpretable failure detection, which is tangentially related to your friend's interest in clever statistical tricks.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.06324": {
        "authors": [
            "Zemin Yang",
            "Yujing Sun",
            "Xidong Peng",
            "Siu Ming Yiu",
            "Yuexin Ma"
        ],
        "title": "UniDemoir\\'e: Towards Universal Image Demoir\\'eing with Data Generation and Synthesis",
        "abstract": "arXiv:2502.06324v1 Announce Type: new  Abstract: Image demoir\\'eing poses one of the most formidable challenges in image restoration, primarily due to the unpredictable and anisotropic nature of moir\\'e patterns. Limited by the quantity and diversity of training data, current methods tend to overfit to a single moir\\'e domain, resulting in performance degradation for new domains and restricting their robustness in real-world applications. In this paper, we propose a universal image demoir\\'eing solution, UniDemoir\\'e, which has superior generalization capability. Notably, we propose innovative and effective data generation and synthesis methods that can automatically provide vast high-quality moir\\'e images to train a universal demoir\\'eing model. Our extensive experiments demonstrate the cutting-edge performance and broad potential of our approach for generalized image demoir\\'eing.",
        "arxiv_id": "2502.06324",
        "ARXIVID": "2502.06324",
        "COMMENT": "Does not match any specific criterion but is relevant to image restoration and generalization in demoir\u00e9ing tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.05800": {
        "authors": [
            "Novendra Setyawan",
            "Chi-Chia Sun",
            "Mao-Hsiu Hsu",
            "Wen-Kai Kuo",
            "Jun-Wei Hsieh"
        ],
        "title": "MicroViT: A Vision Transformer with Low Complexity Self Attention for Edge Device",
        "abstract": "arXiv:2502.05800v1 Announce Type: new  Abstract: The Vision Transformer (ViT) has demonstrated state-of-the-art performance in various computer vision tasks, but its high computational demands make it impractical for edge devices with limited resources. This paper presents MicroViT, a lightweight Vision Transformer architecture optimized for edge devices by significantly reducing computational complexity while maintaining high accuracy. The core of MicroViT is the Efficient Single Head Attention (ESHA) mechanism, which utilizes group convolution to reduce feature redundancy and processes only a fraction of the channels, thus lowering the burden of the self-attention mechanism. MicroViT is designed using a multi-stage MetaFormer architecture, stacking multiple MicroViT encoders to enhance efficiency and performance. Comprehensive experiments on the ImageNet-1K and COCO datasets demonstrate that MicroViT achieves competitive accuracy while significantly improving 3.6 faster inference speed and reducing energy consumption with 40% higher efficiency than the MobileViT series, making it suitable for deployment in resource-constrained environments such as mobile and edge devices.",
        "arxiv_id": "2502.05800",
        "ARXIVID": "2502.05800",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and efficiency improvements in Vision Transformers.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.06655": {
        "authors": [
            "Meilin Chen",
            "Jian Tian",
            "Liang Ma",
            "Di Xie",
            "Weijie Chen",
            "Jiang Zhu"
        ],
        "title": "Unbiased Evaluation of Large Language Models from a Causal Perspective",
        "abstract": "arXiv:2502.06655v1 Announce Type: new  Abstract: Benchmark contamination has become a significant concern in the LLM evaluation community. Previous Agents-as-an-Evaluator address this issue by involving agents in the generation of questions. Despite their success, the biases in Agents-as-an-Evaluator methods remain largely unexplored. In this paper, we present a theoretical formulation of evaluation bias, providing valuable insights into designing unbiased evaluation protocols. Furthermore, we identify two type of bias in Agents-as-an-Evaluator through carefully designed probing tasks on a minimal Agents-as-an-Evaluator setup. To address these issues, we propose the Unbiased Evaluator, an evaluation protocol that delivers a more comprehensive, unbiased, and interpretable assessment of LLMs.Extensive experiments reveal significant room for improvement in current LLMs. Additionally, we demonstrate that the Unbiased Evaluator not only offers strong evidence of benchmark contamination but also provides interpretable evaluation results.",
        "arxiv_id": "2502.06655",
        "ARXIVID": "2502.06655",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of unbiased evaluation of large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.06432": {
        "authors": [
            "Huaqiu Li",
            "Wang Zhang",
            "Xiaowan Hu",
            "Tao Jiang",
            "Zikang Chen",
            "Haoqian Wang"
        ],
        "title": "Prompt-SID: Learning Structural Representation Prompt via Latent Diffusion for Single-Image Denoising",
        "abstract": "arXiv:2502.06432v1 Announce Type: new  Abstract: Many studies have concentrated on constructing supervised models utilizing paired datasets for image denoising, which proves to be expensive and time-consuming. Current self-supervised and unsupervised approaches typically rely on blind-spot networks or sub-image pairs sampling, resulting in pixel information loss and destruction of detailed structural information, thereby significantly constraining the efficacy of such methods. In this paper, we introduce Prompt-SID, a prompt-learning-based single image denoising framework that emphasizes preserving of structural details. This approach is trained in a self-supervised manner using downsampled image pairs. It captures original-scale image information through structural encoding and integrates this prompt into the denoiser. To achieve this, we propose a structural representation generation model based on the latent diffusion process and design a structural attention module within the transformer-based denoiser architecture to decode the prompt. Additionally, we introduce a scale replay training mechanism, which effectively mitigates the scale gap from images of different resolutions. We conduct comprehensive experiments on synthetic, real-world, and fluorescence imaging datasets, showcasing the remarkable effectiveness of Prompt-SID.",
        "arxiv_id": "2502.06432",
        "ARXIVID": "2502.06432",
        "COMMENT": "Does not match any specific criteria but involves image denoising, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.06152": {
        "authors": [
            "Ziyang Guo",
            "Yifan Wu",
            "Jason Hartline",
            "Jessica Hullman"
        ],
        "title": "The Value of Information in Human-AI Decision-making",
        "abstract": "arXiv:2502.06152v1 Announce Type: new  Abstract: Humans and AIs are often paired on decision tasks with the expectation of achieving complementary performance, where the combination of human and AI outperforms either one alone. However, how to improve performance of a human-AI team is often not clear without knowing more about what particular information and strategies each agent employs. We provide a decision-theoretic framework for characterizing the value of information -- and consequently, opportunities for agents to better exploit available information--in AI-assisted decision workflow. We demonstrate the use of the framework for model selection, empirical evaluation of human-AI performance, and explanation design. We propose a novel information-based instance-level explanation technique that adapts a conventional saliency-based explanation to explain information value in decision making.",
        "arxiv_id": "2502.06152",
        "ARXIVID": "2502.06152",
        "COMMENT": "Does not match any specific criteria but involves human-AI decision-making, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.06552": {
        "authors": [
            "Haokai Zhao",
            "Haowei Lou",
            "Lina Yao",
            "Wei Peng",
            "Ehsan Adeli",
            "Kilian M Pohl",
            "Yu Zhang"
        ],
        "title": "Diffusion Models for Computational Neuroimaging: A Survey",
        "abstract": "arXiv:2502.06552v1 Announce Type: new  Abstract: Computational neuroimaging involves analyzing brain images or signals to provide mechanistic insights and predictive tools for human cognition and behavior. While diffusion models have shown stability and high-quality generation in natural images, there is increasing interest in adapting them to analyze brain data for various neurological tasks such as data enhancement, disease diagnosis and brain decoding. This survey provides an overview of recent efforts to integrate diffusion models into computational neuroimaging. We begin by introducing the common neuroimaging data modalities, follow with the diffusion formulations and conditioning mechanisms. Then we discuss how the variations of the denoising starting point, condition input and generation target of diffusion models are developed and enhance specific neuroimaging tasks. For a comprehensive overview of the ongoing research, we provide a publicly available repository at https://github.com/JoeZhao527/dm4neuro.",
        "arxiv_id": "2502.06552",
        "ARXIVID": "2502.06552",
        "COMMENT": "Does not match any specific criterion but is a survey on diffusion models applied to computational neuroimaging.",
        "RELEVANCE": 3,
        "NOVELTY": 3
    }
}