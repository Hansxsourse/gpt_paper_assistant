{
    "2509.04446": {
        "authors": [
            "Kiymet Akdemir",
            "Jing Shi",
            "Kushal Kafle",
            "Brian Price",
            "Pinar Yanardag"
        ],
        "title": "Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models",
        "abstract": "arXiv:2509.04446v1 Announce Type: new  Abstract: Text-to-image diffusion models have demonstrated significant capabilities to generate diverse and detailed visuals in various domains, and story visualization is emerging as a particularly promising application. However, as their use in real-world creative domains increases, the need for providing enhanced control, refinement, and the ability to modify images post-generation in a consistent manner becomes an important challenge. Existing methods often lack the flexibility to apply fine or coarse edits while maintaining visual and narrative consistency across multiple frames, preventing creators from seamlessly crafting and refining their visual stories. To address these challenges, we introduce Plot'n Polish, a zero-shot framework that enables consistent story generation and provides fine-grained control over story visualizations at various levels of detail.",
        "arxiv_id": "2509.04446",
        "ARXIVID": "2509.04446",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.03794": {
        "authors": [
            "Juhun Lee",
            "Simon S. Woo"
        ],
        "title": "Fitting Image Diffusion Models on Video Datasets",
        "abstract": "arXiv:2509.03794v1 Announce Type: new  Abstract: Image diffusion models are trained on independently sampled static images. While this is the bedrock task protocol in generative modeling, capturing the temporal world through the lens of static snapshots is information-deficient by design. This limitation leads to slower convergence, limited distributional coverage, and reduced generalization. In this work, we propose a simple and effective training strategy that leverages the temporal inductive bias present in continuous video frames to improve diffusion training. Notably, the proposed method requires no architectural modification and can be seamlessly integrated into standard diffusion training pipelines. We evaluate our method on the HandCo dataset, where hand-object interactions exhibit dense temporal coherence and subtle variations in finger articulation often result in semantically distinct motions. Empirically, our method accelerates convergence by over 2$\\text{x}$ faster and achieves lower FID on both training and validation distributions. It also improves generative diversity by encouraging the model to capture meaningful temporal variations. We further provide an optimization analysis showing that our regularization reduces the gradient variance, which contributes to faster convergence.",
        "arxiv_id": "2509.03794",
        "ARXIVID": "2509.03794",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}