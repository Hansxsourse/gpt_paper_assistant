{
    "2503.09143": {
        "authors": [
            "Haoyu Zhang",
            "Qiaohui Chu",
            "Meng Liu",
            "Yunxiao Wang",
            "Bin Wen",
            "Fan Yang",
            "Tingting Gao",
            "Di Zhang",
            "Yaowei Wang",
            "Liqiang Nie"
        ],
        "title": "Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video Understanding",
        "abstract": "arXiv:2503.09143v1 Announce Type: new  Abstract: AI personal assistants, deployed through robots or wearables, require embodied understanding to collaborate effectively with humans. Current Multimodal Large Language Models (MLLMs) primarily focus on third-person (exocentric) vision, overlooking the unique aspects of first-person (egocentric) videos. Additionally, high acquisition costs limit data size, impairing MLLM performance. To address these challenges, we propose learning the mapping between exocentric and egocentric domains, leveraging the extensive exocentric knowledge within existing MLLMs to enhance egocentric video understanding. To this end, we introduce Ego-ExoClip, a pre-training dataset comprising 1.1M synchronized ego-exo clip-text pairs derived from Ego-Exo4D. Our approach features a progressive training pipeline with three stages: Teacher Self-Preparation, Teacher-Student Guidance, and Student Self-Practice. Additionally, we propose an instruction-tuning data EgoIT from multiple sources to strengthen the model's instruction-following capabilities, along with the EgoBench benchmark comprising eight different tasks for thorough evaluation. Extensive experiments across diverse egocentric tasks reveal that existing MLLMs perform inadequately in egocentric video understanding, while our model significantly outperforms these leading models.",
        "arxiv_id": "2503.09143",
        "ARXIVID": "2503.09143",
        "COMMENT": "This paper matches criterion 2 and 3 as it introduces a new MLLM for egocentric video understanding and builds a new benchmark (EgoBench) for evaluation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.09160": {
        "authors": [
            "Hao Feng",
            "Zhi Zuo",
            "Jia-hui Pan",
            "Ka-hei Hui",
            "Yi-hua Shao",
            "Qi Dou",
            "Wei Xie",
            "Zheng-zhe Liu"
        ],
        "title": "WonderVerse: Extendable 3D Scene Generation with Video Generative Models",
        "abstract": "arXiv:2503.09160v1 Announce Type: new  Abstract: We introduce \\textit{WonderVerse}, a simple but effective framework for generating extendable 3D scenes. Unlike existing methods that rely on iterative depth estimation and image inpainting, often leading to geometric distortions and inconsistencies, WonderVerse leverages the powerful world-level priors embedded within video generative foundation models to create highly immersive and geometrically coherent 3D environments. Furthermore, we propose a new technique for controllable 3D scene extension to substantially increase the scale of the generated environments. Besides, we introduce a novel abnormal sequence detection module that utilizes camera trajectory to address geometric inconsistency in the generated videos. Finally, WonderVerse is compatible with various 3D reconstruction methods, allowing both efficient and high-quality generation. Extensive experiments on 3D scene generation demonstrate that our WonderVerse, with an elegant and simple pipeline, delivers extendable and highly-realistic 3D scenes, markedly outperforming existing works that rely on more complex architectures.",
        "arxiv_id": "2503.09160",
        "ARXIVID": "2503.09160",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework (WonderVerse) for 3D scene generation using video generative models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2503.09445": {
        "authors": [
            "Xiaoda Yang",
            "JunYu Lu",
            "Hongshun Qiu",
            "Sijing Li",
            "Hao Li",
            "Shengpeng Ji",
            "Xudong Tang",
            "Jiayang Xu",
            "Jiaqi Duan",
            "Ziyue Jiang",
            "Cong Lin",
            "Sihang Cai",
            "Zejian Xie",
            "Zhuoyang Song",
            "Songxin Zhang"
        ],
        "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
        "abstract": "arXiv:2503.09445v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) based on Mixture-of-Experts (MoE) architectures have emerged as a pivotal paradigm in multimodal understanding, offering a powerful framework for integrating visual and linguistic information. However, the increasing complexity and diversity of tasks present significant challenges in coordinating load balancing across heterogeneous visual experts, where optimizing one specialist's performance often compromises others' capabilities. To address task heterogeneity and expert load imbalance, we propose Astrea, a novel multi-expert collaborative VLM architecture based on progressive pre-alignment. Astrea introduces three key innovations: 1) A heterogeneous expert coordination mechanism that integrates four specialized models (detection, segmentation, classification, captioning) into a comprehensive expert matrix covering essential visual comprehension elements; 2) A dynamic knowledge fusion strategy featuring progressive pre-alignment to harmonize experts within the VLM latent space through contrastive learning, complemented by probabilistically activated stochastic residual connections to preserve knowledge continuity; 3) An enhanced optimization framework utilizing momentum contrastive learning for long-range dependency modeling and adaptive weight allocators for real-time expert contribution calibration. Extensive evaluations across 12 benchmark tasks spanning VQA, image captioning, and cross-modal retrieval demonstrate Astrea's superiority over state-of-the-art models, achieving an average performance gain of +4.7\\%. This study provides the first empirical demonstration that progressive pre-alignment strategies enable VLMs to overcome task heterogeneity limitations, establishing new methodological foundations for developing general-purpose multimodal agents.",
        "arxiv_id": "2503.09445",
        "ARXIVID": "2503.09445",
        "COMMENT": "Matches criterion 2 as it introduces a novel VLM architecture with progressive alignment and multi-expert collaboration.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2503.08741": {
        "authors": [
            "Letian Zhang",
            "Quan Cui",
            "Bingchen Zhao",
            "Cheng Yang"
        ],
        "title": "Oasis: One Image is All You Need for Multimodal Instruction Data Synthesis",
        "abstract": "arXiv:2503.08741v1 Announce Type: new  Abstract: The success of multi-modal large language models (MLLMs) has been largely attributed to the large-scale training data. However, the training data of many MLLMs is unavailable due to privacy concerns. The expensive and labor-intensive process of collecting multi-modal data further exacerbates the problem. Is it possible to synthesize multi-modal training data automatically without compromising diversity and quality? In this paper, we propose a new method, Oasis, to synthesize high-quality multi-modal data with only images. Oasis breaks through traditional methods by prompting only images to the MLLMs, thus extending the data diversity by a large margin. Our method features a delicate quality control method which ensures the data quality. We collected over 500k data and conducted incremental experiments on LLaVA-NeXT. Extensive experiments demonstrate that our method can significantly improve the performance of MLLMs. The image-based synthesis also allows us to focus on the specific-domain ability of MLLMs. Code and data will be publicly available.",
        "arxiv_id": "2503.08741",
        "ARXIVID": "2503.08741",
        "COMMENT": "This paper matches criterion 2 as it proposes a novel method for synthesizing multi-modal training data for MLLMs, which is a significant contribution to visual large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.09474": {
        "authors": [
            "Jiayuan Huang",
            "Runlong He",
            "Danyal Z. Khan",
            "Evangelos Mazomenos",
            "Danail Stoyanov",
            "Hani J. Marcus",
            "Matthew J. Clarkson",
            "Mobarakol Islam"
        ],
        "title": "SurgicalVLM-Agent: Towards an Interactive AI Co-Pilot for Pituitary Surgery",
        "abstract": "arXiv:2503.09474v1 Announce Type: new  Abstract: Image-guided surgery demands adaptive, real-time decision support, yet static AI models struggle with structured task planning and providing interactive guidance. Large vision-language models (VLMs) offer a promising solution by enabling dynamic task planning and predictive decision support. We introduce SurgicalVLM-Agent, an AI co-pilot for image-guided pituitary surgery, capable of conversation, planning, and task execution. The agent dynamically processes surgeon queries and plans the tasks such as MRI tumor segmentation, endoscope anatomy segmentation, overlaying preoperative imaging with intraoperative views, instrument tracking, and surgical visual question answering (VQA). To enable structured task planning, we develop the PitAgent dataset, a surgical context-aware dataset covering segmentation, overlaying, instrument localization, tool tracking, tool-tissue interactions, phase identification, and surgical activity recognition. Additionally, we propose FFT-GaLore, a fast Fourier transform (FFT)-based gradient projection technique for efficient low-rank adaptation, optimizing fine-tuning for LLaMA 3.2 in surgical environments. We validate SurgicalVLM-Agent by assessing task planning and prompt generation on our PitAgent dataset and evaluating zero-shot VQA using a public pituitary dataset. Results demonstrate state-of-the-art performance in task planning and query interpretation, with highly semantically meaningful VQA responses, advancing AI-driven surgical assistance.",
        "arxiv_id": "2503.09474",
        "ARXIVID": "2503.09474",
        "COMMENT": "Matches criterion 2 as it introduces a vision-language model (VLM) for surgical applications, which is a novel application of VLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.09332": {
        "authors": [
            "Dai Sun",
            "Huhao Guan",
            "Kun Zhang",
            "Xike Xie",
            "S. Kevin Zhou"
        ],
        "title": "SDD-4DGS: Static-Dynamic Aware Decoupling in Gaussian Splatting for 4D Scene Reconstruction",
        "abstract": "arXiv:2503.09332v1 Announce Type: new  Abstract: Dynamic and static components in scenes often exhibit distinct properties, yet most 4D reconstruction methods treat them indiscriminately, leading to suboptimal performance in both cases. This work introduces SDD-4DGS, the first framework for static-dynamic decoupled 4D scene reconstruction based on Gaussian Splatting. Our approach is built upon a novel probabilistic dynamic perception coefficient that is naturally integrated into the Gaussian reconstruction pipeline, enabling adaptive separation of static and dynamic components. With carefully designed implementation strategies to realize this theoretical framework, our method effectively facilitates explicit learning of motion patterns for dynamic elements while maintaining geometric stability for static structures. Extensive experiments on five benchmark datasets demonstrate that SDD-4DGS consistently outperforms state-of-the-art methods in reconstruction fidelity, with enhanced detail restoration for static structures and precise modeling of dynamic motions. The code will be released.",
        "arxiv_id": "2503.09332",
        "ARXIVID": "2503.09332",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for 4D scene reconstruction with a focus on static-dynamic decoupling, which is a new angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.09146": {
        "authors": [
            "Linli Yao",
            "Haoning Wu",
            "Kun Ouyang",
            "Yuanxing Zhang",
            "Caiming Xiong",
            "Bei Chen",
            "Xu Sun",
            "Junnan Li"
        ],
        "title": "Generative Frame Sampler for Long Video Understanding",
        "abstract": "arXiv:2503.09146v1 Announce Type: new  Abstract: Despite recent advances in Video Large Language Models (VideoLLMs), effectively understanding long-form videos remains a significant challenge. Perceiving lengthy videos containing thousands of frames poses substantial computational burden. To mitigate this issue, this paper introduces Generative Frame Sampler (GenS), a plug-and-play module integrated with VideoLLMs to facilitate efficient lengthy video perception. Built upon a lightweight VideoLLM, GenS leverages its inherent vision-language capabilities to identify question-relevant frames. To facilitate effective retrieval, we construct GenS-Video-150K, a large-scale video instruction dataset with dense frame relevance annotations. Extensive experiments demonstrate that GenS consistently boosts the performance of various VideoLLMs, including open-source models (Qwen2-VL-7B, Aria-25B, VILA-40B, LLaVA-Video-7B/72B) and proprietary assistants (GPT-4o, Gemini). When equipped with GenS, open-source VideoLLMs achieve impressive state-of-the-art results on long-form video benchmarks: LLaVA-Video-72B reaches 66.8 (+4.3) on LongVideoBench and 77.0 (+2.7) on MLVU, while Aria obtains 39.2 on HourVideo surpassing the Gemini-1.5-pro by 1.9 points. We will release all datasets and models at https://generative-sampler.github.io.",
        "arxiv_id": "2503.09146",
        "ARXIVID": "2503.09146",
        "COMMENT": "Matches criterion 2 as it introduces a generative frame sampler for VideoLLMs, improving long video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.09402": {
        "authors": [
            "Kevin Qinghong Lin",
            "Mike Zheng Shou"
        ],
        "title": "VLog: Video-Language Models by Generative Retrieval of Narration Vocabulary",
        "abstract": "arXiv:2503.09402v1 Announce Type: new  Abstract: Human daily activities can be concisely narrated as sequences of routine events (e.g., turning off an alarm) in video streams, forming an event vocabulary. Motivated by this, we introduce VLog, a novel video understanding framework that define video narrations as vocabulary, going beyond the typical subword vocabularies in existing generative video-language models. Built on the lightweight language model GPT-2, VLog feature three key innovations: (i) A generative retrieval model, marrying language model's complex reasoning capabilities with contrastive retrieval's efficient similarity search. (ii) A hierarchical vocabulary derived from large-scale video narrations using our narration pair encoding algorithm, enabling efficient indexing of specific events (e.g., cutting a tomato) by identifying broader scenarios (e.g., kitchen) with expressive postfixes (e.g., by the left hand). (iii) A vocabulary update strategy leveraging generative models to extend the vocabulary for novel events encountered during inference. To validate our approach, we introduce VidCap-Eval, a development set requiring concise narrations with reasoning relationships (e.g., before and after). Experiments on EgoSchema, COIN, and HiREST further demonstrate the effectiveness of VLog, highlighting its ability to generate concise, contextually accurate, and efficient narrations, offering a novel perspective on video understanding. Codes are released at https://github.com/showlab/VLog.",
        "arxiv_id": "2503.09402",
        "ARXIVID": "2503.09402",
        "COMMENT": "Matches criterion 2 as it introduces a novel video-language model (VLog) with generative retrieval and hierarchical vocabulary.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.09447": {
        "authors": [
            "Saimouli Katragadda",
            "Cho-Ying Wu",
            "Yuliang Guo",
            "Xinyu Huang",
            "Guoquan Huang",
            "Liu Ren"
        ],
        "title": "Online Language Splatting",
        "abstract": "arXiv:2503.09447v1 Announce Type: new  Abstract: To enable AI agents to interact seamlessly with both humans and 3D environments, they must not only perceive the 3D world accurately but also align human language with 3D spatial representations. While prior work has made significant progress by integrating language features into geometrically detailed 3D scene representations using 3D Gaussian Splatting (GS), these approaches rely on computationally intensive offline preprocessing of language features for each input image, limiting adaptability to new environments. In this work, we introduce Online Language Splatting, the first framework to achieve online, near real-time, open-vocabulary language mapping within a 3DGS-SLAM system without requiring pre-generated language features. The key challenge lies in efficiently fusing high-dimensional language features into 3D representations while balancing the computation speed, memory usage, rendering quality and open-vocabulary capability. To this end, we innovatively design: (1) a high-resolution CLIP embedding module capable of generating detailed language feature maps in 18ms per frame, (2) a two-stage online auto-encoder that compresses 768-dimensional CLIP features to 15 dimensions while preserving open-vocabulary capabilities, and (3) a color-language disentangled optimization approach to improve rendering quality. Experimental results show that our online method not only surpasses the state-of-the-art offline methods in accuracy but also achieves more than 40x efficiency boost, demonstrating the potential for dynamic and interactive AI applications.",
        "arxiv_id": "2503.09447",
        "ARXIVID": "2503.09447",
        "COMMENT": "Matches criterion 1 as it focuses on aligning human language with 3D spatial representations in real-time, which is a methodological improvement in spatial understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.08751": {
        "authors": [
            "Qi Wang",
            "Zhipeng Zhang",
            "Baao Xie",
            "Xin Jin",
            "Yunbo Wang",
            "Shiyu Wang",
            "Liaomo Zheng",
            "Xiaokang Yang",
            "Wenjun Zeng"
        ],
        "title": "Disentangled World Models: Learning to Transfer Semantic Knowledge from Distracting Videos for Reinforcement Learning",
        "abstract": "arXiv:2503.08751v1 Announce Type: new  Abstract: Training visual reinforcement learning (RL) in practical scenarios presents a significant challenge, $\\textit{i.e.,}$ RL agents suffer from low sample efficiency in environments with variations. While various approaches have attempted to alleviate this issue by disentanglement representation learning, these methods usually start learning from scratch without prior knowledge of the world. This paper, in contrast, tries to learn and understand underlying semantic variations from distracting videos via offline-to-online latent distillation and flexible disentanglement constraints. To enable effective cross-domain semantic knowledge transfer, we introduce an interpretable model-based RL framework, dubbed Disentangled World Models (DisWM). Specifically, we pretrain the action-free video prediction model offline with disentanglement regularization to extract semantic knowledge from distracting videos. The disentanglement capability of the pretrained model is then transferred to the world model through latent distillation. For finetuning in the online environment, we exploit the knowledge from the pretrained model and introduce a disentanglement constraint to the world model. During the adaptation phase, the incorporation of actions and rewards from online environment interactions enriches the diversity of the data, which in turn strengthens the disentangled representation learning. Experimental results validate the superiority of our approach on various benchmarks.",
        "arxiv_id": "2503.08751",
        "ARXIVID": "2503.08751",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for disentangled representation learning in reinforcement learning with semantic knowledge transfer.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.09248": {
        "authors": [
            "Lihua Zhou",
            "Mao Ye",
            "Shuaifeng Li",
            "Nianxin Li",
            "Xiatian Zhu",
            "Lei Deng",
            "Hongbin Liu",
            "Zhen Lei"
        ],
        "title": "Bayesian Test-Time Adaptation for Vision-Language Models",
        "abstract": "arXiv:2503.09248v1 Announce Type: new  Abstract: Test-time adaptation with pre-trained vision-language models, such as CLIP, aims to adapt the model to new, potentially out-of-distribution test data. Existing methods calculate the similarity between visual embedding and learnable class embeddings, which are initialized by text embeddings, for zero-shot image classification. In this work, we first analyze this process based on Bayes theorem, and observe that the core factors influencing the final prediction are the likelihood and the prior. However, existing methods essentially focus on adapting class embeddings to adapt likelihood, but they often ignore the importance of prior. To address this gap, we propose a novel approach, \\textbf{B}ayesian \\textbf{C}lass \\textbf{A}daptation (BCA), which in addition to continuously updating class embeddings to adapt likelihood, also uses the posterior of incoming samples to continuously update the prior for each class embedding. This dual updating mechanism allows the model to better adapt to distribution shifts and achieve higher prediction accuracy. Our method not only surpasses existing approaches in terms of performance metrics but also maintains superior inference rates and memory usage, making it highly efficient and practical for real-world applications.",
        "arxiv_id": "2503.09248",
        "ARXIVID": "2503.09248",
        "COMMENT": "Matches criterion 2 as it proposes a novel Bayesian test-time adaptation method for vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2503.09537": {
        "authors": [
            "Shuokang Huang",
            "Julie A. McCann"
        ],
        "title": "GenHPE: Generative Counterfactuals for 3D Human Pose Estimation with Radio Frequency Signals",
        "abstract": "arXiv:2503.09537v1 Announce Type: new  Abstract: Human pose estimation (HPE) detects the positions of human body joints for various applications. Compared to using cameras, HPE using radio frequency (RF) signals is non-intrusive and more robust to adverse conditions, exploiting the signal variations caused by human interference. However, existing studies focus on single-domain HPE confined by domain-specific confounders, which cannot generalize to new domains and result in diminished HPE performance. Specifically, the signal variations caused by different human body parts are entangled, containing subject-specific confounders. RF signals are also intertwined with environmental noise, involving environment-specific confounders. In this paper, we propose GenHPE, a 3D HPE approach that generates counterfactual RF signals to eliminate domain-specific confounders. GenHPE trains generative models conditioned on human skeleton labels, learning how human body parts and confounders interfere with RF signals. We manipulate skeleton labels (i.e., removing body parts) as counterfactual conditions for generative models to synthesize counterfactual RF signals. The differences between counterfactual signals approximately eliminate domain-specific confounders and regularize an encoder-decoder model to learn domain-independent representations. Such representations help GenHPE generalize to new subjects/environments for cross-domain 3D HPE. We evaluate GenHPE on three public datasets from WiFi, ultra-wideband, and millimeter wave. Experimental results show that GenHPE outperforms state-of-the-art methods and reduces estimation errors by up to 52.2mm for cross-subject HPE and 10.6mm for cross-environment HPE.",
        "arxiv_id": "2503.09537",
        "ARXIVID": "2503.09537",
        "COMMENT": "Matches criterion 3 as it focuses on a novel method for 3D human pose estimation using RF signals, addressing domain-specific confounders.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.08906": {
        "authors": [
            "Xiwen Chen",
            "Wenhui Zhu",
            "Peijie Qiu",
            "Hao Wang",
            "Huayu Li",
            "Haiyu Wu",
            "Aristeidis Sotiras",
            "Yalin Wang",
            "Abolfazl Razi"
        ],
        "title": "Prompt-OT: An Optimal Transport Regularization Paradigm for Knowledge Preservation in Vision-Language Model Adaptation",
        "abstract": "arXiv:2503.08906v1 Announce Type: new  Abstract: Vision-language models (VLMs) such as CLIP demonstrate strong performance but struggle when adapted to downstream tasks. Prompt learning has emerged as an efficient and effective strategy to adapt VLMs while preserving their pre-trained knowledge. However, existing methods still lead to overfitting and degrade zero-shot generalization. To address this challenge, we propose an optimal transport (OT)-guided prompt learning framework that mitigates forgetting by preserving the structural consistency of feature distributions between pre-trained and fine-tuned models. Unlike conventional point-wise constraints, OT naturally captures cross-instance relationships and expands the feasible parameter space for prompt tuning, allowing a better trade-off between adaptation and generalization. Our approach enforces joint constraints on both vision and text representations, ensuring a holistic feature alignment. Extensive experiments on benchmark datasets demonstrate that our simple yet effective method can outperform existing prompt learning strategies in base-to-novel generalization, cross-dataset evaluation, and domain generalization without additional augmentation or ensemble techniques. The code is available at https://github.com/ChongQingNoSubway/Prompt-OT",
        "arxiv_id": "2503.08906",
        "ARXIVID": "2503.08906",
        "COMMENT": "Matches criterion 2 as it proposes a novel optimal transport-guided prompt learning framework for vision-language model adaptation.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2503.09527": {
        "authors": [
            "Peng Chen",
            "Pi Bu",
            "Yingyao Wang",
            "Xinyi Wang",
            "Ziming Wang",
            "Jie Guo",
            "Yingxiu Zhao",
            "Qi Zhu",
            "Jun Song",
            "Siran Yang",
            "Jiamang Wang",
            "Bo Zheng"
        ],
        "title": "CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games",
        "abstract": "arXiv:2503.09527v1 Announce Type: new  Abstract: Recent advances in Vision-Language-Action models (VLAs) have expanded the capabilities of embodied intelligence. However, significant challenges remain in real-time decision-making in complex 3D environments, which demand second-level responses, high-resolution perception, and tactical reasoning under dynamic conditions. To advance the field, we introduce CombatVLA, an efficient VLA model optimized for combat tasks in 3D action role-playing games(ARPGs). Specifically, our CombatVLA is a 3B model trained on video-action pairs collected by an action tracker, where the data is formatted as action-of-thought (AoT) sequences. Thereafter, CombatVLA seamlessly integrates into an action execution framework, allowing efficient inference through our truncated AoT strategy. Experimental results demonstrate that CombatVLA not only outperforms all existing models on the combat understanding benchmark but also achieves a 50-fold acceleration in game combat. Moreover, it has a higher task success rate than human players. We will open-source all resources, including the action tracker, dataset, benchmark, model weights, training code, and the implementation of the framework at https://combatvla.github.io/.",
        "arxiv_id": "2503.09527",
        "ARXIVID": "2503.09527",
        "COMMENT": "Matches criterion 3 as it focuses on embodied AI with a new benchmark and method for combat tasks in 3D ARPGs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2503.09321": {
        "authors": [
            "Gorjan Radevski",
            "Teodora Popordanoska",
            "Matthew B. Blaschko",
            "Tinne Tuytelaars"
        ],
        "title": "DAVE: Diagnostic benchmark for Audio Visual Evaluation",
        "abstract": "arXiv:2503.09321v1 Announce Type: new  Abstract: Audio-visual understanding is a rapidly evolving field that seeks to integrate and interpret information from both auditory and visual modalities. Despite recent advances in multi-modal learning, existing benchmarks often suffer from strong visual bias -- where answers can be inferred from visual data alone -- and provide only aggregate scores that conflate multiple sources of error. This makes it difficult to determine whether models struggle with visual understanding, audio interpretation, or audio-visual alignment. In this work, we introduce DAVE (Diagnostic Audio Visual Evaluation), a novel benchmark dataset designed to systematically evaluate audio-visual models across controlled challenges. DAVE alleviates existing limitations by (i) ensuring both modalities are necessary to answer correctly and (ii) decoupling evaluation into atomic subcategories. Our detailed analysis of state-of-the-art models reveals specific failure modes and provides targeted insights for improvement. By offering this standardized diagnostic framework, we aim to facilitate more robust development of audio-visual models. The dataset is released: https://github.com/gorjanradevski/dave",
        "arxiv_id": "2503.09321",
        "ARXIVID": "2503.09321",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for audio-visual evaluation with a focus on addressing visual bias and multi-modal alignment.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.08843": {
        "authors": [
            "Rajitha de Silva",
            "Jonathan Cox",
            "Marija Popovic",
            "Cesar Cadena",
            "Cyrill Stachniss",
            "Riccardo Polvara"
        ],
        "title": "Keypoint Semantic Integration for Improved Feature Matching in Outdoor Agricultural Environments",
        "abstract": "arXiv:2503.08843v1 Announce Type: new  Abstract: Robust robot navigation in outdoor environments requires accurate perception systems capable of handling visual challenges such as repetitive structures and changing appearances. Visual feature matching is crucial to vision-based pipelines but remains particularly challenging in natural outdoor settings due to perceptual aliasing. We address this issue in vineyards, where repetitive vine trunks and other natural elements generate ambiguous descriptors that hinder reliable feature matching. We hypothesise that semantic information tied to keypoint positions can alleviate perceptual aliasing by enhancing keypoint descriptor distinctiveness. To this end, we introduce a keypoint semantic integration technique that improves the descriptors in semantically meaningful regions within the image, enabling more accurate differentiation even among visually similar local features. We validate this approach in two vineyard perception tasks: (i) relative pose estimation and (ii) visual localisation. Across all tested keypoint types and descriptors, our method improves matching accuracy by 12.6%, demonstrating its effectiveness over multiple months in challenging vineyard conditions.",
        "arxiv_id": "2503.08843",
        "ARXIVID": "2503.08843",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for improving spatial understanding in outdoor environments for embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.09439": {
        "authors": [
            "Qijian Zhang",
            "Xiaozheng Jian",
            "Xuan Zhang",
            "Wenping Wang",
            "Junhui Hou"
        ],
        "title": "SuperCarver: Texture-Consistent 3D Geometry Super-Resolution for High-Fidelity Surface Detail Generation",
        "abstract": "arXiv:2503.09439v1 Announce Type: new  Abstract: Traditional production workflow of high-precision 3D mesh assets necessitates a cumbersome and laborious process of manual sculpting by specialized modelers. The recent years have witnessed remarkable advances in AI-empowered 3D content creation. However, although the latest state-of-the-arts are already capable of generating plausible structures and intricate appearances from images or text prompts, the actual mesh surfaces are typically over-smoothing and lack geometric details. This paper introduces SuperCarver, a 3D geometry super-resolution framework particularly tailored for adding texture-consistent surface details to given coarse meshes. Technically, we start by rendering the original textured mesh into the image domain from multiple viewpoints. To achieve geometric detail generation, we develop a deterministic prior-guided normal diffusion model fine-tuned on a carefully curated dataset of paired low-poly and high-poly normal renderings. To optimize mesh structures from potentially imperfect normal map predictions, we design a simple yet effective noise-resistant inverse rendering scheme based on distance field deformation. Extensive experiments show that SuperCarver generates realistic and expressive surface details as depicted by specific texture appearances, making it a powerful tool for automatically upgrading massive outdated low-quality assets and shortening the iteration cycle of high-quality mesh production in practical applications.",
        "arxiv_id": "2503.09439",
        "ARXIVID": "2503.09439",
        "COMMENT": "Matches criterion 4 as it focuses on texture-consistent 3D geometry super-resolution, which is related to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.09277": {
        "authors": [
            "Haoxuan Wang",
            "Jinlong Peng",
            "Qingdong He",
            "Hao Yang",
            "Ying Jin",
            "Jiafu Wu",
            "Xiaobin Hu",
            "Yanjie Pan",
            "Zhenye Gan",
            "Mingmin Chi",
            "Bo Peng",
            "Yabiao Wang"
        ],
        "title": "UniCombine: Unified Multi-Conditional Combination with Diffusion Transformer",
        "abstract": "arXiv:2503.09277v1 Announce Type: new  Abstract: With the rapid development of diffusion models in image generation, the demand for more powerful and flexible controllable frameworks is increasing. Although existing methods can guide generation beyond text prompts, the challenge of effectively combining multiple conditional inputs while maintaining consistency with all of them remains unsolved. To address this, we introduce UniCombine, a DiT-based multi-conditional controllable generative framework capable of handling any combination of conditions, including but not limited to text prompts, spatial maps, and subject images. Specifically, we introduce a novel Conditional MMDiT Attention mechanism and incorporate a trainable LoRA module to build both the training-free and training-based versions. Additionally, we propose a new pipeline to construct SubjectSpatial200K, the first dataset designed for multi-conditional generative tasks covering both the subject-driven and spatially-aligned conditions. Extensive experimental results on multi-conditional generation demonstrate the outstanding universality and powerful capability of our approach with state-of-the-art performance.",
        "arxiv_id": "2503.09277",
        "ARXIVID": "2503.09277",
        "COMMENT": "Matches criterion 2 as it discusses a multi-conditional generative framework using diffusion transformers, which is relevant to multi-modal large language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.09387": {
        "authors": [
            "Ruanjun Li",
            "Yuedong Tan",
            "Yuanming Shi",
            "Jiawei Shao"
        ],
        "title": "VideoScan: Enabling Efficient Streaming Video Understanding via Frame-level Semantic Carriers",
        "abstract": "arXiv:2503.09387v1 Announce Type: new  Abstract: This paper introduces VideoScan, an efficient vision-language model (VLM) inference framework designed for real-time video interaction that effectively comprehends and retains streamed video inputs while delivering rapid and accurate responses. A longstanding challenge in video understanding--particularly for long-term or real-time applications--stems from the substantial computational overhead caused by the extensive length of visual tokens. To address this, VideoScan employs a single semantic carrier token to represent each frame, progressively reducing computational and memory overhead during its two-phase inference process: prefilling and decoding. The embedding of the semantic carrier token is derived from an optimized aggregation of frame-level visual features, ensuring compact yet semantically rich representations. Critically, the corresponding key-value pairs are trained to retain contextual semantics from prior frames, enabling efficient memory management without sacrificing temporal coherence. During inference, the visual tokens of each frame are processed only once during the prefilling phase and subsequently discarded in the decoding stage, eliminating redundant computations. This design ensures efficient VLM inference even under stringent real-time constraints. Comprehensive experiments on diverse offline and online benchmarks demonstrate that LLaVA-Video, supported by our method, achieves up to $\\sim 5\\times$ and $1.29\\times$ speedups compared to its original version and previous efficient streaming video understanding approaches, respectively. Crucially, these improvements are attained while maintaining competitive performance and ensuring stable GPU memory consumption (consistently $\\sim 18$GB, independent of video duration).",
        "arxiv_id": "2503.09387",
        "ARXIVID": "2503.09387",
        "COMMENT": "Matches criterion 2 as it introduces a vision-language model (VLM) framework for efficient video understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.08722": {
        "authors": [
            "Aviad Barzilai",
            "Yotam Gigi",
            "Vered Silverman",
            "Yehonathan Refael",
            "Bolous Jaber",
            "Amr Helmy",
            "Tomer Shekel",
            "George Leifman",
            "Genady Beryozkin"
        ],
        "title": "A Recipe for Improving Remote Sensing VLM Zero Shot Generalization",
        "abstract": "arXiv:2503.08722v1 Announce Type: new  Abstract: Foundation models have had a significant impact across various AI applications, enabling use cases that were previously impossible. Contrastive Visual Language Models (VLMs), in particular, have outperformed other techniques in many tasks. However, their prevalence in remote sensing (RS) is still limited, due to the scarcity of diverse remote-sensing visual-language datasets. In this work we introduce two novel image-caption datasets for training of remote sensing foundation models. The first dataset pairs aerial and satellite imagery with captions generated by Gemini using landmarks extracted from Google Maps. The second dataset utilizes public web images and their corresponding alt-text, filtered for the remote sensing domain, resulting in a diverse dataset with greater breadth in image styles and subject matter. These datasets are used to pre-train the MaMMUT~\\citep{kuo2023mammutsimplearchitecturejoint} VLM architecture, resulting in state-of-the-art generalization performance in zero-shot cross-modal retrieval on well-known public benchmarks. Finally, we present our ongoing research to distill image-level knowledge gained in the VLM contrastive training procedure to enhance the model's localization ability. Specifically, we iteratively generate pseudo-labels for image regions based on the model's attention maps and use these labels for further training. To mitigate noisy attention maps and create robust segmentation masks, we introduce a novel attention-pooling mechanism called the Smooth-Attention-Operation.",
        "arxiv_id": "2503.08722",
        "ARXIVID": "2503.08722",
        "COMMENT": "Matches criterion 4 as it introduces new datasets and methods for vision foundation models in remote sensing applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.09197": {
        "authors": [
            "Zicheng Zhang",
            "Haoning Wu",
            "Ziheng Jia",
            "Weisi Lin",
            "Guangtao Zhai"
        ],
        "title": "Teaching LMMs for Image Quality Scoring and Interpreting",
        "abstract": "arXiv:2503.09197v1 Announce Type: new  Abstract: Image quality scoring and interpreting are two fundamental components of Image Quality Assessment (IQA). The former quantifies image quality, while the latter enables descriptive question answering about image quality. Traditionally, these two tasks have been addressed independently. However, from the perspective of the Human Visual System (HVS) and the Perception-Decision Integration Model, they are inherently interconnected: interpreting serves as the foundation for scoring, while scoring provides an abstract summary of interpreting. Thus, unifying these capabilities within a single model is both intuitive and logically coherent. In this paper, we propose Q-SiT (Quality Scoring and Interpreting joint Teaching), a unified framework that enables large multimodal models (LMMs) to learn both image quality scoring and interpreting simultaneously. We achieve this by transforming conventional IQA datasets into learnable question-answering datasets and incorporating human-annotated quality interpreting data for training. Furthermore, we introduce an efficient scoring & interpreting balance strategy, which first determines the optimal data mix ratio on lightweight LMMs and then maps this ratio to primary LMMs for fine-tuning adjustment. This strategy not only mitigates task interference and enhances cross-task knowledge transfer but also significantly reduces computational costs compared to direct optimization on full-scale LMMs. With this joint learning framework and corresponding training strategy, we develop Q-SiT, the first model capable of simultaneously performing image quality scoring and interpreting tasks, along with its lightweight variant, Q-SiT-mini. Experimental results demonstrate that Q-SiT achieves strong performance in both tasks with superior generalization IQA abilities.Project page at https://github.com/Q-Future/Q-SiT.",
        "arxiv_id": "2503.09197",
        "ARXIVID": "2503.09197",
        "COMMENT": "Matches criterion 2 as it proposes a unified framework for large multimodal models (LMMs) for image quality scoring and interpreting.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.09271": {
        "authors": [
            "Chiara Cappellino",
            "Gianluca Mancusi",
            "Matteo Mosconi",
            "Angelo Porrello",
            "Simone Calderara",
            "Rita Cucchiara"
        ],
        "title": "DitHub: A Modular Framework for Incremental Open-Vocabulary Object Detection",
        "abstract": "arXiv:2503.09271v1 Announce Type: new  Abstract: Open-Vocabulary object detectors can recognize a wide range of categories using simple textual prompts. However, improving their ability to detect rare classes or specialize in certain domains remains a challenge. While most recent methods rely on a single set of model weights for adaptation, we take a different approach by using modular deep learning. We introduce DitHub, a framework designed to create and manage a library of efficient adaptation modules. Inspired by Version Control Systems, DitHub organizes expert modules like branches that can be fetched and merged as needed. This modular approach enables a detailed study of how adaptation modules combine, making it the first method to explore this aspect in Object Detection. Our approach achieves state-of-the-art performance on the ODinW-13 benchmark and ODinW-O, a newly introduced benchmark designed to evaluate how well models adapt when previously seen classes reappear. For more details, visit our project page: https://aimagelab.github.io/DitHub/",
        "arxiv_id": "2503.09271",
        "ARXIVID": "2503.09271",
        "COMMENT": "Matches criterion 3 as it introduces a new modular framework for open-vocabulary object detection, which is a novel angle in embodied AI benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.09567": {
        "authors": [
            "Qiguang Chen",
            "Libo Qin",
            "Jinhao Liu",
            "Dengyun Peng",
            "Jiannan Guan",
            "Peng Wang",
            "Mengkang Hu",
            "Yuhang Zhou",
            "Te Gao",
            "Wangxiang Che"
        ],
        "title": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models",
        "abstract": "arXiv:2503.09567v1 Announce Type: new  Abstract: Recent advancements in reasoning with large language models (RLLMs), such as OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in complex domains like mathematics and coding. A central factor in their success lies in the application of long chain-of-thought (Long CoT) characteristics, which enhance reasoning abilities and enable the solution of intricate problems. However, despite these developments, a comprehensive survey on Long CoT is still lacking, limiting our understanding of its distinctions from traditional short chain-of-thought (Short CoT) and complicating ongoing debates on issues like \"overthinking\" and \"test-time scaling.\" This survey seeks to fill this gap by offering a unified perspective on Long CoT. (1) We first distinguish Long CoT from Short CoT and introduce a novel taxonomy to categorize current reasoning paradigms. (2) Next, we explore the key characteristics of Long CoT: deep reasoning, extensive exploration, and feasible reflection, which enable models to handle more complex tasks and produce more efficient, coherent outcomes compared to the shallower Short CoT. (3) We then investigate key phenomena such as the emergence of Long CoT with these characteristics, including overthinking, and test-time scaling, offering insights into how these processes manifest in practice. (4) Finally, we identify significant research gaps and highlight promising future directions, including the integration of multi-modal reasoning, efficiency improvements, and enhanced knowledge frameworks. By providing a structured overview, this survey aims to inspire future research and further the development of logical reasoning in artificial intelligence.",
        "arxiv_id": "2503.09567",
        "ARXIVID": "2503.09567",
        "COMMENT": "Matches criterion 2 as it surveys reasoning capabilities in large language models, focusing on long chain-of-thought reasoning.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2503.09590": {
        "authors": [
            "Md Mohaiminul Islam",
            "Tushar Nagarajan",
            "Huiyu Wang",
            "Gedas Bertasius",
            "Lorenzo Torresani"
        ],
        "title": "BIMBA: Selective-Scan Compression for Long-Range Video Question Answering",
        "abstract": "arXiv:2503.09590v1 Announce Type: new  Abstract: Video Question Answering (VQA) in long videos poses the key challenge of extracting relevant information and modeling long-range dependencies from many redundant frames. The self-attention mechanism provides a general solution for sequence modeling, but it has a prohibitive cost when applied to a massive number of spatiotemporal tokens in long videos. Most prior methods rely on compression strategies to lower the computational cost, such as reducing the input length via sparse frame sampling or compressing the output sequence passed to the large language model (LLM) via space-time pooling. However, these naive approaches over-represent redundant information and often miss salient events or fast-occurring space-time patterns. In this work, we introduce BIMBA, an efficient state-space model to handle long-form videos. Our model leverages the selective scan algorithm to learn to effectively select critical information from high-dimensional video and transform it into a reduced token sequence for efficient LLM processing. Extensive experiments demonstrate that BIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks, including PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and Video-MME. Code, and models are publicly available at https://sites.google.com/view/bimba-mllm.",
        "arxiv_id": "2503.09590",
        "ARXIVID": "2503.09590",
        "COMMENT": "Matches criterion 2 as it discusses a model for video question answering with efficient token selection, aligning with multi-modal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.08933": {
        "authors": [
            "Zhangyu Jin",
            "Andrew Feng",
            "Ankur Chemburkar",
            "Celso M. De Melo"
        ],
        "title": "PromptGAR: Flexible Promptive Group Activity Recognition",
        "abstract": "arXiv:2503.08933v1 Announce Type: new  Abstract: We present PromptGAR, a novel framework that addresses the limitations of current Group Activity Recognition (GAR) approaches by leveraging multi-modal prompts to achieve both input flexibility and high recognition accuracy. The existing approaches suffer from limited real-world applicability due to their reliance on full prompt annotations, the lack of long-term actor consistency, and under-exploration of multi-group scenarios. To bridge the gap, we proposed PromptGAR, which is the first GAR model to provide input flexibility across prompts, frames, and instances without the need for retraining. Specifically, we unify bounding boxes, skeletal keypoints, and areas as point prompts and employ a recognition decoder for cross-updating class and prompt tokens. To ensure long-term consistency for extended activity durations, we also introduce a relative instance attention mechanism that directly encodes instance IDs. Finally, PromptGAR explores the use of area prompts to enable the selective recognition of the particular group activity within videos that contain multiple concurrent groups. Comprehensive evaluations demonstrate that PromptGAR achieves competitive performances both on full prompts and diverse prompt inputs, establishing its effectiveness on input flexibility and generalization ability for real-world applications.",
        "arxiv_id": "2503.08933",
        "ARXIVID": "2503.08933",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework for group activity recognition with multi-modal prompts, addressing under-explored aspects of GAR.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.09344": {
        "authors": [
            "Lehan Yang",
            "Lu Qi",
            "Xiangtai Li",
            "Sheng Li",
            "Varun Jampani",
            "Ming-Hsuan Yang"
        ],
        "title": "Unified Dense Prediction of Video Diffusion",
        "abstract": "arXiv:2503.09344v1 Announce Type: new  Abstract: We present a unified network for simultaneously generating videos and their corresponding entity segmentation and depth maps from text prompts. We utilize colormap to represent entity masks and depth maps, tightly integrating dense prediction with RGB video generation. Introducing dense prediction information improves video generation's consistency and motion smoothness without increasing computational costs. Incorporating learnable task embeddings brings multiple dense prediction tasks into a single model, enhancing flexibility and further boosting performance. We further propose a large-scale dense prediction video dataset~\\datasetname, addressing the issue that existing datasets do not concurrently contain captions, videos, segmentation, or depth maps. Comprehensive experiments demonstrate the high efficiency of our method, surpassing the state-of-the-art in terms of video quality, consistency, and motion smoothness.",
        "arxiv_id": "2503.09344",
        "ARXIVID": "2503.09344",
        "COMMENT": "Matches criterion 2 as it discusses a unified network for video generation with dense prediction tasks, which aligns with multi-modal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.09091": {
        "authors": [
            "Dong Li",
            "Guihong Wan",
            "Xintao Wu",
            "Xinyu Wu",
            "Xiaohui Chen",
            "Yi He",
            "Christine G. Lian",
            "Peter K. Sorger",
            "Yevgeniy R. Semenov",
            "Chen Zhao"
        ],
        "title": "Multi-Modal Foundation Models for Computational Pathology: A Survey",
        "abstract": "arXiv:2503.09091v1 Announce Type: new  Abstract: Foundation models have emerged as a powerful paradigm in computational pathology (CPath), enabling scalable and generalizable analysis of histopathological images. While early developments centered on uni-modal models trained solely on visual data, recent advances have highlighted the promise of multi-modal foundation models that integrate heterogeneous data sources such as textual reports, structured domain knowledge, and molecular profiles. In this survey, we provide a comprehensive and up-to-date review of multi-modal foundation models in CPath, with a particular focus on models built upon hematoxylin and eosin (H&E) stained whole slide images (WSIs) and tile-level representations. We categorize 32 state-of-the-art multi-modal foundation models into three major paradigms: vision-language, vision-knowledge graph, and vision-gene expression. We further divide vision-language models into non-LLM-based and LLM-based approaches. Additionally, we analyze 28 available multi-modal datasets tailored for pathology, grouped into image-text pairs, instruction datasets, and image-other modality pairs. Our survey also presents a taxonomy of downstream tasks, highlights training and evaluation strategies, and identifies key challenges and future directions. We aim for this survey to serve as a valuable resource for researchers and practitioners working at the intersection of pathology and AI.",
        "arxiv_id": "2503.09091",
        "ARXIVID": "2503.09091",
        "COMMENT": "Matches criterion 2 as it surveys multi-modal foundation models, including vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 4
    },
    "2503.08884": {
        "authors": [
            "Parsa Hosseini",
            "Sumit Nawathe",
            "Mazda Moayeri",
            "Sriram Balasubramanian",
            "Soheil Feizi"
        ],
        "title": "Seeing What's Not There: Spurious Correlation in Multimodal LLMs",
        "abstract": "arXiv:2503.08884v1 Announce Type: new  Abstract: Unimodal vision models are known to rely on spurious correlations, but it remains unclear to what extent Multimodal Large Language Models (MLLMs) exhibit similar biases despite language supervision. In this paper, we investigate spurious bias in MLLMs and introduce SpurLens, a pipeline that leverages GPT-4 and open-set object detectors to automatically identify spurious visual cues without human supervision. Our findings reveal that spurious correlations cause two major failure modes in MLLMs: (1) over-reliance on spurious cues for object recognition, where removing these cues reduces accuracy, and (2) object hallucination, where spurious cues amplify the hallucination by over 10x. We validate our findings in various MLLMs and datasets. Beyond diagnosing these failures, we explore potential mitigation strategies, such as prompt ensembling and reasoning-based prompting, and conduct ablation studies to examine the root causes of spurious bias in MLLMs. By exposing the persistence of spurious correlations, our study calls for more rigorous evaluation methods and mitigation strategies to enhance the reliability of MLLMs.",
        "arxiv_id": "2503.08884",
        "ARXIVID": "2503.08884",
        "COMMENT": "Matches criterion 2 as it investigates spurious correlations in multimodal large language models (MLLMs), providing insights into their behavior.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2503.09013": {
        "authors": [
            "Rongxin Liao",
            "Feng Li",
            "Yanyan Wei",
            "Zenglin Shi",
            "Le Zhang",
            "Huihui Bai",
            "Meng Wang"
        ],
        "title": "Prompt to Restore, Restore to Prompt: Cyclic Prompting for Universal Adverse Weather Removal",
        "abstract": "arXiv:2503.09013v1 Announce Type: new  Abstract: Universal adverse weather removal (UAWR) seeks to address various weather degradations within a unified framework. Recent methods are inspired by prompt learning using pre-trained vision-language models (e.g., CLIP), leveraging degradation-aware prompts to facilitate weather-free image restoration, yielding significant improvements. In this work, we propose CyclicPrompt, an innovative cyclic prompt approach designed to enhance the effectiveness, adaptability, and generalizability of UAWR. CyclicPrompt Comprises two key components: 1) a composite context prompt that integrates weather-related information and context-aware representations into the network to guide restoration. This prompt differs from previous methods by marrying learnable input-conditional vectors with weather-specific knowledge, thereby improving adaptability across various degradations. 2) The erase-and-paste mechanism, after the initial guided restoration, substitutes weather-specific knowledge with constrained restoration priors, inducing high-quality weather-free concepts into the composite prompt to further fine-tune the restoration process. Therefore, we can form a cyclic \"Prompt-Restore-Prompt\" pipeline that adeptly harnesses weather-specific knowledge, textual contexts, and reliable textures. Extensive experiments on synthetic and real-world datasets validate the superior performance of CyclicPrompt. The code is available at: https://github.com/RongxinL/CyclicPrompt.",
        "arxiv_id": "2503.09013",
        "ARXIVID": "2503.09013",
        "COMMENT": "Matches criterion 4 as it discusses leveraging vision-language models (CLIP) for weather removal, which is an application of vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.09396": {
        "authors": [
            "Jiatong Xia",
            "Lingqiao Liu"
        ],
        "title": "Close-up-GS: Enhancing Close-Up View Synthesis in 3D Gaussian Splatting with Progressive Self-Training",
        "abstract": "arXiv:2503.09396v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in synthesizing novel views after training on a given set of viewpoints. However, its rendering quality deteriorates when the synthesized view deviates significantly from the training views. This decline occurs due to (1) the model's difficulty in generalizing to out-of-distribution scenarios and (2) challenges in interpolating fine details caused by substantial resolution changes and occlusions. A notable case of this limitation is close-up view generation--producing views that are significantly closer to the object than those in the training set. To tackle this issue, we propose a novel approach for close-up view generation based by progressively training the 3DGS model with self-generated data. Our solution is based on three key ideas. First, we leverage the See3D model, a recently introduced 3D-aware generative model, to enhance the details of rendered views. Second, we propose a strategy to progressively expand the ``trust regions'' of the 3DGS model and update a set of reference views for See3D. Finally, we introduce a fine-tuning strategy to carefully update the 3DGS model with training data generated from the above schemes. We further define metrics for close-up views evaluation to facilitate better research on this problem. By conducting evaluations on specifically selected scenarios for close-up views, our proposed approach demonstrates a clear advantage over competitive solutions.",
        "arxiv_id": "2503.09396",
        "ARXIVID": "2503.09396",
        "COMMENT": "This paper does not match any specific criteria but is related to 3D vision and generative modeling, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.09314": {
        "authors": [
            "Xinghan Li",
            "Jingjing Chen",
            "Yue Yu",
            "Xue Song",
            "Haijun Shan",
            "Yu-Gang Jiang"
        ],
        "title": "Revealing the Implicit Noise-based Imprint of Generative Models",
        "abstract": "arXiv:2503.09314v1 Announce Type: new  Abstract: With the rapid advancement of vision generation models, the potential security risks stemming from synthetic visual content have garnered increasing attention, posing significant challenges for AI-generated image detection. Existing methods suffer from inadequate generalization capabilities, resulting in unsatisfactory performance on emerging generative models. To address this issue, this paper presents a novel framework that leverages noise-based model-specific imprint for the detection task. Specifically, we propose a novel noise-based imprint simulator to capture intrinsic patterns imprinted in images generated by different models. By aggregating imprints from various generative models, imprints of future models can be extrapolated to expand training data, thereby enhancing generalization and robustness. Furthermore, we design a new pipeline that pioneers the use of noise patterns, derived from a noise-based imprint extractor, alongside other visual features for AI-generated image detection, resulting in a significant improvement in performance. Our approach achieves state-of-the-art performance across three public benchmarks including GenImage, Synthbuster and Chameleon.",
        "arxiv_id": "2503.09314",
        "ARXIVID": "2503.09314",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and detection of AI-generated images.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.09151": {
        "authors": [
            "Hyeonho Jeong",
            "Suhyeon Lee",
            "Jong Chul Ye"
        ],
        "title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
        "abstract": "arXiv:2503.09151v1 Announce Type: new  Abstract: We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: https://hyeonho99.github.io/reangle-a-video/",
        "arxiv_id": "2503.09151",
        "ARXIVID": "2503.09151",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling and video understanding.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.09154": {
        "authors": [
            "Chengshu Zhao",
            "Yunyang Ge",
            "Xinhua Cheng",
            "Bin Zhu",
            "Yatian Pang",
            "Bin Lin",
            "Fan Yang",
            "Feng Gao",
            "Li Yuan"
        ],
        "title": "SwapAnyone: Consistent and Realistic Video Synthesis for Swapping Any Person into Any Video",
        "abstract": "arXiv:2503.09154v1 Announce Type: new  Abstract: Video body-swapping aims to replace the body in an existing video with a new body from arbitrary sources, which has garnered more attention in recent years. Existing methods treat video body-swapping as a composite of multiple tasks instead of an independent task and typically rely on various models to achieve video body-swapping sequentially. However, these methods fail to achieve end-to-end optimization for the video body-swapping which causes issues such as variations in luminance among frames, disorganized occlusion relationships, and the noticeable separation between bodies and background. In this work, we define video body-swapping as an independent task and propose three critical consistencies: identity consistency, motion consistency, and environment consistency. We introduce an end-to-end model named SwapAnyone, treating video body-swapping as a video inpainting task with reference fidelity and motion control. To improve the ability to maintain environmental harmony, particularly luminance harmony in the resulting video, we introduce a novel EnvHarmony strategy for training our model progressively. Additionally, we provide a dataset named HumanAction-32K covering various videos about human actions. Extensive experiments demonstrate that our method achieves State-Of-The-Art (SOTA) performance among open-source methods while approaching or surpassing closed-source models across multiple dimensions. All code, model weights, and the HumanAction-32K dataset will be open-sourced at https://github.com/PKU-YuanGroup/SwapAnyone.",
        "arxiv_id": "2503.09154",
        "ARXIVID": "2503.09154",
        "COMMENT": "This paper does not match any specific criteria but is related to generative modeling in video synthesis, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09410": {
        "authors": [
            "Jiale Wang",
            "Chen Zhao",
            "Wei Ke",
            "Tong Zhang"
        ],
        "title": "Monte Carlo Diffusion for Generalizable Learning-Based RANSAC",
        "abstract": "arXiv:2503.09410v1 Announce Type: new  Abstract: Random Sample Consensus (RANSAC) is a fundamental approach for robustly estimating parametric models from noisy data. Existing learning-based RANSAC methods utilize deep learning to enhance the robustness of RANSAC against outliers. However, these approaches are trained and tested on the data generated by the same algorithms, leading to limited generalization to out-of-distribution data during inference. Therefore, in this paper, we introduce a novel diffusion-based paradigm that progressively injects noise into ground-truth data, simulating the noisy conditions for training learning-based RANSAC. To enhance data diversity, we incorporate Monte Carlo sampling into the diffusion paradigm, approximating diverse data distributions by introducing different types of randomness at multiple stages. We evaluate our approach in the context of feature matching through comprehensive experiments on the ScanNet and MegaDepth datasets. The experimental results demonstrate that our Monte Carlo diffusion mechanism significantly improves the generalization ability of learning-based RANSAC. We also develop extensive ablation studies that highlight the effectiveness of key components in our framework.",
        "arxiv_id": "2503.09410",
        "ARXIVID": "2503.09410",
        "COMMENT": "This paper does not directly match any of the criteria but is tangentially related to spatial understanding through robust model estimation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09514": {
        "authors": [
            "Bin Hu",
            "Chenqiang Gao",
            "Shurui Liu",
            "Junjie Guo",
            "Fang Chen",
            "Fangcen Liu"
        ],
        "title": "CM-Diff: A Single Generative Network for Bidirectional Cross-Modality Translation Diffusion Model Between Infrared and Visible Images",
        "abstract": "arXiv:2503.09514v1 Announce Type: new  Abstract: The image translation method represents a crucial approach for mitigating information deficiencies in the infrared and visible modalities, while also facilitating the enhancement of modality-specific datasets. However, existing methods for infrared and visible image translation either achieve unidirectional modality translation or rely on cycle consistency for bidirectional modality translation, which may result in suboptimal performance. In this work, we present the cross-modality translation diffusion model (CM-Diff) for simultaneously modeling data distributions in both the infrared and visible modalities. We address this challenge by combining translation direction labels for guidance during training with cross-modality feature control. Specifically, we view the establishment of the mapping relationship between the two modalities as the process of learning data distributions and understanding modality differences, achieved through a novel Bidirectional Diffusion Training (BDT) strategy. Additionally, we propose a Statistical Constraint Inference (SCI) strategy to ensure the generated image closely adheres to the data distribution of the target modality. Experimental results demonstrate the superiority of our CM-Diff over state-of-the-art methods, highlighting its potential for generating dual-modality datasets.",
        "arxiv_id": "2503.09514",
        "ARXIVID": "2503.09514",
        "COMMENT": "Does not match any specific criteria but is related to cross-modality translation and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09131": {
        "authors": [
            "Zhehui Wu",
            "Yong Chen",
            "Naoto Yokoya",
            "Wei He"
        ],
        "title": "MP-HSIR: A Multi-Prompt Framework for Universal Hyperspectral Image Restoration",
        "abstract": "arXiv:2503.09131v1 Announce Type: new  Abstract: Hyperspectral images (HSIs) often suffer from diverse and unknown degradations during imaging, leading to severe spectral and spatial distortions. Existing HSI restoration methods typically rely on specific degradation assumptions, limiting their effectiveness in complex scenarios. In this paper, we propose MP-HSIR, a novel multi-prompt framework that effectively integrates spectral, textual, and visual prompts to achieve universal HSI restoration across diverse degradation types and intensities. Specifically, we develop a prompt-guided spatial-spectral transformer, which incorporates spatial self-attention and a prompt-guided dual-branch spectral self-attention. Since degradations affect spectral features differently, we introduce spectral prompts in the local spectral branch to provide universal low-rank spectral patterns as prior knowledge for enhancing spectral reconstruction. Furthermore, the text-visual synergistic prompt fuses high-level semantic representations with fine-grained visual features to encode degradation information, thereby guiding the restoration process. Extensive experiments on 9 HSI restoration tasks, including all-in-one scenarios, generalization tests, and real-world cases, demonstrate that MP-HSIR not only consistently outperforms existing all-in-one methods but also surpasses state-of-the-art task-specific approaches across multiple tasks. The code and models will be released at https://github.com/ZhehuiWu/MP-HSIR.",
        "arxiv_id": "2503.09131",
        "ARXIVID": "2503.09131",
        "COMMENT": "Does not match any specific criteria but is related to image restoration and multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09368": {
        "authors": [
            "Nikolai K\\\"orber",
            "Eduard Kromer",
            "Andreas Siebert",
            "Sascha Hauke",
            "Daniel Mueller-Gritschneder",
            "Bj\\\"orn Schuller"
        ],
        "title": "PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with Implicit Hierarchical Masked Image Modeling",
        "abstract": "arXiv:2503.09368v1 Announce Type: new  Abstract: We introduce PerCoV2, a novel and open ultra-low bit-rate perceptual image compression system designed for bandwidth- and storage-constrained applications. Building upon prior work by Careil et al., PerCoV2 extends the original formulation to the Stable Diffusion 3 ecosystem and enhances entropy coding efficiency by explicitly modeling the discrete hyper-latent image distribution. To this end, we conduct a comprehensive comparison of recent autoregressive methods (VAR and MaskGIT) for entropy modeling and evaluate our approach on the large-scale MSCOCO-30k benchmark. Compared to previous work, PerCoV2 (i) achieves higher image fidelity at even lower bit-rates while maintaining competitive perceptual quality, (ii) features a hybrid generation mode for further bit-rate savings, and (iii) is built solely on public components. Code and trained models will be released at https://github.com/Nikolai10/PerCoV2.",
        "arxiv_id": "2503.09368",
        "ARXIVID": "2503.09368",
        "COMMENT": "Does not match any specific criteria but is related to image compression and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09446": {
        "authors": [
            "Zhihua Tian",
            "Sirun Nan",
            "Ming Xu",
            "Shengfang Zhai",
            "Wenjie Qu",
            "Jian Liu",
            "Kui Ren",
            "Ruoxi Jia",
            "Jiaheng Zhang"
        ],
        "title": "Sparse Autoencoder as a Zero-Shot Classifier for Concept Erasing in Text-to-Image Diffusion Models",
        "abstract": "arXiv:2503.09446v1 Announce Type: new  Abstract: Text-to-image (T2I) diffusion models have achieved remarkable progress in generating high-quality images but also raise people's concerns about generating harmful or misleading content. While extensive approaches have been proposed to erase unwanted concepts without requiring retraining from scratch, they inadvertently degrade performance on normal generation tasks. In this work, we propose Interpret then Deactivate (ItD), a novel framework to enable precise concept removal in T2I diffusion models while preserving overall performance. ItD first employs a sparse autoencoder (SAE) to interpret each concept as a combination of multiple features. By permanently deactivating the specific features associated with target concepts, we repurpose SAE as a zero-shot classifier that identifies whether the input prompt includes target concepts, allowing selective concept erasure in diffusion models. Moreover, we demonstrate that ItD can be easily extended to erase multiple concepts without requiring further training. Comprehensive experiments across celebrity identities, artistic styles, and explicit content demonstrate ItD's effectiveness in eliminating targeted concepts without interfering with normal concept generation. Additionally, ItD is also robust against adversarial prompts designed to circumvent content filters. Code is available at: https://github.com/NANSirun/Interpret-then-deactivate.",
        "arxiv_id": "2503.09446",
        "ARXIVID": "2503.09446",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling in text-to-image diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09556": {
        "authors": [
            "Tim B\\\"uchner",
            "Christoph Anders",
            "Orlando Guntinas-Lichius",
            "Joachim Denzler"
        ],
        "title": "Electromyography-Informed Facial Expression Reconstruction for Physiological-Based Synthesis and Analysis",
        "abstract": "arXiv:2503.09556v1 Announce Type: new  Abstract: The relationship between muscle activity and resulting facial expressions is crucial for various fields, including psychology, medicine, and entertainment. The synchronous recording of facial mimicry and muscular activity via surface electromyography (sEMG) provides a unique window into these complex dynamics. Unfortunately, existing methods for facial analysis cannot handle electrode occlusion, rendering them ineffective. Even with occlusion-free reference images of the same person, variations in expression intensity and execution are unmatchable. Our electromyography-informed facial expression reconstruction (EIFER) approach is a novel method to restore faces under sEMG occlusion faithfully in an adversarial manner. We decouple facial geometry and visual appearance (e.g., skin texture, lighting, electrodes) by combining a 3D Morphable Model (3DMM) with neural unpaired image-to-image translation via reference recordings. Then, EIFER learns a bidirectional mapping between 3DMM expression parameters and muscle activity, establishing correspondence between the two domains. We validate the effectiveness of our approach through experiments on a dataset of synchronized sEMG recordings and facial mimicry, demonstrating faithful geometry and appearance reconstruction. Further, we synthesize expressions based on muscle activity and how observed expressions can predict dynamic muscle activity. Consequently, EIFER introduces a new paradigm for facial electromyography, which could be extended to other forms of multi-modal face recordings.",
        "arxiv_id": "2503.09556",
        "ARXIVID": "2503.09556",
        "COMMENT": "Does not match any specific criterion but is relevant to multi-modal learning and facial expression analysis.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09106": {
        "authors": [
            "Chuyu Zhang",
            "Xueyang Yu",
            "Peiyan Gu",
            "Xuming He"
        ],
        "title": "Freeze and Cluster: A Simple Baseline for Rehearsal-Free Continual Category Discovery",
        "abstract": "arXiv:2503.09106v1 Announce Type: new  Abstract: This paper addresses the problem of Rehearsal-Free Continual Category Discovery (RF-CCD), which focuses on continuously identifying novel class by leveraging knowledge from labeled data. Existing methods typically train from scratch, overlooking the potential of base models, and often resort to data storage to prevent forgetting. Moreover, because RF-CCD encompasses both continual learning and novel class discovery, previous approaches have struggled to effectively integrate advanced techniques from these fields, resulting in less convincing comparisons and failing to reveal the unique challenges posed by RF-CCD. To address these challenges, we lead the way in integrating advancements from both domains and conducting extensive experiments and analyses. Our findings demonstrate that this integration can achieve state-of-the-art results, leading to the conclusion that in the presence of pre-trained models, the representation does not improve and may even degrade with the introduction of unlabeled data. To mitigate representation degradation, we propose a straightforward yet highly effective baseline method. This method first utilizes prior knowledge of known categories to estimate the number of novel classes. It then acquires representations using a model specifically trained on the base classes, generates high-quality pseudo-labels through k-means clustering, and trains only the classifier layer. We validate our conclusions and methods by conducting extensive experiments across multiple benchmarks, including the Stanford Cars, CUB, iNat, and Tiny-ImageNet datasets. The results clearly illustrate our findings, demonstrate the effectiveness of our baseline, and pave the way for future advancements in RF-CCD.",
        "arxiv_id": "2503.09106",
        "ARXIVID": "2503.09106",
        "COMMENT": "Does not match any specific criterion but is relevant to continual learning and novel class discovery.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09501": {
        "authors": [
            "Ziyu Wan",
            "Yunxiang Li",
            "Yan Song",
            "Hanjing Wang",
            "Linyi Yang",
            "Mark Schmidt",
            "Jun Wang",
            "Weinan Zhang",
            "Shuyue Hu",
            "Ying Wen"
        ],
        "title": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning",
        "abstract": "arXiv:2503.09501v1 Announce Type: new  Abstract: Recent research on Reasoning of Large Language Models (LLMs) has sought to further enhance their performance by integrating meta-thinking -- enabling models to monitor, evaluate, and control their reasoning processes for more adaptive and effective problem-solving. However, current single-agent work lacks a specialized design for acquiring meta-thinking, resulting in low efficacy. To address this challenge, we introduce Reinforced Meta-thinking Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think about thinking. ReMA decouples the reasoning process into two hierarchical agents: a high-level meta-thinking agent responsible for generating strategic oversight and plans, and a low-level reasoning agent for detailed executions. Through iterative reinforcement learning with aligned objectives, these agents explore and learn collaboration, leading to improved generalization and robustness. Experimental results demonstrate that ReMA outperforms single-agent RL baselines on complex reasoning tasks, including competitive-level mathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation studies further illustrate the evolving dynamics of each distinct agent, providing valuable insights into how the meta-thinking reasoning process enhances the reasoning capabilities of LLMs.",
        "arxiv_id": "2503.09501",
        "ARXIVID": "2503.09501",
        "COMMENT": "Does not match any specific criterion but is relevant to advancements in reasoning for large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09496": {
        "authors": [
            "Junjie Zhou",
            "Jiao Tang",
            "Yingli Zuo",
            "Peng Wan",
            "Daoqiang Zhang",
            "Wei Shao"
        ],
        "title": "Robust Multimodal Survival Prediction with the Latent Differentiation Conditional Variational AutoEncoder",
        "abstract": "arXiv:2503.09496v1 Announce Type: new  Abstract: The integrative analysis of histopathological images and genomic data has received increasing attention for survival prediction of human cancers. However, the existing studies always hold the assumption that full modalities are available. As a matter of fact, the cost for collecting genomic data is high, which sometimes makes genomic data unavailable in testing samples. A common way of tackling such incompleteness is to generate the genomic representations from the pathology images. Nevertheless, such strategy still faces the following two challenges: (1) The gigapixel whole slide images (WSIs) are huge and thus hard for representation. (2) It is difficult to generate the genomic embeddings with diverse function categories in a unified generative framework. To address the above challenges, we propose a Conditional Latent Differentiation Variational AutoEncoder (LD-CVAE) for robust multimodal survival prediction, even with missing genomic data. Specifically, a Variational Information Bottleneck Transformer (VIB-Trans) module is proposed to learn compressed pathological representations from the gigapixel WSIs. To generate different functional genomic features, we develop a novel Latent Differentiation Variational AutoEncoder (LD-VAE) to learn the common and specific posteriors for the genomic embeddings with diverse functions. Finally, we use the product-of-experts technique to integrate the genomic common posterior and image posterior for the joint latent distribution estimation in LD-CVAE. We test the effectiveness of our method on five different cancer datasets, and the experimental results demonstrate its superiority in both complete and missing modality scenarios.",
        "arxiv_id": "2503.09496",
        "ARXIVID": "2503.09496",
        "COMMENT": "Does not match any specific criterion but is relevant to multi-modal learning in general.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09294": {
        "authors": [
            "Peng Hu",
            "Chunming He",
            "Lei Xu",
            "Jingduo Tian",
            "Sina Farsiu",
            "Yulun Zhang",
            "Pei Liu",
            "Xiu Li"
        ],
        "title": "IQPFR: An Image Quality Prior for Blind Face Restoration and Beyond",
        "abstract": "arXiv:2503.09294v1 Announce Type: new  Abstract: Blind Face Restoration (BFR) addresses the challenge of reconstructing degraded low-quality (LQ) facial images into high-quality (HQ) outputs. Conventional approaches predominantly rely on learning feature representations from ground-truth (GT) data; however, inherent imperfections in GT datasets constrain restoration performance to the mean quality level of the training data, rather than attaining maximally attainable visual quality. To overcome this limitation, we propose a novel framework that incorporates an Image Quality Prior (IQP) derived from No-Reference Image Quality Assessment (NR-IQA) models to guide the restoration process toward optimal HQ reconstructions. Our methodology synergizes this IQP with a learned codebook prior through two critical innovations: (1) During codebook learning, we devise a dual-branch codebook architecture that disentangles feature extraction into universal structural components and HQ-specific attributes, ensuring comprehensive representation of both common and high-quality facial characteristics. (2) In the codebook lookup stage, we implement a quality-conditioned Transformer-based framework. NR-IQA-derived quality scores act as dynamic conditioning signals to steer restoration toward the highest feasible quality standard. This score-conditioned paradigm enables plug-and-play enhancement of existing BFR architectures without modifying the original structure. We also formulate a discrete representation-based quality optimization strategy that circumvents over-optimization artifacts prevalent in continuous latent space approaches. Extensive experiments demonstrate that our method outperforms state-of-the-art techniques across multiple benchmarks. Besides, our quality-conditioned framework demonstrates consistent performance improvements when integrated with prior BFR models. The code will be released.",
        "arxiv_id": "2503.09294",
        "ARXIVID": "2503.09294",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and blind face restoration.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09399": {
        "authors": [
            "Tobias Christian Nauen",
            "Brian Moser",
            "Federico Raue",
            "Stanislav Frolov",
            "Andreas Dengel"
        ],
        "title": "ForAug: Recombining Foregrounds and Backgrounds to Improve Vision Transformer Training with Bias Mitigation",
        "abstract": "arXiv:2503.09399v1 Announce Type: new  Abstract: Transformers, particularly Vision Transformers (ViTs), have achieved state-of-the-art performance in large-scale image classification. However, they often require large amounts of data and can exhibit biases that limit their robustness and generalizability. This paper introduces ForAug, a novel data augmentation scheme that addresses these challenges and explicitly includes inductive biases, which commonly are part of the neural network architecture, into the training data. ForAug is constructed by using pretrained foundation models to separate and recombine foreground objects with different backgrounds, enabling fine-grained control over image composition during training. It thus increases the data diversity and effective number of training samples. We demonstrate that training on ForNet, the application of ForAug to ImageNet, significantly improves the accuracy of ViTs and other architectures by up to 4.5 percentage points (p.p.) on ImageNet and 7.3 p.p. on downstream tasks. Importantly, ForAug enables novel ways of analyzing model behavior and quantifying biases. Namely, we introduce metrics for background robustness, foreground focus, center bias, and size bias and show that training on ForNet substantially reduces these biases compared to training on ImageNet. In summary, ForAug provides a valuable tool for analyzing and mitigating biases, enabling the development of more robust and reliable computer vision models. Our code and dataset are publicly available at https://github.com/tobna/ForAug.",
        "arxiv_id": "2503.09399",
        "ARXIVID": "2503.09399",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and bias mitigation in vision transformers.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.08992": {
        "authors": [
            "Xuzhong Hu",
            "Zaipeng Duan",
            "Pei An",
            "Jun zhang",
            "Jie Ma"
        ],
        "title": "Dual-Domain Homogeneous Fusion with Cross-Modal Mamba and Progressive Decoder for 3D Object Detection",
        "abstract": "arXiv:2503.08992v1 Announce Type: new  Abstract: Fusing LiDAR point cloud features and image features in a homogeneous BEV space has been widely adopted for 3D object detection in autonomous driving. However, such methods are limited by the excessive compression of multi-modal features. While some works explore feature fusion in dense voxel spaces, they suffer from high computational costs and inefficiencies in query generation. To address these limitations, we propose a Dual-Domain Homogeneous Fusion network (DDHFusion), which leverages the complementary advantages of both BEV and voxel domains while mitigating their respective drawbacks. Specifically, we first transform image features into BEV and sparse voxel spaces using LSS and our proposed semantic-aware feature sampling module which can significantly reduces computational overhead by filtering unimportant voxels. For feature encoding, we design two networks for BEV and voxel feature fusion, incorporating novel cross-modal voxel and BEV Mamba blocks to resolve feature misalignment and enable efficient yet comprehensive scene perception. The output voxel features are injected into the BEV space to compensate for the loss of 3D details caused by height compression. For feature decoding, a progressive query generation module is implemented in the BEV domain to alleviate false negatives during query selection caused by feature compression and small object sizes. Finally, a progressive decoder can sequentially aggregate not only context-rich BEV features but also geometry-aware voxel features, ensuring more precise confidence prediction and bounding box regression. On the NuScenes dataset, DDHfusion achieves state-of-the-art performance, and further experiments demonstrate its superiority over other homogeneous fusion methods.",
        "arxiv_id": "2503.08992",
        "ARXIVID": "2503.08992",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09122": {
        "authors": [
            "Yuechen Xie",
            "Jie Song",
            "Huiqiong Wang",
            "Mingli Song"
        ],
        "title": "Training Data Provenance Verification: Did Your Model Use Synthetic Data from My Generative Model for Training?",
        "abstract": "arXiv:2503.09122v1 Announce Type: new  Abstract: High-quality open-source text-to-image models have lowered the threshold for obtaining photorealistic images significantly, but also face potential risks of misuse. Specifically, suspects may use synthetic data generated by these generative models to train models for specific tasks without permission, when lacking real data resources especially. Protecting these generative models is crucial for the well-being of their owners. In this work, we propose the first method to this important yet unresolved issue, called Training data Provenance Verification (TrainProVe). The rationale behind TrainProVe is grounded in the principle of generalization error bound, which suggests that, for two models with the same task, if the distance between their training data distributions is smaller, their generalization ability will be closer. We validate the efficacy of TrainProVe across four text-to-image models (Stable Diffusion v1.4, latent consistency model, PixArt-$\\alpha$, and Stable Cascade). The results show that TrainProVe achieves a verification accuracy of over 99\\% in determining the provenance of suspicious model training data, surpassing all previous methods. Code is available at https://github.com/xieyc99/TrainProVe.",
        "arxiv_id": "2503.09122",
        "ARXIVID": "2503.09122",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling and data provenance.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.08883": {
        "authors": [
            "Kunag-Da Wang",
            "Ping-Chun Hsieh",
            "Wen-Chih Peng"
        ],
        "title": "Imitation Learning of Correlated Policies in Stackelberg Games",
        "abstract": "arXiv:2503.08883v1 Announce Type: new  Abstract: Stackelberg games, widely applied in domains like economics and security, involve asymmetric interactions where a leader's strategy drives follower responses. Accurately modeling these dynamics allows domain experts to optimize strategies in interactive scenarios, such as turn-based sports like badminton. In multi-agent systems, agent behaviors are interdependent, and traditional Multi-Agent Imitation Learning (MAIL) methods often fail to capture these complex interactions. Correlated policies, which account for opponents' strategies, are essential for accurately modeling such dynamics. However, even methods designed for learning correlated policies, like CoDAIL, struggle in Stackelberg games due to their asymmetric decision-making, where leaders and followers cannot simultaneously account for each other's actions, often leading to non-correlated policies. Furthermore, existing MAIL methods that match occupancy measures or use adversarial techniques like GAIL or Inverse RL face scalability challenges, particularly in high-dimensional environments, and suffer from unstable training. To address these challenges, we propose a correlated policy occupancy measure specifically designed for Stackelberg games and introduce the Latent Stackelberg Differential Network (LSDN) to match it. LSDN models two-agent interactions as shared latent state trajectories and uses multi-output Geometric Brownian Motion (MO-GBM) to effectively capture joint policies. By leveraging MO-GBM, LSDN disentangles environmental influences from agent-driven transitions in latent space, enabling the simultaneous learning of interdependent policies. This design eliminates the need for adversarial training and simplifies the learning process. Extensive experiments on Iterative Matrix Games and multi-agent particle environments demonstrate that LSDN can better reproduce complex interaction dynamics than existing MAIL methods.",
        "arxiv_id": "2503.08883",
        "ARXIVID": "2503.08883",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to multi-agent systems and imitation learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09491": {
        "authors": [
            "Junjie Zhou",
            "Shouju Wang",
            "Yuxia Tang",
            "Qi Zhu",
            "Daoqiang Zhang",
            "Wei Shao"
        ],
        "title": "DAMM-Diffusion: Learning Divergence-Aware Multi-Modal Diffusion Model for Nanoparticles Distribution Prediction",
        "abstract": "arXiv:2503.09491v1 Announce Type: new  Abstract: The prediction of nanoparticles (NPs) distribution is crucial for the diagnosis and treatment of tumors. Recent studies indicate that the heterogeneity of tumor microenvironment (TME) highly affects the distribution of NPs across tumors. Hence, it has become a research hotspot to generate the NPs distribution by the aid of multi-modal TME components. However, the distribution divergence among multi-modal TME components may cause side effects i.e., the best uni-modal model may outperform the joint generative model. To address the above issues, we propose a \\textbf{D}ivergence-\\textbf{A}ware \\textbf{M}ulti-\\textbf{M}odal \\textbf{Diffusion} model (i.e., \\textbf{DAMM-Diffusion}) to adaptively generate the prediction results from uni-modal and multi-modal branches in a unified network. In detail, the uni-modal branch is composed of the U-Net architecture while the multi-modal branch extends it by introducing two novel fusion modules i.e., Multi-Modal Fusion Module (MMFM) and Uncertainty-Aware Fusion Module (UAFM). Specifically, the MMFM is proposed to fuse features from multiple modalities, while the UAFM module is introduced to learn the uncertainty map for cross-attention computation. Following the individual prediction results from each branch, the Divergence-Aware Multi-Modal Predictor (DAMMP) module is proposed to assess the consistency of multi-modal data with the uncertainty map, which determines whether the final prediction results come from multi-modal or uni-modal predictions. We predict the NPs distribution given the TME components of tumor vessels and cell nuclei, and the experimental results show that DAMM-Diffusion can generate the distribution of NPs with higher accuracy than the comparing methods. Additional results on the multi-modal brain image synthesis task further validate the effectiveness of the proposed method.",
        "arxiv_id": "2503.09491",
        "ARXIVID": "2503.09491",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to multi-modal learning and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09241": {
        "authors": [
            "Pei Yang",
            "Hai Ci",
            "Mike Zheng Shou"
        ],
        "title": "In-Context Defense in Computer Agents: An Empirical Study",
        "abstract": "arXiv:2503.09241v1 Announce Type: new  Abstract: Computer agents powered by vision-language models (VLMs) have significantly advanced human-computer interaction, enabling users to perform complex tasks through natural language instructions. However, these agents are vulnerable to context deception attacks, an emerging threat where adversaries embed misleading content into the agent's operational environment, such as a pop-up window containing deceptive instructions. Existing defenses, such as instructing agents to ignore deceptive elements, have proven largely ineffective. As the first systematic study on protecting computer agents, we introduce textbf{in-context defense}, leveraging in-context learning and chain-of-thought (CoT) reasoning to counter such attacks. Our approach involves augmenting the agent's context with a small set of carefully curated exemplars containing both malicious environments and corresponding defensive responses. These exemplars guide the agent to first perform explicit defensive reasoning before action planning, reducing susceptibility to deceptive attacks. Experiments demonstrate the effectiveness of our method, reducing attack success rates by 91.2% on pop-up window attacks, 74.6% on average on environment injection attacks, while achieving 100% successful defenses against distracting advertisements. Our findings highlight that (1) defensive reasoning must precede action planning for optimal performance, and (2) a minimal number of exemplars (fewer than three) is sufficient to induce an agent's defensive behavior.",
        "arxiv_id": "2503.09241",
        "ARXIVID": "2503.09241",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to vision-language models and embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09132": {
        "authors": [
            "Sota Kawamura",
            "Hirotada Honda",
            "Shugo Nakamura",
            "Takashi Sano"
        ],
        "title": "Investigation of Frame Differences as Motion Cues for Video Object Segmentation",
        "abstract": "arXiv:2503.09132v1 Announce Type: new  Abstract: Automatic Video Object Segmentation (AVOS) refers to the task of autonomously segmenting target objects in video sequences without relying on human-provided annotations in the first frames. In AVOS, the use of motion information is crucial, with optical flow being a commonly employed method for capturing motion cues. However, the computation of optical flow is resource-intensive, making it unsuitable for real-time applications, especially on edge devices with limited computational resources. In this study, we propose using frame differences as an alternative to optical flow for motion cue extraction. We developed an extended U-Net-like AVOS model that takes a frame on which segmentation is performed and a frame difference as inputs, and outputs an estimated segmentation map. Our experimental results demonstrate that the proposed model achieves performance comparable to the model with optical flow as an input, particularly when applied to videos captured by stationary cameras. Our results suggest the usefulness of employing frame differences as motion cues in cases with limited computational resources.",
        "arxiv_id": "2503.09132",
        "ARXIVID": "2503.09132",
        "COMMENT": "Does not match any specific criteria but is related to video object segmentation and motion cues.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.08695": {
        "authors": [
            "Youssef Shoeb",
            "Azarm Nowzad",
            "Hanno Gottschalk"
        ],
        "title": "Out-of-Distribution Segmentation in Autonomous Driving: Problems and State of the Art",
        "abstract": "arXiv:2503.08695v1 Announce Type: new  Abstract: In this paper, we review the state of the art in Out-of-Distribution (OoD) segmentation, with a focus on road obstacle detection in automated driving as a real-world application. We analyse the performance of existing methods on two widely used benchmarks, SegmentMeIfYouCan Obstacle Track and LostAndFound-NoKnown, highlighting their strengths, limitations, and real-world applicability. Additionally, we discuss key challenges and outline potential research directions to advance the field. Our goal is to provide researchers and practitioners with a comprehensive perspective on the current landscape of OoD segmentation and to foster further advancements toward safer and more reliable autonomous driving systems.",
        "arxiv_id": "2503.08695",
        "ARXIVID": "2503.08695",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and autonomous driving.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.09306": {
        "authors": [
            "Kathleen Anderson",
            "Thomas Martinetz"
        ],
        "title": "Revealing Unintentional Information Leakage in Low-Dimensional Facial Portrait Representations",
        "abstract": "arXiv:2503.09306v1 Announce Type: new  Abstract: We evaluate the information that can unintentionally leak into the low dimensional output of a neural network, by reconstructing an input image from a 40- or 32-element feature vector that intends to only describe abstract attributes of a facial portrait. The reconstruction uses blackbox-access to the image encoder which generates the feature vector. Other than previous work, we leverage recent knowledge about image generation and facial similarity, implementing a method that outperforms the current state-of-the-art. Our strategy uses a pretrained StyleGAN and a new loss function that compares the perceptual similarity of portraits by mapping them into the latent space of a FaceNet embedding. Additionally, we present a new technique that fuses the output of an ensemble, to deliberately generate specific aspects of the recreated image.",
        "arxiv_id": "2503.09306",
        "ARXIVID": "2503.09306",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.09408": {
        "authors": [
            "Xiuzhen Guo",
            "Lianyuan Yu",
            "Ji Shi",
            "Na Lei",
            "Hongxiao Wang"
        ],
        "title": "Diff-CL: A Novel Cross Pseudo-Supervision Method for Semi-supervised Medical Image Segmentation",
        "abstract": "arXiv:2503.09408v1 Announce Type: new  Abstract: Semi-supervised learning utilizes insights from unlabeled data to improve model generalization, thereby reducing reliance on large labeled datasets. Most existing studies focus on limited samples and fail to capture the overall data distribution. We contend that combining distributional information with detailed information is crucial for achieving more robust and accurate segmentation results. On the one hand, with its robust generative capabilities, diffusion models (DM) learn data distribution effectively. However, it struggles with fine detail capture, leading to generated images with misleading details. Combining DM with convolutional neural networks (CNNs) enables the former to learn data distribution while the latter corrects fine details. While capturing complete high-frequency details by CNNs requires substantial computational resources and is susceptible to local noise. On the other hand, given that both labeled and unlabeled data come from the same distribution, we believe that regions in unlabeled data similar to overall class semantics to labeled data are likely to belong to the same class, while regions with minimal similarity are less likely to. This work introduces a semi-supervised medical image segmentation framework from the distribution perspective (Diff-CL). Firstly, we propose a cross-pseudo-supervision learning mechanism between diffusion and convolution segmentation networks. Secondly, we design a high-frequency mamba module to capture boundary and detail information globally. Finally, we apply contrastive learning for label propagation from labeled to unlabeled data. Our method achieves state-of-the-art (SOTA) performance across three datasets, including left atrium, brain tumor, and NIH pancreas datasets.",
        "arxiv_id": "2503.09408",
        "ARXIVID": "2503.09408",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of semi-supervised learning and medical image segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.09355": {
        "authors": [
            "Lianyuan Yu",
            "Xiuzhen Guo",
            "Ji Shi",
            "Hongxiao Wang",
            "Hongwei Li"
        ],
        "title": "GIGP: A Global Information Interacting and Geometric Priors Focusing Framework for Semi-supervised Medical Image Segmentation",
        "abstract": "arXiv:2503.09355v1 Announce Type: new  Abstract: Semi-supervised learning enhances medical image segmentation by leveraging unlabeled data, reducing reliance on extensive labeled datasets. On the one hand, the distribution discrepancy between limited labeled data and abundant unlabeled data can hinder model generalization. Most existing methods rely on local similarity matching, which may introduce bias. In contrast, Mamba effectively models global context with linear complexity, learning more comprehensive data representations. On the other hand, medical images usually exhibit consistent anatomical structures defined by geometric features. Most existing methods fail to fully utilize global geometric priors, such as volumes, moments etc. In this work, we introduce a global information interaction and geometric priors focus framework (GIGP). Firstly, we present a Global Information Interaction Mamba module to reduce distribution discrepancy between labeled and unlabeled data. Secondly, we propose a Geometric Moment Attention Mechanism to extract richer global geometric features. Finally, we propose Global Geometric Perturbation Consistency to simulate organ dynamics and geometric variations, enhancing the ability of the model to learn generalized features. The superior performance on the NIH Pancreas and Left Atrium datasets demonstrates the effectiveness of our approach.",
        "arxiv_id": "2503.09355",
        "ARXIVID": "2503.09355",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of semi-supervised learning and medical image segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.08974": {
        "authors": [
            "Yong Li",
            "Yi Ren",
            "Xuesong Niu",
            "Yi Ding",
            "Xiu-Shen Wei",
            "Cuntai Guan"
        ],
        "title": "Beyond Overfitting: Doubly Adaptive Dropout for Generalizable AU Detection",
        "abstract": "arXiv:2503.08974v1 Announce Type: new  Abstract: Facial Action Units (AUs) are essential for conveying psychological states and emotional expressions. While automatic AU detection systems leveraging deep learning have progressed, they often overfit to specific datasets and individual features, limiting their cross-domain applicability. To overcome these limitations, we propose a doubly adaptive dropout approach for cross-domain AU detection, which enhances the robustness of convolutional feature maps and spatial tokens against domain shifts. This approach includes a Channel Drop Unit (CD-Unit) and a Token Drop Unit (TD-Unit), which work together to reduce domain-specific noise at both the channel and token levels. The CD-Unit preserves domain-agnostic local patterns in feature maps, while the TD-Unit helps the model identify AU relationships generalizable across domains. An auxiliary domain classifier, integrated at each layer, guides the selective omission of domain-sensitive features. To prevent excessive feature dropout, a progressive training strategy is used, allowing for selective exclusion of sensitive features at any model layer. Our method consistently outperforms existing techniques in cross-domain AU detection, as demonstrated by extensive experimental evaluations. Visualizations of attention maps also highlight clear and meaningful patterns related to both individual and combined AUs, further validating the approach's effectiveness.",
        "arxiv_id": "2503.08974",
        "ARXIVID": "2503.08974",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and domain generalization.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}