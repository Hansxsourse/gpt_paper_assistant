{
    "2503.23330": {
        "authors": [
            "Hongxiang Jiang",
            "Jihao Yin",
            "Qixiong Wang",
            "Jiaqi Feng",
            "Guo Chen"
        ],
        "title": "EagleVision: Object-level Attribute Multimodal LLM for Remote Sensing",
        "abstract": "arXiv:2503.23330v1 Announce Type: new  Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated impressive results in various visual tasks. However, in remote sensing (RS), high resolution and small proportion of objects pose challenges to existing MLLMs, which struggle with object-centric tasks, particularly in precise localization and fine-grained attribute description for each object. These RS MLLMs have not yet surpassed classical visual perception models, as they only provide coarse image understanding, leading to limited gains in real-world scenarios. To address this gap, we establish EagleVision, an MLLM tailored for remote sensing that excels in object detection and attribute comprehension. Equipped with the Attribute Disentangle module, EagleVision learns disentanglement vision tokens to express distinct attributes. To support object-level visual-language alignment, we construct EVAttrs-95K, the first large-scale object attribute understanding dataset in RS for instruction tuning, along with a novel evaluation benchmark, EVBench. EagleVision achieves state-of-the-art performance on both fine-grained object detection and object attribute understanding tasks, highlighting the mutual promotion between detection and understanding capabilities in MLLMs. The code, model, data, and demo will be available at https://github.com/XiangTodayEatsWhat/EagleVision.",
        "arxiv_id": "2503.23330",
        "ARXIVID": "2503.23330",
        "COMMENT": "Matches criterion 2 as it introduces a new multimodal large language model (MLLM) tailored for remote sensing.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2503.23771": {
        "authors": [
            "Fengxiang Wang",
            "Hongzhen Wang",
            "Mingshuo Chen",
            "Di Wang",
            "Yulin Wang",
            "Zonghao Guo",
            "Qiang Ma",
            "Long Lan",
            "Wenjing Yang",
            "Jing Zhang",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "title": "XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?",
        "abstract": "arXiv:2503.23771v1 Announce Type: new  Abstract: The astonishing breakthrough of multimodal large language models (MLLMs) has necessitated new benchmarks to quantitatively assess their capabilities, reveal their limitations, and indicate future research directions. However, this is challenging in the context of remote sensing (RS), since the imagery features ultra-high resolution that incorporates extremely complex semantic relationships. Existing benchmarks usually adopt notably smaller image sizes than real-world RS scenarios, suffer from limited annotation quality, and consider insufficient dimensions of evaluation. To address these issues, we present XLRS-Bench: a comprehensive benchmark for evaluating the perception and reasoning capabilities of MLLMs in ultra-high-resolution RS scenarios. XLRS-Bench boasts the largest average image size (8500$\\times$8500) observed thus far, with all evaluation samples meticulously annotated manually, assisted by a novel semi-automatic captioner on ultra-high-resolution RS images. On top of the XLRS-Bench, 16 sub-tasks are defined to evaluate MLLMs' 10 kinds of perceptual capabilities and 6 kinds of reasoning capabilities, with a primary emphasis on advanced cognitive processes that facilitate real-world decision-making and the capture of spatiotemporal changes. The results of both general and RS-focused MLLMs on XLRS-Bench indicate that further efforts are needed for real-world RS applications. We have open-sourced XLRS-Bench to support further research in developing more powerful MLLMs for remote sensing.",
        "arxiv_id": "2503.23771",
        "ARXIVID": "2503.23771",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (XLRS-Bench) for evaluating multimodal large language models in remote sensing scenarios.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.22976": {
        "authors": [
            "Jiahui Zhang",
            "Yurui Chen",
            "Yanpeng Zhou",
            "Yueming Xu",
            "Ze Huang",
            "Jilin Mei",
            "Junhui Chen",
            "Yu-Jie Yuan",
            "Xinyue Cai",
            "Guowei Huang",
            "Xingyue Quan",
            "Hang Xu",
            "Li Zhang"
        ],
        "title": "From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D",
        "abstract": "arXiv:2503.22976v1 Announce Type: new  Abstract: Recent advances in LVLMs have improved vision-language understanding, but they still struggle with spatial perception, limiting their ability to reason about complex 3D scenes. Unlike previous approaches that incorporate 3D representations into models to improve spatial understanding, we aim to unlock the potential of VLMs by leveraging spatially relevant image data. To this end, we introduce a novel 2D spatial data generation and annotation pipeline built upon scene data with 3D ground-truth. This pipeline enables the creation of a diverse set of spatial tasks, ranging from basic perception tasks to more complex reasoning tasks. Leveraging this pipeline, we construct SPAR-7M, a large-scale dataset generated from thousands of scenes across multiple public datasets. In addition, we introduce SPAR-Bench, a benchmark designed to offer a more comprehensive evaluation of spatial capabilities compared to existing spatial benchmarks, supporting both single-view and multi-view inputs. Training on both SPAR-7M and large-scale 2D datasets enables our models to achieve state-of-the-art performance on 2D spatial benchmarks. Further fine-tuning on 3D task-specific datasets yields competitive results, underscoring the effectiveness of our dataset in enhancing spatial reasoning.",
        "arxiv_id": "2503.22976",
        "ARXIVID": "2503.22976",
        "COMMENT": "Matches criterion 1 as it focuses on improving spatial understanding in vision-language models through a novel dataset and benchmark.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2503.24379": {
        "authors": [
            "Shengqiong Wu",
            "Weicai Ye",
            "Jiahao Wang",
            "Quande Liu",
            "Xintao Wang",
            "Pengfei Wan",
            "Di Zhang",
            "Kun Gai",
            "Shuicheng Yan",
            "Hao Fei",
            "Tat-Seng Chua"
        ],
        "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation",
        "abstract": "arXiv:2503.24379v1 Announce Type: new  Abstract: To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs--text, images, videos, and specialized cues such as region, motion, and camera poses--into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: https://sqwu.top/Any2Cap/",
        "arxiv_id": "2503.24379",
        "ARXIVID": "2503.24379",
        "COMMENT": "Matches criterion 2 as it introduces a novel framework for controllable video generation using multimodal large language models (MLLMs) to interpret diverse inputs.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.24382": {
        "authors": [
            "Chong Bao",
            "Xiyu Zhang",
            "Zehao Yu",
            "Jiale Shi",
            "Guofeng Zhang",
            "Songyou Peng",
            "Zhaopeng Cui"
        ],
        "title": "Free360: Layered Gaussian Splatting for Unbounded 360-Degree View Synthesis from Extremely Sparse and Unposed Views",
        "abstract": "arXiv:2503.24382v1 Announce Type: new  Abstract: Neural rendering has demonstrated remarkable success in high-quality 3D neural reconstruction and novel view synthesis with dense input views and accurate poses. However, applying it to extremely sparse, unposed views in unbounded 360{\\deg} scenes remains a challenging problem. In this paper, we propose a novel neural rendering framework to accomplish the unposed and extremely sparse-view 3D reconstruction in unbounded 360{\\deg} scenes. To resolve the spatial ambiguity inherent in unbounded scenes with sparse input views, we propose a layered Gaussian-based representation to effectively model the scene with distinct spatial layers. By employing a dense stereo reconstruction model to recover coarse geometry, we introduce a layer-specific bootstrap optimization to refine the noise and fill occluded regions in the reconstruction. Furthermore, we propose an iterative fusion of reconstruction and generation alongside an uncertainty-aware training approach to facilitate mutual conditioning and enhancement between these two processes. Comprehensive experiments show that our approach outperforms existing state-of-the-art methods in terms of rendering quality and surface reconstruction accuracy. Project page: https://zju3dv.github.io/free360/",
        "arxiv_id": "2503.24382",
        "ARXIVID": "2503.24382",
        "COMMENT": "Matches criterion 3 as it introduces a novel neural rendering framework for unposed and sparse-view 3D reconstruction in unbounded 360-degree scenes, which is relevant to embodied AI and spatial understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.22952": {
        "authors": [
            "Yuxuan Wang",
            "Yueqian Wang",
            "Bo Chen",
            "Tong Wu",
            "Dongyan Zhao",
            "Zilong Zheng"
        ],
        "title": "OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts",
        "abstract": "arXiv:2503.22952v1 Announce Type: new  Abstract: The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has propelled the development of Omni language models, designed to process and proactively respond to continuous streams of multi-modal data. Despite their potential, evaluating their real-world interactive capabilities in streaming video contexts remains a formidable challenge. In this work, we introduce OmniMMI, a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six distinct subtasks. Moreover, we propose a novel framework, Multi-modal Multiplexing Modeling (M4), designed to enable an inference-efficient streaming model that can see, listen while generating.",
        "arxiv_id": "2503.22952",
        "ARXIVID": "2503.22952",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (OmniMMI) for evaluating multimodal interaction in streaming video contexts.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2503.23024": {
        "authors": [
            "Zhihao Yuan",
            "Yibo Peng",
            "Jinke Ren",
            "Yinghong Liao",
            "Yatong Han",
            "Chun-Mei Feng",
            "Hengshuang Zhao",
            "Guanbin Li",
            "Shuguang Cui",
            "Zhen Li"
        ],
        "title": "Empowering Large Language Models with 3D Situation Awareness",
        "abstract": "arXiv:2503.23024v1 Announce Type: new  Abstract: Driven by the great success of Large Language Models (LLMs) in the 2D image domain, their applications in 3D scene understanding has emerged as a new trend. A key difference between 3D and 2D is that the situation of an egocentric observer in 3D scenes can change, resulting in different descriptions (e.g., ''left\" or ''right\"). However, current LLM-based methods overlook the egocentric perspective and simply use datasets from a global viewpoint. To address this issue, we propose a novel approach to automatically generate a situation-aware dataset by leveraging the scanning trajectory during data collection and utilizing Vision-Language Models (VLMs) to produce high-quality captions and question-answer pairs. Furthermore, we introduce a situation grounding module to explicitly predict the position and orientation of observer's viewpoint, thereby enabling LLMs to ground situation description in 3D scenes. We evaluate our approach on several benchmarks, demonstrating that our method effectively enhances the 3D situational awareness of LLMs while significantly expanding existing datasets and reducing manual effort.",
        "arxiv_id": "2503.23024",
        "ARXIVID": "2503.23024",
        "COMMENT": "Matches criterion 3 as it introduces a novel dataset and method for 3D situational awareness in embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2503.23297": {
        "authors": [
            "Zhenyang Liu",
            "Yikai Wang",
            "Sixiao Zheng",
            "Tongying Pan",
            "Longfei Liang",
            "Yanwei Fu",
            "Xiangyang Xue"
        ],
        "title": "ReasonGrounder: LVLM-Guided Hierarchical Feature Splatting for Open-Vocabulary 3D Visual Grounding and Reasoning",
        "abstract": "arXiv:2503.23297v1 Announce Type: new  Abstract: Open-vocabulary 3D visual grounding and reasoning aim to localize objects in a scene based on implicit language descriptions, even when they are occluded. This ability is crucial for tasks such as vision-language navigation and autonomous robotics. However, current methods struggle because they rely heavily on fine-tuning with 3D annotations and mask proposals, which limits their ability to handle diverse semantics and common knowledge required for effective reasoning. In this work, we propose ReasonGrounder, an LVLM-guided framework that uses hierarchical 3D feature Gaussian fields for adaptive grouping based on physical scale, enabling open-vocabulary 3D grounding and reasoning. ReasonGrounder interprets implicit instructions using large vision-language models (LVLM) and localizes occluded objects through 3D Gaussian splatting. By incorporating 2D segmentation masks from the SAM and multi-view CLIP embeddings, ReasonGrounder selects Gaussian groups based on object scale, enabling accurate localization through both explicit and implicit language understanding, even in novel, occluded views. We also contribute ReasoningGD, a new dataset containing over 10K scenes and 2 million annotations for evaluating open-vocabulary 3D grounding and amodal perception under occlusion. Experiments show that ReasonGrounder significantly improves 3D grounding accuracy in real-world scenarios.",
        "arxiv_id": "2503.23297",
        "ARXIVID": "2503.23297",
        "COMMENT": "Matches criterion 1 and 3 as it focuses on open-vocabulary 3D visual grounding and reasoning using LVLMs, and introduces a new dataset for evaluation.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2503.22906": {
        "authors": [
            "Heng Yu",
            "Juze Zhang",
            "Changan Chen",
            "Tiange Xiang",
            "Yusu Fang",
            "Juan Carlos Niebles",
            "Ehsan Adeli"
        ],
        "title": "SocialGen: Modeling Multi-Human Social Interaction with Language Models",
        "abstract": "arXiv:2503.22906v1 Announce Type: new  Abstract: Human interactions in everyday life are inherently social, involving engagements with diverse individuals across various contexts. Modeling these social interactions is fundamental to a wide range of real-world applications. In this paper, we introduce SocialGen, the first unified motion-language model capable of modeling interaction behaviors among varying numbers of individuals, to address this crucial yet challenging problem. Unlike prior methods that are limited to two-person interactions, we propose a novel social motion representation that supports tokenizing the motions of an arbitrary number of individuals and aligning them with the language space. This alignment enables the model to leverage rich, pretrained linguistic knowledge to better understand and reason about human social behaviors. To tackle the challenges of data scarcity, we curate a comprehensive multi-human interaction dataset, SocialX, enriched with textual annotations. Leveraging this dataset, we establish the first comprehensive benchmark for multi-human interaction tasks. Our method achieves state-of-the-art performance across motion-language tasks, setting a new standard for multi-human interaction modeling.",
        "arxiv_id": "2503.22906",
        "ARXIVID": "2503.22906",
        "COMMENT": "Matches criterion 2 as it introduces a multi-modal motion-language model for social interaction modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.23963": {
        "authors": [
            "Miao Fan",
            "Shanshan Yu",
            "Shengtong Xu",
            "Kun Jiang",
            "Haoyi Xiong",
            "Xiangzeng Liu"
        ],
        "title": "A Benchmark for Vision-Centric HD Mapping by V2I Systems",
        "abstract": "arXiv:2503.23963v1 Announce Type: new  Abstract: Autonomous driving faces safety challenges due to a lack of global perspective and the semantic information of vectorized high-definition (HD) maps. Information from roadside cameras can greatly expand the map perception range through vehicle-to-infrastructure (V2I) communications. However, there is still no dataset from the real world available for the study on map vectorization onboard under the scenario of vehicle-infrastructure cooperation. To prosper the research on online HD mapping for Vehicle-Infrastructure Cooperative Autonomous Driving (VICAD), we release a real-world dataset, which contains collaborative camera frames from both vehicles and roadside infrastructures, and provides human annotations of HD map elements. We also present an end-to-end neural framework (i.e., V2I-HD) leveraging vision-centric V2I systems to construct vectorized maps. To reduce computation costs and further deploy V2I-HD on autonomous vehicles, we introduce a directionally decoupled self-attention mechanism to V2I-HD. Extensive experiments show that V2I-HD has superior performance in real-time inference speed, as tested by our real-world dataset. Abundant qualitative results also demonstrate stable and robust map construction quality with low cost in complex and various driving scenes. As a benchmark, both source codes and the dataset have been released at OneDrive for the purpose of further study.",
        "arxiv_id": "2503.23963",
        "ARXIVID": "2503.23963",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for vision-centric HD mapping in autonomous driving.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.23064": {
        "authors": [
            "Yufan Ren",
            "Konstantinos Tertikas",
            "Shalini Maiti",
            "Junlin Han",
            "Tong Zhang",
            "Sabine S\\\"usstrunk",
            "Filippos Kokkinos"
        ],
        "title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models",
        "abstract": "arXiv:2503.23064v1 Announce Type: new  Abstract: Large Vision-Language Models (LVLMs) struggle with puzzles, which require precise perception, rule comprehension, and logical reasoning. Assessing and enhancing their performance in this domain is crucial, as it reflects their ability to engage in structured reasoning - an essential skill for real-world problem-solving. However, existing benchmarks primarily evaluate pre-trained models without additional training or fine-tuning, often lack a dedicated focus on reasoning, and fail to establish a systematic evaluation framework. To address these limitations, we introduce VGRP-Bench, a Visual Grid Reasoning Puzzle Benchmark featuring 20 diverse puzzles. VGRP-Bench spans multiple difficulty levels, and includes extensive experiments not only on existing chat LVLMs (e.g., GPT-4o), but also on reasoning LVLMs (e.g., Gemini-Thinking). Our results reveal that even the state-of-the-art LVLMs struggle with these puzzles, highlighting fundamental limitations in their puzzle-solving capabilities. Most importantly, through systematic experiments, we identify and analyze key factors influencing LVLMs' puzzle-solving performance, including the number of clues, grid size, and rule complexity. Furthermore, we explore two Supervised Fine-Tuning (SFT) strategies that can be used in post-training: SFT on solutions (S-SFT) and SFT on synthetic reasoning processes (R-SFT). While both methods significantly improve performance on trained puzzles, they exhibit limited generalization to unseen ones. We will release VGRP-Bench to facilitate further research on LVLMs for complex, real-world problem-solving.",
        "arxiv_id": "2503.23064",
        "ARXIVID": "2503.23064",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (VGRP-Bench) for reasoning in large vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.24182": {
        "authors": [
            "Yingrui Ji",
            "Xi Xiao",
            "Gaofei Chen",
            "Hao Xu",
            "Chenrui Ma",
            "Lijing Zhu",
            "Aokun Liang",
            "Jiansheng Chen"
        ],
        "title": "CIBR: Cross-modal Information Bottleneck Regularization for Robust CLIP Generalization",
        "abstract": "arXiv:2503.24182v1 Announce Type: new  Abstract: Contrastive Language-Image Pretraining (CLIP) has achieved remarkable success in cross-modal tasks such as zero-shot image classification and text-image retrieval by effectively aligning visual and textual representations. However, the theoretical foundations underlying CLIP's strong generalization remain unclear. In this work, we address this gap by proposing the Cross-modal Information Bottleneck (CIB) framework. CIB offers a principled interpretation of CLIP's contrastive learning objective as an implicit Information Bottleneck optimization. Under this view, the model maximizes shared cross-modal information while discarding modality-specific redundancies, thereby preserving essential semantic alignment across modalities. Building on this insight, we introduce a Cross-modal Information Bottleneck Regularization (CIBR) method that explicitly enforces these IB principles during training. CIBR introduces a penalty term to discourage modality-specific redundancy, thereby enhancing semantic alignment between image and text features. We validate CIBR on extensive vision-language benchmarks, including zero-shot classification across seven diverse image datasets and text-image retrieval on MSCOCO and Flickr30K. The results show consistent performance gains over standard CLIP. These findings provide the first theoretical understanding of CLIP's generalization through the IB lens. They also demonstrate practical improvements, offering guidance for future cross-modal representation learning.",
        "arxiv_id": "2503.24182",
        "ARXIVID": "2503.24182",
        "COMMENT": "Matches criterion 2 as it provides a theoretical framework (CIBR) for improving vision-language models like CLIP.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.24267": {
        "authors": [
            "Yixuan Li",
            "Yu Tian",
            "Yipo Huang",
            "Wei Lu",
            "Shiqi Wang",
            "Weisi Lin",
            "Anderson Rocha"
        ],
        "title": "FakeScope: Large Multimodal Expert Model for Transparent AI-Generated Image Forensics",
        "abstract": "arXiv:2503.24267v1 Announce Type: new  Abstract: The rapid and unrestrained advancement of generative artificial intelligence (AI) presents a double-edged sword: while enabling unprecedented creativity, it also facilitates the generation of highly convincing deceptive content, undermining societal trust. As image generation techniques become increasingly sophisticated, detecting synthetic images is no longer just a binary task: it necessitates interpretable, context-aware methodologies that enhance trustworthiness and transparency. However, existing detection models primarily focus on classification, offering limited explanatory insights into image authenticity. In this work, we propose FakeScope, an expert multimodal model (LMM) tailored for AI-generated image forensics, which not only identifies AI-synthetic images with high accuracy but also provides rich, interpretable, and query-driven forensic insights. We first construct FakeChain dataset that contains linguistic authenticity reasoning based on visual trace evidence, developed through a novel human-machine collaborative framework. Building upon it, we further present FakeInstruct, the largest multimodal instruction tuning dataset containing 2 million visual instructions tailored to enhance forensic awareness in LMMs. FakeScope achieves state-of-the-art performance in both closed-ended and open-ended forensic scenarios. It can distinguish synthetic images with high accuracy while offering coherent and insightful explanations, free-form discussions on fine-grained forgery attributes, and actionable enhancement strategies. Notably, despite being trained exclusively on qualitative hard labels, FakeScope demonstrates remarkable zero-shot quantitative capability on detection, enabled by our proposed token-based probability estimation strategy. Furthermore, FakeScope exhibits strong generalization and in-the-wild ability, ensuring its applicability in real-world scenarios.",
        "arxiv_id": "2503.24267",
        "ARXIVID": "2503.24267",
        "COMMENT": "Matches criterion 2 as it introduces a new multimodal large language model (FakeScope) for AI-generated image forensics.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.23121": {
        "authors": [
            "Guohong Huang",
            "Ling-An Zeng",
            "Zexin Zheng",
            "Shengbo Gu",
            "Wei-Shi Zheng"
        ],
        "title": "Efficient Explicit Joint-level Interaction Modeling with Mamba for Text-guided HOI Generation",
        "abstract": "arXiv:2503.23121v1 Announce Type: new  Abstract: We propose a novel approach for generating text-guided human-object interactions (HOIs) that achieves explicit joint-level interaction modeling in a computationally efficient manner. Previous methods represent the entire human body as a single token, making it difficult to capture fine-grained joint-level interactions and resulting in unrealistic HOIs. However, treating each individual joint as a token would yield over twenty times more tokens, increasing computational overhead. To address these challenges, we introduce an Efficient Explicit Joint-level Interaction Model (EJIM). EJIM features a Dual-branch HOI Mamba that separately and efficiently models spatiotemporal HOI information, as well as a Dual-branch Condition Injector for integrating text semantics and object geometry into human and object motions. Furthermore, we design a Dynamic Interaction Block and a progressive masking mechanism to iteratively filter out irrelevant joints, ensuring accurate and nuanced interaction modeling. Extensive quantitative and qualitative evaluations on public datasets demonstrate that EJIM surpasses previous works by a large margin while using only 5\\% of the inference time. Code is available \\href{https://github.com/Huanggh531/EJIM}{here}.",
        "arxiv_id": "2503.23121",
        "ARXIVID": "2503.23121",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for text-guided human-object interaction generation with efficient joint-level modeling, addressing a previously underexplored aspect.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.23282": {
        "authors": [
            "Felix Wimbauer",
            "Weirong Chen",
            "Dominik Muhle",
            "Christian Rupprecht",
            "Daniel Cremers"
        ],
        "title": "AnyCam: Learning to Recover Camera Poses and Intrinsics from Casual Videos",
        "abstract": "arXiv:2503.23282v1 Announce Type: new  Abstract: Estimating camera motion and intrinsics from casual videos is a core challenge in computer vision. Traditional bundle-adjustment based methods, such as SfM and SLAM, struggle to perform reliably on arbitrary data. Although specialized SfM approaches have been developed for handling dynamic scenes, they either require intrinsics or computationally expensive test-time optimization and often fall short in performance. Recently, methods like Dust3r have reformulated the SfM problem in a more data-driven way. While such techniques show promising results, they are still 1) not robust towards dynamic objects and 2) require labeled data for supervised training. As an alternative, we propose AnyCam, a fast transformer model that directly estimates camera poses and intrinsics from a dynamic video sequence in feed-forward fashion. Our intuition is that such a network can learn strong priors over realistic camera poses. To scale up our training, we rely on an uncertainty-based loss formulation and pre-trained depth and flow networks instead of motion or trajectory supervision. This allows us to use diverse, unlabelled video datasets obtained mostly from YouTube. Additionally, we ensure that the predicted trajectory does not accumulate drift over time through a lightweight trajectory refinement step. We test AnyCam on established datasets, where it delivers accurate camera poses and intrinsics both qualitatively and quantitatively. Furthermore, even with trajectory refinement, AnyCam is significantly faster than existing works for SfM in dynamic settings. Finally, by combining camera information, uncertainty, and depth, our model can produce high-quality 4D pointclouds.",
        "arxiv_id": "2503.23282",
        "ARXIVID": "2503.23282",
        "COMMENT": "Matches criterion 1 as it proposes a new method for spatial understanding by estimating camera poses and intrinsics from dynamic videos using a transformer model.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.23463": {
        "authors": [
            "Xingcheng Zhou",
            "Xuyuan Han",
            "Feng Yang",
            "Yunpu Ma",
            "Alois C. Knoll"
        ],
        "title": "OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model",
        "abstract": "arXiv:2503.23463v1 Announce Type: new  Abstract: We present OpenDriveVLA, a Vision-Language Action (VLA) model designed for end-to-end autonomous driving. OpenDriveVLA builds upon open-source pre-trained large Vision-Language Models (VLMs) to generate reliable driving actions, conditioned on 3D environmental perception, ego vehicle states, and driver commands. To bridge the modality gap between driving visual representations and language embeddings, we propose a hierarchical vision-language alignment process, projecting both 2D and 3D structured visual tokens into a unified semantic space. Besides, OpenDriveVLA models the dynamic relationships between the ego vehicle, surrounding agents, and static road elements through an autoregressive agent-env-ego interaction process, ensuring both spatially and behaviorally informed trajectory planning. Extensive experiments on the nuScenes dataset demonstrate that OpenDriveVLA achieves state-of-the-art results across open-loop trajectory planning and driving-related question-answering tasks. Qualitative analyses further illustrate OpenDriveVLA's superior capability to follow high-level driving commands and robustly generate trajectories under challenging scenarios, highlighting its potential for next-generation end-to-end autonomous driving. We will release our code to facilitate further research in this domain.",
        "arxiv_id": "2503.23463",
        "ARXIVID": "2503.23463",
        "COMMENT": "Matches criterion 3 as it introduces a vision-language action model for autonomous driving, focusing on embodied AI and novel methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.23980": {
        "authors": [
            "Yanbo Wang",
            "Yongtao Chen",
            "Chuan Cao",
            "Tianchen Deng",
            "Wentao Zhao",
            "Jingchuan Wang",
            "Weidong Chen"
        ],
        "title": "SALT: A Flexible Semi-Automatic Labeling Tool for General LiDAR Point Clouds with Cross-Scene Adaptability and 4D Consistency",
        "abstract": "arXiv:2503.23980v1 Announce Type: new  Abstract: We propose a flexible Semi-Automatic Labeling Tool (SALT) for general LiDAR point clouds with cross-scene adaptability and 4D consistency. Unlike recent approaches that rely on camera distillation, SALT operates directly on raw LiDAR data, automatically generating pre-segmentation results. To achieve this, we propose a novel zero-shot learning paradigm, termed data alignment, which transforms LiDAR data into pseudo-images by aligning with the training distribution of vision foundation models. Additionally, we design a 4D-consistent prompting strategy and 4D non-maximum suppression module to enhance SAM2, ensuring high-quality, temporally consistent presegmentation. SALT surpasses the latest zero-shot methods by 18.4% PQ on SemanticKITTI and achieves nearly 40-50% of human annotator performance on our newly collected low-resolution LiDAR data and on combined data from three LiDAR types, significantly boosting annotation efficiency. We anticipate that SALT's open-sourcing will catalyze substantial expansion of current LiDAR datasets and lay the groundwork for the future development of LiDAR foundation models. Code is available at https://github.com/Cavendish518/SALT.",
        "arxiv_id": "2503.23980",
        "ARXIVID": "2503.23980",
        "COMMENT": "Matches criterion 3 as it introduces a semi-automatic labeling tool for LiDAR point clouds with cross-scene adaptability and 4D consistency, which is relevant to embodied AI benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.23806": {
        "authors": [
            "Xiaoqing Guo",
            "Wuyang Li",
            "Yixuan Yuan"
        ],
        "title": "Bridge the Gap Between Visual and Linguistic Comprehension for Generalized Zero-shot Semantic Segmentation",
        "abstract": "arXiv:2503.23806v1 Announce Type: new  Abstract: Generalized zero-shot semantic segmentation (GZS3) aims to achieve the human-level capability of segmenting not only seen classes but also novel class regions unseen in the training data through introducing the bridge of semantic representations, e.g., word vector. While effective, the way of utilizing one semantic representation to associate the corresponding class and to enable the knowledge transfer from seen to unseen classes is insufficient as well as incompatible with human cognition. Inspired by the observation that humans often use some `part' and `state' information to comprehend the seen objects and imagine unseen classes, we decouple each class into detailed descriptions, including object parts and states. Based on the decoupling formulation, we propose a Decoupled Vision-Language Matching (DeVLMatch) framework, composed of spatial-part (SPMatch) and channel-state (CSMatch) matching modules, for GZS3. In SPMatch, we comprehend objects with spatial part information from both visual and linguistic perspectives and perform graph matching to bridge the gap. In CSMatch, states of objects from the linguistic perspective are matched to compatible channel information from the visual perspective. By decoupling and matching objects across visual and linguistic comprehension, we can explicitly introspect the relationship between seen and unseen classes in fine-grained object part and state levels, thereby facilitating the knowledge transfer from seen to unseen classes in visual space. The proposed DeVLMatch framework surpasses the previous GZS3 methods on standard benchmarks, including PASCAL VOC, COCO-Stuff, and CATARACTS, demonstrating its effectiveness.",
        "arxiv_id": "2503.23806",
        "ARXIVID": "2503.23806",
        "COMMENT": "Matches criterion 2 as it proposes a novel vision-language matching framework for generalized zero-shot semantic segmentation, focusing on decoupling and matching visual and linguistic comprehension.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.23388": {
        "authors": [
            "Fanding Huang",
            "Jingyan Jiang",
            "Qinting Jiang",
            "Hebei Li",
            "Faisal Nadeem Khan",
            "Zhi Wang"
        ],
        "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP Test-Time Adaptation",
        "abstract": "arXiv:2503.23388v1 Announce Type: new  Abstract: Recent vision-language models (VLMs) face significant challenges in test-time adaptation to novel domains. While cache-based methods show promise by leveraging historical information, they struggle with both caching unreliable feature-label pairs and indiscriminately using single-class information during querying, significantly compromising adaptation accuracy. To address these limitations, we propose COSMIC (Clique-Oriented Semantic Multi-space Integration for CLIP), a robust test-time adaptation framework that enhances adaptability through multi-granular, cross-modal semantic caching and graph-based querying mechanisms. Our framework introduces two key innovations: Dual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual Semantics Graph constructs complementary semantic spaces by incorporating textual features, coarse-grained CLIP features, and fine-grained DINOv2 features to capture rich semantic relationships. Building upon these dual graphs, the Clique Guided Hyper-class component leverages structured class relationships to enhance prediction robustness through correlated class selection. Extensive experiments demonstrate COSMIC's superior performance across multiple benchmarks, achieving significant improvements over state-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on cross-domain generation with CLIP RN-50. Code is available at github.com/hf618/COSMIC.",
        "arxiv_id": "2503.23388",
        "ARXIVID": "2503.23388",
        "COMMENT": "Matches criterion 2 as it proposes a novel framework for test-time adaptation of vision-language models (VLMs) with innovative mechanisms like Dual Semantics Graph and Clique Guided Hyper-class.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.23587": {
        "authors": [
            "Martin Malenick\\'y",
            "Martin C\\'ifka",
            "M\\'ed\\'eric Fourmy",
            "Louis Montaut",
            "Justin Carpentier",
            "Josef Sivic",
            "Vladimir Petrik"
        ],
        "title": "PhysPose: Refining 6D Object Poses with Physical Constraints",
        "abstract": "arXiv:2503.23587v1 Announce Type: new  Abstract: Accurate 6D object pose estimation from images is a key problem in object-centric scene understanding, enabling applications in robotics, augmented reality, and scene reconstruction. Despite recent advances, existing methods often produce physically inconsistent pose estimates, hindering their deployment in real-world scenarios. We introduce PhysPose, a novel approach that integrates physical reasoning into pose estimation through a postprocessing optimization enforcing non-penetration and gravitational constraints. By leveraging scene geometry, PhysPose refines pose estimates to ensure physical plausibility. Our approach achieves state-of-the-art accuracy on the YCB-Video dataset from the BOP benchmark and improves over the state-of-the-art pose estimation methods on the HOPE-Video dataset. Furthermore, we demonstrate its impact in robotics by significantly improving success rates in a challenging pick-and-place task, highlighting the importance of physical consistency in real-world applications.",
        "arxiv_id": "2503.23587",
        "ARXIVID": "2503.23587",
        "COMMENT": "Matches criterion 3 as it focuses on embodied AI with a novel method for refining 6D object poses using physical constraints.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2503.23022": {
        "authors": [
            "Xianglong He",
            "Junyi Chen",
            "Di Huang",
            "Zexiang Liu",
            "Xiaoshui Huang",
            "Wanli Ouyang",
            "Chun Yuan",
            "Yangguang Li"
        ],
        "title": "MeshCraft: Exploring Efficient and Controllable Mesh Generation with Flow-based DiTs",
        "abstract": "arXiv:2503.23022v1 Announce Type: new  Abstract: In the domain of 3D content creation, achieving optimal mesh topology through AI models has long been a pursuit for 3D artists. Previous methods, such as MeshGPT, have explored the generation of ready-to-use 3D objects via mesh auto-regressive techniques. While these methods produce visually impressive results, their reliance on token-by-token predictions in the auto-regressive process leads to several significant limitations. These include extremely slow generation speeds and an uncontrollable number of mesh faces. In this paper, we introduce MeshCraft, a novel framework for efficient and controllable mesh generation, which leverages continuous spatial diffusion to generate discrete triangle faces. Specifically, MeshCraft consists of two core components: 1) a transformer-based VAE that encodes raw meshes into continuous face-level tokens and decodes them back to the original meshes, and 2) a flow-based diffusion transformer conditioned on the number of faces, enabling the generation of high-quality 3D meshes with a predefined number of faces. By utilizing the diffusion model for the simultaneous generation of the entire mesh topology, MeshCraft achieves high-fidelity mesh generation at significantly faster speeds compared to auto-regressive methods. Specifically, MeshCraft can generate an 800-face mesh in just 3.2 seconds (35$\\times$ faster than existing baselines). Extensive experiments demonstrate that MeshCraft outperforms state-of-the-art techniques in both qualitative and quantitative evaluations on ShapeNet dataset and demonstrates superior performance on Objaverse dataset. Moreover, it integrates seamlessly with existing conditional guidance strategies, showcasing its potential to relieve artists from the time-consuming manual work involved in mesh creation.",
        "arxiv_id": "2503.23022",
        "ARXIVID": "2503.23022",
        "COMMENT": "Matches criterion 1 as it introduces a novel framework for efficient and controllable 3D mesh generation, which relates to spatial understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.24219": {
        "authors": [
            "Karim Radouane",
            "Hanane Azzag",
            "Mustapha lebbah"
        ],
        "title": "MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote Sensing",
        "abstract": "arXiv:2503.24219v1 Announce Type: new  Abstract: We propose a unified framework that integrates object detection (OD) and visual grounding (VG) for remote sensing (RS) imagery. To support conventional OD and establish an intuitive prior for VG task, we fine-tune an open-set object detector using referring expression data, framing it as a partially supervised OD task. In the first stage, we construct a graph representation of each image, comprising object queries, class embeddings, and proposal locations. Then, our task-aware architecture processes this graph to perform the VG task. The model consists of: (i) a multi-branch network that integrates spatial, visual, and categorical features to generate task-aware proposals, and (ii) an object reasoning network that assigns probabilities across proposals, followed by a soft selection mechanism for final referring object localization. Our model demonstrates superior performance on the OPT-RSVG and DIOR-RSVG datasets, achieving significant improvements over state-of-the-art methods while retaining classical OD capabilities. The code will be available in our repository: \\url{https://github.com/rd20karim/MB-ORES}.",
        "arxiv_id": "2503.24219",
        "ARXIVID": "2503.24219",
        "COMMENT": "Matches criterion 3 as it proposes a new method for visual grounding in remote sensing with a novel multi-branch reasoning framework.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2503.23905": {
        "authors": [
            "Qihan Huang",
            "Long Chan",
            "Jinlong Liu",
            "Wanggui He",
            "Hao Jiang",
            "Mingli Song",
            "Jingyuan Chen",
            "Chang Yao",
            "Jie Song"
        ],
        "title": "Boosting MLLM Reasoning with Text-Debiased Hint-GRPO",
        "abstract": "arXiv:2503.23905v1 Announce Type: new  Abstract: MLLM reasoning has drawn widespread research for its excellent problem-solving capability. Current reasoning methods fall into two types: PRM, which supervises the intermediate reasoning steps, and ORM, which supervises the final results. Recently, DeepSeek-R1 has challenged the traditional view that PRM outperforms ORM, which demonstrates strong generalization performance using an ORM method (i.e., GRPO). However, current MLLM's GRPO algorithms still struggle to handle challenging and complex multimodal reasoning tasks (e.g., mathematical reasoning). In this work, we reveal two problems that impede the performance of GRPO on the MLLM: Low data utilization and Text-bias. Low data utilization refers to that GRPO cannot acquire positive rewards to update the MLLM on difficult samples, and text-bias is a phenomenon that the MLLM bypasses image condition and solely relies on text condition for generation after GRPO training. To tackle these problems, this work proposes Hint-GRPO that improves data utilization by adaptively providing hints for samples of varying difficulty, and text-bias calibration that mitigates text-bias by calibrating the token prediction logits with image condition in test-time. Experiment results on three base MLLMs across eleven datasets demonstrate that our proposed methods advance the reasoning capability of original MLLM by a large margin, exhibiting superior performance to existing MLLM reasoning methods. Our code is available at https://github.com/hqhQAQ/Hint-GRPO.",
        "arxiv_id": "2503.23905",
        "ARXIVID": "2503.23905",
        "COMMENT": "Matches criterion 2 as it proposes new methods to improve reasoning in MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2503.24008": {
        "authors": [
            "Qi Wu",
            "Quanlong Zheng",
            "Yanhao Zhang",
            "Junlin Xie",
            "Jinguo Luo",
            "Kuo Wang",
            "Peng Liu",
            "Qingsong Xie",
            "Ru Zhen",
            "Haonan Lu",
            "Zhenyu Yang"
        ],
        "title": "H2VU-Benchmark: A Comprehensive Benchmark for Hierarchical Holistic Video Understanding",
        "abstract": "arXiv:2503.24008v1 Announce Type: new  Abstract: With the rapid development of multimodal models, the demand for assessing video understanding capabilities has been steadily increasing. However, existing benchmarks for evaluating video understanding exhibit significant limitations in coverage, task diversity, and scene adaptability. These shortcomings hinder the accurate assessment of models' comprehensive video understanding capabilities. To tackle this challenge, we propose a hierarchical and holistic video understanding (H2VU) benchmark designed to evaluate both general video and online streaming video comprehension. This benchmark contributes three key features:   Extended video duration: Spanning videos from brief 3-second clips to comprehensive 1.5-hour recordings, thereby bridging the temporal gaps found in current benchmarks. Comprehensive assessment tasks: Beyond traditional perceptual and reasoning tasks, we have introduced modules for countercommonsense comprehension and trajectory state tracking. These additions test the models' deep understanding capabilities beyond mere prior knowledge. Enriched video data: To keep pace with the rapid evolution of current AI agents, we have expanded first-person streaming video datasets. This expansion allows for the exploration of multimodal models' performance in understanding streaming videos from a first-person perspective. Extensive results from H2VU reveal that existing multimodal large language models (MLLMs) possess substantial potential for improvement in our newly proposed evaluation tasks. We expect that H2VU will facilitate advancements in video understanding research by offering a comprehensive and in-depth analysis of MLLMs.",
        "arxiv_id": "2503.24008",
        "ARXIVID": "2503.24008",
        "COMMENT": "Matches criterion 3 as it proposes a new benchmark (H2VU) for hierarchical holistic video understanding, focusing on novel evaluation tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.23715": {
        "authors": [
            "Kun Liu",
            "Qi Liu",
            "Xinchen Liu",
            "Jie Li",
            "Yongdong Zhang",
            "Jiebo Luo",
            "Xiaodong He",
            "Wu Liu"
        ],
        "title": "HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video Generation",
        "abstract": "arXiv:2503.23715v1 Announce Type: new  Abstract: Text-to-video (T2V) generation has made tremendous progress in generating complicated scenes based on texts. However, human-object interaction (HOI) often cannot be precisely generated by current T2V models due to the lack of large-scale videos with accurate captions for HOI. To address this issue, we introduce HOIGen-1M, the first largescale dataset for HOI Generation, consisting of over one million high-quality videos collected from diverse sources. In particular, to guarantee the high quality of videos, we first design an efficient framework to automatically curate HOI videos using the powerful multimodal large language models (MLLMs), and then the videos are further cleaned by human annotators. Moreover, to obtain accurate textual captions for HOI videos, we design a novel video description method based on a Mixture-of-Multimodal-Experts (MoME) strategy that not only generates expressive captions but also eliminates the hallucination by individual MLLM. Furthermore, due to the lack of an evaluation framework for generated HOI videos, we propose two new metrics to assess the quality of generated videos in a coarse-to-fine manner. Extensive experiments reveal that current T2V models struggle to generate high-quality HOI videos and confirm that our HOIGen-1M dataset is instrumental for improving HOI video generation. Project webpage is available at https://liuqi-creat.github.io/HOIGen.github.io.",
        "arxiv_id": "2503.23715",
        "ARXIVID": "2503.23715",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset (HOIGen-1M) for human-object interaction video generation, addressing a novel angle in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.23786": {
        "authors": [
            "Haoran Shen",
            "Peixian Zhuang",
            "Jiahao Kou",
            "Yuxin Zeng",
            "Haoying Xu",
            "Jiangyun Li"
        ],
        "title": "MGD-SAM2: Multi-view Guided Detail-enhanced Segment Anything Model 2 for High-Resolution Class-agnostic Segmentation",
        "abstract": "arXiv:2503.23786v1 Announce Type: new  Abstract: Segment Anything Models (SAMs), as vision foundation models, have demonstrated remarkable performance across various image analysis tasks. Despite their strong generalization capabilities, SAMs encounter challenges in fine-grained detail segmentation for high-resolution class-independent segmentation (HRCS), due to the limitations in the direct processing of high-resolution inputs and low-resolution mask predictions, and the reliance on accurate manual prompts. To address these limitations, we propose MGD-SAM2 which integrates SAM2 with multi-view feature interaction between a global image and local patches to achieve precise segmentation. MGD-SAM2 incorporates the pre-trained SAM2 with four novel modules: the Multi-view Perception Adapter (MPAdapter), the Multi-view Complementary Enhancement Module (MCEM), the Hierarchical Multi-view Interaction Module (HMIM), and the Detail Refinement Module (DRM). Specifically, we first introduce MPAdapter to adapt the SAM2 encoder for enhanced extraction of local details and global semantics in HRCS images. Then, MCEM and HMIM are proposed to further exploit local texture and global context by aggregating multi-view features within and across multi-scales. Finally, DRM is designed to generate gradually restored high-resolution mask predictions, compensating for the loss of fine-grained details resulting from directly upsampling the low-resolution prediction maps. Experimental results demonstrate the superior performance and strong generalization of our model on multiple high-resolution and normal-resolution datasets. Code will be available at https://github.com/sevenshr/MGD-SAM2.",
        "arxiv_id": "2503.23786",
        "ARXIVID": "2503.23786",
        "COMMENT": "Matches criterion 4 as it proposes a vision foundation model (SAM2) for high-resolution segmentation tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.22963": {
        "authors": [
            "Peiyu Chen",
            "Fuling Lin",
            "Weipeng Guan",
            "Peng Lu"
        ],
        "title": "SuperEIO: Self-Supervised Event Feature Learning for Event Inertial Odometry",
        "abstract": "arXiv:2503.22963v1 Announce Type: new  Abstract: Event cameras asynchronously output low-latency event streams, promising for state estimation in high-speed motion and challenging lighting conditions. As opposed to frame-based cameras, the motion-dependent nature of event cameras presents persistent challenges in achieving robust event feature detection and matching. In recent years, learning-based approaches have demonstrated superior robustness over traditional handcrafted methods in feature detection and matching, particularly under aggressive motion and HDR scenarios. In this paper, we propose SuperEIO, a novel framework that leverages the learning-based event-only detection and IMU measurements to achieve event-inertial odometry. Our event-only feature detection employs a convolutional neural network under continuous event streams. Moreover, our system adopts the graph neural network to achieve event descriptor matching for loop closure. The proposed system utilizes TensorRT to accelerate the inference speed of deep networks, which ensures low-latency processing and robust real-time operation on resource-limited platforms. Besides, we evaluate our method extensively on multiple public datasets, demonstrating its superior accuracy and robustness compared to other state-of-the-art event-based methods. We have also open-sourced our pipeline to facilitate research in the field: https://github.com/arclab-hku/SuperEIO.",
        "arxiv_id": "2503.22963",
        "ARXIVID": "2503.22963",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework for event-inertial odometry, which is relevant to embodied AI and spatial intelligence.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.23529": {
        "authors": [
            "Shuhei Tarashima",
            "Xinqi Shu",
            "Norio Tagawa"
        ],
        "title": "ViLAaD: Enhancing \"Attracting and Dispersing'' Source-Free Domain Adaptation with Vision-and-Language Model",
        "abstract": "arXiv:2503.23529v1 Announce Type: new  Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to a target dataset from a different domain without access to the source data. Conventional SFDA methods are limited by the information encoded in the pre-trained source model and the unlabeled target data. Recently, approaches leveraging auxiliary resources have emerged, yet remain in their early stages, offering ample opportunities for research. In this work, we propose a novel method that incorporates auxiliary information by extending an existing SFDA framework using Vision-and-Language (ViL) models. Specifically, we build upon Attracting and Dispersing (AaD), a widely adopted SFDA technique, and generalize its core principle to naturally integrate ViL models as a powerful initialization for target adaptation. Our approach, called ViL-enhanced AaD (ViLAaD), preserves the simplicity and flexibility of the AaD framework, while leveraging ViL models to significantly boost adaptation performance. We validate our method through experiments using various ViL models, demonstrating that ViLAaD consistently outperforms both AaD and zero-shot classification by ViL models, especially when both the source model and ViL model provide strong initializations. Moreover, the flexibility of ViLAaD allows it to be seamlessly incorporated into an alternating optimization framework with ViL prompt tuning and extended with additional objectives for target model adaptation. Extensive experiments on four SFDA benchmarks show that this enhanced version, ViLAaD++, achieves state-of-the-art performance across multiple SFDA scenarios, including Closed-set SFDA, Partial-set SFDA, and Open-set SFDA.",
        "arxiv_id": "2503.23529",
        "ARXIVID": "2503.23529",
        "COMMENT": "Matches criterion 2 as it leverages vision-and-language models for source-free domain adaptation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.23452": {
        "authors": [
            "Yuhang Yang",
            "Ke Fan",
            "Shangkun Sun",
            "Hongxiang Li",
            "Ailing Zeng",
            "FeiLin Han",
            "Wei Zhai",
            "Wei Liu",
            "Yang Cao",
            "Zheng-Jun Zha"
        ],
        "title": "VideoGen-Eval: Agent-based System for Video Generation Evaluation",
        "abstract": "arXiv:2503.23452v1 Announce Type: new  Abstract: The rapid advancement of video generation has rendered existing evaluation systems inadequate for assessing state-of-the-art models, primarily due to simple prompts that cannot showcase the model's capabilities, fixed evaluation operators struggling with Out-of-Distribution (OOD) cases, and misalignment between computed metrics and human preferences. To bridge the gap, we propose VideoGen-Eval, an agent evaluation system that integrates LLM-based content structuring, MLLM-based content judgment, and patch tools designed for temporal-dense dimensions, to achieve a dynamic, flexible, and expandable video generation evaluation. Additionally, we introduce a video generation benchmark to evaluate existing cutting-edge models and verify the effectiveness of our evaluation system. It comprises 700 structured, content-rich prompts (both T2V and I2V) and over 12,000 videos generated by 20+ models, among them, 8 cutting-edge models are selected as quantitative evaluation for the agent and human. Extensive experiments validate that our proposed agent-based evaluation system demonstrates strong alignment with human preferences and reliably completes the evaluation, as well as the diversity and richness of the benchmark.",
        "arxiv_id": "2503.23452",
        "ARXIVID": "2503.23452",
        "COMMENT": "Matches criterion 4 as it proposes a benchmark and evaluation system for video generation, leveraging multi-modal models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.23402": {
        "authors": [
            "Junsu Kim",
            "Yunhoe Ku",
            "Dongyoon Han",
            "Seungryul Baek"
        ],
        "title": "Diffusion Meets Few-shot Class Incremental Learning",
        "abstract": "arXiv:2503.23402v1 Announce Type: new  Abstract: Few-shot class-incremental learning (FSCIL) is challenging due to extremely limited training data; while aiming to reduce catastrophic forgetting and learn new information. We propose Diffusion-FSCIL, a novel approach that employs a text-to-image diffusion model as a frozen backbone. Our conjecture is that FSCIL can be tackled using a large generative model's capabilities benefiting from 1) generation ability via large-scale pre-training; 2) multi-scale representation; 3) representational flexibility through the text encoder. To maximize the representation capability, we propose to extract multiple complementary diffusion features to play roles as latent replay with slight support from feature distillation for preventing generative biases. Our framework realizes efficiency through 1) using a frozen backbone; 2) minimal trainable components; 3) batch processing of multiple feature extractions. Extensive experiments on CUB-200, miniImageNet, and CIFAR-100 show that Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on previously learned classes and adapting effectively to new ones.",
        "arxiv_id": "2503.23402",
        "ARXIVID": "2503.23402",
        "COMMENT": "Matches criterion 4 as it explores the use of diffusion models for few-shot class-incremental learning, which is a novel application of generative modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.23993": {
        "authors": [
            "Ming Yuan",
            "Sichao Wang",
            "Chuang Zhang",
            "Lei He",
            "Qing Xu",
            "Jianqiang Wang"
        ],
        "title": "DenseFormer: Learning Dense Depth Map from Sparse Depth and Image via Conditional Diffusion Model",
        "abstract": "arXiv:2503.23993v1 Announce Type: new  Abstract: The depth completion task is a critical problem in autonomous driving, involving the generation of dense depth maps from sparse depth maps and RGB images. Most existing methods employ a spatial propagation network to iteratively refine the depth map after obtaining an initial dense depth. In this paper, we propose DenseFormer, a novel method that integrates the diffusion model into the depth completion task. By incorporating the denoising mechanism of the diffusion model, DenseFormer generates the dense depth map by progressively refining an initial random depth distribution through multiple iterations. We propose a feature extraction module that leverages a feature pyramid structure, along with multi-layer deformable attention, to effectively extract and integrate features from sparse depth maps and RGB images, which serve as the guiding condition for the diffusion process. Additionally, this paper presents a depth refinement module that applies multi-step iterative refinement across various ranges to the dense depth results generated by the diffusion process. The module utilizes image features enriched with multi-scale information and sparse depth input to further enhance the accuracy of the predicted depth map. Extensive experiments on the KITTI outdoor scene dataset demonstrate that DenseFormer outperforms classical depth completion methods.",
        "arxiv_id": "2503.23993",
        "ARXIVID": "2503.23993",
        "COMMENT": "Matches criterion 1 as it proposes a novel method (DenseFormer) for spatial understanding in depth completion tasks using a diffusion model.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.00938": {
        "authors": [
            "Kristen M. Edwards",
            "Farnaz Tehranchi",
            "Scarlett R. Miller",
            "Faez Ahmed"
        ],
        "title": "AI Judges in Design: Statistical Perspectives on Achieving Human Expert Equivalence With Vision-Language Models",
        "abstract": "arXiv:2504.00938v1 Announce Type: new  Abstract: The subjective evaluation of early stage engineering designs, such as conceptual sketches, traditionally relies on human experts. However, expert evaluations are time-consuming, expensive, and sometimes inconsistent. Recent advances in vision-language models (VLMs) offer the potential to automate design assessments, but it is crucial to ensure that these AI ``judges'' perform on par with human experts. However, no existing framework assesses expert equivalence. This paper introduces a rigorous statistical framework to determine whether an AI judge's ratings match those of human experts. We apply this framework in a case study evaluating four VLM-based judges on key design metrics (uniqueness, creativity, usefulness, and drawing quality). These AI judges employ various in-context learning (ICL) techniques, including uni- vs. multimodal prompts and inference-time reasoning. The same statistical framework is used to assess three trained novices for expert-equivalence. Results show that the top-performing AI judge, using text- and image-based ICL with reasoning, achieves expert-level agreement for uniqueness and drawing quality and outperforms or matches trained novices across all metrics. In 6/6 runs for both uniqueness and creativity, and 5/6 runs for both drawing quality and usefulness, its agreement with experts meets or exceeds that of the majority of trained novices. These findings suggest that reasoning-supported VLM models can achieve human-expert equivalence in design evaluation. This has implications for scaling design evaluation in education and practice, and provides a general statistical framework for validating AI judges in other domains requiring subjective content evaluation.",
        "arxiv_id": "2504.00938",
        "ARXIVID": "2504.00938",
        "COMMENT": "Matches criterion 2 as it evaluates vision-language models for subjective evaluation tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.22796": {
        "authors": [
            "Hanling Zhang",
            "Rundong Su",
            "Zhihang Yuan",
            "Pengtao Chen",
            "Mingzhu Shen Yibo Fan",
            "Shengen Yan",
            "Guohao Dai",
            "Yu Wang"
        ],
        "title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality Diffusion Transformers",
        "abstract": "arXiv:2503.22796v1 Announce Type: new  Abstract: Text-to-image generation models, especially Multimodal Diffusion Transformers (MMDiT), have shown remarkable progress in generating high-quality images. However, these models often face significant computational bottlenecks, particularly in attention mechanisms, which hinder their scalability and efficiency. In this paper, we introduce DiTFastAttnV2, a post-training compression method designed to accelerate attention in MMDiT. Through an in-depth analysis of MMDiT's attention patterns, we identify key differences from prior DiT-based methods and propose head-wise arrow attention and caching mechanisms to dynamically adjust attention heads, effectively bridging this gap. We also design an Efficient Fused Kernel for further acceleration. By leveraging local metric methods and optimization techniques, our approach significantly reduces the search time for optimal compression schemes to just minutes while maintaining generation quality. Furthermore, with the customized kernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x end-to-end speedup on 2K image generation without compromising visual fidelity.",
        "arxiv_id": "2503.22796",
        "ARXIVID": "2503.22796",
        "COMMENT": "Matches criterion 2 as it introduces a post-training compression method for multimodal diffusion transformers, improving efficiency in text-to-image generation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.24026": {
        "authors": [
            "Boyuan Wang",
            "Xiaofeng Wang",
            "Chaojun Ni",
            "Guosheng Zhao",
            "Zhiqin Yang",
            "Zheng Zhu",
            "Muyang Zhang",
            "Yukun Zhou",
            "Xinze Chen",
            "Guan Huang",
            "Lihong Liu",
            "Xingang Wang"
        ],
        "title": "HumanDreamer: Generating Controllable Human-Motion Videos via Decoupled Generation",
        "abstract": "arXiv:2503.24026v2 Announce Type: new  Abstract: Human-motion video generation has been a challenging task, primarily due to the difficulty inherent in learning human body movements. While some approaches have attempted to drive human-centric video generation explicitly through pose control, these methods typically rely on poses derived from existing videos, thereby lacking flexibility. To address this, we propose HumanDreamer, a decoupled human video generation framework that first generates diverse poses from text prompts and then leverages these poses to generate human-motion videos. Specifically, we propose MotionVid, the largest dataset for human-motion pose generation. Based on the dataset, we present MotionDiT, which is trained to generate structured human-motion poses from text prompts. Besides, a novel LAMA loss is introduced, which together contribute to a significant improvement in FID by 62.4%, along with respective enhancements in R-precision for top1, top2, and top3 by 41.8%, 26.3%, and 18.3%, thereby advancing both the Text-to-Pose control accuracy and FID metrics. Our experiments across various Pose-to-Video baselines demonstrate that the poses generated by our method can produce diverse and high-quality human-motion videos. Furthermore, our model can facilitate other downstream tasks, such as pose sequence prediction and 2D-3D motion lifting.",
        "arxiv_id": "2503.24026",
        "ARXIVID": "2503.24026",
        "COMMENT": "Matches criterion 4 as it proposes a framework for generating human-motion videos using a decoupled approach, leveraging a large dataset and novel loss function.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.23796": {
        "authors": [
            "Bosung Kim",
            "Kyuhwan Lee",
            "Isu Jeong",
            "Jungmin Cheon",
            "Yeojin Lee",
            "Seulki Lee"
        ],
        "title": "On-device Sora: Enabling Training-Free Diffusion-based Text-to-Video Generation for Mobile Devices",
        "abstract": "arXiv:2503.23796v2 Announce Type: new  Abstract: We present On-device Sora, the first model training-free solution for diffusion-based on-device text-to-video generation that operates efficiently on smartphone-grade devices. To address the challenges of diffusion-based text-to-video generation on computation- and memory-limited mobile devices, the proposed On-device Sora applies three novel techniques to pre-trained video generative models. First, Linear Proportional Leap (LPL) reduces the excessive denoising steps required in video diffusion through an efficient leap-based approach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive token-processing computation in attention layers by merging consecutive tokens along the temporal dimension. Third, Concurrent Inference with Dynamic Loading (CI-DL) dynamically partitions large models into smaller blocks and loads them into memory for concurrent model inference, effectively addressing the challenges of limited device memory. We implement On-device Sora on the iPhone 15 Pro, and the experimental evaluations show that it is capable of generating high-quality videos on the device, comparable to those produced by high-end GPUs. These results show that On-device Sora enables efficient and high-quality video generation on resource-constrained mobile devices. We envision the proposed On-device Sora as a significant first step toward democratizing state-of-the-art generative technologies, enabling video generation on commodity mobile and embedded devices without resource-intensive re-training for model optimization (compression). The code implementation is available at a GitHub repository(https://github.com/eai-lab/On-device-Sora).",
        "arxiv_id": "2503.23796",
        "ARXIVID": "2503.23796",
        "COMMENT": "Matches criterion 4 as it focuses on applying generative models for text-to-video generation on mobile devices, which is a novel application of vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.23368": {
        "authors": [
            "Xindi Yang",
            "Baolu Li",
            "Yiming Zhang",
            "Zhenfei Yin",
            "Lei Bai",
            "Liqian Ma",
            "Zhiyong Wang",
            "Jianfei Cai",
            "Tien-Tsin Wong",
            "Huchuan Lu",
            "Xu Jia"
        ],
        "title": "Towards Physically Plausible Video Generation via VLM Planning",
        "abstract": "arXiv:2503.23368v1 Announce Type: new  Abstract: Video diffusion models (VDMs) have advanced significantly in recent years, enabling the generation of highly realistic videos and drawing the attention of the community in their potential as world simulators. However, despite their capabilities, VDMs often fail to produce physically plausible videos due to an inherent lack of understanding of physics, resulting in incorrect dynamics and event sequences. To address this limitation, we propose a novel two-stage image-to-video generation framework that explicitly incorporates physics. In the first stage, we employ a Vision Language Model (VLM) as a coarse-grained motion planner, integrating chain-of-thought and physics-aware reasoning to predict a rough motion trajectories/changes that approximate real-world physical dynamics while ensuring the inter-frame consistency. In the second stage, we use the predicted motion trajectories/changes to guide the video generation of a VDM. As the predicted motion trajectories/changes are rough, noise is added during inference to provide freedom to the VDM in generating motion with more fine details. Extensive experimental results demonstrate that our framework can produce physically plausible motion, and comparative evaluations highlight the notable superiority of our approach over existing methods. More video results are available on our Project Page: https://madaoer.github.io/projects/physically_plausible_video_generation.",
        "arxiv_id": "2503.23368",
        "ARXIVID": "2503.23368",
        "COMMENT": "Matches criterion 4 as it integrates vision-language models (VLMs) for physically plausible video generation, which is related to vision foundation models and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2503.24270": {
        "authors": [
            "Yuelei Li",
            "Hyunjin Kim",
            "Fangneng Zhan",
            "Ri-Zhao Qiu",
            "Mazeyu Ji",
            "Xiaojun Shan",
            "Xueyan Zou",
            "Paul Liang",
            "Hanspeter Pfister",
            "Xiaolong Wang"
        ],
        "title": "Visual Acoustic Fields",
        "abstract": "arXiv:2503.24270v2 Announce Type: new  Abstract: Objects produce different sounds when hit, and humans can intuitively infer how an object might sound based on its appearance and material properties. Inspired by this intuition, we propose Visual Acoustic Fields, a framework that bridges hitting sounds and visual signals within a 3D space using 3D Gaussian Splatting (3DGS). Our approach features two key modules: sound generation and sound localization. The sound generation module leverages a conditional diffusion model, which takes multiscale features rendered from a feature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, the sound localization module enables querying the 3D scene, represented by the feature-augmented 3DGS, to localize hitting positions based on the sound sources. To support this framework, we introduce a novel pipeline for collecting scene-level visual-sound sample pairs, achieving alignment between captured images, impact locations, and corresponding sounds. To the best of our knowledge, this is the first dataset to connect visual and acoustic signals in a 3D context. Extensive experiments on our dataset demonstrate the effectiveness of Visual Acoustic Fields in generating plausible impact sounds and accurately localizing impact sources. Our project page is at https://yuelei0428.github.io/projects/Visual-Acoustic-Fields/.",
        "arxiv_id": "2503.24270",
        "ARXIVID": "2503.24270",
        "COMMENT": "Matches criterion 4 as it connects visual and acoustic signals in a 3D context using 3D Gaussian Splatting, which is a novel application of vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.23044": {
        "authors": [
            "Yuanyuan Gao",
            "Hao Li",
            "Jiaqi Chen",
            "Zhengyu Zou",
            "Zhihang Zhong",
            "Dingwen Zhang",
            "Xiao Sun",
            "Junwei Han"
        ],
        "title": "CityGS-X: A Scalable Architecture for Efficient and Geometrically Accurate Large-Scale Scene Reconstruction",
        "abstract": "arXiv:2503.23044v1 Announce Type: new  Abstract: Despite its significant achievements in large-scale scene reconstruction, 3D Gaussian Splatting still faces substantial challenges, including slow processing, high computational costs, and limited geometric accuracy. These core issues arise from its inherently unstructured design and the absence of efficient parallelization. To overcome these challenges simultaneously, we introduce CityGS-X, a scalable architecture built on a novel parallelized hybrid hierarchical 3D representation (PH^2-3D). As an early attempt, CityGS-X abandons the cumbersome merge-and-partition process and instead adopts a newly-designed batch-level multi-task rendering process. This architecture enables efficient multi-GPU rendering through dynamic Level-of-Detail voxel allocations, significantly improving scalability and performance. Through extensive experiments, CityGS-X consistently outperforms existing methods in terms of faster training times, larger rendering capacities, and more accurate geometric details in large-scale scenes. Notably, CityGS-X can train and render a scene with 5,000+ images in just 5 hours using only 4 * 4090 GPUs, a task that would make other alternative methods encounter Out-Of-Memory (OOM) issues and fail completely. This implies that CityGS-X is far beyond the capacity of other existing methods.",
        "arxiv_id": "2503.23044",
        "ARXIVID": "2503.23044",
        "COMMENT": "Matches criterion 3 as it introduces a scalable architecture for large-scale scene reconstruction with novel methods for efficient multi-GPU rendering.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.24374": {
        "authors": [
            "Maxim V. Shugaev",
            "Vincent Chen",
            "Maxim Karrenbach",
            "Kyle Ashley",
            "Bridget Kennedy",
            "Naresh P. Cuntoor"
        ],
        "title": "ERUPT: Efficient Rendering with Unposed Patch Transformer",
        "abstract": "arXiv:2503.24374v1 Announce Type: new  Abstract: This work addresses the problem of novel view synthesis in diverse scenes from small collections of RGB images. We propose ERUPT (Efficient Rendering with Unposed Patch Transformer) a state-of-the-art scene reconstruction model capable of efficient scene rendering using unposed imagery. We introduce patch-based querying, in contrast to existing pixel-based queries, to reduce the compute required to render a target view. This makes our model highly efficient both during training and at inference, capable of rendering at 600 fps on commercial hardware. Notably, our model is designed to use a learned latent camera pose which allows for training using unposed targets in datasets with sparse or inaccurate ground truth camera pose. We show that our approach can generalize on large real-world data and introduce a new benchmark dataset (MSVS-1M) for latent view synthesis using street-view imagery collected from Mapillary. In contrast to NeRF and Gaussian Splatting, which require dense imagery and precise metadata, ERUPT can render novel views of arbitrary scenes with as few as five unposed input images. ERUPT achieves better rendered image quality than current state-of-the-art methods for unposed image synthesis tasks, reduces labeled data requirements by ~95\\% and decreases computational requirements by an order of magnitude, providing efficient novel view synthesis for diverse real-world scenes.",
        "arxiv_id": "2503.24374",
        "ARXIVID": "2503.24374",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset (MSVS-1M) for latent view synthesis and focuses on novel methods for scene reconstruction.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.23888": {
        "authors": [
            "Xin Zhang",
            "Siting Huang",
            "Xiangyang Luo",
            "Yifan Xie",
            "Weijiang Yu",
            "Heng Chang",
            "Fei Ma",
            "Fei Yu"
        ],
        "title": "MuseFace: Text-driven Face Editing via Diffusion-based Mask Generation Approach",
        "abstract": "arXiv:2503.23888v1 Announce Type: new  Abstract: Face editing modifies the appearance of face, which plays a key role in customization and enhancement of personal images. Although much work have achieved remarkable success in text-driven face editing, they still face significant challenges as none of them simultaneously fulfill the characteristics of diversity, controllability and flexibility. To address this challenge, we propose MuseFace, a text-driven face editing framework, which relies solely on text prompt to enable face editing. Specifically, MuseFace integrates a Text-to-Mask diffusion model and a semantic-aware face editing model, capable of directly generating fine-grained semantic masks from text and performing face editing. The Text-to-Mask diffusion model provides \\textit{diversity} and \\textit{flexibility} to the framework, while the semantic-aware face editing model ensures \\textit{controllability} of the framework. Our framework can create fine-grained semantic masks, making precise face editing possible, and significantly enhancing the controllability and flexibility of face editing models. Extensive experiments demonstrate that MuseFace achieves superior high-fidelity performance.",
        "arxiv_id": "2503.23888",
        "ARXIVID": "2503.23888",
        "COMMENT": "Matches criterion 4 as it introduces a text-driven face editing framework using diffusion models, which is related to vision foundation models and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.23897": {
        "authors": [
            "Yufei Wang",
            "Lanqing Guo",
            "Zhihao Li",
            "Jiaxing Huang",
            "Pichao Wang",
            "Bihan Wen",
            "Jian Wang"
        ],
        "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model",
        "abstract": "arXiv:2503.23897v1 Announce Type: new  Abstract: Text-guided image editing is an essential task that enables users to modify images through natural language descriptions. Recent advances in diffusion models and rectified flows have significantly improved editing quality, primarily relying on inversion techniques to extract structured noise from input images. However, inaccuracies in inversion can propagate errors, leading to unintended modifications and compromising fidelity. Moreover, even with perfect inversion, the entanglement between textual prompts and image features often results in global changes when only local edits are intended. To address these challenges, we propose a novel text-guided image editing framework based on VAR (Visual AutoRegressive modeling), which eliminates the need for explicit inversion while ensuring precise and controlled modifications. Our method introduces a caching mechanism that stores token indices and probability distributions from the original image, capturing the relationship between the source prompt and the image. Using this cache, we design an adaptive fine-grained masking strategy that dynamically identifies and constrains modifications to relevant regions, preventing unintended changes. A token reassembling approach further refines the editing process, enhancing diversity, fidelity, and control. Our framework operates in a training-free manner and achieves high-fidelity editing with faster inference speeds, processing a 1K resolution image in as fast as 1.2 seconds. Extensive experiments demonstrate that our method achieves performance comparable to, or even surpassing, existing diffusion- and rectified flow-based approaches in both quantitative metrics and visual quality. The code will be released.",
        "arxiv_id": "2503.23897",
        "ARXIVID": "2503.23897",
        "COMMENT": "Matches criterion 2 as it proposes a novel framework for text-guided image editing using a visual autoregressive model, relevant to vision-language models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.23573": {
        "authors": [
            "Maximilian Augustin",
            "Yannic Neuhaus",
            "Matthias Hein"
        ],
        "title": "DASH: Detection and Assessment of Systematic Hallucinations of VLMs",
        "abstract": "arXiv:2503.23573v1 Announce Type: new  Abstract: Vision-language models (VLMs) are prone to object hallucinations, where they erroneously indicate the presenceof certain objects in an image. Existing benchmarks quantify hallucinations using relatively small, labeled datasets. However, this approach is i) insufficient to assess hallucinations that arise in open-world settings, where VLMs are widely used, and ii) inadequate for detecting systematic errors in VLMs. We propose DASH (Detection and Assessment of Systematic Hallucinations), an automatic, large-scale pipeline designed to identify systematic hallucinations of VLMs on real-world images in an open-world setting. A key component is DASH-OPT for image-based retrieval, where we optimize over the ''natural image manifold'' to generate images that mislead the VLM. The output of DASH consists of clusters of real and semantically similar images for which the VLM hallucinates an object. We apply DASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in total, find more than 19k clusters with 950k images. We study the transfer of the identified systematic hallucinations to other VLMs and show that fine-tuning PaliGemma with the model-specific images obtained with DASH mitigates object hallucinations. Code and data are available at https://YanNeu.github.io/DASH.",
        "arxiv_id": "2503.23573",
        "ARXIVID": "2503.23573",
        "COMMENT": "Matches criterion 2 as it focuses on systematic hallucinations in vision-language models (VLMs), providing insights into their limitations.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.22869": {
        "authors": [
            "Alexey Gavryushin",
            "Florian Redhardt",
            "Gaia Di Lorenzo",
            "Luc Van Gool",
            "Marc Pollefeys",
            "Kaichun Mo",
            "Xi Wang"
        ],
        "title": "SIGHT: Single-Image Conditioned Generation of Hand Trajectories for Hand-Object Interaction",
        "abstract": "arXiv:2503.22869v1 Announce Type: new  Abstract: We introduce a novel task of generating realistic and diverse 3D hand trajectories given a single image of an object, which could be involved in a hand-object interaction scene or pictured by itself. When humans grasp an object, appropriate trajectories naturally form in our minds to use it for specific tasks. Hand-object interaction trajectory priors can greatly benefit applications in robotics, embodied AI, augmented reality and related fields. However, synthesizing realistic and appropriate hand trajectories given a single object or hand-object interaction image is a highly ambiguous task, requiring to correctly identify the object of interest and possibly even the correct interaction among many possible alternatives. To tackle this challenging problem, we propose the SIGHT-Fusion system, consisting of a curated pipeline for extracting visual features of hand-object interaction details from egocentric videos involving object manipulation, and a diffusion-based conditional motion generation model processing the extracted features. We train our method given video data with corresponding hand trajectory annotations, without supervision in the form of action labels. For the evaluation, we establish benchmarks utilizing the first-person FPHAB and HOI4D datasets, testing our method against various baselines and using multiple metrics. We also introduce task simulators for executing the generated hand trajectories and reporting task success rates as an additional metric. Experiments show that our method generates more appropriate and realistic hand trajectories than baselines and presents promising generalization capability on unseen objects. The accuracy of the generated hand trajectories is confirmed in a physics simulation setting, showcasing the authenticity of the created sequences and their applicability in downstream uses.",
        "arxiv_id": "2503.22869",
        "ARXIVID": "2503.22869",
        "COMMENT": "Matches criterion 3 as it introduces a novel method (SIGHT-Fusion) for generating hand trajectories in embodied AI tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.23377": {
        "authors": [
            "Kai Liu",
            "Wei Li",
            "Lai Chen",
            "Shengqiong Wu",
            "Yanhao Zheng",
            "Jiayi Ji",
            "Fan Zhou",
            "Rongxin Jiang",
            "Jiebo Luo",
            "Hao Fei",
            "Tat-Seng Chua"
        ],
        "title": "JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchronization",
        "abstract": "arXiv:2503.23377v1 Announce Type: new  Abstract: This paper introduces JavisDiT, a novel Joint Audio-Video Diffusion Transformer designed for synchronized audio-video generation (JAVG). Built upon the powerful Diffusion Transformer (DiT) architecture, JavisDiT is able to generate high-quality audio and video content simultaneously from open-ended user prompts. To ensure optimal synchronization, we introduce a fine-grained spatio-temporal alignment mechanism through a Hierarchical Spatial-Temporal Synchronized Prior (HiST-Sypo) Estimator. This module extracts both global and fine-grained spatio-temporal priors, guiding the synchronization between the visual and auditory components. Furthermore, we propose a new benchmark, JavisBench, consisting of 10,140 high-quality text-captioned sounding videos spanning diverse scenes and complex real-world scenarios. Further, we specifically devise a robust metric for evaluating the synchronization between generated audio-video pairs in real-world complex content. Experimental results demonstrate that JavisDiT significantly outperforms existing methods by ensuring both high-quality generation and precise synchronization, setting a new standard for JAVG tasks. Our code, model, and dataset will be made publicly available at https://javisdit.github.io/.",
        "arxiv_id": "2503.23377",
        "ARXIVID": "2503.23377",
        "COMMENT": "Matches criterion 2 as it introduces a novel multi-modal large language model (JavisDiT) for synchronized audio-video generation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.23012": {
        "authors": [
            "Xinlei Shao",
            "Hongruixuan Chen",
            "Fan Zhao",
            "Kirsty Magson",
            "Jundong Chen",
            "Peiran Li",
            "Jiaqi Wang",
            "Jun Sasaki"
        ],
        "title": "Multi-label classification for multi-temporal, multi-spatial coral reef condition monitoring using vision foundation model with adapter learning",
        "abstract": "arXiv:2503.23012v1 Announce Type: new  Abstract: Coral reef ecosystems provide essential ecosystem services, but face significant threats from climate change and human activities. Although advances in deep learning have enabled automatic classification of coral reef conditions, conventional deep models struggle to achieve high performance when processing complex underwater ecological images. Vision foundation models, known for their high accuracy and cross-domain generalizability, offer promising solutions. However, fine-tuning these models requires substantial computational resources and results in high carbon emissions. To address these challenges, adapter learning methods such as Low-Rank Adaptation (LoRA) have emerged as a solution. This study introduces an approach integrating the DINOv2 vision foundation model with the LoRA fine-tuning method. The approach leverages multi-temporal field images collected through underwater surveys at 15 dive sites at Koh Tao, Thailand, with all images labeled according to universal standards used in citizen science-based conservation programs. The experimental results demonstrate that the DINOv2-LoRA model achieved superior accuracy, with a match ratio of 64.77%, compared to 60.34% achieved by the best conventional model. Furthermore, incorporating LoRA reduced the trainable parameters from 1,100M to 5.91M. Transfer learning experiments conducted under different temporal and spatial settings highlight the exceptional generalizability of DINOv2-LoRA across different seasons and sites. This study is the first to explore the efficient adaptation of foundation models for multi-label classification of coral reef conditions under multi-temporal and multi-spatial settings. The proposed method advances the classification of coral reef conditions and provides a tool for monitoring, conserving, and managing coral reef ecosystems.",
        "arxiv_id": "2503.23012",
        "ARXIVID": "2503.23012",
        "COMMENT": "Matches criterion 4 as it applies vision foundation models with adapter learning to coral reef monitoring.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.24166": {
        "authors": [
            "Fabian Fuchs",
            "Mario Ruben Fernandez",
            "Norman Ettrich",
            "Janis Keuper"
        ],
        "title": "Foundation Models For Seismic Data Processing: An Extensive Review",
        "abstract": "arXiv:2503.24166v1 Announce Type: new  Abstract: Seismic processing plays a crucial role in transforming raw data into high-quality subsurface images, pivotal for various geoscience applications. Despite its importance, traditional seismic processing techniques face challenges such as noisy and damaged data and the reliance on manual, time-consuming workflows. The emergence of deep learning approaches has introduced effective and user-friendly alternatives, yet many of these deep learning approaches rely on synthetic datasets and specialized neural networks. Recently, foundation models have gained traction in the seismic domain, due to their success in natural imaging. This paper investigates the application of foundation models in seismic processing on the tasks: demultiple, interpolation, and denoising. It evaluates the impact of different model characteristics, such as pre-training technique and neural network architecture, on performance and efficiency. Rather than proposing a single seismic foundation model, this paper critically examines various natural image foundation models and suggest some promising candidates for future exploration.",
        "arxiv_id": "2503.24166",
        "ARXIVID": "2503.24166",
        "COMMENT": "Matches criterion 4 as it discusses foundation models and their applications in seismic data processing.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.24306": {
        "authors": [
            "Adam Schmidt",
            "Mert Asim Karaoglu",
            "Soham Sinha",
            "Mingang Jang",
            "Ho-Gun Ha",
            "Kyungmin Jung",
            "Kyeongmo Gu",
            "Ihsan Ullah",
            "Hyunki Lee",
            "Jon\\'a\\v{s} \\v{S}er\\'ych",
            "Michal Neoral",
            "Ji\\v{r}\\'i Matas",
            "Rulin Zhou",
            "Wenlong He",
            "An Wang",
            "Hongliang Ren",
            "Bruno Silva",
            "Sandro Queir\\'os",
            "Est\\^ev\\~ao Lima",
            "Jo\\~ao L. Vila\\c{c}a",
            "Shunsuke Kikuchi",
            "Atsushi Kouno",
            "Hiroki Matsuzaki",
            "Tongtong Li",
            "Yulu Chen",
            "Ling Li",
            "Xiang Ma",
            "Xiaojian Li",
            "Mona Sheikh Zeinoddin",
            "Xu Wang",
            "Zafer Tandogdu",
            "Greg Shaw",
            "Evangelos Mazomenos",
            "Danail Stoyanov",
            "Yuxin Chen",
            "Zijian Wu",
            "Alexander Ladikos",
            "Simon DiMaio",
            "Septimiu E. Salcudean",
            "Omid Mohareri"
        ],
        "title": "Point Tracking in Surgery--The 2024 Surgical Tattoos in Infrared (STIR) Challenge",
        "abstract": "arXiv:2503.24306v1 Announce Type: new  Abstract: Understanding tissue motion in surgery is crucial to enable applications in downstream tasks such as segmentation, 3D reconstruction, virtual tissue landmarking, autonomous probe-based scanning, and subtask autonomy. Labeled data are essential to enabling algorithms in these downstream tasks since they allow us to quantify and train algorithms. This paper introduces a point tracking challenge to address this, wherein participants can submit their algorithms for quantification. The submitted algorithms are evaluated using a dataset named surgical tattoos in infrared (STIR), with the challenge aptly named the STIR Challenge 2024. The STIR Challenge 2024 comprises two quantitative components: accuracy and efficiency. The accuracy component tests the accuracy of algorithms on in vivo and ex vivo sequences. The efficiency component tests the latency of algorithm inference. The challenge was conducted as a part of MICCAI EndoVis 2024. In this challenge, we had 8 total teams, with 4 teams submitting before and 4 submitting after challenge day. This paper details the STIR Challenge 2024, which serves to move the field towards more accurate and efficient algorithms for spatial understanding in surgery. In this paper we summarize the design, submissions, and results from the challenge. The challenge dataset is available here: https://zenodo.org/records/14803158 , and the code for baseline models and metric calculation is available here: https://github.com/athaddius/STIRMetrics",
        "arxiv_id": "2503.24306",
        "ARXIVID": "2503.24306",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (STIR Challenge 2024) for spatial understanding in surgery, focusing on tissue motion tracking.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.23951": {
        "authors": [
            "Fangda Chen",
            "Shanshan Zhao",
            "Chuanfu Xu",
            "Long Lan"
        ],
        "title": "JointTuner: Appearance-Motion Adaptive Joint Training for Customized Video Generation",
        "abstract": "arXiv:2503.23951v1 Announce Type: new  Abstract: Recent text-to-video advancements have enabled coherent video synthesis from prompts and expanded to fine-grained control over appearance and motion. However, existing methods either suffer from concept interference due to feature domain mismatch caused by naive decoupled optimizations or exhibit appearance contamination induced by spatial feature leakage resulting from the entanglement of motion and appearance in reference video reconstructions. In this paper, we propose JointTuner, a novel adaptive joint training framework, to alleviate these issues. Specifically, we develop Adaptive LoRA, which incorporates a context-aware gating mechanism, and integrate the gated LoRA components into the spatial and temporal Transformers within the diffusion model. These components enable simultaneous optimization of appearance and motion, eliminating concept interference. In addition, we introduce the Appearance-independent Temporal Loss, which decouples motion patterns from intrinsic appearance in reference video reconstructions through an appearance-agnostic noise prediction task. The key innovation lies in adding frame-wise offset noise to the ground-truth Gaussian noise, perturbing its distribution, thereby disrupting spatial attributes associated with frames while preserving temporal coherence. Furthermore, we construct a benchmark comprising 90 appearance-motion customized combinations and 10 multi-type automatic metrics across four dimensions, facilitating a more comprehensive evaluation for this customization task. Extensive experiments demonstrate the superior performance of our method compared to current advanced approaches.",
        "arxiv_id": "2503.23951",
        "ARXIVID": "2503.23951",
        "COMMENT": "Matches criterion 2 as it proposes a novel framework for text-to-video generation with fine-grained control over appearance and motion.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.23508": {
        "authors": [
            "Yuming Chen",
            "Jiangyan Feng",
            "Haodong Zhang",
            "Lijun Gong",
            "Feng Zhu",
            "Rui Zhao",
            "Qibin Hou",
            "Ming-Ming Cheng",
            "Yibing Song"
        ],
        "title": "Re-Aligning Language to Visual Objects with an Agentic Workflow",
        "abstract": "arXiv:2503.23508v1 Announce Type: new  Abstract: Language-based object detection (LOD) aims to align visual objects with language expressions. A large amount of paired data is utilized to improve LOD model generalizations. During the training process, recent studies leverage vision-language models (VLMs) to automatically generate human-like expressions for visual objects, facilitating training data scaling up. In this process, we observe that VLM hallucinations bring inaccurate object descriptions (e.g., object name, color, and shape) to deteriorate VL alignment quality. To reduce VLM hallucinations, we propose an agentic workflow controlled by an LLM to re-align language to visual objects via adaptively adjusting image and text prompts. We name this workflow Real-LOD, which includes planning, tool use, and reflection steps. Given an image with detected objects and VLM raw language expressions, Real-LOD reasons its state automatically and arranges action based on our neural symbolic designs (i.e., planning). The action will adaptively adjust the image and text prompts and send them to VLMs for object re-description (i.e., tool use). Then, we use another LLM to analyze these refined expressions for feedback (i.e., reflection). These steps are conducted in a cyclic form to gradually improve language descriptions for re-aligning to visual objects. We construct a dataset that contains a tiny amount of 0.18M images with re-aligned language expression and train a prevalent LOD model to surpass existing LOD methods by around 50% on the standard benchmarks. Our Real-LOD workflow, with automatic VL refinement, reveals a potential to preserve data quality along with scaling up data quantity, which further improves LOD performance from a data-alignment perspective.",
        "arxiv_id": "2503.23508",
        "ARXIVID": "2503.23508",
        "COMMENT": "Matches criterion 2 as it discusses vision-language models (VLMs) and introduces a novel agentic workflow for improving language-object alignment.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.23456": {
        "authors": [
            "Maofu Liu",
            "Xin Jiang",
            "Xiaokang Zhang"
        ],
        "title": "CADFormer: Fine-Grained Cross-modal Alignment and Decoding Transformer for Referring Remote Sensing Image Segmentation",
        "abstract": "arXiv:2503.23456v1 Announce Type: new  Abstract: Referring Remote Sensing Image Segmentation (RRSIS) is a challenging task, aiming to segment specific target objects in remote sensing (RS) images based on a given language expression. Existing RRSIS methods typically employ coarse-grained unidirectional alignment approaches to obtain multimodal features, and they often overlook the critical role of language features as contextual information during the decoding process. Consequently, these methods exhibit weak object-level correspondence between visual and language features, leading to incomplete or erroneous predicted masks, especially when handling complex expressions and intricate RS image scenes. To address these challenges, we propose a fine-grained cross-modal alignment and decoding Transformer, CADFormer, for RRSIS. Specifically, we design a semantic mutual guidance alignment module (SMGAM) to achieve both vision-to-language and language-to-vision alignment, enabling comprehensive integration of visual and textual features for fine-grained cross-modal alignment. Furthermore, a textual-enhanced cross-modal decoder (TCMD) is introduced to incorporate language features during decoding, using refined textual information as context to enhance the relationship between cross-modal features. To thoroughly evaluate the performance of CADFormer, especially for inconspicuous targets in complex scenes, we constructed a new RRSIS dataset, called RRSIS-HR, which includes larger high-resolution RS image patches and semantically richer language expressions. Extensive experiments on the RRSIS-HR dataset and the popular RRSIS-D dataset demonstrate the effectiveness and superiority of CADFormer. Datasets and source codes will be available at https://github.com/zxk688.",
        "arxiv_id": "2503.23456",
        "ARXIVID": "2503.23456",
        "COMMENT": "Matches criterion 4 as it proposes a cross-modal alignment and decoding transformer for remote sensing image segmentation.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2503.23162": {
        "authors": [
            "Zhenyu Tang",
            "Chaoran Feng",
            "Xinhua Cheng",
            "Wangbo Yu",
            "Junwu Zhang",
            "Yuan Liu",
            "Xiaoxiao Long",
            "Wenping Wang",
            "Li Yuan"
        ],
        "title": "NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations",
        "abstract": "arXiv:2503.23162v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) demonstrates superior quality and rendering speed, but with millions of 3D Gaussians and significant storage and transmission costs. Recent 3DGS compression methods mainly concentrate on compressing Scaffold-GS, achieving impressive performance but with an additional voxel structure and a complex encoding and quantization strategy. In this paper, we aim to develop a simple yet effective method called NeuralGS that explores in another way to compress the original 3DGS into a compact representation without the voxel structure and complex quantization strategies. Our observation is that neural fields like NeRF can represent complex 3D scenes with Multi-Layer Perceptron (MLP) neural networks using only a few megabytes. Thus, NeuralGS effectively adopts the neural field representation to encode the attributes of 3D Gaussians with MLPs, only requiring a small storage size even for a large-scale scene. To achieve this, we adopt a clustering strategy and fit the Gaussians with different tiny MLPs for each cluster, based on importance scores of Gaussians as fitting weights. We experiment on multiple datasets, achieving a 45-times average model size reduction without harming the visual quality. The compression performance of our method on original 3DGS is comparable to the dedicated Scaffold-GS-based compression methods, which demonstrate the huge potential of directly compressing original 3DGS with neural fields.",
        "arxiv_id": "2503.23162",
        "ARXIVID": "2503.23162",
        "COMMENT": "Matches criterion 4 as it focuses on compact 3D representations and neural fields, which are related to vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2503.23125": {
        "authors": [
            "Shuhao Fu",
            "Andrew Jun Lee",
            "Anna Wang",
            "Ida Momennejad",
            "Trevor Bihl",
            "Hongjing Lu",
            "Taylor W. Webb"
        ],
        "title": "Evaluating Compositional Scene Understanding in Multimodal Generative Models",
        "abstract": "arXiv:2503.23125v1 Announce Type: new  Abstract: The visual world is fundamentally compositional. Visual scenes are defined by the composition of objects and their relations. Hence, it is essential for computer vision systems to reflect and exploit this compositionality to achieve robust and generalizable scene understanding. While major strides have been made toward the development of general-purpose, multimodal generative models, including both text-to-image models and multimodal vision-language models, it remains unclear whether these systems are capable of accurately generating and interpreting scenes involving the composition of multiple objects and relations. In this work, we present an evaluation of the compositional visual processing capabilities in the current generation of text-to-image (DALL-E 3) and multimodal vision-language models (GPT-4V, GPT-4o, Claude Sonnet 3.5, QWEN2-VL-72B, and InternVL2.5-38B), and compare the performance of these systems to human participants. The results suggest that these systems display some ability to solve compositional and relational tasks, showing notable improvements over the previous generation of multimodal models, but with performance nevertheless well below the level of human participants, particularly for more complex scenes involving many ($>5$) objects and multiple relations. These results highlight the need for further progress toward compositional understanding of visual scenes.",
        "arxiv_id": "2503.23125",
        "ARXIVID": "2503.23125",
        "COMMENT": "Matches criterion 2 as it evaluates compositional scene understanding in multimodal generative models, providing insights into their capabilities and limitations.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2503.23030": {
        "authors": [
            "Huajie Jiang",
            "Zhengxian Li",
            "Xiaohan Yu",
            "Yongli Hu",
            "Baocai Yin",
            "Jian Yang",
            "Yuankai Qi"
        ],
        "title": "Visual and Semantic Prompt Collaboration for Generalized Zero-Shot Learning",
        "abstract": "arXiv:2503.23030v1 Announce Type: new  Abstract: Generalized zero-shot learning aims to recognize both seen and unseen classes with the help of semantic information that is shared among different classes. It inevitably requires consistent visual-semantic alignment. Existing approaches fine-tune the visual backbone by seen-class data to obtain semantic-related visual features, which may cause overfitting on seen classes with a limited number of training images. This paper proposes a novel visual and semantic prompt collaboration framework, which utilizes prompt tuning techniques for efficient feature adaptation. Specifically, we design a visual prompt to integrate the visual information for discriminative feature learning and a semantic prompt to integrate the semantic formation for visualsemantic alignment. To achieve effective prompt information integration, we further design a weak prompt fusion mechanism for the shallow layers and a strong prompt fusion mechanism for the deep layers in the network. Through the collaboration of visual and semantic prompts, we can obtain discriminative semantic-related features for generalized zero-shot image recognition. Extensive experiments demonstrate that our framework consistently achieves favorable performance in both conventional zero-shot learning and generalized zero-shot learning benchmarks compared to other state-of-the-art methods.",
        "arxiv_id": "2503.23030",
        "ARXIVID": "2503.23030",
        "COMMENT": "Matches criterion 2 as it introduces a novel visual and semantic prompt collaboration framework, which is relevant to advancements in multi-modal learning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.23538": {
        "authors": [
            "Jiyeon Han",
            "Dahee Kwon",
            "Gayoung Lee",
            "Junho Kim",
            "Jaesik Choi"
        ],
        "title": "Enhancing Creative Generation on Stable Diffusion-based Models",
        "abstract": "arXiv:2503.23538v1 Announce Type: new  Abstract: Recent text-to-image generative models, particularly Stable Diffusion and its distilled variants, have achieved impressive fidelity and strong text-image alignment. However, their creative capability remains constrained, as including `creative' in prompts seldom yields the desired results. This paper introduces C3 (Creative Concept Catalyst), a training-free approach designed to enhance creativity in Stable Diffusion-based models. C3 selectively amplifies features during the denoising process to foster more creative outputs. We offer practical guidelines for choosing amplification factors based on two main aspects of creativity. C3 is the first study to enhance creativity in diffusion models without extensive computational costs. We demonstrate its effectiveness across various Stable Diffusion-based models.",
        "arxiv_id": "2503.23538",
        "ARXIVID": "2503.23538",
        "COMMENT": "Matches criterion 4 as it focuses on enhancing creativity in Stable Diffusion-based models, which are related to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.23623": {
        "authors": [
            "Zahra TehraniNasab",
            "Amar Kumar",
            "Tal Arbel"
        ],
        "title": "Language-Guided Trajectory Traversal in Disentangled Stable Diffusion Latent Space for Factorized Medical Image Generation",
        "abstract": "arXiv:2503.23623v1 Announce Type: new  Abstract: Text-to-image diffusion models have demonstrated a remarkable ability to generate photorealistic images from natural language prompts. These high-resolution, language-guided synthesized images are essential for the explainability of disease or exploring causal relationships. However, their potential for disentangling and controlling latent factors of variation in specialized domains like medical imaging remains under-explored. In this work, we present the first investigation of the power of pre-trained vision-language foundation models, once fine-tuned on medical image datasets, to perform latent disentanglement for factorized medical image generation and interpolation. Through extensive experiments on chest X-ray and skin datasets, we illustrate that fine-tuned, language-guided Stable Diffusion inherently learns to factorize key attributes for image generation, such as the patient's anatomical structures or disease diagnostic features. We devise a framework to identify, isolate, and manipulate key attributes through latent space trajectory traversal of generative models, facilitating precise control over medical image synthesis.",
        "arxiv_id": "2503.23623",
        "ARXIVID": "2503.23623",
        "COMMENT": "Matches criterion 4 as it explores vision-language foundation models (Stable Diffusion) for medical image generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.00906": {
        "authors": [
            "Saaket Agashe",
            "Kyle Wong",
            "Vincent Tu",
            "Jiachen Yang",
            "Ang Li",
            "Xin Eric Wang"
        ],
        "title": "Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents",
        "abstract": "arXiv:2504.00906v1 Announce Type: new  Abstract: Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.",
        "arxiv_id": "2504.00906",
        "ARXIVID": "2504.00906",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework (Agent S2) for embodied agents with new methods for GUI interaction and task planning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.24376": {
        "authors": [
            "Yi Chen",
            "Yuying Ge",
            "Rui Wang",
            "Yixiao Ge",
            "Lu Qiu",
            "Ying Shan",
            "Xihui Liu"
        ],
        "title": "Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1",
        "abstract": "arXiv:2503.24376v1 Announce Type: new  Abstract: Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals.",
        "arxiv_id": "2503.24376",
        "ARXIVID": "2503.24376",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (SEED-Bench-R1) for evaluating multimodal large language models in video understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.23137": {
        "authors": [
            "Tuo Liang",
            "Zhe Hu",
            "Jing Li",
            "Hao Zhang",
            "Yiren Lu",
            "Yunlai Zhou",
            "Yiran Qiao",
            "Disheng Liu",
            "Jeirui Peng",
            "Jing Ma",
            "Yu Yin"
        ],
        "title": "When 'YES' Meets 'BUT': Can Large Models Comprehend Contradictory Humor Through Comparative Reasoning?",
        "abstract": "arXiv:2503.23137v1 Announce Type: new  Abstract: Understanding humor-particularly when it involves complex, contradictory narratives that require comparative reasoning-remains a significant challenge for large vision-language models (VLMs). This limitation hinders AI's ability to engage in human-like reasoning and cultural expression. In this paper, we investigate this challenge through an in-depth analysis of comics that juxtapose panels to create humor through contradictions. We introduce the YesBut (V2), a novel benchmark with 1,262 comic images from diverse multilingual and multicultural contexts, featuring comprehensive annotations that capture various aspects of narrative understanding. Using this benchmark, we systematically evaluate a wide range of VLMs through four complementary tasks spanning from surface content comprehension to deep narrative reasoning, with particular emphasis on comparative reasoning between contradictory elements. Our extensive experiments reveal that even the most advanced models significantly underperform compared to humans, with common failures in visual perception, key element identification, comparative analysis and hallucinations. We further investigate text-based training strategies and social knowledge augmentation methods to enhance model performance. Our findings not only highlight critical weaknesses in VLMs' understanding of cultural and creative expressions but also provide pathways toward developing context-aware models capable of deeper narrative understanding though comparative reasoning.",
        "arxiv_id": "2503.23137",
        "ARXIVID": "2503.23137",
        "COMMENT": "Matches criterion 2 as it evaluates vision-language models (VLMs) and their limitations in understanding complex narratives.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.24368": {
        "authors": [
            "Xiaoran Zhang",
            "Eric Z. Chen",
            "Lin Zhao",
            "Xiao Chen",
            "Yikang Liu",
            "Boris Maihe",
            "James S. Duncan",
            "Terrence Chen",
            "Shanhui Sun"
        ],
        "title": "Adapting Vision Foundation Models for Real-time Ultrasound Image Segmentation",
        "abstract": "arXiv:2503.24368v1 Announce Type: new  Abstract: We propose a novel approach that adapts hierarchical vision foundation models for real-time ultrasound image segmentation. Existing ultrasound segmentation methods often struggle with adaptability to new tasks, relying on costly manual annotations, while real-time approaches generally fail to match state-of-the-art performance. To overcome these limitations, we introduce an adaptive framework that leverages the vision foundation model Hiera to extract multi-scale features, interleaved with DINOv2 representations to enhance visual expressiveness. These enriched features are then decoded to produce precise and robust segmentation. We conduct extensive evaluations on six public datasets and one in-house dataset, covering both cardiac and thyroid ultrasound segmentation. Experiments show that our approach outperforms state-of-the-art methods across multiple datasets and excels with limited supervision, surpassing nnUNet by over 20\\% on average in the 1\\% and 10\\% data settings. Our method achieves $\\sim$77 FPS inference speed with TensorRT on a single GPU, enabling real-time clinical applications.",
        "arxiv_id": "2503.24368",
        "ARXIVID": "2503.24368",
        "COMMENT": "Matches criterion 4 as it adapts vision foundation models for real-time ultrasound segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.24320": {
        "authors": [
            "Wenyan Cong",
            "Hanqing Zhu",
            "Peihao Wang",
            "Bangya Liu",
            "Dejia Xu",
            "Kevin Wang",
            "David Z. Pan",
            "Yan Wang",
            "Zhiwen Fan",
            "Zhangyang Wang"
        ],
        "title": "Can Test-Time Scaling Improve World Foundation Model?",
        "abstract": "arXiv:2503.24320v1 Announce Type: new  Abstract: World foundation models, which simulate the physical world by predicting future states from current observations and inputs, have become central to many applications in physical intelligence, including autonomous driving and robotics. However, these models require substantial computational resources for pretraining and are further constrained by available data during post-training. As such, scaling computation at test time emerges as both a critical and practical alternative to traditional model enlargement or re-training. In this work, we introduce SWIFT, a test-time scaling framework tailored for WFMs. SWIFT integrates our extensible WFM evaluation toolkit with process-level inference strategies, including fast tokenization, probability-based Top-K pruning, and efficient beam search. Empirical results on the COSMOS model demonstrate that test-time scaling exists even in a compute-optimal way. Our findings reveal that test-time scaling laws hold for WFMs and that SWIFT provides a scalable and effective pathway for improving WFM inference without retraining or increasing model size. The code is available at https://github.com/Mia-Cong/SWIFT.git.",
        "arxiv_id": "2503.24320",
        "ARXIVID": "2503.24320",
        "COMMENT": "Matches criterion 4 as it discusses test-time scaling for world foundation models, which are relevant to physical intelligence and robotics.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.23106": {
        "authors": [
            "Chao Tao",
            "Dandan Zhong",
            "Weiliang Mu",
            "Zhuofei Du",
            "Haiyang Wu"
        ],
        "title": "A large-scale image-text dataset benchmark for farmland segmentation",
        "abstract": "arXiv:2503.23106v1 Announce Type: new  Abstract: The traditional deep learning paradigm that solely relies on labeled data has limitations in representing the spatial relationships between farmland elements and the surrounding environment.It struggles to effectively model the dynamic temporal evolution and spatial heterogeneity of farmland. Language,as a structured knowledge carrier,can explicitly express the spatiotemporal characteristics of farmland, such as its shape, distribution,and surrounding environmental information.Therefore,a language-driven learning paradigm can effectively alleviate the challenges posed by the spatiotemporal heterogeneity of farmland.However,in the field of remote sensing imagery of farmland,there is currently no comprehensive benchmark dataset to support this research direction.To fill this gap,we introduced language based descriptions of farmland and developed FarmSeg-VL dataset,the first fine-grained image-text dataset designed for spatiotemporal farmland segmentation.Firstly, this article proposed a semi-automatic annotation method that can accurately assign caption to each image, ensuring high data quality and semantic richness while improving the efficiency of dataset construction.Secondly,the FarmSeg-VL exhibits significant spatiotemporal characteristics.In terms of the temporal dimension,it covers all four seasons.In terms of the spatial dimension,it covers eight typical agricultural regions across China.In addition, in terms of captions,FarmSeg-VL covers rich spatiotemporal characteristics of farmland,including its inherent properties,phenological characteristics, spatial distribution,topographic and geomorphic features,and the distribution of surrounding environments.Finally,we present a performance analysis of VLMs and the deep learning models that rely solely on labels trained on the FarmSeg-VL,demonstrating its potential as a standard benchmark for farmland segmentation.",
        "arxiv_id": "2503.23106",
        "ARXIVID": "2503.23106",
        "COMMENT": "Matches criterion 4 as it introduces a vision-language dataset for farmland segmentation, focusing on spatiotemporal characteristics.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.23131": {
        "authors": [
            "Alexander Vogel",
            "Omar Moured",
            "Yufan Chen",
            "Jiaming Zhang",
            "Rainer Stiefelhagen"
        ],
        "title": "RefChartQA: Grounding Visual Answer on Chart Images through Instruction Tuning",
        "abstract": "arXiv:2503.23131v1 Announce Type: new  Abstract: Recently, Vision Language Models (VLMs) have increasingly emphasized document visual grounding to achieve better human-computer interaction, accessibility, and detailed understanding. However, its application to visualizations such as charts remains under-explored due to the inherent complexity of interleaved visual-numerical relationships in chart images. Existing chart understanding methods primarily focus on answering questions without explicitly identifying the visual elements that support their predictions. To bridge this gap, we introduce RefChartQA, a novel benchmark that integrates Chart Question Answering (ChartQA) with visual grounding, enabling models to refer elements at multiple granularities within chart images. Furthermore, we conduct a comprehensive evaluation by instruction-tuning 5 state-of-the-art VLMs across different categories. Our experiments demonstrate that incorporating spatial awareness via grounding improves response accuracy by over 15%, reducing hallucinations, and improving model reliability. Additionally, we identify key factors influencing text-spatial alignment, such as architectural improvements in TinyChart, which leverages a token-merging module for enhanced feature fusion. Our dataset is open-sourced for community development and further advancements. All models and code will be publicly available at https://github.com/moured/RefChartQA.",
        "arxiv_id": "2503.23131",
        "ARXIVID": "2503.23131",
        "COMMENT": "Matches criterion 2 as it evaluates vision-language models (VLMs) and introduces a novel benchmark for visual grounding in chart images.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.24108": {
        "authors": [
            "Anwesa Choudhuri",
            "Zhongpai Gao",
            "Meng Zheng",
            "Benjamin Planche",
            "Terrence Chen",
            "Ziyan Wu"
        ],
        "title": "PolypSegTrack: Unified Foundation Model for Colonoscopy Video Analysis",
        "abstract": "arXiv:2503.24108v1 Announce Type: new  Abstract: Early detection, accurate segmentation, classification and tracking of polyps during colonoscopy are critical for preventing colorectal cancer. Many existing deep-learning-based methods for analyzing colonoscopic videos either require task-specific fine-tuning, lack tracking capabilities, or rely on domain-specific pre-training. In this paper, we introduce \\textit{PolypSegTrack}, a novel foundation model that jointly addresses polyp detection, segmentation, classification and unsupervised tracking in colonoscopic videos. Our approach leverages a novel conditional mask loss, enabling flexible training across datasets with either pixel-level segmentation masks or bounding box annotations, allowing us to bypass task-specific fine-tuning. Our unsupervised tracking module reliably associates polyp instances across frames using object queries, without relying on any heuristics. We leverage a robust vision foundation model backbone that is pre-trained unsupervisedly on natural images, thereby removing the need for domain-specific pre-training. Extensive experiments on multiple polyp benchmarks demonstrate that our method significantly outperforms existing state-of-the-art approaches in detection, segmentation, classification, and tracking.",
        "arxiv_id": "2503.24108",
        "ARXIVID": "2503.24108",
        "COMMENT": "Matches criterion 4 as it discusses a vision foundation model applied to medical imaging tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.23109": {
        "authors": [
            "Xiaolu Liu",
            "Ruizi Yang",
            "Song Wang",
            "Wentong Li",
            "Junbo Chen",
            "Jianke Zhu"
        ],
        "title": "Uncertainty-Instructed Structure Injection for Generalizable HD Map Construction",
        "abstract": "arXiv:2503.23109v1 Announce Type: new  Abstract: Reliable high-definition (HD) map construction is crucial for the driving safety of autonomous vehicles. Although recent studies demonstrate improved performance, their generalization capability across unfamiliar driving scenes remains unexplored. To tackle this issue, we propose UIGenMap, an uncertainty-instructed structure injection approach for generalizable HD map vectorization, which concerns the uncertainty resampling in statistical distribution and employs explicit instance features to reduce excessive reliance on training data. Specifically, we introduce the perspective-view (PV) detection branch to obtain explicit structural features, in which the uncertainty-aware decoder is designed to dynamically sample probability distributions considering the difference in scenes. With probabilistic embedding and selection, UI2DPrompt is proposed to construct PV-learnable prompts. These PV prompts are integrated into the map decoder by designed hybrid injection to compensate for neglected instance structures. To ensure real-time inference, a lightweight Mimic Query Distillation is designed to learn from PV prompts, which can serve as an efficient alternative to the flow of PV branches. Extensive experiments on challenging geographically disjoint (geo-based) data splits demonstrate that our UIGenMap achieves superior performance, with +5.7 mAP improvement on the nuScenes dataset. Source code will be available at https://github.com/xiaolul2/UIGenMap.",
        "arxiv_id": "2503.23109",
        "ARXIVID": "2503.23109",
        "COMMENT": "Matches criterion 3 as it proposes a novel method (UIGenMap) for HD map construction with generalization capabilities.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.23618": {
        "authors": [
            "Amar Kumar",
            "Anita Kriz",
            "Barak Pertzov",
            "Tal Arbel"
        ],
        "title": "Leveraging Vision-Language Foundation Models to Reveal Hidden Image-Attribute Relationships in Medical Imaging",
        "abstract": "arXiv:2503.23618v1 Announce Type: new  Abstract: Vision-language foundation models (VLMs) have shown impressive performance in guiding image generation through text, with emerging applications in medical imaging. In this work, we are the first to investigate the question: 'Can fine-tuned foundation models help identify critical, and possibly unknown, data properties?' By evaluating our proposed method on a chest x-ray dataset, we show that these models can generate high-resolution, precisely edited images compared to methods that rely on Structural Causal Models (SCMs) according to numerous metrics. For the first time, we demonstrate that fine-tuned VLMs can reveal hidden data relationships that were previously obscured due to available metadata granularity and model capacity limitations. Our experiments demonstrate both the potential of these models to reveal underlying dataset properties while also exposing the limitations of fine-tuned VLMs for accurate image editing and susceptibility to biases and spurious correlations.",
        "arxiv_id": "2503.23618",
        "ARXIVID": "2503.23618",
        "COMMENT": "Matches criterion 4 as it explores vision-language foundation models (VLMs) and their applications in medical imaging.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.22884": {
        "authors": [
            "Yi-Ting Shen",
            "Sungmin Eum",
            "Doheon Lee",
            "Rohit Shete",
            "Chiao-Yi Wang",
            "Heesung Kwon",
            "Shuvra S. Bhattacharyya"
        ],
        "title": "AutoComPose: Automatic Generation of Pose Transition Descriptions for Composed Pose Retrieval Using Multimodal LLMs",
        "abstract": "arXiv:2503.22884v1 Announce Type: new  Abstract: Composed pose retrieval (CPR) enables users to search for human poses by specifying a reference pose and a transition description, but progress in this field is hindered by the scarcity and inconsistency of annotated pose transitions. Existing CPR datasets rely on costly human annotations or heuristic-based rule generation, both of which limit scalability and diversity. In this work, we introduce AutoComPose, the first framework that leverages multimodal large language models (MLLMs) to automatically generate rich and structured pose transition descriptions. Our method enhances annotation quality by structuring transitions into fine-grained body part movements and introducing mirrored/swapped variations, while a cyclic consistency constraint ensures logical coherence between forward and reverse transitions. To advance CPR research, we construct and release two dedicated benchmarks, AIST-CPR and PoseFixCPR, supplementing prior datasets with enhanced attributes. Extensive experiments demonstrate that training retrieval models with AutoComPose yields superior performance over human-annotated and heuristic-based methods, significantly reducing annotation costs while improving retrieval quality. Our work pioneers the automatic annotation of pose transitions, establishing a scalable foundation for future CPR research.",
        "arxiv_id": "2503.22884",
        "ARXIVID": "2503.22884",
        "COMMENT": "Matches criterion 2 as it leverages multimodal large language models (MLLMs) for automatic annotation in pose retrieval tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.24381": {
        "authors": [
            "Yuping Wang",
            "Xiangyu Huang",
            "Xiaokang Sun",
            "Mingxuan Yan",
            "Shuo Xing",
            "Zhengzhong Tu",
            "Jiachen Li"
        ],
        "title": "UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving",
        "abstract": "arXiv:2503.24381v1 Announce Type: new  Abstract: We introduce UniOcc, a comprehensive, unified benchmark for occupancy forecasting (i.e., predicting future occupancies based on historical information) and current-frame occupancy prediction from camera images. UniOcc unifies data from multiple real-world datasets (i.e., nuScenes, Waymo) and high-fidelity driving simulators (i.e., CARLA, OpenCOOD), which provides 2D/3D occupancy labels with per-voxel flow annotations and support for cooperative autonomous driving. In terms of evaluation, unlike existing studies that rely on suboptimal pseudo labels for evaluation, UniOcc incorporates novel metrics that do not depend on ground-truth occupancy, enabling robust assessment of additional aspects of occupancy quality. Through extensive experiments on state-of-the-art models, we demonstrate that large-scale, diverse training data and explicit flow information significantly enhance occupancy prediction and forecasting performance.",
        "arxiv_id": "2503.24381",
        "ARXIVID": "2503.24381",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (UniOcc) for occupancy forecasting and prediction in autonomous driving.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.23844": {
        "authors": [
            "Xuyang Li",
            "Chenyu Li",
            "Pedram Ghamisi",
            "Danfeng Hong"
        ],
        "title": "FlexiMo: A Flexible Remote Sensing Foundation Model",
        "abstract": "arXiv:2503.23844v1 Announce Type: new  Abstract: The rapid expansion of multi-source satellite imagery drives innovation in Earth observation, opening unprecedented opportunities for Remote Sensing Foundation Models to harness diverse data. However, many existing models remain constrained by fixed spatial resolutions and patch sizes, limiting their ability to fully exploit the heterogeneous spatial characteristics inherent in satellite imagery. To address these challenges, we propose FlexiMo, a flexible remote sensing foundation model that endows the pre-trained model with the flexibility to adapt to arbitrary spatial resolutions. Central to FlexiMo is a spatial resolution-aware module that employs a parameter-free alignment embedding mechanism to dynamically recalibrate patch embeddings based on the input image's resolution and dimensions. This design not only preserves critical token characteristics and ensures multi-scale feature fidelity but also enables efficient feature extraction without requiring modifications to the underlying network architecture. In addition, FlexiMo incorporates a lightweight channel adaptation module that leverages prior spectral information from sensors. This mechanism allows the model to process images with varying numbers of channels while maintaining the data's intrinsic physical properties. Extensive experiments on diverse multimodal, multi-resolution, and multi-scale datasets demonstrate that FlexiMo significantly enhances model generalization and robustness. In particular, our method achieves outstanding performance across a range of downstream tasks, including scene classification, land cover classification, urban building segmentation, and cloud detection. By enabling parameter-efficient and physically consistent adaptation, FlexiMo paves the way for more adaptable and effective foundation models in real-world remote sensing applications.",
        "arxiv_id": "2503.23844",
        "ARXIVID": "2503.23844",
        "COMMENT": "Matches criterion 4 as it discusses a vision foundation model (FlexiMo) and its applications in remote sensing.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.23313": {
        "authors": [
            "Harshvardhan Takawale",
            "Nirupam Roy"
        ],
        "title": "SpINR: Neural Volumetric Reconstruction for FMCW Radars",
        "abstract": "arXiv:2503.23313v1 Announce Type: new  Abstract: In this paper, we introduce SpINR, a novel framework for volumetric reconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar data. Traditional radar imaging techniques, such as backprojection, often assume ideal signal models and require dense aperture sampling, leading to limitations in resolution and generalization. To address these challenges, SpINR integrates a fully differentiable forward model that operates natively in the frequency domain with implicit neural representations (INRs). This integration leverages the linear relationship between beat frequency and scatterer distance inherent in FMCW radar systems, facilitating more efficient and accurate learning of scene geometry. Additionally, by computing outputs for only the relevant frequency bins, our forward model achieves greater computational efficiency compared to time-domain approaches that process the entire signal before transformation. Through extensive experiments, we demonstrate that SpINR significantly outperforms classical backprojection methods and existing learning-based approaches, achieving higher resolution and more accurate reconstructions of complex scenes. This work represents the first application of neural volumetic reconstruction in the radar domain, offering a promising direction for future research in radar-based imaging and perception systems.",
        "arxiv_id": "2503.23313",
        "ARXIVID": "2503.23313",
        "COMMENT": "Does not match any specific criterion but is related to spatial understanding in radar imaging, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2503.23011": {
        "authors": [
            "Hoigi Seo",
            "Junseo Bang",
            "Haechang Lee",
            "Joohoon Lee",
            "Byung Hyun Lee",
            "Se Young Chun"
        ],
        "title": "On Geometrical Properties of Text Token Embeddings for Strong Semantic Binding in Text-to-Image Generation",
        "abstract": "arXiv:2503.23011v1 Announce Type: new  Abstract: Text-to-Image (T2I) models often suffer from text-image misalignment in complex scenes involving multiple objects and attributes. Semantic binding aims to mitigate this issue by accurately associating the generated attributes and objects with their corresponding noun phrases (NPs). Existing methods rely on text or latent optimizations, yet the factors influencing semantic binding remain underexplored. Here we investigate the geometrical properties of text token embeddings and their cross-attention (CA) maps. We empirically and theoretically analyze that the geometrical properties of token embeddings, specifically both angular distances and norms, play a crucial role in CA map differentiation. Then, we propose \\textbf{TeeMo}, a training-free text embedding-aware T2I framework with strong semantic binding. TeeMo consists of Causality-Aware Projection-Out (CAPO) for distinct inter-NP CA maps and Adaptive Token Mixing (ATM) with our loss to enhance inter-NP separation while maintaining intra-NP cohesion in CA maps. Extensive experiments confirm TeeMo consistently outperforms prior arts across diverse baselines and datasets.",
        "arxiv_id": "2503.23011",
        "ARXIVID": "2503.23011",
        "COMMENT": "Does not match any specific criteria but is related to text-to-image generation and semantic binding.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.23307": {
        "authors": [
            "Cong Wei",
            "Bo Sun",
            "Haoyu Ma",
            "Ji Hou",
            "Felix Juefei-Xu",
            "Zecheng He",
            "Xiaoliang Dai",
            "Luxin Zhang",
            "Kunpeng Li",
            "Tingbo Hou",
            "Animesh Sinha",
            "Peter Vajda",
            "Wenhu Chen"
        ],
        "title": "MoCha: Towards Movie-Grade Talking Character Synthesis",
        "abstract": "arXiv:2503.23307v1 Announce Type: new  Abstract: Recent advancements in video generation have achieved impressive motion realism, yet they often overlook character-driven storytelling, a crucial task for automated film, animation generation. We introduce Talking Characters, a more realistic task to generate talking character animations directly from speech and text. Unlike talking head, Talking Characters aims at generating the full portrait of one or more characters beyond the facial region. In this paper, we propose MoCha, the first of its kind to generate talking characters. To ensure precise synchronization between video and speech, we propose a speech-video window attention mechanism that effectively aligns speech and video tokens. To address the scarcity of large-scale speech-labeled video datasets, we introduce a joint training strategy that leverages both speech-labeled and text-labeled video data, significantly improving generalization across diverse character actions. We also design structured prompt templates with character tags, enabling, for the first time, multi-character conversation with turn-based dialogue-allowing AI-generated characters to engage in context-aware conversations with cinematic coherence. Extensive qualitative and quantitative evaluations, including human preference studies and benchmark comparisons, demonstrate that MoCha sets a new standard for AI-generated cinematic storytelling, achieving superior realism, expressiveness, controllability and generalization.",
        "arxiv_id": "2503.23307",
        "ARXIVID": "2503.23307",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling for talking character synthesis.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.23130": {
        "authors": [
            "Boyi Ma",
            "Yanguang Zhao",
            "Jie Wang",
            "Guankun Wang",
            "Kun Yuan",
            "Tong Chen",
            "Long Bai",
            "Hongliang Ren"
        ],
        "title": "Can DeepSeek-V3 Reason Like a Surgeon? An Empirical Evaluation for Vision-Language Understanding in Robotic-Assisted Surgery",
        "abstract": "arXiv:2503.23130v1 Announce Type: new  Abstract: DeepSeek-V3, a recently emerging Large Language Model (LLM), demonstrates outstanding performance in general scene understanding, question-answering (QA), and text generation tasks, owing to its efficient training paradigm and strong reasoning capabilities. In this study, we investigate the dialogue capabilities of DeepSeek-V3 in robotic surgery scenarios, focusing on tasks such as Single Phrase QA, Visual QA, and Detailed Description. The Single Phrase QA tasks further include sub-tasks such as surgical instrument recognition, action understanding, and spatial position analysis. We conduct extensive evaluations using publicly available datasets, including EndoVis18 and CholecT50, along with their corresponding dialogue data. Our comprehensive evaluation results indicate that, when provided with specific prompts, DeepSeek-V3 performs well in surgical instrument and tissue recognition tasks However, DeepSeek-V3 exhibits significant limitations in spatial position analysis and struggles to understand surgical actions accurately. Additionally, our findings reveal that, under general prompts, DeepSeek-V3 lacks the ability to effectively analyze global surgical concepts and fails to provide detailed insights into surgical scenarios. Based on our observations, we argue that the DeepSeek-V3 is not ready for vision-language tasks in surgical contexts without fine-tuning on surgery-specific datasets.",
        "arxiv_id": "2503.23130",
        "ARXIVID": "2503.23130",
        "COMMENT": "Matches criterion 2 as it evaluates a vision-language model (DeepSeek-V3) in robotic surgery scenarios, though it highlights limitations.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2503.23509": {
        "authors": [
            "Tianming Liang",
            "Haichao Jiang",
            "Wei-Shi Zheng",
            "Jian-Fang Hu"
        ],
        "title": "ReferDINO-Plus: 2nd Solution for 4th PVUW MeViS Challenge at CVPR 2025",
        "abstract": "arXiv:2503.23509v1 Announce Type: new  Abstract: Referring Video Object Segmentation (RVOS) aims to segment target objects throughout a video based on a text description. This task has attracted increasing attention in the field of computer vision due to its promising applications in video editing and human-agent interaction. Recently, ReferDINO has demonstrated promising performance in this task by adapting object-level vision-language knowledge from pretrained foundational image models. In this report, we further enhance its capabilities by incorporating the advantages of SAM2 in mask quality and object consistency. In addition, to effectively balance performance between single-object and multi-object scenarios, we introduce a conditional mask fusion strategy that adaptively fuses the masks from ReferDINO and SAM2. Our solution, termed ReferDINO-Plus, achieves 60.43 \\(\\mathcal{J}\\&\\mathcal{F}\\) on MeViS test set, securing 2nd place in the MeViS PVUW challenge at CVPR 2025. The code is available at: https://github.com/iSEE-Laboratory/ReferDINO-Plus.",
        "arxiv_id": "2503.23509",
        "ARXIVID": "2503.23509",
        "COMMENT": "Matches criterion 2 as it discusses enhancements to a vision-language model (ReferDINO) for video object segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2503.23717": {
        "authors": [
            "Yi Liu",
            "Wengen Li",
            "Jihong Guan",
            "Shuigeng Zhou",
            "Yichao Zhang"
        ],
        "title": "Effective Cloud Removal for Remote Sensing Images by an Improved Mean-Reverting Denoising Model with Elucidated Design Space",
        "abstract": "arXiv:2503.23717v1 Announce Type: new  Abstract: Cloud removal (CR) remains a challenging task in remote sensing image processing. Although diffusion models (DM) exhibit strong generative capabilities, their direct applications to CR are suboptimal, as they generate cloudless images from random noise, ignoring inherent information in cloudy inputs. To overcome this drawback, we develop a new CR model EMRDM based on mean-reverting diffusion models (MRDMs) to establish a direct diffusion process between cloudy and cloudless images. Compared to current MRDMs, EMRDM offers a modular framework with updatable modules and an elucidated design space, based on a reformulated forward process and a new ordinary differential equation (ODE)-based backward process. Leveraging our framework, we redesign key MRDM modules to boost CR performance, including restructuring the denoiser via a preconditioning technique, reorganizing the training process, and improving the sampling process by introducing deterministic and stochastic samplers. To achieve multi-temporal CR, we further develop a denoising network for simultaneously denoising sequential images. Experiments on mono-temporal and multi-temporal datasets demonstrate the superior performance of EMRDM. Our code is available at https://github.com/Ly403/EMRDM.",
        "arxiv_id": "2503.23717",
        "ARXIVID": "2503.23717",
        "COMMENT": "Does not match any specific criterion but discusses generative modeling for cloud removal in remote sensing, which is tangentially relevant to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.23507": {
        "authors": [
            "Siladittya Manna",
            "Suresh Das",
            "Sayantari Ghosh",
            "Saumik Bhattacharya"
        ],
        "title": "Federated Self-Supervised Learning for One-Shot Cross-Modal and Cross-Imaging Technique Segmentation",
        "abstract": "arXiv:2503.23507v1 Announce Type: new  Abstract: Decentralized federated learning enables learning of data representations from multiple sources without compromising the privacy of the clients. In applications like medical image segmentation, where obtaining a large annotated dataset from a single source is a distressing problem, federated self-supervised learning can provide some solace. In this work, we push the limits further by exploring a federated self-supervised one-shot segmentation task representing a more data-scarce scenario. We adopt a pre-existing self-supervised few-shot segmentation framework CoWPro and adapt it to the federated learning scenario. To the best of our knowledge, this work is the first to attempt a self-supervised few-shot segmentation task in the federated learning domain. Moreover, we consider the clients to be constituted of data from different modalities and imaging techniques like MR or CT, which makes the problem even harder. Additionally, we reinforce and improve the baseline CoWPro method using a fused dice loss which shows considerable improvement in performance over the baseline CoWPro. Finally, we evaluate this novel framework on a completely unseen held-out part of the local client dataset. We observe that the proposed framework can achieve performance at par or better than the FedAvg version of the CoWPro framework on the held-out validation dataset.",
        "arxiv_id": "2503.23507",
        "ARXIVID": "2503.23507",
        "COMMENT": "This paper does not match any specific criteria. It focuses on federated self-supervised learning for one-shot segmentation, which is tangential to the specified interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.00762": {
        "authors": [
            "Jianhao Chen",
            "Zishuo Xun",
            "Bocheng Zhou",
            "Han Qi",
            "Qiaosheng Zhang",
            "Yang Chen",
            "Wei Hu",
            "Yuzhong Qu",
            "Wanli Ouyang",
            "Shuyue Hu"
        ],
        "title": "Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling Efficiently Scale Test-Time Compute",
        "abstract": "arXiv:2504.00762v1 Announce Type: new  Abstract: This paper presents a simple, effective, and cost-efficient strategy to improve LLM performance by scaling test-time compute. Our strategy builds upon the repeated-sampling-then-voting framework, with a novel twist: incorporating multiple models, even weaker ones, to leverage their complementary strengths that potentially arise from diverse training data and paradigms. By using consistency as a signal, our strategy dynamically switches between models. Theoretical analysis highlights the efficiency and performance advantages of our strategy. Extensive experiments on six datasets demonstrate that our strategy not only outperforms self-consistency and state-of-the-art multi-agent debate approaches, but also significantly reduces inference costs. Additionally, ModelSwitch requires only a few comparable LLMs to achieve optimal performance and can be extended with verification methods, demonstrating the potential of leveraging multiple LLMs in the generation-verification paradigm.",
        "arxiv_id": "2504.00762",
        "ARXIVID": "2504.00762",
        "COMMENT": "This paper does not match any specific criteria. It focuses on improving LLM performance through multi-model sampling strategies, which is tangential to the specified interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.23450": {
        "authors": [
            "Bohao Xing",
            "Kaishen Yuan",
            "Zitong Yu",
            "Xin Liu",
            "Heikki K\\\"alvi\\\"ainen"
        ],
        "title": "AU-TTT: Vision Test-Time Training model for Facial Action Unit Detection",
        "abstract": "arXiv:2503.23450v1 Announce Type: new  Abstract: Facial Action Units (AUs) detection is a cornerstone of objective facial expression analysis and a critical focus in affective computing. Despite its importance, AU detection faces significant challenges, such as the high cost of AU annotation and the limited availability of datasets. These constraints often lead to overfitting in existing methods, resulting in substantial performance degradation when applied across diverse datasets. Addressing these issues is essential for improving the reliability and generalizability of AU detection methods. Moreover, many current approaches leverage Transformers for their effectiveness in long-context modeling, but they are hindered by the quadratic complexity of self-attention. Recently, Test-Time Training (TTT) layers have emerged as a promising solution for long-sequence modeling. Additionally, TTT applies self-supervised learning for iterative updates during both training and inference, offering a potential pathway to mitigate the generalization challenges inherent in AU detection tasks. In this paper, we propose a novel vision backbone tailored for AU detection, incorporating bidirectional TTT blocks, named AU-TTT. Our approach introduces TTT Linear to the AU detection task and optimizes image scanning mechanisms for enhanced performance. Additionally, we design an AU-specific Region of Interest (RoI) scanning mechanism to capture fine-grained facial features critical for AU detection. Experimental results demonstrate that our method achieves competitive performance in both within-domain and cross-domain scenarios.",
        "arxiv_id": "2503.23450",
        "ARXIVID": "2503.23450",
        "COMMENT": "This paper does not directly match any of the specified criteria. It focuses on facial action unit detection using a novel vision backbone and test-time training, which is not directly related to spatial understanding, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23722": {
        "authors": [
            "Xiang Hu",
            "Yuhao Wang",
            "Pingping Zhang",
            "Huchuan Lu"
        ],
        "title": "LATex: Leveraging Attribute-based Text Knowledge for Aerial-Ground Person Re-Identification",
        "abstract": "arXiv:2503.23722v1 Announce Type: new  Abstract: Aerial-Ground person Re-IDentification (AG-ReID) aims to retrieve specific persons across heterogeneous cameras in different views. Previous methods usually adopt large-scale models, focusing on view-invariant features. However, they overlook the semantic information in person attributes. Additionally, existing training strategies often rely on full fine-tuning large-scale models, which significantly increases training costs. To address these issues, we propose a novel framework named LATex for AG-ReID, which adopts prompt-tuning strategies to leverage attribute-based text knowledge. More specifically, we first introduce the Contrastive Language-Image Pre-training (CLIP) model as the backbone, and propose an Attribute-aware Image Encoder (AIE) to extract global semantic features and attribute-aware features. Then, with these features, we propose a Prompted Attribute Classifier Group (PACG) to generate person attribute predictions and obtain the encoded representations of predicted attributes. Finally, we design a Coupled Prompt Template (CPT) to transform attribute tokens and view information into structured sentences. These sentences are processed by the text encoder of CLIP to generate more discriminative features. As a result, our framework can fully leverage attribute-based text knowledge to improve the AG-ReID. Extensive experiments on three AG-ReID benchmarks demonstrate the effectiveness of our proposed LATex. The source code will be available.",
        "arxiv_id": "2503.23722",
        "ARXIVID": "2503.23722",
        "COMMENT": "Does not match any specific criteria. Focuses on aerial-ground person re-identification using attribute-based text knowledge.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.24387": {
        "authors": [
            "Lee Hsin-Ying",
            "Kelvin C. K. Chan",
            "Ming-Hsuan Yang"
        ],
        "title": "Consistent Subject Generation via Contrastive Instantiated Concepts",
        "abstract": "arXiv:2503.24387v1 Announce Type: new  Abstract: While text-to-image generative models can synthesize diverse and faithful contents, subject variation across multiple creations limits the application in long content generation. Existing approaches require time-consuming tuning, references for all subjects, or access to other creations. We introduce Contrastive Concept Instantiation (CoCoIns) to effectively synthesize consistent subjects across multiple independent creations. The framework consists of a generative model and a mapping network, which transforms input latent codes into pseudo-words associated with certain instances of concepts. Users can generate consistent subjects with the same latent codes. To construct such associations, we propose a contrastive learning approach that trains the network to differentiate the combination of prompts and latent codes. Extensive evaluations of human faces with a single subject show that CoCoIns performs comparably to existing methods while maintaining higher flexibility. We also demonstrate the potential of extending CoCoIns to multiple subjects and other object categories.",
        "arxiv_id": "2503.24387",
        "ARXIVID": "2503.24387",
        "COMMENT": "Does not match any specific criteria. Focuses on consistent subject generation in text-to-image models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.24258": {
        "authors": [
            "Lorenzo Tronchin",
            "Tommy L\\\"ofstedt",
            "Paolo Soda",
            "Valerio Guarrasi"
        ],
        "title": "Beyond a Single Mode: GAN Ensembles for Diverse Medical Data Generation",
        "abstract": "arXiv:2503.24258v1 Announce Type: new  Abstract: The advancement of generative AI, particularly in medical imaging, confronts the trilemma of ensuring high fidelity, diversity, and efficiency in synthetic data generation. While Generative Adversarial Networks (GANs) have shown promise across various applications, they still face challenges like mode collapse and insufficient coverage of real data distributions. This work explores the use of GAN ensembles to overcome these limitations, specifically in the context of medical imaging. By solving a multi-objective optimisation problem that balances fidelity and diversity, we propose a method for selecting an optimal ensemble of GANs tailored for medical data. The selected ensemble is capable of generating diverse synthetic medical images that are representative of true data distributions and computationally efficient. Each model in the ensemble brings a unique contribution, ensuring minimal redundancy. We conducted a comprehensive evaluation using three distinct medical datasets, testing 22 different GAN architectures with various loss functions and regularisation techniques. By sampling models at different training epochs, we crafted 110 unique configurations. The results highlight the capability of GAN ensembles to enhance the quality and utility of synthetic medical images, thereby improving the efficacy of downstream tasks such as diagnostic modelling.",
        "arxiv_id": "2503.24258",
        "ARXIVID": "2503.24258",
        "COMMENT": "Does not match any specific criteria. Focuses on GAN ensembles for medical data generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23353": {
        "authors": [
            "Xiangyang Luo",
            "Junhao Cheng",
            "Yifan Xie",
            "Xin Zhang",
            "Tao Feng",
            "Zhou Liu",
            "Fei Ma",
            "Fei Yu"
        ],
        "title": "Object Isolated Attention for Consistent Story Visualization",
        "abstract": "arXiv:2503.23353v1 Announce Type: new  Abstract: Open-ended story visualization is a challenging task that involves generating coherent image sequences from a given storyline. One of the main difficulties is maintaining character consistency while creating natural and contextually fitting scenes--an area where many existing methods struggle. In this paper, we propose an enhanced Transformer module that uses separate self attention and cross attention mechanisms, leveraging prior knowledge from pre-trained diffusion models to ensure logical scene creation. The isolated self attention mechanism improves character consistency by refining attention maps to reduce focus on irrelevant areas and highlight key features of the same character. Meanwhile, the isolated cross attention mechanism independently processes each character's features, avoiding feature fusion and further strengthening consistency. Notably, our method is training-free, allowing the continuous generation of new characters and storylines without re-tuning. Both qualitative and quantitative evaluations show that our approach outperforms current methods, demonstrating its effectiveness.",
        "arxiv_id": "2503.23353",
        "ARXIVID": "2503.23353",
        "COMMENT": "Does not match any specific criteria. Focuses on story visualization with enhanced attention mechanisms.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.22929": {
        "authors": [
            "Pei-Kai Huang",
            "Jun-Xiong Chong",
            "Ming-Tsung Hsu",
            "Fang-Yu Hsu",
            "Yi-Ting Lin",
            "Kai-Heng Chien",
            "Hao-Chiang Shao",
            "Chiou-Ting Hsu"
        ],
        "title": "Unsupervised Feature Disentanglement and Augmentation Network for One-class Face Anti-spoofing",
        "abstract": "arXiv:2503.22929v1 Announce Type: new  Abstract: Face anti-spoofing (FAS) techniques aim to enhance the security of facial identity authentication by distinguishing authentic live faces from deceptive attempts. While two-class FAS methods risk overfitting to training attacks to achieve better performance, one-class FAS approaches handle unseen attacks well but are less robust to domain information entangled within the liveness features. To address this, we propose an Unsupervised Feature Disentanglement and Augmentation Network (\\textbf{UFDANet}), a one-class FAS technique that enhances generalizability by augmenting face images via disentangled features. The \\textbf{UFDANet} employs a novel unsupervised feature disentangling method to separate the liveness and domain features, facilitating discriminative feature learning. It integrates an out-of-distribution liveness feature augmentation scheme to synthesize new liveness features of unseen spoof classes, which deviate from the live class, thus enhancing the representability and discriminability of liveness features. Additionally, \\textbf{UFDANet} incorporates a domain feature augmentation routine to synthesize unseen domain features, thereby achieving better generalizability. Extensive experiments demonstrate that the proposed \\textbf{UFDANet} outperforms previous one-class FAS methods and achieves comparable performance to state-of-the-art two-class FAS methods.",
        "arxiv_id": "2503.22929",
        "ARXIVID": "2503.22929",
        "COMMENT": "Does not match any specific criteria but is related to one-class learning and feature disentanglement.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.24088": {
        "authors": [
            "Lars M\\\"ollenbrok",
            "Behnood Rasti",
            "Beg\\\"um Demir"
        ],
        "title": "A Plasticity-Aware Method for Continual Self-Supervised Learning in Remote Sensing",
        "abstract": "arXiv:2503.24088v1 Announce Type: new  Abstract: Continual self-supervised learning (CSSL) methods have gained increasing attention in remote sensing (RS) due to their capability to learn new tasks sequentially from continuous streams of unlabeled data.   Existing CSSL methods, while learning new tasks, focus on preventing catastrophic forgetting. To this end, most of them use regularization strategies to retain knowledge of previous tasks. This reduces the model's ability to adapt to the data of new tasks (i.e., learning plasticity), which can degrade performance. To address this problem, in this paper, we propose a novel CSSL method that aims to learn tasks sequentially, while achieving high learning plasticity. To this end, the proposed method uses a knowledge distillation strategy with an integrated decoupling mechanism. The decoupling is achieved by first dividing the feature dimensions into task-common and task-specific parts. Then, the task-common features are forced to be correlated to ensure memory stability while the task-specific features are forced to be de-correlated facilitating the learning of new features. Experimental results show the effectiveness of the proposed method compared to CaSSLe, which is a widely used CSSL framework, with improvements of up to 1.12% in average accuracy and 2.33% in intransigence in a task-incremental scenario, and 1.24% in average accuracy and 2.01% in intransigence in a class-incremental scenario.",
        "arxiv_id": "2503.24088",
        "ARXIVID": "2503.24088",
        "COMMENT": "Does not match any specific criteria but is related to continual learning in remote sensing.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23712": {
        "authors": [
            "Jie Cheng",
            "Hao Zheng",
            "Meiguang Zheng",
            "Lei Wang",
            "Hao Wu",
            "Jian Zhang"
        ],
        "title": "ElimPCL: Eliminating Noise Accumulation with Progressive Curriculum Labeling for Source-Free Domain Adaptation",
        "abstract": "arXiv:2503.23712v1 Announce Type: new  Abstract: Source-Free Domain Adaptation (SFDA) aims to train a target model without source data, and the key is to generate pseudo-labels using a pre-trained source model. However, we observe that the source model often produces highly uncertain pseudo-labels for hard samples, particularly those heavily affected by domain shifts, leading to these noisy pseudo-labels being introduced even before adaptation and further reinforced through parameter updates. Additionally, they continuously influence neighbor samples through propagation in the feature space.To eliminate the issue of noise accumulation, we propose a novel Progressive Curriculum Labeling (ElimPCL) method, which iteratively filters trustworthy pseudo-labeled samples based on prototype consistency to exclude high-noise samples from training. Furthermore, a Dual MixUP technique is designed in the feature space to enhance the separability of hard samples, thereby mitigating the interference of noisy samples on their neighbors.Extensive experiments validate the effectiveness of ElimPCL, achieving up to a 3.4% improvement on challenging tasks compared to state-of-the-art methods.",
        "arxiv_id": "2503.23712",
        "ARXIVID": "2503.23712",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in domain adaptation and pseudo-labeling techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23185": {
        "authors": [
            "Shota Hirose",
            "Kazuki Kotoyori",
            "Kasidis Arunruangsirilert",
            "Fangzheng Lin",
            "Heming Sun",
            "Jiro Katto"
        ],
        "title": "Real-time Video Prediction With Fast Video Interpolation Model and Prediction Training",
        "abstract": "arXiv:2503.23185v1 Announce Type: new  Abstract: Transmission latency significantly affects users' quality of experience in real-time interaction and actuation. As latency is principally inevitable, video prediction can be utilized to mitigate the latency and ultimately enable zero-latency transmission. However, most of the existing video prediction methods are computationally expensive and impractical for real-time applications. In this work, we therefore propose real-time video prediction towards the zero-latency interaction over networks, called IFRVP (Intermediate Feature Refinement Video Prediction). Firstly, we propose three training methods for video prediction that extend frame interpolation models, where we utilize a simple convolution-only frame interpolation network based on IFRNet. Secondly, we introduce ELAN-based residual blocks into the prediction models to improve both inference speed and accuracy. Our evaluations show that our proposed models perform efficiently and achieve the best trade-off between prediction accuracy and computational speed among the existing video prediction methods. A demonstration movie is also provided at http://bit.ly/IFRVPDemo.",
        "arxiv_id": "2503.23185",
        "ARXIVID": "2503.23185",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in video prediction and computational efficiency.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23379": {
        "authors": [
            "Haiduo Huang",
            "Yadong Zhang",
            "Pengju Ren"
        ],
        "title": "KernelDNA: Dynamic Kernel Sharing via Decoupled Naive Adapters",
        "abstract": "arXiv:2503.23379v1 Announce Type: new  Abstract: Dynamic convolution enhances model capacity by adaptively combining multiple kernels, yet faces critical trade-offs: prior works either (1) incur significant parameter overhead by scaling kernel numbers linearly, (2) compromise inference speed through complex kernel interactions, or (3) struggle to jointly optimize dynamic attention and static kernels. We also observe that pre-trained Convolutional Neural Networks (CNNs) exhibit inter-layer redundancy akin to that in Large Language Models (LLMs). Specifically, dense convolutional layers can be efficiently replaced by derived ``child\" layers generated from a shared ``parent\" convolutional kernel through an adapter.   To address these limitations and implement the weight-sharing mechanism, we propose a lightweight convolution kernel plug-in, named KernelDNA. It decouples kernel adaptation into input-dependent dynamic routing and pre-trained static modulation, ensuring both parameter efficiency and hardware-friendly inference. Unlike existing dynamic convolutions that expand parameters via multi-kernel ensembles, our method leverages cross-layer weight sharing and adapter-based modulation, enabling dynamic kernel specialization without altering the standard convolution structure. This design preserves the native computational efficiency of standard convolutions while enhancing representation power through input-adaptive kernel adjustments. Experiments on image classification and dense prediction tasks demonstrate that KernelDNA achieves state-of-the-art accuracy-efficiency balance among dynamic convolution variants. Our codes are available at https://github.com/haiduo/KernelDNA.",
        "arxiv_id": "2503.23379",
        "ARXIVID": "2503.23379",
        "COMMENT": "Does not match any specific criterion but is relevant to general interest in computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23947": {
        "authors": [
            "Guhnoo Yun",
            "Juhan Yoo",
            "Kijung Kim",
            "Jeongho Lee",
            "Paul Hongsuck Seo",
            "Dong Hwan Kim"
        ],
        "title": "Spectral-Adaptive Modulation Networks for Visual Perception",
        "abstract": "arXiv:2503.23947v1 Announce Type: new  Abstract: Recent studies have shown that 2D convolution and self-attention exhibit distinct spectral behaviors, and optimizing their spectral properties can enhance vision model performance. However, theoretical analyses remain limited in explaining why 2D convolution is more effective in high-pass filtering than self-attention and why larger kernels favor shape bias, akin to self-attention. In this paper, we employ graph spectral analysis to theoretically simulate and compare the frequency responses of 2D convolution and self-attention within a unified framework. Our results corroborate previous empirical findings and reveal that node connectivity, modulated by window size, is a key factor in shaping spectral functions. Leveraging this insight, we introduce a \\textit{spectral-adaptive modulation} (SPAM) mixer, which processes visual features in a spectral-adaptive manner using multi-scale convolutional kernels and a spectral re-scaling mechanism to refine spectral components. Based on SPAM, we develop SPANetV2 as a novel vision backbone. Extensive experiments demonstrate that SPANetV2 outperforms state-of-the-art models across multiple vision tasks, including ImageNet-1K classification, COCO object detection, and ADE20K semantic segmentation.",
        "arxiv_id": "2503.23947",
        "ARXIVID": "2503.23947",
        "COMMENT": "Does not match any specific criteria but is related to vision model performance improvements through spectral analysis.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23907": {
        "authors": [
            "Zhichao Liao",
            "Xiaokun Liu",
            "Wenyu Qin",
            "Qingyu Li",
            "Qiulin Wang",
            "Pengfei Wan",
            "Di Zhang",
            "Long Zeng",
            "Pingfa Feng"
        ],
        "title": "HumanAesExpert: Advancing a Multi-Modality Foundation Model for Human Image Aesthetic Assessment",
        "abstract": "arXiv:2503.23907v1 Announce Type: new  Abstract: Image Aesthetic Assessment (IAA) is a long-standing and challenging research task. However, its subset, Human Image Aesthetic Assessment (HIAA), has been scarcely explored, even though HIAA is widely used in social media, AI workflows, and related domains. To bridge this research gap, our work pioneers a holistic implementation framework tailored for HIAA. Specifically, we introduce HumanBeauty, the first dataset purpose-built for HIAA, which comprises 108k high-quality human images with manual annotations. To achieve comprehensive and fine-grained HIAA, 50K human images are manually collected through a rigorous curation process and annotated leveraging our trailblazing 12-dimensional aesthetic standard, while the remaining 58K with overall aesthetic labels are systematically filtered from public datasets. Based on the HumanBeauty database, we propose HumanAesExpert, a powerful Vision Language Model for aesthetic evaluation of human images. We innovatively design an Expert head to incorporate human knowledge of aesthetic sub-dimensions while jointly utilizing the Language Modeling (LM) and Regression head. This approach empowers our model to achieve superior proficiency in both overall and fine-grained HIAA. Furthermore, we introduce a MetaVoter, which aggregates scores from all three heads, to effectively balance the capabilities of each head, thereby realizing improved assessment precision. Extensive experiments demonstrate that our HumanAesExpert models deliver significantly better performance in HIAA than other state-of-the-art models. Our datasets, models, and codes are publicly released to advance the HIAA community. Project webpage: https://humanaesexpert.github.io/HumanAesExpert/",
        "arxiv_id": "2503.23907",
        "ARXIVID": "2503.23907",
        "COMMENT": "Does not match any specific criteria but is related to vision-language models for aesthetic assessment.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23764": {
        "authors": [
            "Md Mahfuz Al Hasan",
            "Mahdi Zaman",
            "Abdul Jawad",
            "Alberto Santamaria-Pang",
            "Ho Hin Lee",
            "Ivan Tarapov",
            "Kyle See",
            "Md Shah Imran",
            "Antika Roy",
            "Yaser Pourmohammadi Fallah",
            "Navid Asadizanjani",
            "Reza Forghani"
        ],
        "title": "WaveFormer: A 3D Transformer with Wavelet-Driven Feature Representation for Efficient Medical Image Segmentation",
        "abstract": "arXiv:2503.23764v2 Announce Type: new  Abstract: Transformer-based architectures have advanced medical image analysis by effectively modeling long-range dependencies, yet they often struggle in 3D settings due to substantial memory overhead and insufficient capture of fine-grained local features. We address these limitations with WaveFormer, a novel 3D-transformer that: i) leverages the fundamental frequency-domain properties of features for contextual representation, and ii) is inspired by the top-down mechanism of the human visual recognition system, making it a biologically motivated architecture. By employing discrete wavelet transformations (DWT) at multiple scales, WaveFormer preserves both global context and high-frequency details while replacing heavy upsampling layers with efficient wavelet-based summarization and reconstruction. This significantly reduces the number of parameters, which is critical for real-world deployment where computational resources and training times are constrained. Furthermore, the model is generic and easily adaptable to diverse applications. Evaluations on BraTS2023, FLARE2021, and KiTS2023 demonstrate performance on par with state-of-the-art methods while offering substantially lower computational complexity.",
        "arxiv_id": "2503.23764",
        "ARXIVID": "2503.23764",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and transformer-based architectures.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23398": {
        "authors": [
            "Leander Girrbach",
            "Stephan Alaniz",
            "Genevieve Smith",
            "Zeynep Akata"
        ],
        "title": "A Large Scale Analysis of Gender Biases in Text-to-Image Generative Models",
        "abstract": "arXiv:2503.23398v1 Announce Type: new  Abstract: With the increasing use of image generation technology, understanding its social biases, including gender bias, is essential. This paper presents the first large-scale study on gender bias in text-to-image (T2I) models, focusing on everyday situations. While previous research has examined biases in occupations, we extend this analysis to gender associations in daily activities, objects, and contexts. We create a dataset of 3,217 gender-neutral prompts and generate 200 images per prompt from five leading T2I models. We automatically detect the perceived gender of people in the generated images and filter out images with no person or multiple people of different genders, leaving 2,293,295 images. To enable a broad analysis of gender bias in T2I models, we group prompts into semantically similar concepts and calculate the proportion of male- and female-gendered images for each prompt. Our analysis shows that T2I models reinforce traditional gender roles, reflect common gender stereotypes in household roles, and underrepresent women in financial related activities. Women are predominantly portrayed in care- and human-centered scenarios, and men in technical or physical labor scenarios.",
        "arxiv_id": "2503.23398",
        "ARXIVID": "2503.23398",
        "COMMENT": "Does not match any specific criterion but is relevant to understanding biases in generative models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23580": {
        "authors": [
            "Zheng-Peng Duan",
            "Jiawei Zhang",
            "Xin Jin",
            "Ziheng Zhang",
            "Zheng Xiong",
            "Dongqing Zou",
            "Jimmy Ren",
            "Chun-Le Guo",
            "Chongyi Li"
        ],
        "title": "DiT4SR: Taming Diffusion Transformer for Real-World Image Super-Resolution",
        "abstract": "arXiv:2503.23580v1 Announce Type: new  Abstract: Large-scale pre-trained diffusion models are becoming increasingly popular in solving the Real-World Image Super-Resolution (Real-ISR) problem because of their rich generative priors. The recent development of diffusion transformer (DiT) has witnessed overwhelming performance over the traditional UNet-based architecture in image generation, which also raises the question: Can we adopt the advanced DiT-based diffusion model for Real-ISR? To this end, we propose our DiT4SR, one of the pioneering works to tame the large-scale DiT model for Real-ISR. Instead of directly injecting embeddings extracted from low-resolution (LR) images like ControlNet, we integrate the LR embeddings into the original attention mechanism of DiT, allowing for the bidirectional flow of information between the LR latent and the generated latent. The sufficient interaction of these two streams allows the LR stream to evolve with the diffusion process, producing progressively refined guidance that better aligns with the generated latent at each diffusion step. Additionally, the LR guidance is injected into the generated latent via a cross-stream convolution layer, compensating for DiT's limited ability to capture local information. These simple but effective designs endow the DiT model with superior performance in Real-ISR, which is demonstrated by extensive experiments. Project Page: https://adam-duan.github.io/projects/dit4sr/.",
        "arxiv_id": "2503.23580",
        "ARXIVID": "2503.23580",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.24345": {
        "authors": [
            "Fang Yan",
            "Jianfeng Wu",
            "Jiawen Li",
            "Wei Wang",
            "Jiaxuan Lu",
            "Wen Chen",
            "Zizhao Gao",
            "Jianan Li",
            "Hong Yan",
            "Jiabo Ma",
            "Minda Chen",
            "Yang Lu",
            "Qing Chen",
            "Yizhi Wang",
            "Xitong Ling",
            "Xuenian Wang",
            "Zihan Wang",
            "Qiang Huang",
            "Shengyi Hua",
            "Mianxin Liu",
            "Lei Ma",
            "Tian Shen",
            "Xiaofan Zhang",
            "Yonghong He",
            "Hao Chen",
            "Shaoting Zhang",
            "Zhe Wang"
        ],
        "title": "PathOrchestra: A Comprehensive Foundation Model for Computational Pathology with Over 100 Diverse Clinical-Grade Tasks",
        "abstract": "arXiv:2503.24345v1 Announce Type: new  Abstract: The complexity and variability inherent in high-resolution pathological images present significant challenges in computational pathology. While pathology foundation models leveraging AI have catalyzed transformative advancements, their development demands large-scale datasets, considerable storage capacity, and substantial computational resources. Furthermore, ensuring their clinical applicability and generalizability requires rigorous validation across a broad spectrum of clinical tasks. Here, we present PathOrchestra, a versatile pathology foundation model trained via self-supervised learning on a dataset comprising 300K pathological slides from 20 tissue and organ types across multiple centers. The model was rigorously evaluated on 112 clinical tasks using a combination of 61 private and 51 public datasets. These tasks encompass digital slide preprocessing, pan-cancer classification, lesion identification, multi-cancer subtype classification, biomarker assessment, gene expression prediction, and the generation of structured reports. PathOrchestra demonstrated exceptional performance across 27,755 WSIs and 9,415,729 ROIs, achieving over 0.950 accuracy in 47 tasks, including pan-cancer classification across various organs, lymphoma subtype diagnosis, and bladder cancer screening. Notably, it is the first model to generate structured reports for high-incidence colorectal cancer and diagnostically complex lymphoma-areas that are infrequently addressed by foundational models but hold immense clinical potential. Overall, PathOrchestra exemplifies the feasibility and efficacy of a large-scale, self-supervised pathology foundation model, validated across a broad range of clinical-grade tasks. Its high accuracy and reduced reliance on extensive data annotation underline its potential for clinical integration, offering a pathway toward more efficient and high-quality medical services.",
        "arxiv_id": "2503.24345",
        "ARXIVID": "2503.24345",
        "COMMENT": "Does not match any specific criterion but is related to vision foundation models applied to computational pathology, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23709": {
        "authors": [
            "Xulong Shi",
            "Caiyi Sun",
            "Zhi Qi",
            "Liu Hao",
            "Xiaodong Yang"
        ],
        "title": "Expanding-and-Shrinking Binary Neural Networks",
        "abstract": "arXiv:2503.23709v1 Announce Type: new  Abstract: While binary neural networks (BNNs) offer significant benefits in terms of speed, memory and energy, they encounter substantial accuracy degradation in challenging tasks compared to their real-valued counterparts. Due to the binarization of weights and activations, the possible values of each entry in the feature maps generated by BNNs are strongly constrained. To tackle this limitation, we propose the expanding-and-shrinking operation, which enhances binary feature maps with negligible increase of computation complexity, thereby strengthening the representation capacity. Extensive experiments conducted on multiple benchmarks reveal that our approach generalizes well across diverse applications ranging from image classification, object detection to generative diffusion model, while also achieving remarkable improvement over various leading binarization algorithms based on different architectures including both CNNs and Transformers.",
        "arxiv_id": "2503.23709",
        "ARXIVID": "2503.23709",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.24282": {
        "authors": [
            "Jian Wang",
            "Xin Lan",
            "Jizhe Zhou",
            "Yuxin Tian",
            "Jiancheng Lv"
        ],
        "title": "Style Quantization for Data-Efficient GAN Training",
        "abstract": "arXiv:2503.24282v1 Announce Type: new  Abstract: Under limited data setting, GANs often struggle to navigate and effectively exploit the input latent space. Consequently, images generated from adjacent variables in a sparse input latent space may exhibit significant discrepancies in realism, leading to suboptimal consistency regularization (CR) outcomes. To address this, we propose \\textit{SQ-GAN}, a novel approach that enhances CR by introducing a style space quantization scheme. This method transforms the sparse, continuous input latent space into a compact, structured discrete proxy space, allowing each element to correspond to a specific real data point, thereby improving CR performance. Instead of direct quantization, we first map the input latent variables into a less entangled ``style'' space and apply quantization using a learnable codebook. This enables each quantized code to control distinct factors of variation. Additionally, we optimize the optimal transport distance to align the codebook codes with features extracted from the training data by a foundation model, embedding external knowledge into the codebook and establishing a semantically rich vocabulary that properly describes the training dataset. Extensive experiments demonstrate significant improvements in both discriminator robustness and generation quality with our method.",
        "arxiv_id": "2503.24282",
        "ARXIVID": "2503.24282",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to generative modeling and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23911": {
        "authors": [
            "Ruisheng Han",
            "Kanglei Zhou",
            "Amir Atapour-Abarghouei",
            "Xiaohui Liang",
            "Hubert P. H. Shum"
        ],
        "title": "FineCausal: A Causal-Based Framework for Interpretable Fine-Grained Action Quality Assessment",
        "abstract": "arXiv:2503.23911v1 Announce Type: new  Abstract: Action quality assessment (AQA) is critical for evaluating athletic performance, informing training strategies, and ensuring safety in competitive sports. However, existing deep learning approaches often operate as black boxes and are vulnerable to spurious correlations, limiting both their reliability and interpretability. In this paper, we introduce FineCausal, a novel causal-based framework that achieves state-of-the-art performance on the FineDiving-HM dataset. Our approach leverages a Graph Attention Network-based causal intervention module to disentangle human-centric foreground cues from background confounders, and incorporates a temporal causal attention module to capture fine-grained temporal dependencies across action stages. This dual-module strategy enables FineCausal to generate detailed spatio-temporal representations that not only achieve state-of-the-art scoring performance but also provide transparent, interpretable feedback on which features drive the assessment. Despite its strong performance, FineCausal requires extensive expert knowledge to define causal structures and depends on high-quality annotations, challenges that we discuss and address as future research directions. Code is available at https://github.com/Harrison21/FineCausal.",
        "arxiv_id": "2503.23911",
        "ARXIVID": "2503.23911",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23679": {
        "authors": [
            "Mingkai Tian",
            "Guorong Li",
            "Yuankai Qi",
            "Amin Beheshti",
            "Javen Qinfeng Shi",
            "Anton van den Hengel",
            "Qingming Huang"
        ],
        "title": "The Devil is in the Distributions: Explicit Modeling of Scene Content is Key in Zero-Shot Video Captioning",
        "abstract": "arXiv:2503.23679v1 Announce Type: new  Abstract: Zero-shot video captioning requires that a model generate high-quality captions without human-annotated video-text pairs for training. State-of-the-art approaches to the problem leverage CLIP to extract visual-relevant textual prompts to guide language models in generating captions. These methods tend to focus on one key aspect of the scene and build a caption that ignores the rest of the visual input. To address this issue, and generate more accurate and complete captions, we propose a novel progressive multi-granularity textual prompting strategy for zero-shot video captioning. Our approach constructs three distinct memory banks, encompassing noun phrases, scene graphs of noun phrases, and entire sentences. Moreover, we introduce a category-aware retrieval mechanism that models the distribution of natural language surrounding the specific topics in question. Extensive experiments demonstrate the effectiveness of our method with 5.7%, 16.2%, and 3.4% improvements in terms of the main metric CIDEr on MSR-VTT, MSVD, and VATEX benchmarks compared to existing state-of-the-art.",
        "arxiv_id": "2503.23679",
        "ARXIVID": "2503.23679",
        "COMMENT": "Does not match any specific criterion but is related to video captioning and generative modeling, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23746": {
        "authors": [
            "Dizhan Xue",
            "Jing Cui",
            "Shengsheng Qian",
            "Chuanrui Hu",
            "Changsheng Xu"
        ],
        "title": "Short-video Propagation Influence Rating: A New Real-world Dataset and A New Large Graph Model",
        "abstract": "arXiv:2503.23746v1 Announce Type: new  Abstract: Short-video platforms have gained immense popularity, captivating the interest of millions, if not billions, of users globally. Recently, researchers have highlighted the significance of analyzing the propagation of short-videos, which typically involves discovering commercial values, public opinions, user behaviors, etc. This paper proposes a new Short-video Propagation Influence Rating (SPIR) task and aims to promote SPIR from both the dataset and method perspectives. First, we propose a new Cross-platform Short-Video (XS-Video) dataset, which aims to provide a large-scale and real-world short-video propagation network across various platforms to facilitate the research on short-video propagation. Our XS-Video dataset includes 117,720 videos, 381,926 samples, and 535 topics across 5 biggest Chinese platforms, annotated with the propagation influence from level 0 to 9. To the best of our knowledge, this is the first large-scale short-video dataset that contains cross-platform data or provides all of the views, likes, shares, collects, fans, comments, and comment content. Second, we propose a Large Graph Model (LGM) named NetGPT, based on a novel three-stage training mechanism, to bridge heterogeneous graph-structured data with the powerful reasoning ability and knowledge of Large Language Models (LLMs). Our NetGPT can comprehend and analyze the short-video propagation graph, enabling it to predict the long-term propagation influence of short-videos. Comprehensive experimental results evaluated by both classification and regression metrics on our XS-Video dataset indicate the superiority of our method for SPIR.",
        "arxiv_id": "2503.23746",
        "ARXIVID": "2503.23746",
        "COMMENT": "Does not match any specific criterion but is related to large graph models and dataset creation, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23455": {
        "authors": [
            "Junzhu Mao",
            "Yang Shen",
            "Jinyang Guo",
            "Yazhou Yao",
            "Xiansheng Hua"
        ],
        "title": "Efficient Token Compression for Vision Transformer with Spatial Information Preserved",
        "abstract": "arXiv:2503.23455v1 Announce Type: new  Abstract: Token compression is essential for reducing the computational and memory requirements of transformer models, enabling their deployment in resource-constrained environments. In this work, we propose an efficient and hardware-compatible token compression method called Prune and Merge. Our approach integrates token pruning and merging operations within transformer models to achieve layer-wise token compression. By introducing trainable merge and reconstruct matrices and utilizing shortcut connections, we efficiently merge tokens while preserving important information and enabling the restoration of pruned tokens. Additionally, we introduce a novel gradient-weighted attention scoring mechanism that computes token importance scores during the training phase, eliminating the need for separate computations during inference and enhancing compression efficiency. We also leverage gradient information to capture the global impact of tokens and automatically identify optimal compression structures. Extensive experiments on the ImageNet-1k and ADE20K datasets validate the effectiveness of our approach, achieving significant speed-ups with minimal accuracy degradation compared to state-of-the-art methods. For instance, on DeiT-Small, we achieve a 1.64$\\times$ speed-up with only a 0.2\\% drop in accuracy on ImageNet-1k. Moreover, by compressing segmenter models and comparing with existing methods, we demonstrate the superior performance of our approach in terms of efficiency and effectiveness. Code and models have been made available at https://github.com/NUST-Machine-Intelligence-Laboratory/prune_and_merge.",
        "arxiv_id": "2503.23455",
        "ARXIVID": "2503.23455",
        "COMMENT": "Does not match any specific criterion but discusses token compression in vision transformers, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.00299": {
        "authors": [
            "Min Zhang",
            "Yuzhe Lu",
            "Yun Zhou",
            "Panpan Xu",
            "Lin Lee Cheong",
            "Chang-Tien Lu",
            "Haozhu Wang"
        ],
        "title": "Collaborative LLM Numerical Reasoning with Local Data Protection",
        "abstract": "arXiv:2504.00299v1 Announce Type: new  Abstract: Numerical reasoning over documents, which demands both contextual understanding and logical inference, is challenging for low-capacity local models deployed on computation-constrained devices. Although such complex reasoning queries could be routed to powerful remote models like GPT-4, exposing local data raises significant data leakage concerns. Existing mitigation methods generate problem descriptions or examples for remote assistance. However, the inherent complexity of numerical reasoning hinders the local model from generating logically equivalent queries and accurately inferring answers with remote guidance. In this paper, we present a model collaboration framework with two key innovations: (1) a context-aware synthesis strategy that shifts the query domains while preserving logical consistency; and (2) a tool-based answer reconstruction approach that reuses the remote-generated problem-solving pattern with code snippets. Experimental results demonstrate that our method achieves better reasoning accuracy than solely using local models while providing stronger data protection than fully relying on remote models. Furthermore, our method improves accuracy by 16.2% - 43.6% while reducing data leakage by 2.3% - 44.6% compared to existing data protection approaches.",
        "arxiv_id": "2504.00299",
        "ARXIVID": "2504.00299",
        "COMMENT": "Does not match any specific criterion but discusses numerical reasoning and data protection, which is tangentially relevant to general AI interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23793": {
        "authors": [
            "Zhongnan Cai",
            "Yingying Wang",
            "Yunlong Lin",
            "Hui Zheng",
            "Ge Meng",
            "Zixu Lin",
            "Jiaxin Xie",
            "Junbin Lu",
            "Yue Huang",
            "Xinghao Ding"
        ],
        "title": "Pan-LUT: Efficient Pan-sharpening via Learnable Look-Up Tables",
        "abstract": "arXiv:2503.23793v1 Announce Type: new  Abstract: Recently, deep learning-based pan-sharpening algorithms have achieved notable advancements over traditional methods. However, many deep learning-based approaches incur substantial computational overhead during inference, especially with high-resolution images. This excessive computational demand limits the applicability of these methods in real-world scenarios, particularly in the absence of dedicated computing devices such as GPUs and TPUs. To address these challenges, we propose Pan-LUT, a novel learnable look-up table (LUT) framework for pan-sharpening that strikes a balance between performance and computational efficiency for high-resolution remote sensing images. To finely control the spectral transformation, we devise the PAN-guided look-up table (PGLUT) for channel-wise spectral mapping. To effectively capture fine-grained spatial details and adaptively learn local contexts, we introduce the spatial details look-up table (SDLUT) and adaptive aggregation look-up table (AALUT). Our proposed method contains fewer than 300K parameters and processes a 8K resolution image in under 1 ms using a single NVIDIA GeForce RTX 2080 Ti GPU, demonstrating significantly faster performance compared to other methods. Experiments reveal that Pan-LUT efficiently processes large remote sensing images in a lightweight manner, bridging the gap to real-world applications. Furthermore, our model surpasses SOTA methods in full-resolution scenes under real-world conditions, highlighting its effectiveness and efficiency.",
        "arxiv_id": "2503.23793",
        "ARXIVID": "2503.23793",
        "COMMENT": "Does not match any specific criterion but is related to computational efficiency in vision tasks, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23381": {
        "authors": [
            "Jiexin Wang",
            "Wenwen Qiang",
            "Zhao Yang",
            "Bing Su"
        ],
        "title": "Enhancing Human Motion Prediction via Multi-range Decoupling Decoding with Gating-adjusting Aggregation",
        "abstract": "arXiv:2503.23381v1 Announce Type: new  Abstract: Expressive representation of pose sequences is crucial for accurate motion modeling in human motion prediction (HMP). While recent deep learning-based methods have shown promise in learning motion representations, these methods tend to overlook the varying relevance and dependencies between historical information and future moments, with a stronger correlation for short-term predictions and weaker for distant future predictions. This limits the learning of motion representation and then hampers prediction performance. In this paper, we propose a novel approach called multi-range decoupling decoding with gating-adjusting aggregation ($MD2GA$), which leverages the temporal correlations to refine motion representation learning. This approach employs a two-stage strategy for HMP. In the first stage, a multi-range decoupling decoding adeptly adjusts feature learning by decoding the shared features into distinct future lengths, where different decoders offer diverse insights into motion patterns. In the second stage, a gating-adjusting aggregation dynamically combines the diverse insights guided by input motion data. Extensive experiments demonstrate that the proposed method can be easily integrated into other motion prediction methods and enhance their prediction performance.",
        "arxiv_id": "2503.23381",
        "ARXIVID": "2503.23381",
        "COMMENT": "Does not match any specific criterion but discusses human motion prediction, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.00389": {
        "authors": [
            "Chengshuai Zhao",
            "Riccardo De Maria",
            "Tharindu Kumarage",
            "Kumar Satvik Chaudhary",
            "Garima Agrawal",
            "Yiwen Li",
            "Jongchan Park",
            "Yuli Deng",
            "Ying-Chih Chen",
            "Huan Liu"
        ],
        "title": "CyberBOT: Towards Reliable Cybersecurity Education via Ontology-Grounded Retrieval Augmented Generation",
        "abstract": "arXiv:2504.00389v1 Announce Type: new  Abstract: Advancements in large language models (LLMs) have enabled the development of intelligent educational tools that support inquiry-based learning across technical domains. In cybersecurity education, where accuracy and safety are paramount, systems must go beyond surface-level relevance to provide information that is both trustworthy and domain-appropriate. To address this challenge, we introduce CyberBOT, a question-answering chatbot that leverages a retrieval-augmented generation (RAG) pipeline to incorporate contextual information from course-specific materials and validate responses using a domain-specific cybersecurity ontology. The ontology serves as a structured reasoning layer that constrains and verifies LLM-generated answers, reducing the risk of misleading or unsafe guidance. CyberBOT has been deployed in a large graduate-level course at Arizona State University (ASU), where more than one hundred students actively engage with the system through a dedicated web-based platform. Computational evaluations in lab environments highlight the potential capacity of CyberBOT, and a forthcoming field study will evaluate its pedagogical impact. By integrating structured domain reasoning with modern generative capabilities, CyberBOT illustrates a promising direction for developing reliable and curriculum-aligned AI applications in specialized educational contexts.",
        "arxiv_id": "2504.00389",
        "ARXIVID": "2504.00389",
        "COMMENT": "Does not match any specific criterion but discusses retrieval-augmented generation in a specific educational domain, which is tangentially relevant to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.00424": {
        "authors": [
            "Jianshu She",
            "Zhuohao Li",
            "Zhemin Huang",
            "Qi Li",
            "Peiran Xu",
            "Haonan Li",
            "Qirong Ho"
        ],
        "title": "Hawkeye:Efficient Reasoning with Model Collaboration",
        "abstract": "arXiv:2504.00424v1 Announce Type: new  Abstract: Chain-of-Thought (CoT) reasoning has demonstrated remarkable effectiveness in enhancing the reasoning abilities of large language models (LLMs). However, its efficiency remains a challenge due to the generation of excessive intermediate reasoning tokens, which introduce semantic redundancy and overly detailed reasoning steps. Moreover, computational expense and latency are significant concerns, as the cost scales with the number of output tokens, including those intermediate steps. In this work, we observe that most CoT tokens are unnecessary, and retaining only a small portion of them is sufficient for producing high-quality responses. Inspired by this, we propose HAWKEYE, a novel post-training and inference framework where a large model produces concise CoT instructions to guide a smaller model in response generation. HAWKEYE quantifies redundancy in CoT reasoning and distills high-density information via reinforcement learning. By leveraging these concise CoTs, HAWKEYE is able to expand responses while reducing token usage and computational cost significantly. Our evaluation shows that HAWKEYE can achieve comparable response quality using only 35% of the full CoTs, while improving clarity, coherence, and conciseness by approximately 10%. Furthermore, HAWKEYE can accelerate end-to-end reasoning by up to 3.4x on complex math tasks while reducing inference cost by up to 60%. HAWKEYE will be open-sourced and the models will be available soon.",
        "arxiv_id": "2504.00424",
        "ARXIVID": "2504.00424",
        "COMMENT": "Does not match any specific criterion but discusses reasoning efficiency in large language models, which is tangentially relevant to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23367": {
        "authors": [
            "Hang Guo",
            "Yawei Li",
            "Taolin Zhang",
            "Jiangshan Wang",
            "Tao Dai",
            "Shu-Tao Xia",
            "Luca Benini"
        ],
        "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
        "abstract": "arXiv:2503.23367v1 Announce Type: new  Abstract: Visual Autoregressive (VAR) modeling has gained popularity for its shift towards next-scale prediction. However, existing VAR paradigms process the entire token map at each scale step, leading to the complexity and runtime scaling dramatically with image resolution. To address this challenge, we propose FastVAR, a post-training acceleration method for efficient resolution scaling with VARs. Our key finding is that the majority of latency arises from the large-scale step where most tokens have already converged. Leveraging this observation, we develop the cached token pruning strategy that only forwards pivotal tokens for scale-specific modeling while using cached tokens from previous scale steps to restore the pruned slots. This significantly reduces the number of forwarded tokens and improves the efficiency at larger resolutions. Experiments show the proposed FastVAR can further speedup FlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop of <1%. We further extend FastVAR to zero-shot generation of higher resolution images. In particular, FastVAR can generate one 2K image with 15GB memory footprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at https://github.com/csguoh/FastVAR.",
        "arxiv_id": "2503.23367",
        "ARXIVID": "2503.23367",
        "COMMENT": "This paper does not match any specific criteria. It focuses on accelerating visual autoregressive modeling, which is not directly related to the specified interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23725": {
        "authors": [
            "Hongwei Ren",
            "Xiaopeng Lin",
            "Hongxiang Huang",
            "Yue Zhou",
            "Bojun Cheng"
        ],
        "title": "Exploring Temporal Dynamics in Event-based Eye Tracker",
        "abstract": "arXiv:2503.23725v1 Announce Type: new  Abstract: Eye-tracking is a vital technology for human-computer interaction, especially in wearable devices such as AR, VR, and XR. The realization of high-speed and high-precision eye-tracking using frame-based image sensors is constrained by their limited temporal resolution, which impairs the accurate capture of rapid ocular dynamics, such as saccades and blinks. Event cameras, inspired by biological vision systems, are capable of perceiving eye movements with extremely low power consumption and ultra-high temporal resolution. This makes them a promising solution for achieving high-speed, high-precision tracking with rich temporal dynamics. In this paper, we propose TDTracker, an effective eye-tracking framework that captures rapid eye movements by thoroughly modeling temporal dynamics from both implicit and explicit perspectives. TDTracker utilizes 3D convolutional neural networks to capture implicit short-term temporal dynamics and employs a cascaded structure consisting of a Frequency-aware Module, GRU, and Mamba to extract explicit long-term temporal dynamics. Ultimately, a prediction heatmap is used for eye coordinate regression. Experimental results demonstrate that TDTracker achieves state-of-the-art (SOTA) performance on the synthetic SEET dataset and secured Third place in the CVPR event-based eye-tracking challenge 2025. Our code is available at https://github.com/rhwxmx/TDTracker.",
        "arxiv_id": "2503.23725",
        "ARXIVID": "2503.23725",
        "COMMENT": "This paper does not match any specific criteria. It focuses on eye-tracking using event cameras, which is not directly related to the specified interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.24121": {
        "authors": [
            "Valentin Boussot",
            "C\\'edric H\\'emon",
            "Jean-Claude Nunes",
            "Jason Downling",
            "Simon Rouz\\'e",
            "Caroline Lafond",
            "Ana\\\"is Barateau",
            "Jean-Louis Dillenseger"
        ],
        "title": "IMPACT: A Generic Semantic Loss for Multimodal Medical Image Registration",
        "abstract": "arXiv:2503.24121v1 Announce Type: new  Abstract: Image registration is fundamental in medical imaging, enabling precise alignment of anatomical structures for diagnosis, treatment planning, image-guided treatment or longitudinal monitoring. This work introduces IMPACT (Image Metric with Pretrained model-Agnostic Comparison for Transmodality registration), a generic semantic similarity metric designed for seamless integration into diverse image registration frameworks (such as Elastix and Voxelmorph). It compares deep learning-based features extracted from medical images without requiring task-specific training, ensuring broad applicability across various modalities. By leveraging the features of the large-scale pretrained TotalSegmentator models and the ability to integrate Segment Anything Model (SAM) and other large-scale segmentation networks, this approach offers significant advantages. It provides robust, scalable, and efficient solutions for multimodal image registration. The IMPACT loss was evaluated on five challenging registration tasks involving thoracic CT/CBCT, and pelvic MR/CT datasets. Quantitative metrics, such as Target Registration Error and Dice Similarity Coefficient, demonstrated significant improvements in anatomical alignment compared to baseline methods. Qualitative analyses further confirmed the increased robustness of the proposed metric in the face of noise, artifacts, and modality variations. IMPACT's versatility and efficiency make it a valuable tool for advancing registration performance in clinical and research applications, addressing critical challenges in multimodal medical imaging.",
        "arxiv_id": "2503.24121",
        "ARXIVID": "2503.24121",
        "COMMENT": "Does not match any specific criterion but is tangentially related to general interest in multimodal image registration.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23355": {
        "authors": [
            "Linfeng Tang",
            "Chunyu Li",
            "Guoqing Wang",
            "Yixuan Yuan",
            "Jiayi Ma"
        ],
        "title": "DSPFusion: Image Fusion via Degradation and Semantic Dual-Prior Guidance",
        "abstract": "arXiv:2503.23355v1 Announce Type: new  Abstract: Existing fusion methods are tailored for high-quality images but struggle with degraded images captured under harsh circumstances, thus limiting the practical potential of image fusion. This work presents a \\textbf{D}egradation and \\textbf{S}emantic \\textbf{P}rior dual-guided framework for degraded image \\textbf{Fusion} (\\textbf{DSPFusion}), utilizing degradation priors and high-quality scene semantic priors restored via diffusion models to guide both information recovery and fusion in a unified model. In specific, it first individually extracts modality-specific degradation priors, while jointly capturing comprehensive low-quality semantic priors. Subsequently, a diffusion model is developed to iteratively restore high-quality semantic priors in a compact latent space, enabling our method to be over $20 \\times$ faster than mainstream diffusion model-based image fusion schemes. Finally, the degradation priors and high-quality semantic priors are employed to guide information enhancement and aggregation via the dual-prior guidance and prior-guided fusion modules. Extensive experiments demonstrate that DSPFusion mitigates most typical degradations while integrating complementary context with minimal computational cost, greatly broadening the application scope of image fusion.",
        "arxiv_id": "2503.23355",
        "ARXIVID": "2503.23355",
        "COMMENT": "Does not match any specific criterion but is tangentially related to general interest in image fusion and vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.23212": {
        "authors": [
            "Max Gupta",
            "Sunayana Rane",
            "R. Thomas McCoy",
            "Thomas L. Griffiths"
        ],
        "title": "Convolutional Neural Networks Can (Meta-)Learn the Same-Different Relation",
        "abstract": "arXiv:2503.23212v2 Announce Type: new  Abstract: While convolutional neural networks (CNNs) have come to match and exceed human performance in many settings, the tasks these models optimize for are largely constrained to the level of individual objects, such as classification and captioning. Humans remain vastly superior to CNNs in visual tasks involving relations, including the ability to identify two objects as `same' or `different'. A number of studies have shown that while CNNs can be coaxed into learning the same-different relation in some settings, they tend to generalize poorly to other instances of this relation. In this work we show that the same CNN architectures that fail to generalize the same-different relation with conventional training are able to succeed when trained via meta-learning, which explicitly encourages abstraction and generalization across tasks.",
        "arxiv_id": "2503.23212",
        "ARXIVID": "2503.23212",
        "COMMENT": "Does not match any specific criterion but is tangentially related to general interest in machine learning and vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.22936": {
        "authors": [
            "Pei-Kai Huanga",
            "Jun-Xiong Chong",
            "Ming-Tsung Hsu",
            "Fang-Yu Hsu",
            "Chiou-Ting Hsu"
        ],
        "title": "Enhancing Learnable Descriptive Convolutional Vision Transformer for Face Anti-Spoofing",
        "abstract": "arXiv:2503.22936v1 Announce Type: new  Abstract: Face anti-spoofing (FAS) heavily relies on identifying live/spoof discriminative features to counter face presentation attacks. Recently, we proposed LDCformer to successfully incorporate the Learnable Descriptive Convolution (LDC) into ViT, to model long-range dependency of locally descriptive features for FAS. In this paper, we propose three novel training strategies to effectively enhance the training of LDCformer to largely boost its feature characterization capability. The first strategy, dual-attention supervision, is developed to learn fine-grained liveness features guided by regional live/spoof attentions. The second strategy, self-challenging supervision, is designed to enhance the discriminability of the features by generating challenging training data. In addition, we propose a third training strategy, transitional triplet mining strategy, through narrowing the cross-domain gap while maintaining the transitional relationship between live and spoof features, to enlarge the domain-generalization capability of LDCformer. Extensive experiments show that LDCformer under joint supervision of the three novel training strategies outperforms previous methods.",
        "arxiv_id": "2503.22936",
        "ARXIVID": "2503.22936",
        "COMMENT": "Does not match any specific criteria. Focuses on face anti-spoofing with training strategies for a vision transformer.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.23275": {
        "authors": [
            "Deeksha Arun",
            "Kagan Ozturk",
            "Kevin W. Bowyer",
            "Patrick Flynn"
        ],
        "title": "Improved Ear Verification with Vision Transformers and Overlapping Patches",
        "abstract": "arXiv:2503.23275v1 Announce Type: new  Abstract: Ear recognition has emerged as a promising biometric modality due to the relative stability in appearance during adulthood. Although Vision Transformers (ViTs) have been widely used in image recognition tasks, their efficiency in ear recognition has been hampered by a lack of attention to overlapping patches, which is crucial for capturing intricate ear features. In this study, we evaluate ViT-Tiny (ViT-T), ViT-Small (ViT-S), ViT-Base (ViT-B) and ViT-Large (ViT-L) configurations on a diverse set of datasets (OPIB, AWE, WPUT, and EarVN1.0), using an overlapping patch selection strategy. Results demonstrate the critical importance of overlapping patches, yielding superior performance in 44 of 48 experiments in a structured study. Moreover, upon comparing the results of the overlapping patches with the non-overlapping configurations, the increase is significant, reaching up to 10% for the EarVN1.0 dataset. In terms of model performance, the ViT-T model consistently outperformed the ViT-S, ViT-B, and ViT-L models on the AWE, WPUT, and EarVN1.0 datasets. The highest scores were achieved in a configuration with a patch size of 28x28 and a stride of 14 pixels. This patch-stride configuration represents 25% of the normalized image area (112x112 pixels) for the patch size and 12.5% of the row or column size for the stride. This study confirms that transformer architectures with overlapping patch selection can serve as an efficient and high-performing option for ear-based biometric recognition tasks in verification scenarios.",
        "arxiv_id": "2503.23275",
        "ARXIVID": "2503.23275",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and transformer-based architectures for biometric recognition.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.00615": {
        "authors": [
            "Danial Hooshyar",
            "Eve Kikas",
            "Yeongwook Yang",
            "Gustav \\v{S}\\'ir",
            "Raija H\\\"am\\\"al\\\"ainen",
            "Tommi K\\\"arkk\\\"ainen",
            "Roger Azevedo"
        ],
        "title": "Towards Responsible and Trustworthy Educational Data Mining: Comparing Symbolic, Sub-Symbolic, and Neural-Symbolic AI Methods",
        "abstract": "arXiv:2504.00615v1 Announce Type: new  Abstract: Given the demand for responsible and trustworthy AI for education, this study evaluates symbolic, sub-symbolic, and neural-symbolic AI (NSAI) in terms of generalizability and interpretability. Our extensive experiments on balanced and imbalanced self-regulated learning datasets of Estonian primary school students predicting 7th-grade mathematics national test performance showed that symbolic and sub-symbolic methods performed well on balanced data but struggled to identify low performers in imbalanced datasets. Interestingly, symbolic and sub-symbolic methods emphasized different factors in their decision-making: symbolic approaches primarily relied on cognitive and motivational factors, while sub-symbolic methods focused more on cognitive aspects, learned knowledge, and the demographic variable of gender -- yet both largely overlooked metacognitive factors. The NSAI method, on the other hand, showed advantages by: (i) being more generalizable across both classes -- even in imbalanced datasets -- as its symbolic knowledge component compensated for the underrepresented class; and (ii) relying on a more integrated set of factors in its decision-making, including motivation, (meta)cognition, and learned knowledge, thus offering a comprehensive and theoretically grounded interpretability framework. These contrasting findings highlight the need for a holistic comparison of AI methods before drawing conclusions based solely on predictive performance. They also underscore the potential of hybrid, human-centered NSAI methods to address the limitations of other AI families and move us closer to responsible AI for education. Specifically, by enabling stakeholders to contribute to AI design, NSAI aligns learned patterns with theoretical constructs, incorporates factors like motivation and metacognition, and strengthens the trustworthiness and responsibility of educational data mining.",
        "arxiv_id": "2504.00615",
        "ARXIVID": "2504.00615",
        "COMMENT": "Does not match any specific criterion but is relevant to AI methods and their applications in education.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.23214": {
        "authors": [
            "Vincent Gbouna Zakka",
            "Zhuangzhuang Dai",
            "Luis J. Manso"
        ],
        "title": "Action Recognition in Real-World Ambient Assisted Living Environment",
        "abstract": "arXiv:2503.23214v1 Announce Type: new  Abstract: The growing ageing population and their preference to maintain independence by living in their own homes require proactive strategies to ensure safety and support. Ambient Assisted Living (AAL) technologies have emerged to facilitate ageing in place by offering continuous monitoring and assistance within the home. Within AAL technologies, action recognition plays a crucial role in interpreting human activities and detecting incidents like falls, mobility decline, or unusual behaviours that may signal worsening health conditions. However, action recognition in practical AAL applications presents challenges, including occlusions, noisy data, and the need for real-time performance. While advancements have been made in accuracy, robustness to noise, and computation efficiency, achieving a balance among them all remains a challenge. To address this challenge, this paper introduces the Robust and Efficient Temporal Convolution network (RE-TCN), which comprises three main elements: Adaptive Temporal Weighting (ATW), Depthwise Separable Convolutions (DSC), and data augmentation techniques. These elements aim to enhance the model's accuracy, robustness against noise and occlusion, and computational efficiency within real-world AAL contexts. RE-TCN outperforms existing models in terms of accuracy, noise and occlusion robustness, and has been validated on four benchmark datasets: NTU RGB+D 60, Northwestern-UCLA, SHREC'17, and DHG-14/28. The code is publicly available at: https://github.com/Gbouna/RE-TCN",
        "arxiv_id": "2503.23214",
        "ARXIVID": "2503.23214",
        "COMMENT": "Does not match any specific criterion but is relevant to action recognition in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.22912": {
        "authors": [
            "Xin Liang",
            "Yogesh S Rawat"
        ],
        "title": "DIFFER: Disentangling Identity Features via Semantic Cues for Clothes-Changing Person Re-ID",
        "abstract": "arXiv:2503.22912v1 Announce Type: new  Abstract: Clothes-changing person re-identification (CC-ReID) aims to recognize individuals under different clothing scenarios. Current CC-ReID approaches either concentrate on modeling body shape using additional modalities including silhouette, pose, and body mesh, potentially causing the model to overlook other critical biometric traits such as gender, age, and style, or they incorporate supervision through additional labels that the model tries to disregard or emphasize, such as clothing or personal attributes. However, these annotations are discrete in nature and do not capture comprehensive descriptions.   In this work, we propose DIFFER: Disentangle Identity Features From Entangled Representations, a novel adversarial learning method that leverages textual descriptions to disentangle identity features. Recognizing that image features inherently mix inseparable information, DIFFER introduces NBDetach, a mechanism designed for feature disentanglement by leveraging the separable nature of text descriptions as supervision. It partitions the feature space into distinct subspaces and, through gradient reversal layers, effectively separates identity-related features from non-biometric features. We evaluate DIFFER on 4 different benchmark datasets (LTCC, PRCC, CelebreID-Light, and CCVID) to demonstrate its effectiveness and provide state-of-the-art performance across all the benchmarks. DIFFER consistently outperforms the baseline method, with improvements in top-1 accuracy of 3.6% on LTCC, 3.4% on PRCC, 2.5% on CelebReID-Light, and 1% on CCVID. Our code can be found here.",
        "arxiv_id": "2503.22912",
        "ARXIVID": "2503.22912",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision applications.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.23422": {
        "authors": [
            "Xin Zuo",
            "Jiaran Jiang",
            "Jifeng Shen",
            "Wankou Yang"
        ],
        "title": "Improving underwater semantic segmentation with underwater image quality attention and muti-scale aggregation attention",
        "abstract": "arXiv:2503.23422v1 Announce Type: new  Abstract: Underwater image understanding is crucial for both submarine navigation and seabed exploration. However, the low illumination in underwater environments degrades the imaging quality, which in turn seriously deteriorates the performance of underwater semantic segmentation, particularly for outlining the object region boundaries. To tackle this issue, we present UnderWater SegFormer (UWSegFormer), a transformer-based framework for semantic segmentation of low-quality underwater images. Firstly, we propose the Underwater Image Quality Attention (UIQA) module. This module enhances the representation of highquality semantic information in underwater image feature channels through a channel self-attention mechanism. In order to address the issue of loss of imaging details due to the underwater environment, the Multi-scale Aggregation Attention(MAA) module is proposed. This module aggregates sets of semantic features at different scales by extracting discriminative information from high-level features,thus compensating for the semantic loss of detail in underwater objects. Finally, during training, we introduce Edge Learning Loss (ELL) in order to enhance the model's learning of underwater object edges and improve the model's prediction accuracy. Experiments conducted on the SUIM and DUT-USEG (DUT) datasets have demonstrated that the proposed method has advantages in terms of segmentation completeness, boundary clarity, and subjective perceptual details when compared to SOTA methods. In addition, the proposed method achieves the highest mIoU of 82.12 and 71.41 on the SUIM and DUT datasets, respectively. Code will be available at https://github.com/SAWRJJ/UWSegFormer.",
        "arxiv_id": "2503.23422",
        "ARXIVID": "2503.23422",
        "COMMENT": "Does not match any specific criterion but is related to computer vision applications in underwater environments, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.23965": {
        "authors": [
            "Miao Fan",
            "Xuxu Kong",
            "Shengtong Xu",
            "Haoyi Xiong",
            "Xiangzeng Liu"
        ],
        "title": "Video-based Traffic Light Recognition by Rockchip RV1126 for Autonomous Driving",
        "abstract": "arXiv:2503.23965v1 Announce Type: new  Abstract: Real-time traffic light recognition is fundamental for autonomous driving safety and navigation in urban environments. While existing approaches rely on single-frame analysis from onboard cameras, they struggle with complex scenarios involving occlusions and adverse lighting conditions. We present \\textit{ViTLR}, a novel video-based end-to-end neural network that processes multiple consecutive frames to achieve robust traffic light detection and state classification. The architecture leverages a transformer-like design with convolutional self-attention modules, which is optimized specifically for deployment on the Rockchip RV1126 embedded platform. Extensive evaluations on two real-world datasets demonstrate that \\textit{ViTLR} achieves state-of-the-art performance while maintaining real-time processing capabilities (>25 FPS) on RV1126's NPU. The system shows superior robustness across temporal stability, varying target distances, and challenging environmental conditions compared to existing single-frame approaches. We have successfully integrated \\textit{ViTLR} into an ego-lane traffic light recognition system using HD maps for autonomous driving applications. The complete implementation, including source code and datasets, is made publicly available to facilitate further research in this domain.",
        "arxiv_id": "2503.23965",
        "ARXIVID": "2503.23965",
        "COMMENT": "Does not match any specific criterion but is related to video-based traffic light recognition for autonomous driving, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.23519": {
        "authors": [
            "Haruya Ishikawa",
            "Yoshimitsu Aoki"
        ],
        "title": "BoundMatch: Boundary detection applied to semi-supervised segmentation for urban-driving scenes",
        "abstract": "arXiv:2503.23519v1 Announce Type: new  Abstract: Semi-supervised semantic segmentation (SS-SS) aims to mitigate the heavy annotation burden of dense pixel labeling by leveraging abundant unlabeled images alongside a small labeled set. While current teacher-student consistency regularization methods achieve strong results, they often overlook a critical challenge: the precise delineation of object boundaries. In this paper, we propose BoundMatch, a novel multi-task SS-SS framework that explicitly integrates semantic boundary detection into the consistency regularization pipeline. Our core mechanism, Boundary Consistency Regularized Multi-Task Learning (BCRM), enforces prediction agreement between teacher and student models on both segmentation masks and detailed semantic boundaries. To further enhance performance and sharpen contours, BoundMatch incorporates two lightweight fusion modules: Boundary-Semantic Fusion (BSF) injects learned boundary cues into the segmentation decoder, while Spatial Gradient Fusion (SGF) refines boundary predictions using mask gradients, leading to higher-quality boundary pseudo-labels. This framework is built upon SAMTH, a strong teacher-student baseline featuring a Harmonious Batch Normalization (HBN) update strategy for improved stability. Extensive experiments on diverse datasets including Cityscapes, BDD100K, SYNTHIA, ADE20K, and Pascal VOC show that BoundMatch achieves competitive performance against state-of-the-art methods while significantly improving boundary-specific evaluation metrics. We also demonstrate its effectiveness in realistic large-scale unlabeled data scenarios and on lightweight architectures designed for mobile deployment.",
        "arxiv_id": "2503.23519",
        "ARXIVID": "2503.23519",
        "COMMENT": "Does not match any specific criterion but is related to semi-supervised segmentation and boundary detection, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.24096": {
        "authors": [
            "Adrienne Deganutti",
            "Simon Hadfield",
            "Andrew Gilbert"
        ],
        "title": "DANTE-AD: Dual-Vision Attention Network for Long-Term Audio Description",
        "abstract": "arXiv:2503.24096v1 Announce Type: new  Abstract: Audio Description is a narrated commentary designed to aid vision-impaired audiences in perceiving key visual elements in a video. While short-form video understanding has advanced rapidly, a solution for maintaining coherent long-term visual storytelling remains unresolved. Existing methods rely solely on frame-level embeddings, effectively describing object-based content but lacking contextual information across scenes. We introduce DANTE-AD, an enhanced video description model leveraging a dual-vision Transformer-based architecture to address this gap. DANTE-AD sequentially fuses both frame and scene level embeddings to improve long-term contextual understanding. We propose a novel, state-of-the-art method for sequential cross-attention to achieve contextual grounding for fine-grained audio description generation. Evaluated on a broad range of key scenes from well-known movie clips, DANTE-AD outperforms existing methods across traditional NLP metrics and LLM-based evaluations.",
        "arxiv_id": "2503.24096",
        "ARXIVID": "2503.24096",
        "COMMENT": "Does not match any specific criterion but is related to video understanding and generative modeling for audio description, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}