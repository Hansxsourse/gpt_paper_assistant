{
    "2502.08639": {
        "authors": [
            "Qinghe Wang",
            "Yawen Luo",
            "Xiaoyu Shi",
            "Xu Jia",
            "Huchuan Lu",
            "Tianfan Xue",
            "Xintao Wang",
            "Pengfei Wan",
            "Di Zhang",
            "Kun Gai"
        ],
        "title": "CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation",
        "abstract": "arXiv:2502.08639v1 Announce Type: new  Abstract: In this work, we present CineMaster, a novel framework for 3D-aware and controllable text-to-video generation. Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera in 3D space, and intuitive layout control over the rendered frames. To achieve this, CineMaster operates in two stages. In the first stage, we design an interactive workflow that allows users to intuitively construct 3D-aware conditional signals by positioning object bounding boxes and defining camera movements within the 3D space. In the second stage, these control signals--comprising rendered depth maps, camera trajectories and object class labels--serve as the guidance for a text-to-video diffusion model, ensuring to generate the user-intended video content. Furthermore, to overcome the scarcity of in-the-wild datasets with 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline that extracts 3D bounding boxes and camera trajectories from large-scale video data. Extensive qualitative and quantitative experiments demonstrate that CineMaster significantly outperforms existing methods and implements prominent 3D-aware text-to-video generation. Project page: https://cinemaster-dev.github.io/.",
        "arxiv_id": "2502.08639",
        "ARXIVID": "2502.08639",
        "COMMENT": "This paper introduces a 3D-aware and controllable text-to-video generation framework, which aligns with criterion 2 on VLLMs/MLLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2502.08636": {
        "authors": [
            "Xingrui Wang",
            "Wufei Ma",
            "Tiezheng Zhang",
            "Celso M de Melo",
            "Jieneng Chen",
            "Alan Yuille"
        ],
        "title": "PulseCheck457: A Diagnostic Benchmark for Comprehensive Spatial Reasoning of Large Multimodal Models",
        "abstract": "arXiv:2502.08636v1 Announce Type: new  Abstract: Although large multimodal models (LMMs) have demonstrated remarkable capabilities in visual scene interpretation and reasoning, their capacity for complex and precise 3-dimensional spatial reasoning remains uncertain. Existing benchmarks focus predominantly on 2D spatial understanding and lack a framework to comprehensively evaluate 6D spatial reasoning across varying complexities. To address this limitation, we present PulseCheck457, a scalable and unbiased synthetic dataset designed with 4 key capability for spatial reasoning: multi-object recognition, 2D location, 3D location, and 3D orientation. We develop a cascading evaluation structure, constructing 7 question types across 5 difficulty levels that range from basic single object recognition to our new proposed complex 6D spatial reasoning tasks. We evaluated various large multimodal models (LMMs) on PulseCheck457, observing a general decline in performance as task complexity increases, particularly in 3D reasoning and 6D spatial tasks. To quantify these challenges, we introduce the Relative Performance Dropping Rate (RPDR), highlighting key weaknesses in 3D reasoning capabilities. Leveraging the unbiased attribute design of our dataset, we also uncover prediction biases across different attributes, with similar patterns observed in real-world image settings.",
        "arxiv_id": "2502.08636",
        "ARXIVID": "2502.08636",
        "COMMENT": "This paper introduces a new benchmark for spatial reasoning in large multimodal models, directly matching criterion 3.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2502.07869": {
        "authors": [
            "Christen Millerdurai",
            "Hiroyasu Akada",
            "Jian Wang",
            "Diogo Luvizon",
            "Alain Pagani",
            "Didier Stricker",
            "Christian Theobalt",
            "Vladislav Golyanik"
        ],
        "title": "EventEgo3D++: 3D Human Motion Capture from a Head-Mounted Event Camera",
        "abstract": "arXiv:2502.07869v1 Announce Type: new  Abstract: Monocular egocentric 3D human motion capture remains a significant challenge, particularly under conditions of low lighting and fast movements, which are common in head-mounted device applications. Existing methods that rely on RGB cameras often fail under these conditions. To address these limitations, we introduce EventEgo3D++, the first approach that leverages a monocular event camera with a fisheye lens for 3D human motion capture. Event cameras excel in high-speed scenarios and varying illumination due to their high temporal resolution, providing reliable cues for accurate 3D human motion capture. EventEgo3D++ leverages the LNES representation of event streams to enable precise 3D reconstructions. We have also developed a mobile head-mounted device (HMD) prototype equipped with an event camera, capturing a comprehensive dataset that includes real event observations from both controlled studio environments and in-the-wild settings, in addition to a synthetic dataset. Additionally, to provide a more holistic dataset, we include allocentric RGB streams that offer different perspectives of the HMD wearer, along with their corresponding SMPL body model. Our experiments demonstrate that EventEgo3D++ achieves superior 3D accuracy and robustness compared to existing solutions, even in challenging conditions. Moreover, our method supports real-time 3D pose updates at a rate of 140Hz. This work is an extension of the EventEgo3D approach (CVPR 2024) and further advances the state of the art in egocentric 3D human motion capture. For more details, visit the project page at https://eventego3d.mpi-inf.mpg.de.",
        "arxiv_id": "2502.07869",
        "ARXIVID": "2502.07869",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for egocentric 3D human motion capture using event cameras, which is a new angle in embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2502.08079": {
        "authors": [
            "Peng-Fei Zhang",
            "Guangdong Bai",
            "Zi Huang"
        ],
        "title": "MAA: Meticulous Adversarial Attack against Vision-Language Pre-trained Models",
        "abstract": "arXiv:2502.08079v1 Announce Type: new  Abstract: Current adversarial attacks for evaluating the robustness of vision-language pre-trained (VLP) models in multi-modal tasks suffer from limited transferability, where attacks crafted for a specific model often struggle to generalize effectively across different models, limiting their utility in assessing robustness more broadly. This is mainly attributed to the over-reliance on model-specific features and regions, particularly in the image modality. In this paper, we propose an elegant yet highly effective method termed Meticulous Adversarial Attack (MAA) to fully exploit model-independent characteristics and vulnerabilities of individual samples, achieving enhanced generalizability and reduced model dependence. MAA emphasizes fine-grained optimization of adversarial images by developing a novel resizing and sliding crop (RScrop) technique, incorporating a multi-granularity similarity disruption (MGSD) strategy. Extensive experiments across diverse VLP models, multiple benchmark datasets, and a variety of downstream tasks demonstrate that MAA significantly enhances the effectiveness and transferability of adversarial attacks. A large cohort of performance studies is conducted to generate insights into the effectiveness of various model configurations, guiding future advancements in this domain.",
        "arxiv_id": "2502.08079",
        "ARXIVID": "2502.08079",
        "COMMENT": "Matches criterion 2 as it focuses on vision-language pre-trained models and proposes a novel adversarial attack method, which is relevant to VLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.07709": {
        "authors": [
            "Loris Gaven",
            "Thomas Carta",
            "Cl\\'ement Romac",
            "C\\'edric Colas",
            "Sylvain Lamprier",
            "Olivier Sigaud",
            "Pierre-Yves Oudeyer"
        ],
        "title": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces",
        "abstract": "arXiv:2502.07709v2 Announce Type: new  Abstract: Open-ended learning agents must efficiently prioritize goals in vast possibility spaces, focusing on those that maximize learning progress (LP). When such autotelic exploration is achieved by LLM agents trained with online RL in high-dimensional and evolving goal spaces, a key challenge for LP prediction is modeling one's own competence, a form of metacognitive monitoring. Traditional approaches either require extensive sampling or rely on brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive framework that lets LLM agents learn to predict their competence and LP online. By capturing semantic relationships between goals, MAGELLAN enables sample-efficient LP estimation and dynamic adaptation to evolving goal spaces through generalization. In an interactive learning environment, we show that MAGELLAN improves LP prediction efficiency and goal prioritization, being the only method allowing the agent to fully master a large and evolving goal space. These results demonstrate how augmenting LLM agents with a metacognitive ability for LP predictions can effectively scale curriculum learning to open-ended goal spaces.",
        "arxiv_id": "2502.07709",
        "ARXIVID": "2502.07709",
        "COMMENT": "Matches criterion 1 as it introduces a metacognitive framework for LLM agents to improve spatial understanding and goal prioritization in large goal spaces.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.08468": {
        "authors": [
            "Haonan Chen",
            "Liang Wang",
            "Nan Yang",
            "Yutao Zhu",
            "Ziliang Zhao",
            "Furu Wei",
            "Zhicheng Dou"
        ],
        "title": "mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data",
        "abstract": "arXiv:2502.08468v1 Announce Type: new  Abstract: Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. In this work, we identify three criteria for high-quality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets and models are released in https://github.com/haon-chen/mmE5.",
        "arxiv_id": "2502.08468",
        "ARXIVID": "2502.08468",
        "COMMENT": "Matches criterion 2 as it discusses improvements in multimodal multilingual embeddings using synthetic data.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.07838": {
        "authors": [
            "Mukund Agarwalla",
            "Himanshu Kumar",
            "Raj Dandekar",
            "Rajat Dandekar",
            "Sreedath Panat"
        ],
        "title": "NanoVLMs: How small can we go and still make coherent Vision Language Models?",
        "abstract": "arXiv:2502.07838v1 Announce Type: new  Abstract: Vision-Language Models (VLMs), such as GPT-4V and Llama 3.2 vision, have garnered significant research attention for their ability to leverage Large Language Models (LLMs) in multimodal tasks. However, their potential is constrained by inherent challenges, including proprietary restrictions, substantial computational demands, and limited accessibility. Smaller models, such as GIT and BLIP, exhibit marked limitations, often failing to generate coherent and consistent text beyond a few tokens, even with extensive training. This underscores a pivotal inquiry: how small can a VLM be and still produce fluent and consistent text? Drawing inspiration from the exceptional learning process of 3-4 year old children, who rely heavily on visual cues for understanding and communication, we introduce two novel datasets: ShortDesc (featuring concise image descriptions) and LongDesc (containing more detailed image descriptions). These datasets consist of image-text pairs where the text is restricted to the simple vocabulary and syntax typically used by young children, generated with a scaled- down model, GPT-4o. Using these datasets, we demonstrate that it is possible to train VLMs that are significantly smaller, up to 10 times smaller than state of the art(SOTA) small VLMs while maintaining architectural simplicity. To evaluate the outputs, we leverage GPT-4o to grade the text, as if stories written by students, on creativity, meaningfulness, and consistency, assigning scores out of 10. This method addresses limitations of standard benchmarks by accommodating unstructured outputs and providing a multidimensional evaluation of the model capabilities. Our findings contribute to the development of lightweight, accessible multimodal models for resource constrained environments.",
        "arxiv_id": "2502.07838",
        "ARXIVID": "2502.07838",
        "COMMENT": "Matches criterion 2 as it discusses new methodological improvements for smaller Vision-Language Models (VLMs).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.08169": {
        "authors": [
            "Yunjiang Xu",
            "Lingzhi Li",
            "Jin Wang",
            "Benyuan Yang",
            "Zhiwen Wu",
            "Xinhong Chen",
            "Jianping Wang"
        ],
        "title": "CoDynTrust: Robust Asynchronous Collaborative Perception via Dynamic Feature Trust Modulus",
        "abstract": "arXiv:2502.08169v1 Announce Type: new  Abstract: Collaborative perception, fusing information from multiple agents, can extend perception range so as to improve perception performance. However, temporal asynchrony in real-world environments, caused by communication delays, clock misalignment, or sampling configuration differences, can lead to information mismatches. If this is not well handled, then the collaborative performance is patchy, and what's worse safety accidents may occur. To tackle this challenge, we propose CoDynTrust, an uncertainty-encoded asynchronous fusion perception framework that is robust to the information mismatches caused by temporal asynchrony. CoDynTrust generates dynamic feature trust modulus (DFTM) for each region of interest by modeling aleatoric and epistemic uncertainty as well as selectively suppressing or retaining single-vehicle features, thereby mitigating information mismatches. We then design a multi-scale fusion module to handle multi-scale feature maps processed by DFTM. Compared to existing works that also consider asynchronous collaborative perception, CoDynTrust combats various low-quality information in temporally asynchronous scenarios and allows uncertainty to be propagated to downstream tasks such as planning and control. Experimental results demonstrate that CoDynTrust significantly reduces performance degradation caused by temporal asynchrony across multiple datasets, achieving state-of-the-art detection performance even with temporal asynchrony. The code is available at https://github.com/CrazyShout/CoDynTrust.",
        "arxiv_id": "2502.08169",
        "ARXIVID": "2502.08169",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework for asynchronous collaborative perception, which is relevant to embodied AI and introduces a new method for handling temporal asynchrony.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.08391": {
        "authors": [
            "Jiangbo Shi",
            "Chen Li",
            "Tieliang Gong",
            "Yefeng Zheng",
            "Huazhu Fu"
        ],
        "title": "ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for Whole Slide Image Classification",
        "abstract": "arXiv:2502.08391v1 Announce Type: new  Abstract: Multiple instance learning (MIL)-based framework has become the mainstream for processing the whole slide image (WSI) with giga-pixel size and hierarchical image context in digital pathology. However, these methods heavily depend on a substantial number of bag-level labels and solely learn from the original slides, which are easily affected by variations in data distribution. Recently, vision language model (VLM)-based methods introduced the language prior by pre-training on large-scale pathological image-text pairs. However, the previous text prompt lacks the consideration of pathological prior knowledge, therefore does not substantially boost the model's performance. Moreover, the collection of such pairs and the pre-training process are very time-consuming and source-intensive.To solve the above problems, we propose a dual-scale vision-language multiple instance learning (ViLa-MIL) framework for whole slide image classification. Specifically, we propose a dual-scale visual descriptive text prompt based on the frozen large language model (LLM) to boost the performance of VLM effectively. To transfer the VLM to process WSI efficiently, for the image branch, we propose a prototype-guided patch decoder to aggregate the patch features progressively by grouping similar patches into the same prototype; for the text branch, we introduce a context-guided text decoder to enhance the text features by incorporating the multi-granular image contexts. Extensive studies on three multi-cancer and multi-center subtyping datasets demonstrate the superiority of ViLa-MIL.",
        "arxiv_id": "2502.08391",
        "ARXIVID": "2502.08391",
        "COMMENT": "Matches criterion 4 as it proposes a dual-scale vision-language model for whole slide image classification, aligning with vision foundation models and their applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.07811": {
        "authors": [
            "Shihab Aaqil Ahamed",
            "Malitha Gunawardhana",
            "Liel David",
            "Michael Sidorov",
            "Daniel Harari",
            "Muhammad Haris Khan"
        ],
        "title": "CrossVideoMAE: Self-Supervised Image-Video Representation Learning with Masked Autoencoders",
        "abstract": "arXiv:2502.07811v1 Announce Type: new  Abstract: Current video-based Masked Autoencoders (MAEs) primarily focus on learning effective spatiotemporal representations from a visual perspective, which may lead the model to prioritize general spatial-temporal patterns but often overlook nuanced semantic attributes like specific interactions or sequences that define actions - such as action-specific features that align more closely with human cognition for space-time correspondence. This can limit the model's ability to capture the essence of certain actions that are contextually rich and continuous. Humans are capable of mapping visual concepts, object view invariance, and semantic attributes available in static instances to comprehend natural dynamic scenes or videos. Existing MAEs for videos and static images rely on separate datasets for videos and images, which may lack the rich semantic attributes necessary for fully understanding the learned concepts, especially when compared to using video and corresponding sampled frame images together. To this end, we propose CrossVideoMAE an end-to-end self-supervised cross-modal contrastive learning MAE that effectively learns both video-level and frame-level rich spatiotemporal representations and semantic attributes. Our method integrates mutual spatiotemporal information from videos with spatial information from sampled frames within a feature-invariant space, while encouraging invariance to augmentations within the video domain. This objective is achieved through jointly embedding features of visible tokens and combining feature correspondence within and across modalities, which is critical for acquiring rich, label-free guiding signals from both video and frame image modalities in a self-supervised manner. Extensive experiments demonstrate that our approach surpasses previous state-of-the-art methods and ablation studies validate the effectiveness of our approach.",
        "arxiv_id": "2502.07811",
        "ARXIVID": "2502.07811",
        "COMMENT": "Matches criterion 4 as it proposes a novel self-supervised learning method for video and image representation learning, which aligns with vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.08333": {
        "authors": [
            "Mohsin Bilal",
            "Aadam",
            "Manahil Raza",
            "Youssef Altherwy",
            "Anas Alsuhaibani",
            "Abdulrahman Abduljabbar",
            "Fahdah Almarshad",
            "Paul Golding",
            "Nasir Rajpoot"
        ],
        "title": "Foundation Models in Computational Pathology: A Review of Challenges, Opportunities, and Impact",
        "abstract": "arXiv:2502.08333v1 Announce Type: new  Abstract: From self-supervised, vision-only models to contrastive visual-language frameworks, computational pathology has rapidly evolved in recent years. Generative AI \"co-pilots\" now demonstrate the ability to mine subtle, sub-visual tissue cues across the cellular-to-pathology spectrum, generate comprehensive reports, and respond to complex user queries. The scale of data has surged dramatically, growing from tens to millions of multi-gigapixel tissue images, while the number of trainable parameters in these models has risen to several billion. The critical question remains: how will this new wave of generative and multi-purpose AI transform clinical diagnostics? In this article, we explore the true potential of these innovations and their integration into clinical practice. We review the rapid progress of foundation models in pathology, clarify their applications and significance. More precisely, we examine the very definition of foundational models, identifying what makes them foundational, general, or multipurpose, and assess their impact on computational pathology. Additionally, we address the unique challenges associated with their development and evaluation. These models have demonstrated exceptional predictive and generative capabilities, but establishing global benchmarks is crucial to enhancing evaluation standards and fostering their widespread clinical adoption. In computational pathology, the broader impact of frontier AI ultimately depends on widespread adoption and societal acceptance. While direct public exposure is not strictly necessary, it remains a powerful tool for dispelling misconceptions, building trust, and securing regulatory support.",
        "arxiv_id": "2502.08333",
        "ARXIVID": "2502.08333",
        "COMMENT": "Matches criterion 4 as it reviews the applications of vision foundation models in computational pathology.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.07202": {
        "authors": [
            "Jaesik Yoon",
            "Hyeonseo Cho",
            "Doojin Baek",
            "Yoshua Bengio",
            "Sungjin Ahn"
        ],
        "title": "Monte Carlo Tree Diffusion for System 2 Planning",
        "abstract": "arXiv:2502.07202v1 Announce Type: new  Abstract: Diffusion models have recently emerged as a powerful tool for planning. However, unlike Monte Carlo Tree Search (MCTS)-whose performance naturally improves with additional test-time computation (TTC), standard diffusion-based planners offer only limited avenues for TTC scalability. In this paper, we introduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates the generative strength of diffusion models with the adaptive search capabilities of MCTS. Our method reconceptualizes denoising as a tree-structured process, allowing partially denoised plans to be iteratively evaluated, pruned, and refined. By selectively expanding promising trajectories while retaining the flexibility to revisit and improve suboptimal branches, MCTD achieves the benefits of MCTS such as controlling exploration-exploitation trade-offs within the diffusion framework. Empirical results on challenging long-horizon tasks show that MCTD outperforms diffusion baselines, yielding higher-quality solutions as TTC increases.",
        "arxiv_id": "2502.07202",
        "ARXIVID": "2502.07202",
        "COMMENT": "Matches criterion 3 as it introduces a novel planning framework combining diffusion models and MCTS, which is relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2502.08254": {
        "authors": [
            "Maximilian Jaritz",
            "Matthieu Guillaumin",
            "Sabine Sternig",
            "Loris Bazzani"
        ],
        "title": "UniCoRN: Unified Commented Retrieval Network with LMMs",
        "abstract": "arXiv:2502.08254v1 Announce Type: new  Abstract: Multimodal retrieval methods have limitations in handling complex, compositional queries that require reasoning about the visual content of both the query and the retrieved entities. On the other hand, Large Multimodal Models (LMMs) can answer with language to more complex visual questions, but without the inherent ability to retrieve relevant entities to support their answers. We aim to address these limitations with UniCoRN, a Unified Commented Retrieval Network that combines the strengths of composed multimodal retrieval methods and generative language approaches, going beyond Retrieval-Augmented Generation (RAG). We introduce an entity adapter module to inject the retrieved multimodal entities back into the LMM, so it can attend to them while generating answers and comments. By keeping the base LMM frozen, UniCoRN preserves its original capabilities while being able to perform both retrieval and text generation tasks under a single integrated framework. To assess these new abilities, we introduce the Commented Retrieval task (CoR) and a corresponding dataset, with the goal of retrieving an image that accurately answers a given question and generate an additional textual response that provides further clarification and details about the visual information. We demonstrate the effectiveness of UniCoRN on several datasets showing improvements of +4.5% recall over the state of the art for composed multimodal retrieval and of +14.9% METEOR / +18.4% BEM over RAG for commenting in CoR.",
        "arxiv_id": "2502.08254",
        "ARXIVID": "2502.08254",
        "COMMENT": "Matches criterion 2 as it introduces a new multimodal retrieval framework leveraging LMMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.08285": {
        "authors": [
            "Weijie Wang",
            "Guofeng Mei",
            "Jian Zhang",
            "Nicu Sebe",
            "Bruno Lepri",
            "Fabio Poiesi"
        ],
        "title": "Fully-Geometric Cross-Attention for Point Cloud Registration",
        "abstract": "arXiv:2502.08285v1 Announce Type: new  Abstract: Point cloud registration approaches often fail when the overlap between point clouds is low due to noisy point correspondences. This work introduces a novel cross-attention mechanism tailored for Transformer-based architectures that tackles this problem, by fusing information from coordinates and features at the super-point level between point clouds. This formulation has remained unexplored primarily because it must guarantee rotation and translation invariance since point clouds reside in different and independent reference frames. We integrate the Gromov-Wasserstein distance into the cross-attention formulation to jointly compute distances between points across different point clouds and account for their geometric structure. By doing so, points from two distinct point clouds can attend to each other under arbitrary rigid transformations. At the point level, we also devise a self-attention mechanism that aggregates the local geometric structure information into point features for fine matching. Our formulation boosts the number of inlier correspondences, thereby yielding more precise registration results compared to state-of-the-art approaches. We have conducted an extensive evaluation on 3DMatch, 3DLoMatch, KITTI, and 3DCSR datasets.",
        "arxiv_id": "2502.08285",
        "ARXIVID": "2502.08285",
        "COMMENT": "This paper introduces a novel cross-attention mechanism for point cloud registration, which could be relevant to spatial understanding (criterion 1).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.07830": {
        "authors": [
            "Wenhao Wang",
            "Adam Dziedzic",
            "Grace C. Kim",
            "Michael Backes",
            "Franziska Boenisch"
        ],
        "title": "Captured by Captions: On Memorization and its Mitigation in CLIP Models",
        "abstract": "arXiv:2502.07830v1 Announce Type: new  Abstract: Multi-modal models, such as CLIP, have demonstrated strong performance in aligning visual and textual representations, excelling in tasks like image retrieval and zero-shot classification. Despite this success, the mechanisms by which these models utilize training data, particularly the role of memorization, remain unclear. In uni-modal models, both supervised and self-supervised, memorization has been shown to be essential for generalization. However, it is not well understood how these findings would apply to CLIP, which incorporates elements from both supervised learning via captions that provide a supervisory signal similar to labels, and from self-supervised learning via the contrastive objective. To bridge this gap in understanding, we propose a formal definition of memorization in CLIP (CLIPMem) and use it to quantify memorization in CLIP models. Our results indicate that CLIP's memorization behavior falls between the supervised and self-supervised paradigms, with \"mis-captioned\" samples exhibiting highest levels of memorization. Additionally, we find that the text encoder contributes more to memorization than the image encoder, suggesting that mitigation strategies should focus on the text domain. Building on these insights, we propose multiple strategies to reduce memorization while at the same time improving utility--something that had not been shown before for traditional learning paradigms where reducing memorization typically results in utility decrease.",
        "arxiv_id": "2502.07830",
        "ARXIVID": "2502.07830",
        "COMMENT": "Matches criterion 4 as it investigates memorization in CLIP models, which are vision-language foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2502.07527": {
        "authors": [
            "Yingce Xia",
            "Peiran Jin",
            "Shufang Xie",
            "Liang He",
            "Chuan Cao",
            "Renqian Luo",
            "Guoqing Liu",
            "Yue Wang",
            "Zequn Liu",
            "Yuan-Jyue Chen",
            "Zekun Guo",
            "Yeqi Bai",
            "Pan Deng",
            "Yaosen Min",
            "Ziheng Lu",
            "Hongxia Hao",
            "Han Yang",
            "Jielan Li",
            "Chang Liu",
            "Jia Zhang",
            "Jianwei Zhu",
            "Kehan Wu",
            "Wei Zhang",
            "Kaiyuan Gao",
            "Qizhi Pei",
            "Qian Wang",
            "Xixian Liu",
            "Yanting Li",
            "Houtian Zhu",
            "Yeqing Lu",
            "Mingqian Ma",
            "Zun Wang",
            "Tian Xie",
            "Krzysztof Maziarz",
            "Marwin Segler",
            "Zhao Yang",
            "Zilong Chen",
            "Yu Shi",
            "Shuxin Zheng",
            "Lijun Wu",
            "Chen Hu",
            "Peggy Dai",
            "Tie-Yan Liu",
            "Haiguang Liu",
            "Tao Qin"
        ],
        "title": "NatureLM: Deciphering the Language of Nature for Scientific Discovery",
        "abstract": "arXiv:2502.07527v1 Announce Type: new  Abstract: Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, and RNA. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the \"language of nature\", we introduce Nature Language Model (briefly, NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) achieving state-of-the-art performance in tasks like SMILES-to-IUPAC translation and retrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.",
        "arxiv_id": "2502.07527",
        "ARXIVID": "2502.07527",
        "COMMENT": "Does not match any specific criteria but introduces a foundation model for scientific discovery across multiple domains.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2502.08352": {
        "authors": [
            "Tianle Liu",
            "Shuangming Zhao",
            "Wanshou Jiang",
            "Bingxuan Guo"
        ],
        "title": "Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images with Depth and Normal Supervision",
        "abstract": "arXiv:2502.08352v1 Announce Type: new  Abstract: With advancements in satellite imaging technology, acquiring high-resolution multi-view satellite imagery has become increasingly accessible, enabling rapid and location-independent ground model reconstruction. However, traditional stereo matching methods struggle to capture fine details, and while neural radiance fields (NeRFs) achieve high-quality reconstructions, their training time is prohibitively long. Moreover, challenges such as low visibility of building facades, illumination and style differences between pixels, and weakly textured regions in satellite imagery further make it hard to reconstruct reasonable terrain geometry and detailed building facades. To address these issues, we propose Sat-DN, a novel framework leveraging a progressively trained multi-resolution hash grid reconstruction architecture with explicit depth guidance and surface normal consistency constraints to enhance reconstruction quality. The multi-resolution hash grid accelerates training, while the progressive strategy incrementally increases the learning frequency, using coarse low-frequency geometry to guide the reconstruction of fine high-frequency details. The depth and normal constraints ensure a clear building outline and correct planar distribution. Extensive experiments on the DFC2019 dataset demonstrate that Sat-DN outperforms existing methods, achieving state-of-the-art results in both qualitative and quantitative evaluations. The code is available at https://github.com/costune/SatDN.",
        "arxiv_id": "2502.08352",
        "ARXIVID": "2502.08352",
        "COMMENT": "This paper introduces a novel framework for implicit surface reconstruction from satellite images, which could be tangentially related to spatial understanding (criterion 1).",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2502.07945": {
        "authors": [
            "Yannik Frisch",
            "Ssharvien Kumar Sivakumar",
            "\\c{C}a\\u{g}han K\\\"oksal",
            "Elsa B\\\"ohm",
            "Felix Wagner",
            "Adrian Gericke",
            "Ghazal Ghazaei",
            "Anirban Mukhopadhyay"
        ],
        "title": "SurGrID: Controllable Surgical Simulation via Scene Graph to Image Diffusion",
        "abstract": "arXiv:2502.07945v1 Announce Type: new  Abstract: Surgical simulation offers a promising addition to conventional surgical training. However, available simulation tools lack photorealism and rely on hardcoded behaviour. Denoising Diffusion Models are a promising alternative for high-fidelity image synthesis, but existing state-of-the-art conditioning methods fall short in providing precise control or interactivity over the generated scenes.   We introduce SurGrID, a Scene Graph to Image Diffusion Model, allowing for controllable surgical scene synthesis by leveraging Scene Graphs. These graphs encode a surgical scene's components' spatial and semantic information, which are then translated into an intermediate representation using our novel pre-training step that explicitly captures local and global information.   Our proposed method improves the fidelity of generated images and their coherence with the graph input over the state-of-the-art. Further, we demonstrate the simulation's realism and controllability in a user assessment study involving clinical experts.   Scene Graphs can be effectively used for precise and interactive conditioning of Denoising Diffusion Models for simulating surgical scenes, enabling high fidelity and interactive control over the generated content.",
        "arxiv_id": "2502.07945",
        "ARXIVID": "2502.07945",
        "COMMENT": "This paper introduces a novel method for surgical simulation using scene graphs, which could be tangentially related to embodied AI benchmarks (criterion 3).",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2502.07822": {
        "authors": [
            "Ao Liang",
            "Haiyang Hua",
            "Jian Fang",
            "Wenyu Chen",
            "Huaici Zhao"
        ],
        "title": "PDM-SSD: Single-Stage Three-Dimensional Object Detector With Point Dilation",
        "abstract": "arXiv:2502.07822v1 Announce Type: new  Abstract: Current Point-based detectors can only learn from the provided points, with limited receptive fields and insufficient global learning capabilities for such targets. In this paper, we present a novel Point Dilation Mechanism for single-stage 3D detection (PDM-SSD) that takes advantage of these two representations. Specifically, we first use a PointNet-style 3D backbone for efficient feature encoding. Then, a neck with Point Dilation Mechanism (PDM) is used to expand the feature space, which involves two key steps: point dilation and feature filling. The former expands points to a certain size grid centered around the sampled points in Euclidean space. The latter fills the unoccupied grid with feature for backpropagation using spherical harmonic coefficients and Gaussian density function in terms of direction and scale. Next, we associate multiple dilation centers and fuse coefficients to obtain sparse grid features through height compression. Finally, we design a hybrid detection head for joint learning, where on one hand, the scene heatmap is predicted to complement the voting point set for improved detection accuracy, and on the other hand, the target probability of detected boxes are calibrated through feature fusion. On the challenging Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) dataset, PDM-SSD achieves state-of-the-art results for multi-class detection among single-modal methods with an inference speed of 68 frames. We also demonstrate the advantages of PDM-SSD in detecting sparse and incomplete objects through numerous object-level instances. Additionally, PDM can serve as an auxiliary network to establish a connection between sampling points and object centers, thereby improving the accuracy of the model without sacrificing inference speed. Our code will be available at https://github.com/AlanLiangC/PDM-SSD.git.",
        "arxiv_id": "2502.07822",
        "ARXIVID": "2502.07822",
        "COMMENT": "Does not match any specific criteria but introduces a novel 3D object detection method with point dilation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.08642": {
        "authors": [
            "Ellie Arar",
            "Yarden Frenkel",
            "Daniel Cohen-Or",
            "Ariel Shamir",
            "Yael Vinker"
        ],
        "title": "SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation",
        "abstract": "arXiv:2502.08642v1 Announce Type: new  Abstract: Recent advancements in large vision-language models have enabled highly expressive and diverse vector sketch generation. However, state-of-the-art methods rely on a time-consuming optimization process involving repeated feedback from a pretrained model to determine stroke placement. Consequently, despite producing impressive sketches, these methods are limited in practical applications. In this work, we introduce SwiftSketch, a diffusion model for image-conditioned vector sketch generation that can produce high-quality sketches in less than a second. SwiftSketch operates by progressively denoising stroke control points sampled from a Gaussian distribution. Its transformer-decoder architecture is designed to effectively handle the discrete nature of vector representation and capture the inherent global dependencies between strokes. To train SwiftSketch, we construct a synthetic dataset of image-sketch pairs, addressing the limitations of existing sketch datasets, which are often created by non-artists and lack professional quality. For generating these synthetic sketches, we introduce ControlSketch, a method that enhances SDS-based techniques by incorporating precise spatial control through a depth-aware ControlNet. We demonstrate that SwiftSketch generalizes across diverse concepts, efficiently producing sketches that combine high fidelity with a natural and visually appealing style.",
        "arxiv_id": "2502.08642",
        "ARXIVID": "2502.08642",
        "COMMENT": "Does not match any specific criteria but introduces a diffusion model for vector sketch generation, which is related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.08556": {
        "authors": [
            "Shixiang Tang",
            "Yizhou Wang",
            "Lu Chen",
            "Yuan Wang",
            "Sida Peng",
            "Dan Xu",
            "Wanli Ouyang"
        ],
        "title": "Human-Centric Foundation Models: Perception, Generation and Agentic Modeling",
        "abstract": "arXiv:2502.08556v1 Announce Type: new  Abstract: Human understanding and generation are critical for modeling digital humans and humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs) inspired by the success of generalist models, such as large language and vision models, have emerged to unify diverse human-centric tasks into a single framework, surpassing traditional task-specific approaches. In this survey, we present a comprehensive overview of HcFMs by proposing a taxonomy that categorizes current approaches into four groups: (1) Human-centric Perception Foundation Models that capture fine-grained features for multi-modal 2D and 3D understanding. (2) Human-centric AIGC Foundation Models that generate high-fidelity, diverse human-related content. (3) Unified Perception and Generation Models that integrate these capabilities to enhance both human understanding and synthesis. (4) Human-centric Agentic Foundation Models that extend beyond perception and generation to learn human-like intelligence and interactive behaviors for humanoid embodied tasks. We review state-of-the-art techniques, discuss emerging challenges and future research directions. This survey aims to serve as a roadmap for researchers and practitioners working towards more robust, versatile, and intelligent digital human and embodiments modeling.",
        "arxiv_id": "2502.08556",
        "ARXIVID": "2502.08556",
        "COMMENT": "This paper surveys human-centric foundation models, which aligns with criterion 4 on vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2502.08075": {
        "authors": [
            "Mingyu Xing",
            "Lechao Cheng",
            "Shenggeng Tang",
            "Yaxiong Wang",
            "Zhun Zhong",
            "Meng Wang"
        ],
        "title": "Knowledge Swapping via Learning and Unlearning",
        "abstract": "arXiv:2502.08075v1 Announce Type: new  Abstract: We introduce \\textbf{Knowledge Swapping}, a novel task designed to selectively regulate knowledge of a pretrained model by enabling the forgetting of user\\-specified information, retaining essential knowledge, and acquiring new knowledge simultaneously. By delving into the analysis of knock-on feature hierarchy, we find that incremental learning typically progresses from low\\-level representations to higher\\-level semantics, whereas forgetting tends to occur in the opposite direction\\-starting from high-level semantics and moving down to low-level features. Building upon this, we propose to benchmark the knowledge swapping task with the strategy of \\textit{Learning Before Forgetting}. Comprehensive experiments on various tasks like image classification, object detection, and semantic segmentation validate the effectiveness of the proposed strategy. The source code is available at \\href{https://github.com/xingmingyu123456/KnowledgeSwapping}{https://github.com/xingmingyu123456/KnowledgeSwapping}.",
        "arxiv_id": "2502.08075",
        "ARXIVID": "2502.08075",
        "COMMENT": "This paper does not match any specific criteria but introduces a novel task of knowledge swapping, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.07644": {
        "authors": [
            "Shihao Xia",
            "Mengting He",
            "Shuai Shao",
            "Tingting Yu",
            "Yiying Zhang",
            "Linhai Song"
        ],
        "title": "SymGPT: Auditing Smart Contracts via Combining Symbolic Execution with Large Language Models",
        "abstract": "arXiv:2502.07644v2 Announce Type: new  Abstract: To govern smart contracts running on Ethereum, multiple Ethereum Request for Comment (ERC) standards have been developed, each having a set of rules to guide the behaviors of smart contracts. Violating the ERC rules could cause serious security issues and financial loss, signifying the importance of verifying smart contracts follow ERCs. Today's practices of such verification are to manually audit each single contract, use expert-developed program-analysis tools, or use large language models (LLMs), all of which are far from effective in identifying ERC rule violations. This paper introduces SymGPT, a tool that combines the natural language understanding of large language models (LLMs) with the formal guarantees of symbolic execution to automatically verify smart contracts' compliance with ERC rules. To develop SymGPT, we conduct an empirical study of 132 ERC rules from three widely used ERC standards, examining their content, security implications, and natural language descriptions. Based on this study, we design SymGPT by first instructing an LLM to translate ERC rules into a defined EBNF grammar. We then synthesize constraints from the formalized rules to represent scenarios where violations may occur and use symbolic execution to detect them. Our evaluation shows that SymGPT identifies 5,783 ERC rule violations in 4,000 real-world contracts, including 1,375 violations with clear attack paths for stealing financial assets, demonstrating its effectiveness. Furthermore, SymGPT outperforms six automated techniques and a security-expert auditing service, underscoring its superiority over current smart contract analysis methods.",
        "arxiv_id": "2502.07644",
        "ARXIVID": "2502.07644",
        "COMMENT": "Does not match any specific criteria but is relevant to LLMs and symbolic execution, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.07266": {
        "authors": [
            "Yuyang Wu",
            "Yifei Wang",
            "Tianqi Du",
            "Stefanie Jegelka",
            "Yisen Wang"
        ],
        "title": "When More is Less: Understanding Chain-of-Thought Length in LLMs",
        "abstract": "arXiv:2502.07266v1 Announce Type: new  Abstract: Chain-of-thought (CoT) reasoning enhances the multi-step reasoning capabilities of large language models (LLMs) by breaking complex tasks into smaller, manageable sub-tasks. Researchers have been exploring ways to guide models to generate more complex CoT processes to improve the reasoning ability of LLMs, such as long CoT and the test-time scaling law. However, for most models and tasks, does an increase in CoT length consistently lead to improved reasoning accuracy? In this paper, we observe a nuanced relationship: as the number of reasoning steps increases, performance initially improves but eventually decreases. To understand this phenomenon, we provide a piece of evidence that longer reasoning processes are increasingly susceptible to noise. We theoretically prove the existence of an optimal CoT length and derive a scaling law for this optimal length based on model capability and task difficulty. Inspired by our theory, we conduct experiments on both synthetic and real world datasets and propose Length-filtered Vote to alleviate the effects of excessively long or short CoTs. Our findings highlight the critical need to calibrate CoT length to align with model capabilities and task demands, offering a principled framework for optimizing multi-step reasoning in LLMs.",
        "arxiv_id": "2502.07266",
        "ARXIVID": "2502.07266",
        "COMMENT": "Does not match any specific criteria but provides insights into reasoning processes in LLMs, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.08244": {
        "authors": [
            "Wonjoon Jin",
            "Qi Dai",
            "Chong Luo",
            "Seung-Hwan Baek",
            "Sunghyun Cho"
        ],
        "title": "FloVD: Optical Flow Meets Video Diffusion Model for Enhanced Camera-Controlled Video Synthesis",
        "abstract": "arXiv:2502.08244v1 Announce Type: new  Abstract: This paper presents FloVD, a novel optical-flow-based video diffusion model for camera-controllable video generation. FloVD leverages optical flow maps to represent motions of the camera and moving objects. This approach offers two key benefits. Since optical flow can be directly estimated from videos, our approach allows for the use of arbitrary training videos without ground-truth camera parameters. Moreover, as background optical flow encodes 3D correlation across different viewpoints, our method enables detailed camera control by leveraging the background motion. To synthesize natural object motion while supporting detailed camera control, our framework adopts a two-stage video synthesis pipeline consisting of optical flow generation and flow-conditioned video synthesis. Extensive experiments demonstrate the superiority of our method over previous approaches in terms of accurate camera control and natural object motion synthesis.",
        "arxiv_id": "2502.08244",
        "ARXIVID": "2502.08244",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in video synthesis.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.08097": {
        "authors": [
            "Qianrui Teng",
            "Xing Cui",
            "Xuannan Liu",
            "Peipei Li",
            "Zekun Li",
            "Huaibo Huang",
            "Ran He"
        ],
        "title": "ID-Cloak: Crafting Identity-Specific Cloaks Against Personalized Text-to-Image Generation",
        "abstract": "arXiv:2502.08097v1 Announce Type: new  Abstract: Personalized text-to-image models allow users to generate images of new concepts from several reference photos, thereby leading to critical concerns regarding civil privacy. Although several anti-personalization techniques have been developed, these methods typically assume that defenders can afford to design a privacy cloak corresponding to each specific image. However, due to extensive personal images shared online, image-specific methods are limited by real-world practical applications. To address this issue, we are the first to investigate the creation of identity-specific cloaks (ID-Cloak) that safeguard all images belong to a specific identity. Specifically, we first model an identity subspace that preserves personal commonalities and learns diverse contexts to capture the image distribution to be protected. Then, we craft identity-specific cloaks with the proposed novel objective that encourages the cloak to guide the model away from its normal output within the subspace. Extensive experiments show that the generated universal cloak can effectively protect the images. We believe our method, along with the proposed identity-specific cloak setting, marks a notable advance in realistic privacy protection.",
        "arxiv_id": "2502.08097",
        "ARXIVID": "2502.08097",
        "COMMENT": "Does not match any specific criterion but is related to privacy protection in generative models, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.07443": {
        "authors": [
            "Vince Trencsenyi",
            "Agnieszka Mensfelt",
            "Kostas Stathis"
        ],
        "title": "Approximating Human Strategic Reasoning with LLM-Enhanced Recursive Reasoners Leveraging Multi-agent Hypergames",
        "abstract": "arXiv:2502.07443v1 Announce Type: new  Abstract: LLM-driven multi-agent-based simulations have been gaining traction with applications in game-theoretic and social simulations. While most implementations seek to exploit or evaluate LLM-agentic reasoning, they often do so with a weak notion of agency and simplified architectures. We implement a role-based multi-agent strategic interaction framework tailored to sophisticated recursive reasoners, providing the means for systematic in-depth development and evaluation of strategic reasoning. Our game environment is governed by the umpire responsible for facilitating games, from matchmaking through move validation to environment management. Players incorporate state-of-the-art LLMs in their decision mechanism, relying on a formal hypergame-based model of hierarchical beliefs. We use one-shot, 2-player beauty contests to evaluate the recursive reasoning capabilities of the latest LLMs, providing a comparison to an established baseline model from economics and data from human experiments. Furthermore, we introduce the foundations of an alternative semantic measure of reasoning to the k-level theory. Our experiments show that artificial reasoners can outperform the baseline model in terms of both approximating human behaviour and reaching the optimal solution.",
        "arxiv_id": "2502.07443",
        "ARXIVID": "2502.07443",
        "COMMENT": "Does not match any specific criterion but is tangentially related to multi-agent reasoning and LLMs, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.08544": {
        "authors": [
            "Kevin Flanagan",
            "Dima Damen",
            "Michael Wray"
        ],
        "title": "Moment of Untruth: Dealing with Negative Queries in Video Moment Retrieval",
        "abstract": "arXiv:2502.08544v1 Announce Type: new  Abstract: Video Moment Retrieval is a common task to evaluate the performance of visual-language models - it involves localising start and end times of moments in videos from query sentences. The current task formulation assumes that the queried moment is present in the video, resulting in false positive moment predictions when irrelevant query sentences are provided.   In this paper we propose the task of Negative-Aware Video Moment Retrieval (NA-VMR), which considers both moment retrieval accuracy and negative query rejection accuracy. We make the distinction between In-Domain and Out-of-Domain negative queries and provide new evaluation benchmarks for two popular video moment retrieval datasets: QVHighlights and Charades-STA. We analyse the ability of current SOTA video moment retrieval approaches to adapt to Negative-Aware Video Moment Retrieval and propose UniVTG-NA, an adaptation of UniVTG designed to tackle NA-VMR. UniVTG-NA achieves high negative rejection accuracy (avg. $98.4\\%$) scores while retaining moment retrieval scores to within $3.87\\%$ Recall@1. Dataset splits and code are available at https://github.com/keflanagan/MomentofUntruth",
        "arxiv_id": "2502.08544",
        "ARXIVID": "2502.08544",
        "COMMENT": "Does not match any specific criteria but introduces a new task for video moment retrieval with negative queries.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.07056": {
        "authors": [
            "Amy Yu",
            "Erik Lebedev",
            "Lincoln Everett",
            "Xiaoxin Chen",
            "Terry Chen"
        ],
        "title": "Autonomous Deep Agent",
        "abstract": "arXiv:2502.07056v1 Announce Type: new  Abstract: This technical brief introduces Deep Agent, an advanced autonomous AI system designed to manage complex multi-phase tasks through a novel hierarchical task management architecture. The system's foundation is built on our Hierarchical Task DAG (HTDAG) framework, which dynamically decomposes high-level objectives into manageable sub-tasks while rigorously maintaining dependencies and execution coherence. Deep Agent advances beyond traditional agent systems through three key innovations: First, it implements a recursive two-stage planner-executor architecture that enables continuous task refinement and adaptation as circumstances change. Second, it features an Autonomous API & Tool Creation (AATC) system that automatically generates reusable components from UI interactions, substantially reducing operational costs for similar tasks. Third, it incorporates Prompt Tweaking Engine and Autonomous Prompt Feedback Learning components that optimize Large Language Model prompts for specific scenarios, enhancing both inference accuracy and operational stability. These components are integrated to form a service infrastructure that manages user contexts, handles complex task dependencies, and orchestrates end-to-end agentic workflow execution. Through this sophisticated architecture, Deep Agent establishes a novel paradigm in self-governing AI systems, demonstrating robust capability to independently handle intricate, multi-step tasks while maintaining consistent efficiency and reliability through continuous self-optimization.",
        "arxiv_id": "2502.07056",
        "ARXIVID": "2502.07056",
        "COMMENT": "Does not match any specific criteria but discusses an autonomous AI system with hierarchical task management.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.08189": {
        "authors": [
            "Zhao Wang",
            "Hao Wen",
            "Lingting Zhu",
            "Chenming Shang",
            "Yujiu Yang",
            "Qi Dou"
        ],
        "title": "AnyCharV: Bootstrap Controllable Character Video Generation with Fine-to-Coarse Guidance",
        "abstract": "arXiv:2502.08189v1 Announce Type: new  Abstract: Character video generation is a significant real-world application focused on producing high-quality videos featuring specific characters. Recent advancements have introduced various control signals to animate static characters, successfully enhancing control over the generation process. However, these methods often lack flexibility, limiting their applicability and making it challenging for users to synthesize a source character into a desired target scene. To address this issue, we propose a novel framework, AnyCharV, that flexibly generates character videos using arbitrary source characters and target scenes, guided by pose information. Our approach involves a two-stage training process. In the first stage, we develop a base model capable of integrating the source character with the target scene using pose guidance. The second stage further bootstraps controllable generation through a self-boosting mechanism, where we use the generated video in the first stage and replace the fine mask with the coarse one, enabling training outcomes with better preservation of character details. Experimental results demonstrate the effectiveness and robustness of our proposed method. Our project page is https://anycharv.github.io.",
        "arxiv_id": "2502.08189",
        "ARXIVID": "2502.08189",
        "COMMENT": "Does not match any specific criteria but focuses on character video generation, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.07951": {
        "authors": [
            "Xinyi Tan",
            "Jiacheng Wang",
            "Liansheng Wang"
        ],
        "title": "Federated Self-supervised Domain Generalization for Label-efficient Polyp Segmentation",
        "abstract": "arXiv:2502.07951v1 Announce Type: new  Abstract: Employing self-supervised learning (SSL) methodologies assumes par-amount significance in handling unlabeled polyp datasets when building deep learning-based automatic polyp segmentation models. However, the intricate privacy dynamics surrounding medical data often preclude seamless data sharing among disparate medical centers. Federated learning (FL) emerges as a formidable solution to this privacy conundrum, yet within the realm of FL, optimizing model generalization stands as a pressing imperative. Robust generalization capabilities are imperative to ensure the model's efficacy across diverse geographical domains post-training on localized client datasets. In this paper, a Federated self-supervised Domain Generalization method is proposed to enhance the generalization capacity of federated and Label-efficient intestinal polyp segmentation, named LFDG. Based on a classical SSL method, DropPos, LFDG proposes an adversarial learning-based data augmentation method (SSADA) to enhance the data diversity. LFDG further proposes a relaxation module based on Source-reconstruction and Augmentation-masking (SRAM) to maintain stability in feature learning. We have validated LFDG on polyp images from six medical centers. The performance of our method achieves 3.80% and 3.92% better than the baseline and other recent FL methods and SSL methods, respectively.",
        "arxiv_id": "2502.07951",
        "ARXIVID": "2502.07951",
        "COMMENT": "This paper focuses on federated self-supervised learning for medical segmentation, which does not match any specific criteria but is tangentially related to general interest in machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.07423": {
        "authors": [
            "Erik M. Lintunen",
            "Nadia M. Ady",
            "Sebastian Deterding",
            "Christian Guckelsberger"
        ],
        "title": "Towards a Formal Theory of the Need for Competence via Computational Intrinsic Motivation",
        "abstract": "arXiv:2502.07423v1 Announce Type: new  Abstract: Computational models offer powerful tools for formalising psychological theories, making them both testable and applicable in digital contexts. However, they remain little used in the study of motivation within psychology. We focus on the \"need for competence\", postulated as a key basic human need within Self-Determination Theory (SDT) -- arguably the most influential psychological framework for studying intrinsic motivation (IM). The need for competence is treated as a single construct across SDT texts. Yet, recent research has identified multiple, ambiguously defined facets of competence in SDT. We propose that these inconsistencies may be alleviated by drawing on computational models from the field of artificial intelligence, specifically from the domain of reinforcement learning (RL). By aligning the aforementioned facets of competence -- effectance, skill use, task performance, and capacity growth -- with existing RL formalisms, we provide a foundation for advancing competence-related theory in SDT and motivational psychology more broadly. The formalisms reveal underlying preconditions that SDT fails to make explicit, demonstrating how computational models can improve our understanding of IM. Additionally, our work can support a cycle of theory development by inspiring new computational models formalising aspects of the theory, which can then be tested empirically to refine the theory. While our research lays a promising foundation, empirical studies of these models in both humans and machines are needed, inviting collaboration across disciplines.",
        "arxiv_id": "2502.07423",
        "ARXIVID": "2502.07423",
        "COMMENT": "This paper does not match any specific criteria but discusses computational intrinsic motivation, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.08580": {
        "authors": [
            "Benoit Freiche",
            "Anthony El-Khoury",
            "Ali Nasiri-Sarvi",
            "Mahdi S. Hosseini",
            "Damien Garcia",
            "Adrian Basarab",
            "Mathieu Boily",
            "Hassan Rivaz"
        ],
        "title": "Ultrasound Image Generation using Latent Diffusion Models",
        "abstract": "arXiv:2502.08580v1 Announce Type: new  Abstract: Diffusion models for image generation have been a subject of increasing interest due to their ability to generate diverse, high-quality images. Image generation has immense potential in medical imaging because open-source medical images are difficult to obtain compared to natural images, especially for rare conditions. The generated images can be used later to train classification and segmentation models. In this paper, we propose simulating realistic ultrasound (US) images by successive fine-tuning of large diffusion models on different publicly available databases. To do so, we fine-tuned Stable Diffusion, a state-of-the-art latent diffusion model, on BUSI (Breast US Images) an ultrasound breast image dataset. We successfully generated high-quality US images of the breast using simple prompts that specify the organ and pathology, which appeared realistic to three experienced US scientists and a US radiologist. Additionally, we provided user control by conditioning the model with segmentations through ControlNet. We will release the source code at http://code.sonography.ai/ to allow fast US image generation to the scientific community.",
        "arxiv_id": "2502.08580",
        "ARXIVID": "2502.08580",
        "COMMENT": "This paper does not match any of the specific criteria but is related to generative modeling in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.08590": {
        "authors": [
            "Yujie Zhou",
            "Jiazi Bu",
            "Pengyang Ling",
            "Pan Zhang",
            "Tong Wu",
            "Qidong Huang",
            "Jinsong Li",
            "Xiaoyi Dong",
            "Yuhang Zang",
            "Yuhang Cao",
            "Anyi Rao",
            "Jiaqi Wang",
            "Li Niu"
        ],
        "title": "Light-A-Video: Training-free Video Relighting via Progressive Light Fusion",
        "abstract": "arXiv:2502.08590v1 Announce Type: new  Abstract: Recent advancements in image relighting models, driven by large-scale datasets and pre-trained diffusion models, have enabled the imposition of consistent lighting. However, video relighting still lags, primarily due to the excessive training costs and the scarcity of diverse, high-quality video relighting datasets. A simple application of image relighting models on a frame-by-frame basis leads to several issues: lighting source inconsistency and relighted appearance inconsistency, resulting in flickers in the generated videos. In this work, we propose Light-A-Video, a training-free approach to achieve temporally smooth video relighting. Adapted from image relighting models, Light-A-Video introduces two key techniques to enhance lighting consistency. First, we design a Consistent Light Attention (CLA) module, which enhances cross-frame interactions within the self-attention layers to stabilize the generation of the background lighting source. Second, leveraging the physical principle of light transport independence, we apply linear blending between the source video's appearance and the relighted appearance, using a Progressive Light Fusion (PLF) strategy to ensure smooth temporal transitions in illumination. Experiments show that Light-A-Video improves the temporal consistency of relighted video while maintaining the image quality, ensuring coherent lighting transitions across frames. Project page: https://bujiazi.github.io/light-a-video.github.io/.",
        "arxiv_id": "2502.08590",
        "ARXIVID": "2502.08590",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision and video processing.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.08373": {
        "authors": [
            "Ziyue Yang",
            "Kehan Wang",
            "Yuhang Ming",
            "Yong Peng",
            "Han Yang",
            "Qiong Chen",
            "Wanzeng Kong"
        ],
        "title": "Uncertainty Aware Human-machine Collaboration in Camouflaged Object Detection",
        "abstract": "arXiv:2502.08373v1 Announce Type: new  Abstract: Camouflaged Object Detection (COD), the task of identifying objects concealed within their environments, has seen rapid growth due to its wide range of practical applications. A key step toward developing trustworthy COD systems is the estimation and effective utilization of uncertainty. In this work, we propose a human-machine collaboration framework for classifying the presence of camouflaged objects, leveraging the complementary strengths of computer vision (CV) models and noninvasive brain-computer interfaces (BCIs). Our approach introduces a multiview backbone to estimate uncertainty in CV model predictions, utilizes this uncertainty during training to improve efficiency, and defers low-confidence cases to human evaluation via RSVP-based BCIs during testing for more reliable decision-making. We evaluated the framework in the CAMO dataset, achieving state-of-the-art results with an average improvement of 4.56\\% in balanced accuracy (BA) and 3.66\\% in the F1 score compared to existing methods. For the best-performing participants, the improvements reached 7.6\\% in BA and 6.66\\% in the F1 score. Analysis of the training process revealed a strong correlation between our confidence measures and precision, while an ablation study confirmed the effectiveness of the proposed training policy and the human-machine collaboration strategy. In general, this work reduces human cognitive load, improves system reliability, and provides a strong foundation for advancements in real-world COD applications and human-computer interaction. Our code and data are available at: https://github.com/ziyuey/Uncertainty-aware-human-machine-collaboration-in-camouflaged-object-identification.",
        "arxiv_id": "2502.08373",
        "ARXIVID": "2502.08373",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision and human-machine collaboration.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.07374": {
        "authors": [
            "Dacheng Li",
            "Shiyi Cao",
            "Tyler Griggs",
            "Shu Liu",
            "Xiangxi Mo",
            "Shishir G. Patil",
            "Matei Zaharia",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!",
        "abstract": "arXiv:2502.07374v1 Announce Type: new  Abstract: Large reasoning models (LRMs) tackle complex reasoning problems by following long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking, and self-validation. However, the training techniques and data requirements to elicit Long CoT remain poorly understood. In this work, we find that a Large Language model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). With just 17k long CoT training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0% (+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's score of 44.6% and 59.1%. More importantly, we find that the structure of Long CoT is critical to the learning process, whereas the content of individual reasoning steps has minimal impact. Perturbations affecting content, such as training on incorrect samples or removing reasoning keywords, have little impact on performance. In contrast, structural modifications that disrupt logical consistency in the Long CoT, such as shuffling or deleting reasoning steps, significantly degrade accuracy. For example, a model trained on Long CoT samples with incorrect answers still achieves only 3.2% lower accuracy compared to training with fully correct samples. These insights deepen our understanding of how to elicit reasoning capabilities in LLMs and highlight key considerations for efficiently training the next generation of reasoning models. This is the academic paper of our previous released Sky-T1-32B-Preview model. Codes are available at https://github.com/NovaSky-AI/SkyThought.",
        "arxiv_id": "2502.07374",
        "ARXIVID": "2502.07374",
        "COMMENT": "Does not match any specific criteria but discusses reasoning in LLMs, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.08347": {
        "authors": [
            "Fenghe Tang",
            "Qingsong Yao",
            "Wenxin Ma",
            "Chenxu Wu",
            "Zihang Jiang",
            "S. Kevin Zhou"
        ],
        "title": "Hi-End-MAE: Hierarchical encoder-driven masked autoencoders are stronger vision learners for medical image segmentation",
        "abstract": "arXiv:2502.08347v1 Announce Type: new  Abstract: Medical image segmentation remains a formidable challenge due to the label scarcity. Pre-training Vision Transformer (ViT) through masked image modeling (MIM) on large-scale unlabeled medical datasets presents a promising solution, providing both computational efficiency and model generalization for various downstream tasks. However, current ViT-based MIM pre-training frameworks predominantly emphasize local aggregation representations in output layers and fail to exploit the rich representations across different ViT layers that better capture fine-grained semantic information needed for more precise medical downstream tasks. To fill the above gap, we hereby present Hierarchical Encoder-driven MAE (Hi-End-MAE), a simple yet effective ViT-based pre-training solution, which centers on two key innovations: (1) Encoder-driven reconstruction, which encourages the encoder to learn more informative features to guide the reconstruction of masked patches; and (2) Hierarchical dense decoding, which implements a hierarchical decoding structure to capture rich representations across different layers. We pre-train Hi-End-MAE on a large-scale dataset of 10K CT scans and evaluated its performance across seven public medical image segmentation benchmarks. Extensive experiments demonstrate that Hi-End-MAE achieves superior transfer learning capabilities across various downstream tasks, revealing the potential of ViT in medical imaging applications. The code is available at: https://github.com/FengheTan9/Hi-End-MAE",
        "arxiv_id": "2502.08347",
        "ARXIVID": "2502.08347",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision and machine learning in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.07802": {
        "authors": [
            "Feng Liang",
            "Haoyu Ma",
            "Zecheng He",
            "Tingbo Hou",
            "Ji Hou",
            "Kunpeng Li",
            "Xiaoliang Dai",
            "Felix Juefei-Xu",
            "Samaneh Azadi",
            "Animesh Sinha",
            "Peizhao Zhang",
            "Peter Vajda",
            "Diana Marculescu"
        ],
        "title": "Movie Weaver: Tuning-Free Multi-Concept Video Personalization with Anchored Prompts",
        "abstract": "arXiv:2502.07802v1 Announce Type: new  Abstract: Video personalization, which generates customized videos using reference images, has gained significant attention. However, prior methods typically focus on single-concept personalization, limiting broader applications that require multi-concept integration. Attempts to extend these models to multiple concepts often lead to identity blending, which results in composite characters with fused attributes from multiple sources. This challenge arises due to the lack of a mechanism to link each concept with its specific reference image. We address this with anchored prompts, which embed image anchors as unique tokens within text prompts, guiding accurate referencing during generation. Additionally, we introduce concept embeddings to encode the order of reference images. Our approach, Movie Weaver, seamlessly weaves multiple concepts-including face, body, and animal images-into one video, allowing flexible combinations in a single model. The evaluation shows that Movie Weaver outperforms existing methods for multi-concept video personalization in identity preservation and overall quality.",
        "arxiv_id": "2502.07802",
        "ARXIVID": "2502.07802",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in video personalization.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.08221": {
        "authors": [
            "Xiang Chen",
            "Shuying Gan",
            "Chenyuan Feng",
            "Xijun Wang",
            "Tony Q. S. Quek"
        ],
        "title": "Take What You Need: Flexible Multi-Task Semantic Communications with Channel Adaptation",
        "abstract": "arXiv:2502.08221v1 Announce Type: new  Abstract: The growing demand for efficient semantic communication systems capable of managing diverse tasks and adapting to fluctuating channel conditions has driven the development of robust, resource-efficient frameworks. This article introduces a novel channel-adaptive and multi-task-aware semantic communication framework based on a masked auto-encoder architecture. Our framework optimizes the transmission of meaningful information by incorporating a multi-task-aware scoring mechanism that identifies and prioritizes semantically significant data across multiple concurrent tasks. A channel-aware extractor is employed to dynamically select relevant information in response to real-time channel conditions. By jointly optimizing semantic relevance and transmission efficiency, the framework ensures minimal performance degradation under resource constraints. Experimental results demonstrate the superior performance of our framework compared to conventional methods in tasks such as image reconstruction and object detection. These results underscore the framework's adaptability to heterogeneous channel environments and its scalability for multi-task applications, positioning it as a promising solution for next-generation semantic communication networks.",
        "arxiv_id": "2502.08221",
        "ARXIVID": "2502.08221",
        "COMMENT": "Does not match any specific criterion but is relevant to semantic communication and multi-task learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.07870": {
        "authors": [
            "Alex Jinpeng Wang",
            "Dongxing Mao",
            "Jiawei Zhang",
            "Weiming Han",
            "Zhuobai Dong",
            "Linjie Li",
            "Yiqi Lin",
            "Zhengyuan Yang",
            "Libo Qin",
            "Fuwei Zhang",
            "Lijuan Wang",
            "Min Li"
        ],
        "title": "TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation",
        "abstract": "arXiv:2502.07870v1 Announce Type: new  Abstract: Text-conditioned image generation has gained significant attention in recent years and are processing increasingly longer and comprehensive text prompt. In everyday life, dense and intricate text appears in contexts like advertisements, infographics, and signage, where the integration of both text and visuals is essential for conveying complex information. However, despite these advances, the generation of images containing long-form text remains a persistent challenge, largely due to the limitations of existing datasets, which often focus on shorter and simpler text. To address this gap, we introduce TextAtlas5M, a novel dataset specifically designed to evaluate long-text rendering in text-conditioned image generation. Our dataset consists of 5 million long-text generated and collected images across diverse data types, enabling comprehensive evaluation of large-scale generative models on long-text image generation. We further curate 3000 human-improved test set TextAtlasEval across 3 data domains, establishing one of the most extensive benchmarks for text-conditioned generation. Evaluations suggest that the TextAtlasEval benchmarks present significant challenges even for the most advanced proprietary models (e.g. GPT4o with DallE-3), while their open-source counterparts show an even larger performance gap. These evidences position TextAtlas5M as a valuable dataset for training and evaluating future-generation text-conditioned image generation models.",
        "arxiv_id": "2502.07870",
        "ARXIVID": "2502.07870",
        "COMMENT": "Does not match any specific criterion but is relevant to text-conditioned image generation and dataset creation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.07856": {
        "authors": [
            "Ao Li",
            "Wei Fang",
            "Hongbo Zhao",
            "Le Lu",
            "Ge Yang",
            "Minfeng Xu"
        ],
        "title": "MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers",
        "abstract": "arXiv:2502.07856v1 Announce Type: new  Abstract: In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation.",
        "arxiv_id": "2502.07856",
        "ARXIVID": "2502.07856",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and sampling techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.08549": {
        "authors": [
            "Fei Zheng",
            "Nicolas Duchateau"
        ],
        "title": "Copula-based mixture model identification for subgroup clustering with imaging applications",
        "abstract": "arXiv:2502.08549v1 Announce Type: new  Abstract: Model-based clustering techniques have been widely applied to various application areas, while most studies focus on canonical mixtures with unique component distribution form. However, this strict assumption is often hard to satisfy. In this paper, we consider the more flexible Copula-Based Mixture Models (CBMMs) for clustering, which allow heterogeneous component distributions composed by flexible choices of marginal and copula forms. More specifically, we propose an adaptation of the Generalized Iterative Conditional Estimation (GICE) algorithm to identify the CBMMs in an unsupervised manner, where the marginal and copula forms and their parameters are estimated iteratively. GICE is adapted from its original version developed for switching Markov model identification with the choice of realization time. Our CBMM-GICE clustering method is then tested on synthetic two-cluster data (N=2000 samples) with discussion of the factors impacting its convergence. Finally, it is compared to the Expectation Maximization identified mixture models with unique component form on the entire MNIST database (N=70000), and on real cardiac magnetic resonance data (N=276) to illustrate its value for imaging applications.",
        "arxiv_id": "2502.08549",
        "ARXIVID": "2502.08549",
        "COMMENT": "Does not match any specific criterion but is related to clustering and imaging applications, which might be of tangential interest.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.07191": {
        "authors": [
            "Fan Liu",
            "Wenshuo Chao",
            "Naiqiang Tan",
            "Hao Liu"
        ],
        "title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
        "abstract": "arXiv:2502.07191v2 Announce Type: new  Abstract: With the advancement of large language models (LLMs), solving complex reasoning tasks has gained increasing attention. Inference-time computation methods (e.g., Best-of-N, beam search, et al.) are particularly valuable as they can enhance reasoning performance without modifying model parameters or requiring additional training. However, these techniques come with implementation challenges, and most existing methods remain at the proof-of-concept stage with limited practical adoption due to their computational complexity and varying effectiveness across different tasks. In this paper, we investigate and benchmark diverse inference-time computation strategies across reasoning tasks of varying complexity. Since most current methods rely on a proposer-verifier pipeline that first generates candidate solutions (e.g., reasoning solutions) and then selects the best one based on reward signals (e.g., RLHF rewards, process rewards), our research focuses on optimizing both candidate solution generation (e.g., instructing prompts, hyperparameters such as temperature and top-p) and reward mechanisms (e.g., self-evaluation, reward types). Through extensive experiments (more than 20,000 A100-80G GPU hours with over 1,000 experiments) across a variety of models (e.g., Llama, Qwen, and Mistral families) of various sizes, our ablation studies reveal that previously overlooked strategies can significantly enhance performance (e.g., tuning temperature can improve reasoning task performance by up to 5%). Furthermore, we establish a standardized benchmark for inference-time computation by systematically evaluating six representative methods across eight reasoning tasks. These findings provide a stronger foundation for future research. The code is available at https://github.com/usail-hkust/benchmark_inference_time_computation_LL",
        "arxiv_id": "2502.07191",
        "ARXIVID": "2502.07191",
        "COMMENT": "Does not match any specific criteria but discusses inference-time computation for reasoning tasks in LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}