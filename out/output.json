{
    "2502.09560": {
        "authors": [
            "Rui Yang",
            "Hanyang Chen",
            "Junyu Zhang",
            "Mark Zhao",
            "Cheng Qian",
            "Kangrui Wang",
            "Qineng Wang",
            "Teja Venkat Koripella",
            "Marziyeh Movahedi",
            "Manling Li",
            "Heng Ji",
            "Huan Zhang",
            "Tong Zhang"
        ],
        "title": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents",
        "abstract": "arXiv:2502.09560v1 Announce Type: new  Abstract: Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 13 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code is available at https://embodiedbench.github.io.",
        "arxiv_id": "2502.09560",
        "ARXIVID": "2502.09560",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (EmbodiedBench) for evaluating MLLM-based embodied agents.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2502.09093": {
        "authors": [
            "Mingxiao Li",
            "Fang Qu",
            "Zhanpeng Chen",
            "Na Su",
            "Zhizhou Zhong",
            "Ziyang Chen",
            "Nan Du",
            "Xiaolong Li"
        ],
        "title": "From Visuals to Vocabulary: Establishing Equivalence Between Image and Text Token Through Autoregressive Pre-training in MLLMs",
        "abstract": "arXiv:2502.09093v1 Announce Type: new  Abstract: While MLLMs perform well on perceptual tasks, they lack precise multimodal alignment, limiting performance. To address this challenge, we propose Vision Dynamic Embedding-Guided Pretraining (VDEP), a hybrid autoregressive training paradigm for MLLMs. Utilizing dynamic embeddings from the MLP following the visual encoder, this approach supervises image hidden states and integrates image tokens into autoregressive training. Existing MLLMs primarily focused on recovering information from textual inputs, often neglecting the effective processing of image data. In contrast, the key improvement of this work is the reinterpretation of multimodal alignment as a process of recovering information from input data, with particular emphasis on reconstructing detailed visual features.The proposed method seamlessly integrates into standard models without architectural changes. Experiments on 13 benchmarks show VDEP outperforms baselines, surpassing existing methods.",
        "arxiv_id": "2502.09093",
        "ARXIVID": "2502.09093",
        "COMMENT": "Matches criterion 2. Proposes a new training paradigm for MLLMs with improved multimodal alignment, which is directly relevant to visual large language models.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2502.09615": {
        "authors": [
            "Isabella Liu",
            "Zhan Xu",
            "Wang Yifan",
            "Hao Tan",
            "Zexiang Xu",
            "Xiaolong Wang",
            "Hao Su",
            "Zifan Shi"
        ],
        "title": "RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets",
        "abstract": "arXiv:2502.09615v1 Announce Type: new  Abstract: We present RigAnything, a novel autoregressive transformer-based model, which makes 3D assets rig-ready by probabilistically generating joints, skeleton topologies, and assigning skinning weights in a template-free manner. Unlike most existing auto-rigging methods, which rely on predefined skeleton template and are limited to specific categories like humanoid, RigAnything approaches the rigging problem in an autoregressive manner, iteratively predicting the next joint based on the global input shape and the previous prediction. While autoregressive models are typically used to generate sequential data, RigAnything extends their application to effectively learn and represent skeletons, which are inherently tree structures. To achieve this, we organize the joints in a breadth-first search (BFS) order, enabling the skeleton to be defined as a sequence of 3D locations and the parent index. Furthermore, our model improves the accuracy of position prediction by leveraging diffusion modeling, ensuring precise and consistent placement of joints within the hierarchy. This formulation allows the autoregressive model to efficiently capture both spatial and hierarchical relationships within the skeleton. Trained end-to-end on both RigNet and Objaverse datasets, RigAnything demonstrates state-of-the-art performance across diverse object types, including humanoids, quadrupeds, marine creatures, insects, and many more, surpassing prior methods in quality, robustness, generalizability, and efficiency. Please check our website for more details: https://www.liuisabella.com/RigAnything.",
        "arxiv_id": "2502.09615",
        "ARXIVID": "2502.09615",
        "COMMENT": "Matches criterion 3 as it introduces a novel autoregressive rigging method for diverse 3D assets, focusing on a new angle for embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2502.08859": {
        "authors": [
            "Clinton J. Wang",
            "Dean Lee",
            "Cristina Menghini",
            "Johannes Mols",
            "Jack Doughty",
            "Adam Khoja",
            "Jayson Lynch",
            "Sean Hendryx",
            "Summer Yue",
            "Dan Hendrycks"
        ],
        "title": "EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges",
        "abstract": "arXiv:2502.08859v1 Announce Type: new  Abstract: As language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities, making them a unique testbed for evaluating frontier language models. We introduce EnigmaEval, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks, puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution paths. The benchmark comprises 1184 puzzles of varying complexity -- each typically requiring teams of skilled solvers hours to days to complete -- with unambiguous, verifiable solutions that enable efficient evaluation. State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning.",
        "arxiv_id": "2502.08859",
        "ARXIVID": "2502.08859",
        "COMMENT": "Matches criterion 3. Introduces a new benchmark dataset for multimodal reasoning challenges, which is relevant to embodied AI benchmarks and novel methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.09020": {
        "authors": [
            "Xiao Wang",
            "Jingtao Jiang",
            "Dong Li",
            "Futian Wang",
            "Lin Zhu",
            "Yaowei Wang",
            "Yongyong Tian",
            "Jin Tang"
        ],
        "title": "EventSTR: A Benchmark Dataset and Baselines for Event Stream based Scene Text Recognition",
        "abstract": "arXiv:2502.09020v1 Announce Type: new  Abstract: Mainstream Scene Text Recognition (STR) algorithms are developed based on RGB cameras which are sensitive to challenging factors such as low illumination, motion blur, and cluttered backgrounds. In this paper, we propose to recognize the scene text using bio-inspired event cameras by collecting and annotating a large-scale benchmark dataset, termed EventSTR. It contains 9,928 high-definition (1280 * 720) event samples and involves both Chinese and English characters. We also benchmark multiple STR algorithms as the baselines for future works to compare. In addition, we propose a new event-based scene text recognition framework, termed SimC-ESTR. It first extracts the event features using a visual encoder and projects them into tokens using a Q-former module. More importantly, we propose to augment the vision tokens based on a memory mechanism before feeding into the large language models. A similarity-based error correction mechanism is embedded within the large language model to correct potential minor errors fundamentally based on contextual information. Extensive experiments on the newly proposed EventSTR dataset and two simulation STR datasets fully demonstrate the effectiveness of our proposed model. We believe that the dataset and algorithmic model can innovatively propose an event-based STR task and are expected to accelerate the application of event cameras in various industries. The source code and pre-trained models will be released on https://github.com/Event-AHU/EventSTR",
        "arxiv_id": "2502.09020",
        "ARXIVID": "2502.09020",
        "COMMENT": "Matches criterion 3. Proposes a new benchmark dataset for event-based scene text recognition, which is relevant to embodied AI benchmarks and novel methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.09563": {
        "authors": [
            "Youming Deng",
            "Wenqi Xian",
            "Guandao Yang",
            "Leonidas Guibas",
            "Gordon Wetzstein",
            "Steve Marschner",
            "Paul Debevec"
        ],
        "title": "Self-Calibrating Gaussian Splatting for Large Field of View Reconstruction",
        "abstract": "arXiv:2502.09563v1 Announce Type: new  Abstract: In this paper, we present a self-calibrating framework that jointly optimizes camera parameters, lens distortion and 3D Gaussian representations, enabling accurate and efficient scene reconstruction. In particular, our technique enables high-quality scene reconstruction from Large field-of-view (FOV) imagery taken with wide-angle lenses, allowing the scene to be modeled from a smaller number of images. Our approach introduces a novel method for modeling complex lens distortions using a hybrid network that combines invertible residual networks with explicit grids. This design effectively regularizes the optimization process, achieving greater accuracy than conventional camera models. Additionally, we propose a cubemap-based resampling strategy to support large FOV images without sacrificing resolution or introducing distortion artifacts. Our method is compatible with the fast rasterization of Gaussian Splatting, adaptable to a wide variety of camera lens distortion, and demonstrates state-of-the-art performance on both synthetic and real-world datasets.",
        "arxiv_id": "2502.09563",
        "ARXIVID": "2502.09563",
        "COMMENT": "Matches criterion 1 as it introduces a self-calibrating framework for spatial understanding in large field-of-view reconstruction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.09620": {
        "authors": [
            "Yiwen Tang",
            "Zoey Guo",
            "Zhuhao Wang",
            "Ray Zhang",
            "Qizhi Chen",
            "Junli Liu",
            "Delin Qu",
            "Zhigang Wang",
            "Dong Wang",
            "Xuelong Li",
            "Bin Zhao"
        ],
        "title": "Exploring the Potential of Encoder-free Architectures in 3D LMMs",
        "abstract": "arXiv:2502.09620v1 Announce Type: new  Abstract: Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to overcome the challenges of encoder-based 3D Large Multimodal Models (LMMs). These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM early layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the current state-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the classification, captioning, and VQA tasks, respectively. Our results demonstrate that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at https://github.com/Ivan-Tang-3D/ENEL",
        "arxiv_id": "2502.09620",
        "ARXIVID": "2502.09620",
        "COMMENT": "Matches criterion 2 as it explores encoder-free architectures in 3D large multi-modal models, presenting a novel approach (ENEL).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.09111": {
        "authors": [
            "Mingrui Li",
            "Shuhong Liu",
            "Tianchen Deng",
            "Hongyu Wang"
        ],
        "title": "DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance Prior",
        "abstract": "arXiv:2502.09111v1 Announce Type: new  Abstract: Gaussian SLAM systems excel in real-time rendering and fine-grained reconstruction compared to NeRF-based systems. However, their reliance on extensive keyframes is impractical for deployment in real-world robotic systems, which typically operate under sparse-view conditions that can result in substantial holes in the map. To address these challenges, we introduce DenseSplat, the first SLAM system that effectively combines the advantages of NeRF and 3DGS. DenseSplat utilizes sparse keyframes and NeRF priors for initializing primitives that densely populate maps and seamlessly fill gaps. It also implements geometry-aware primitive sampling and pruning strategies to manage granularity and enhance rendering efficiency. Moreover, DenseSplat integrates loop closure and bundle adjustment, significantly enhancing frame-to-frame tracking accuracy. Extensive experiments on multiple large-scale datasets demonstrate that DenseSplat achieves superior performance in tracking and mapping compared to current state-of-the-art methods.",
        "arxiv_id": "2502.09111",
        "ARXIVID": "2502.09111",
        "COMMENT": "Matches criterion 3 as it introduces a new SLAM system, DenseSplat, with novel methods for combining NeRF and 3DGS for mapping and tracking in sparse-view conditions.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.09623": {
        "authors": [
            "Francesco Ballerini",
            "Pierluigi Zama Ramirez",
            "Samuele Salti",
            "Luigi Di Stefano"
        ],
        "title": "Embed Any NeRF: Graph Meta-Networks for Neural Tasks on Arbitrary NeRF Architectures",
        "abstract": "arXiv:2502.09623v1 Announce Type: new  Abstract: Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for representing 3D objects and scenes by encoding shape and appearance information into the weights of a neural network. Recent works have shown how such weights can be used as input to frameworks processing them to solve deep learning tasks. Yet, these frameworks can only process NeRFs with a specific, predefined architecture. In this paper, we present the first framework that can ingest NeRFs with multiple architectures and perform inference on architectures unseen at training time. We achieve this goal by training a Graph Meta-Network in a representation learning framework. Moreover, we show how a contrastive objective is conducive to obtaining an architecture-agnostic latent space. In experiments on both MLP-based and tri-planar NeRFs, our approach demonstrates robust performance in classification and retrieval tasks that either matches or exceeds that of existing frameworks constrained to single architectures, thus providing the first architecture-agnostic method to perform tasks on NeRFs by processing their weights.",
        "arxiv_id": "2502.09623",
        "ARXIVID": "2502.09623",
        "COMMENT": "This paper presents a framework for processing NeRFs across architectures, which aligns with criterion 4 (vision foundation models) and has potential applications in spatial understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 8
    },
    "2502.08904": {
        "authors": [
            "Xinxin You",
            "Xien Liu",
            "Qixin Sun",
            "Huan Zhang",
            "Kaiyin Zhou",
            "Shaohui Liu",
            "GuoPing Hu",
            "ShiJin Wang",
            "Si Liu",
            "Ji Wu"
        ],
        "title": "MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via Event-Driven Text-Code Cyclic Training",
        "abstract": "arXiv:2502.08904v1 Announce Type: new  Abstract: Recent methodologies utilizing synthetic datasets have aimed to address inconsistent hallucinations in large language models (LLMs); however,these approaches are primarily tailored to specific tasks, limiting their generalizability. Inspired by the strong performance of code-trained models in logic-intensive domains, we propose a novel framework that leverages event-based text to generate corresponding code and employs cyclic training to transfer the logical consistency of code to natural language effectively. Our method significantly reduces inconsistent hallucinations across three leading LLMs and two categories of natural language tasks while maintaining overall performance. This framework effectively alleviates hallucinations without necessitating adaptation to downstream tasks, demonstrating generality and providing new perspectives to tackle the challenge of inconsistent hallucinations.",
        "arxiv_id": "2502.08904",
        "ARXIVID": "2502.08904",
        "COMMENT": "Matches criterion 2 as it addresses hallucinations in large language models, which is relevant to improving VLLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.08884": {
        "authors": [
            "R. Kenny Jones",
            "Paul Guerrero",
            "Niloy J. Mitra",
            "Daniel Ritchie"
        ],
        "title": "ShapeLib: designing a library of procedural 3D shape abstractions with Large Language Models",
        "abstract": "arXiv:2502.08884v1 Announce Type: new  Abstract: Procedural representations are desirable, versatile, and popular shape encodings. Authoring them, either manually or using data-driven procedures, remains challenging, as a well-designed procedural representation should be compact, intuitive, and easy to manipulate. A long-standing problem in shape analysis studies how to discover a reusable library of procedural functions, with semantically aligned exposed parameters, that can explain an entire shape family. We present ShapeLib as the first method that leverages the priors of frontier LLMs to design a library of 3D shape abstraction functions. Our system accepts two forms of design intent: text descriptions of functions to include in the library and a seed set of exemplar shapes. We discover procedural abstractions that match this design intent by proposing, and then validating, function applications and implementations. The discovered shape functions in the library are not only expressive but also generalize beyond the seed set to a full family of shapes. We train a recognition network that learns to infer shape programs based on our library from different visual modalities (primitives, voxels, point clouds). Our shape functions have parameters that are semantically interpretable and can be modified to produce plausible shape variations. We show that this allows inferred programs to be successfully manipulated by an LLM given a text prompt. We evaluate ShapeLib on different datasets and show clear advantages over existing methods and alternative formulations.",
        "arxiv_id": "2502.08884",
        "ARXIVID": "2502.08884",
        "COMMENT": "Matches criterion 4 as it leverages large language models for procedural 3D shape abstractions, which is related to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.09325": {
        "authors": [
            "Haoran Chen",
            "Dong Yi",
            "Moyan Cao",
            "Chensen Huang",
            "Guibo Zhu",
            "Jinqiao Wang"
        ],
        "title": "A Benchmark for Crime Surveillance Video Analysis with Large Models",
        "abstract": "arXiv:2502.09325v1 Announce Type: new  Abstract: Anomaly analysis in surveillance videos is a crucial topic in computer vision. In recent years, multimodal large language models (MLLMs) have outperformed task-specific models in various domains. Although MLLMs are particularly versatile, their abilities to understand anomalous concepts and details are insufficiently studied because of the outdated benchmarks of this field not providing MLLM-style QAs and efficient algorithms to assess the model's open-ended text responses. To fill this gap, we propose a benchmark for crime surveillance video analysis with large models denoted as UCVL, including 1,829 videos and reorganized annotations from the UCF-Crime and UCF-Crime Annotation datasets. We design six types of questions and generate diverse QA pairs. Then we develop detailed instructions and use OpenAI's GPT-4o for accurate assessment. We benchmark eight prevailing MLLMs ranging from 0.5B to 40B parameters, and the results demonstrate the reliability of this bench. Moreover, we finetune LLaVA-OneVision on UCVL's training set. The improvement validates our data's high quality for video anomaly analysis.",
        "arxiv_id": "2502.09325",
        "ARXIVID": "2502.09325",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (UCVL) for crime surveillance video analysis with large models, focusing on anomaly detection.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.09285": {
        "authors": [
            "Xingyu Qi",
            "He Li",
            "Linjie Li",
            "Zhenyu Wu"
        ],
        "title": "EmoAssist: Emotional Assistant for Visual Impairment Community",
        "abstract": "arXiv:2502.09285v1 Announce Type: new  Abstract: The rapid advancement of large multi-modality models (LMMs) has significantly propelled the integration of artificial intelligence into practical applications. Visual Question Answering (VQA) systems, which can process multi-modal data including vision, text, and audio, hold great potential for assisting the Visual Impairment (VI) community in navigating complex and dynamic real-world environments. However, existing VI assistive LMMs overlook the emotional needs of VI individuals, and current benchmarks lack emotional evaluation of these LMMs. To address these gaps, this paper introduces the EmoAssist Benchmark, a comprehensive benchmark designed to evaluate the assistive performance of LMMs for the VI community. To the best of our knowledge, this is the first benchmark that incorporates emotional intelligence as a key consideration. Furthermore, we propose the EmoAssist Model, an Emotion-Assistive LMM specifically designed for the VI community. The EmoAssist Model utilizes Direct Preference Optimization (DPO) to align outputs with human emotional preferences. Experiment results demonstrate that the EmoAssist Model significantly enhances the recognition of implicit emotions and intentions of VI users, delivers empathetic responses, and provides actionable guidance. Specifically, it shows respective improvements of 147.8% and 89.7% in the Empathy and Suggestion metrics on the EmoAssist Benchmark, compared to the pre-tuning LMM, and even outperforms state-of-the-art LLMs such as GPT-4o.",
        "arxiv_id": "2502.09285",
        "ARXIVID": "2502.09285",
        "COMMENT": "Matches criterion 2 as it introduces a new multi-modal large language model (EmoAssist) designed for assisting visually impaired individuals with emotional intelligence.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.09447": {
        "authors": [
            "Dexian Cai",
            "Xiaocui Yang",
            "Yongkang Liu",
            "Daling Wang",
            "Shi Feng",
            "Yifei Zhang",
            "Soujanya Poria"
        ],
        "title": "Pixel-Level Reasoning Segmentation via Multi-turn Conversations",
        "abstract": "arXiv:2502.09447v1 Announce Type: new  Abstract: Existing visual perception systems focus on region-level segmentation in single-turn dialogues, relying on complex and explicit query instructions. Such systems cannot reason at the pixel level and comprehend dynamic user intent that changes over interaction. Our work tackles this issue by introducing a novel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on multi-turn conversations, tracking evolving user intent via multi-turn interactions for fine-grained segmentation. To establish a benchmark for this novel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on Multi-Turn Conversations (PRIST), comprising 24k utterances from 8.3k multi-turn conversational scenarios with segmentation targets. Building on PRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning Segmentation framework, integrates pixel-level segmentation with robust multi-turn conversation understanding, generating pixel-grounded explanations aligned with user intent. The PRIST dataset and MIRSA framework fill the gap in pixel-level reasoning segmentation. Experimental results on the PRIST dataset demonstrate that our method outperforms current segmentation-specific baselines in terms of segmentation and LLM-based reasoning metrics. The code and data are available at: https://github.com/ccccai239/PixelRIST.",
        "arxiv_id": "2502.09447",
        "ARXIVID": "2502.09447",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (PRIST) and a novel framework (MIRAS) for pixel-level reasoning segmentation in multi-turn conversations.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.08974": {
        "authors": [
            "Yiming Yang",
            "Yueru Luo",
            "Bingkun He",
            "Erlong Li",
            "Zhipeng Cao",
            "Chao Zheng",
            "Shuqi Mei",
            "Zhen Li"
        ],
        "title": "Topo2Seq: Enhanced Topology Reasoning via Topology Sequence Learning",
        "abstract": "arXiv:2502.08974v1 Announce Type: new  Abstract: Extracting lane topology from perspective views (PV) is crucial for planning and control in autonomous driving. This approach extracts potential drivable trajectories for self-driving vehicles without relying on high-definition (HD) maps. However, the unordered nature and weak long-range perception of the DETR-like framework can result in misaligned segment endpoints and limited topological prediction capabilities. Inspired by the learning of contextual relationships in language models, the connectivity relations in roads can be characterized as explicit topology sequences. In this paper, we introduce Topo2Seq, a novel approach for enhancing topology reasoning via topology sequences learning. The core concept of Topo2Seq is a randomized order prompt-to-sequence learning between lane segment decoder and topology sequence decoder. The dual-decoder branches simultaneously learn the lane topology sequences extracted from the Directed Acyclic Graph (DAG) and the lane graph containing geometric information. Randomized order prompt-to-sequence learning extracts unordered key points from the lane graph predicted by the lane segment decoder, which are then fed into the prompt design of the topology sequence decoder to reconstruct an ordered and complete lane graph. In this way, the lane segment decoder learns powerful long-range perception and accurate topological reasoning from the topology sequence decoder. Notably, topology sequence decoder is only introduced during training and does not affect the inference efficiency. Experimental evaluations on the OpenLane-V2 dataset demonstrate the state-of-the-art performance of Topo2Seq in topology reasoning.",
        "arxiv_id": "2502.08974",
        "ARXIVID": "2502.08974",
        "COMMENT": "This paper introduces Topo2Seq for topology reasoning in autonomous driving, which aligns with criterion 1 (spatial understanding) and criterion 3 (embodied AI methods).",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2502.09613": {
        "authors": [
            "Chaoyi Zhou",
            "Xi Liu",
            "Feng Luo",
            "Siyu Huang"
        ],
        "title": "Latent Radiance Fields with 3D-aware 2D Representations",
        "abstract": "arXiv:2502.09613v1 Announce Type: new  Abstract: Latent 3D reconstruction has shown great promise in empowering 3D semantic understanding and 3D generation by distilling 2D features into the 3D space. However, existing approaches struggle with the domain gap between 2D feature space and 3D representations, resulting in degraded rendering performance. To address this challenge, we propose a novel framework that integrates 3D awareness into the 2D latent space. The framework consists of three stages: (1) a correspondence-aware autoencoding method that enhances the 3D consistency of 2D latent representations, (2) a latent radiance field (LRF) that lifts these 3D-aware 2D representations into 3D space, and (3) a VAE-Radiance Field (VAE-RF) alignment strategy that improves image decoding from the rendered 2D representations. Extensive experiments demonstrate that our method outperforms the state-of-the-art latent 3D reconstruction approaches in terms of synthesis performance and cross-dataset generalizability across diverse indoor and outdoor scenes. To our knowledge, this is the first work showing the radiance field representations constructed from 2D latent representations can yield photorealistic 3D reconstruction performance.",
        "arxiv_id": "2502.09613",
        "ARXIVID": "2502.09613",
        "COMMENT": "This paper introduces a novel framework for 3D-aware 2D representations and latent radiance fields, which aligns with criterion 1 (spatial understanding) and criterion 4 (vision foundation models).",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2502.09608": {
        "authors": [
            "Mia Tang",
            "Yael Vinker",
            "Chuan Yan",
            "Lvmin Zhang",
            "Maneesh Agrawala"
        ],
        "title": "Instance Segmentation of Scene Sketches Using Natural Image Priors",
        "abstract": "arXiv:2502.09608v1 Announce Type: new  Abstract: Sketch segmentation involves grouping pixels within a sketch that belong to the same object or instance. It serves as a valuable tool for sketch editing tasks, such as moving, scaling, or removing specific components. While image segmentation models have demonstrated remarkable capabilities in recent years, sketches present unique challenges for these models due to their sparse nature and wide variation in styles. We introduce SketchSeg, a method for instance segmentation of raster scene sketches. Our approach adapts state-of-the-art image segmentation and object detection models to the sketch domain by employing class-agnostic fine-tuning and refining segmentation masks using depth cues. Furthermore, our method organizes sketches into sorted layers, where occluded instances are inpainted, enabling advanced sketch editing applications. As existing datasets in this domain lack variation in sketch styles, we construct a synthetic scene sketch segmentation dataset featuring sketches with diverse brush strokes and varying levels of detail. We use this dataset to demonstrate the robustness of our approach and will release it to promote further research in the field.   Project webpage: https://sketchseg.github.io/sketch-seg/",
        "arxiv_id": "2502.09608",
        "ARXIVID": "2502.09608",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models and their application to sketch segmentation, introducing a new dataset and method.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.09528": {
        "authors": [
            "Jack Erhardt",
            "Ziang Li",
            "Reid Pinkham",
            "Andrew Berkovich",
            "Zhengya Zhang"
        ],
        "title": "SteROI-D: System Design and Mapping for Stereo Depth Inference on Regions of Interest",
        "abstract": "arXiv:2502.09528v1 Announce Type: new  Abstract: Machine learning algorithms have enabled high quality stereo depth estimation to run on Augmented and Virtual Reality (AR/VR) devices. However, high energy consumption across the full image processing stack prevents stereo depth algorithms from running effectively on battery-limited devices. This paper introduces SteROI-D, a full stereo depth system paired with a mapping methodology. SteROI-D exploits Region-of-Interest (ROI) and temporal sparsity at the system level to save energy. SteROI-D's flexible and heterogeneous compute fabric supports diverse ROIs. Importantly, we introduce a systematic mapping methodology to effectively handle dynamic ROIs, thereby maximizing energy savings. Using these techniques, our 28nm prototype SteROI-D design achieves up to 4.35x reduction in total system energy compared to a baseline ASIC.",
        "arxiv_id": "2502.09528",
        "ARXIVID": "2502.09528",
        "COMMENT": "This paper introduces SteROI-D, a stereo depth system for AR/VR devices, which aligns with criterion 1 (spatial understanding) and criterion 3 (embodied AI methods).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.09242": {
        "authors": [
            "Lukas Buess",
            "Matthias Keicher",
            "Nassir Navab",
            "Andreas Maier",
            "Soroosh Tayebi Arasteh"
        ],
        "title": "From large language models to multimodal AI: A scoping review on the potential of generative AI in medicine",
        "abstract": "arXiv:2502.09242v1 Announce Type: new  Abstract: Generative artificial intelligence (AI) models, such as diffusion models and OpenAI's ChatGPT, are transforming medicine by enhancing diagnostic accuracy and automating clinical workflows. The field has advanced rapidly, evolving from text-only large language models for tasks such as clinical documentation and decision support to multimodal AI systems capable of integrating diverse data modalities, including imaging, text, and structured data, within a single model. The diverse landscape of these technologies, along with rising interest, highlights the need for a comprehensive review of their applications and potential. This scoping review explores the evolution of multimodal AI, highlighting its methods, applications, datasets, and evaluation in clinical settings. Adhering to PRISMA-ScR guidelines, we systematically queried PubMed, IEEE Xplore, and Web of Science, prioritizing recent studies published up to the end of 2024. After rigorous screening, 144 papers were included, revealing key trends and challenges in this dynamic field. Our findings underscore a shift from unimodal to multimodal approaches, driving innovations in diagnostic support, medical report generation, drug discovery, and conversational AI. However, critical challenges remain, including the integration of heterogeneous data types, improving model interpretability, addressing ethical concerns, and validating AI systems in real-world clinical settings. This review summarizes the current state of the art, identifies critical gaps, and provides insights to guide the development of scalable, trustworthy, and clinically impactful multimodal AI solutions in healthcare.",
        "arxiv_id": "2502.09242",
        "ARXIVID": "2502.09242",
        "COMMENT": "Matches criterion 2 as it discusses the evolution of multimodal AI, including visual large language models, and their applications in medicine.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2502.09520": {
        "authors": [
            "Francesco Pezone",
            "Sergio Barbarossa",
            "Giuseppe Caire"
        ],
        "title": "SQ-GAN: Semantic Image Communications Using Masked Vector Quantization",
        "abstract": "arXiv:2502.09520v1 Announce Type: new  Abstract: This work introduces Semantically Masked VQ-GAN (SQ-GAN), a novel approach integrating generative models to optimize image compression for semantic/task-oriented communications. SQ-GAN employs off-the-shelf semantic semantic segmentation and a new specifically developed semantic-conditioned adaptive mask module (SAMM) to selectively encode semantically significant features of the images. SQ-GAN outperforms state-of-the-art image compression schemes such as JPEG2000 and BPG across multiple metrics, including perceptual quality and semantic segmentation accuracy on the post-decoding reconstructed image, at extreme low compression rates expressed in bits per pixel.",
        "arxiv_id": "2502.09520",
        "ARXIVID": "2502.09520",
        "COMMENT": "This paper introduces SQ-GAN for semantic image compression, which is tangentially related to multi-modal learning but does not directly match any criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.09533": {
        "authors": [
            "Fei Shen",
            "Cong Wang",
            "Junyao Gao",
            "Qin Guo",
            "Jisheng Dang",
            "Jinhui Tang",
            "Tat-Seng Chua"
        ],
        "title": "Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion Model",
        "abstract": "arXiv:2502.09533v1 Announce Type: new  Abstract: Recent advances in conditional diffusion models have shown promise for generating realistic TalkingFace videos, yet challenges persist in achieving consistent head movement, synchronized facial expressions, and accurate lip synchronization over extended generations. To address these, we introduce the \\textbf{M}otion-priors \\textbf{C}onditional \\textbf{D}iffusion \\textbf{M}odel (\\textbf{MCDM}), which utilizes both archived and current clip motion priors to enhance motion prediction and ensure temporal consistency. The model consists of three key elements: (1) an archived-clip motion-prior that incorporates historical frames and a reference frame to preserve identity and context; (2) a present-clip motion-prior diffusion model that captures multimodal causality for accurate predictions of head movements, lip sync, and expressions; and (3) a memory-efficient temporal attention mechanism that mitigates error accumulation by dynamically storing and updating motion features. We also release the \\textbf{TalkingFace-Wild} dataset, a multilingual collection of over 200 hours of footage across 10 languages. Experimental results demonstrate the effectiveness of MCDM in maintaining identity and motion continuity for long-term TalkingFace generation. Code, models, and datasets will be publicly available.",
        "arxiv_id": "2502.09533",
        "ARXIVID": "2502.09533",
        "COMMENT": "Does not match any specific criteria. Focuses on long-term talking face generation, which is not directly related to spatial understanding, embodied AI, or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.09164": {
        "authors": [
            "Trung X. Pham",
            "Zhang Kang",
            "Ji Woo Hong",
            "Xuran Zheng",
            "Chang D. Yoo"
        ],
        "title": "E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot Object Customization",
        "abstract": "arXiv:2502.09164v1 Announce Type: new  Abstract: We propose E-MD3C ($\\underline{E}$fficient $\\underline{M}$asked $\\underline{D}$iffusion Transformer with Disentangled $\\underline{C}$onditions and $\\underline{C}$ompact $\\underline{C}$ollector), a highly efficient framework for zero-shot object image customization. Unlike prior works reliant on resource-intensive Unet architectures, our approach employs lightweight masked diffusion transformers operating on latent patches, offering significantly improved computational efficiency. The framework integrates three core components: (1) an efficient masked diffusion transformer for processing autoencoder latents, (2) a disentangled condition design that ensures compactness while preserving background alignment and fine details, and (3) a learnable Conditions Collector that consolidates multiple inputs into a compact representation for efficient denoising and learning. E-MD3C outperforms the existing approach on the VITON-HD dataset across metrics such as PSNR, FID, SSIM, and LPIPS, demonstrating clear advantages in parameters, memory efficiency, and inference speed. With only $\\frac{1}{4}$ of the parameters, our Transformer-based 468M model delivers $2.5\\times$ faster inference and uses $\\frac{2}{3}$ of the GPU memory compared to an 1720M Unet-based latent diffusion model.",
        "arxiv_id": "2502.09164",
        "ARXIVID": "2502.09164",
        "COMMENT": "Does not match any specific criteria. Focuses on efficient diffusion transformers for object customization, which is not directly related to spatial understanding, embodied AI, or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.09617": {
        "authors": [
            "Jing Wen",
            "Alexander G. Schwing",
            "Shenlong Wang"
        ],
        "title": "LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback Over Multi-Resolution Gaussians-on-Mesh",
        "abstract": "arXiv:2502.09617v1 Announce Type: new  Abstract: Generalizable rendering of an animatable human avatar from sparse inputs relies on data priors and inductive biases extracted from training on large data to avoid scene-specific optimization and to enable fast reconstruction. This raises two main challenges: First, unlike iterative gradient-based adjustment in scene-specific optimization, generalizable methods must reconstruct the human shape representation in a single pass at inference time. Second, rendering is preferably computationally efficient yet of high resolution. To address both challenges we augment the recently proposed dual shape representation, which combines the benefits of a mesh and Gaussian points, in two ways. To improve reconstruction, we propose an iterative feedback update framework, which successively improves the canonical human shape representation during reconstruction. To achieve computationally efficient yet high-resolution rendering, we study a coupled-multi-resolution Gaussians-on-Mesh representation. We evaluate the proposed approach on the challenging THuman2.0, XHuman and AIST++ data. Our approach reconstructs an animatable representation from sparse inputs in less than 1s, renders views with 95.1FPS at $1024 \\times 1024$, and achieves PSNR/LPIPS*/FID of 24.65/110.82/51.27 on THuman2.0, outperforming the state-of-the-art in rendering quality.",
        "arxiv_id": "2502.09617",
        "ARXIVID": "2502.09617",
        "COMMENT": "Does not match any specific criteria. Focuses on human rendering and reconstruction, which is not directly related to spatial understanding, embodied AI, or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.09596": {
        "authors": [
            "Zitao Li",
            "Fei Wei",
            "Yuexiang Xie",
            "Dawei Gao",
            "Weirui Kuang",
            "Zhijian Ma",
            "Bingchen Qian",
            "Yaliang Li",
            "Bolin Ding"
        ],
        "title": "KIMAs: A Configurable Knowledge Integrated Multi-Agent System",
        "abstract": "arXiv:2502.09596v1 Announce Type: new  Abstract: Knowledge-intensive conversations supported by large language models (LLMs) have become one of the most popular and helpful applications that can assist people in different aspects. Many current knowledge-intensive applications are centered on retrieval-augmented generation (RAG) techniques. While many open-source RAG frameworks facilitate the development of RAG-based applications, they often fall short in handling practical scenarios complicated by heterogeneous data in topics and formats, conversational context management, and the requirement of low-latency response times. This technical report presents a configurable knowledge integrated multi-agent system, KIMAs, to address these challenges. KIMAs features a flexible and configurable system for integrating diverse knowledge sources with 1) context management and query rewrite mechanisms to improve retrieval accuracy and multi-turn conversational coherency, 2) efficient knowledge routing and retrieval, 3) simple but effective filter and reference generation mechanisms, and 4) optimized parallelizable multi-agent pipeline execution. Our work provides a scalable framework for advancing the deployment of LLMs in real-world settings. To show how KIMAs can help developers build knowledge-intensive applications with different scales and emphases, we demonstrate how we configure the system to three applications already running in practice with reliable performance.",
        "arxiv_id": "2502.09596",
        "ARXIVID": "2502.09596",
        "COMMENT": "This paper introduces KIMAs, a multi-agent system for knowledge-intensive conversations, which is tangentially related to multi-modal learning but does not directly match any criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.08922": {
        "authors": [
            "Xin Zhou",
            "Yiwen Guo",
            "Ruotian Ma",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "title": "Self-Consistency of the Internal Reward Models Improves Self-Rewarding Language Models",
        "abstract": "arXiv:2502.08922v1 Announce Type: new  Abstract: Aligning Large Language Models (LLMs) with human preferences is crucial for their deployment in real-world applications. Recent advancements in Self-Rewarding Language Models suggest that an LLM can use its internal reward models (such as LLM-as-a-Judge) \\cite{yuanself} to generate preference data, improving alignment performance without costly human annotation. However, we find that different internal reward models within the same LLM often generate inconsistent preferences. This inconsistency raises concerns about the reliability of self-generated preference data, hinders overall alignment performance, and highlights the need for further research to ensure reliable and coherent alignment with human preferences. To address this limitation, we propose Self-Consistent Internal Rewards (SCIR), a novel framework designed to enhance consistency among internal reward models during training. In each training step, we collect preference predictions from multiple pre-defined internal reward models and enforce consistency and confidence through an inconsistency penalty mechanism, thereby improving the reliability of these internal reward models. We selectively use data with consistent predictions for preference optimization, ensuring the quality of the preference data. By employing self-consistent internal rewards, our method significantly improves the alignment performance and reward modeling capability of LLMs, outperforming baseline methods by a notable margin.",
        "arxiv_id": "2502.08922",
        "ARXIVID": "2502.08922",
        "COMMENT": "This paper proposes a framework for improving self-rewarding language models, which is tangentially related to multi-modal learning but does not directly match any criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.08940": {
        "authors": [
            "Jingyang Li",
            "Jiachun Pan",
            "Kim-Chuan Toh",
            "Pan Zhou"
        ],
        "title": "Towards Understanding Why Data Augmentation Improves Generalization",
        "abstract": "arXiv:2502.08940v1 Announce Type: new  Abstract: Data augmentation is a cornerstone technique in deep learning, widely used to improve model generalization. Traditional methods like random cropping and color jittering, as well as advanced techniques such as CutOut, Mixup, and CutMix, have achieved notable success across various domains. However, the mechanisms by which data augmentation improves generalization remain poorly understood, and existing theoretical analyses typically focus on individual techniques without a unified explanation. In this work, we present a unified theoretical framework that elucidates how data augmentation enhances generalization through two key effects: partial semantic feature removal and feature mixing. Partial semantic feature removal reduces the model's reliance on individual feature, promoting diverse feature learning and better generalization. Feature mixing, by scaling down original semantic features and introducing noise, increases training complexity, driving the model to develop more robust features. Advanced methods like CutMix integrate both effects, achieving complementary benefits. Our theoretical insights are further supported by experimental results, validating the effectiveness of this unified perspective.",
        "arxiv_id": "2502.08940",
        "ARXIVID": "2502.08940",
        "COMMENT": "Does not match any specific criteria. Focuses on understanding data augmentation, which is not directly related to spatial understanding, embodied AI, or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.09125": {
        "authors": [
            "Xiang Liu",
            "Mingchen Li",
            "Xia Li",
            "Leigang Qu",
            "Zifan Peng",
            "Yijun Song",
            "Zemin Liu",
            "Linshan Jiang",
            "Jialin Li"
        ],
        "title": "Automatic Pruning via Structured Lasso with Class-wise Information",
        "abstract": "arXiv:2502.09125v1 Announce Type: new  Abstract: Most pruning methods concentrate on unimportant filters of neural networks. However, they face the loss of statistical information due to a lack of consideration for class-wise data. In this paper, from the perspective of leveraging precise class-wise information for model pruning, we utilize structured lasso with guidance from Information Bottleneck theory. Our approach ensures that statistical information is retained during the pruning process. With these techniques, we introduce two innovative adaptive network pruning schemes: sparse graph-structured lasso pruning with Information Bottleneck (\\textbf{sGLP-IB}) and sparse tree-guided lasso pruning with Information Bottleneck (\\textbf{sTLP-IB}). The key aspect is pruning model filters using sGLP-IB and sTLP-IB to better capture class-wise relatedness. Compared to multiple state-of-the-art methods, our approaches demonstrate superior performance across three datasets and six model architectures in extensive experiments. For instance, using the VGG16 model on the CIFAR-10 dataset, we achieve a parameter reduction of 85%, a decrease in FLOPs by 61%, and maintain an accuracy of 94.10% (0.14% higher than the original model); we reduce the parameters by 55% with the accuracy at 76.12% using the ResNet architecture on ImageNet (only drops 0.03%). In summary, we successfully reduce model size and computational resource usage while maintaining accuracy. Our codes are at https://anonymous.4open.science/r/IJCAI-8104.",
        "arxiv_id": "2502.09125",
        "ARXIVID": "2502.09125",
        "COMMENT": "Does not match any specific criteria. Focuses on pruning neural networks with class-wise information, which is not directly related to spatial understanding, embodied AI, or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.09143": {
        "authors": [
            "Adjovi Sim",
            "Zhengkui Wang",
            "Aik Beng Ng",
            "Shalini De Mello",
            "Simon See",
            "Wonmin Byeon"
        ],
        "title": "Feature-based Graph Attention Networks Improve Online Continual Learning",
        "abstract": "arXiv:2502.09143v1 Announce Type: new  Abstract: Online continual learning for image classification is crucial for models to adapt to new data while retaining knowledge of previously learned tasks. This capability is essential to address real-world challenges involving dynamic environments and evolving data distributions. Traditional approaches predominantly employ Convolutional Neural Networks, which are limited to processing images as grids and primarily capture local patterns rather than relational information. Although the emergence of transformer architectures has improved the ability to capture relationships, these models often require significantly larger resources. In this paper, we present a novel online continual learning framework based on Graph Attention Networks (GATs), which effectively capture contextual relationships and dynamically update the task-specific representation via learned attention weights. Our approach utilizes a pre-trained feature extractor to convert images into graphs using hierarchical feature maps, representing information at varying levels of granularity. These graphs are then processed by a GAT and incorporate an enhanced global pooling strategy to improve classification performance for continual learning. In addition, we propose the rehearsal memory duplication technique that improves the representation of the previous tasks while maintaining the memory budget. Comprehensive evaluations on benchmark datasets, including SVHN, CIFAR10, CIFAR100, and MiniImageNet, demonstrate the superiority of our method compared to the state-of-the-art methods.",
        "arxiv_id": "2502.09143",
        "ARXIVID": "2502.09143",
        "COMMENT": "Does not match any specific criteria. Focuses on continual learning with Graph Attention Networks, which is not directly related to spatial understanding, embodied AI, or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.09274": {
        "authors": [
            "Bin Yang",
            "Alexandru Paul Condurache"
        ],
        "title": "FLARES: Fast and Accurate LiDAR Multi-Range Semantic Segmentation",
        "abstract": "arXiv:2502.09274v1 Announce Type: new  Abstract: 3D scene understanding is a critical yet challenging task in autonomous driving, primarily due to the irregularity and sparsity of LiDAR data, as well as the computational demands of processing large-scale point clouds. Recent methods leverage the range-view representation to improve processing efficiency. To mitigate the performance drop caused by information loss inherent to the \"many-to-one\" problem, where multiple nearby 3D points are mapped to the same 2D grids and only the closest is retained, prior works tend to choose a higher azimuth resolution for range-view projection. However, this can bring the drawback of reducing the proportion of pixels that carry information and heavier computation within the network. We argue that it is not the optimal solution and show that, in contrast, decreasing the resolution is more advantageous in both efficiency and accuracy. In this work, we present a comprehensive re-design of the workflow for range-view-based LiDAR semantic segmentation. Our approach addresses data representation, augmentation, and post-processing methods for improvements. Through extensive experiments on two public datasets, we demonstrate that our pipeline significantly enhances the performance of various network architectures over their baselines, paving the way for more effective LiDAR-based perception in autonomous systems.",
        "arxiv_id": "2502.09274",
        "ARXIVID": "2502.09274",
        "COMMENT": "Does not match any specific criterion but is related to LiDAR-based semantic segmentation, which is tangentially relevant to spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.08754": {
        "authors": [
            "Valentina Vadori",
            "Jean-Marie Gra\\\"ic",
            "Antonella Peruffo",
            "Livio Finos",
            "Ujwala Kiran Chaudhari",
            "Enrico Grisan"
        ],
        "title": "HistoSmith: Single-Stage Histology Image-Label Generation via Conditional Latent Diffusion for Enhanced Cell Segmentation and Classification",
        "abstract": "arXiv:2502.08754v1 Announce Type: new  Abstract: Precise segmentation and classification of cell instances are vital for analyzing the tissue microenvironment in histology images, supporting medical diagnosis, prognosis, treatment planning, and studies of brain cytoarchitecture. However, the creation of high-quality annotated datasets for training remains a major challenge. This study introduces a novel single-stage approach (HistoSmith) for generating image-label pairs to augment histology datasets. Unlike state-of-the-art methods that utilize diffusion models with separate components for label and image generation, our approach employs a latent diffusion model to learn the joint distribution of cellular layouts, classification masks, and histology images. This model enables tailored data generation by conditioning on user-defined parameters such as cell types, quantities, and tissue types. Trained on the Conic H&E histopathology dataset and the Nissl-stained CytoDArk0 dataset, the model generates realistic and diverse labeled samples. Experimental results demonstrate improvements in cell instance segmentation and classification, particularly for underrepresented cell types like neutrophils in the Conic dataset. These findings underscore the potential of our approach to address data scarcity challenges.",
        "arxiv_id": "2502.08754",
        "ARXIVID": "2502.08754",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling for histology image-label generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.09296": {
        "authors": [
            "Mojtaba Safari",
            "Shansong Wang",
            "Zach Eidex",
            "Richard Qiu",
            "Chih-Wei Chang",
            "David S. Yu",
            "Xiaofeng Yang"
        ],
        "title": "A Physics-Informed Deep Learning Model for MRI Brain Motion Correction",
        "abstract": "arXiv:2502.09296v1 Announce Type: new  Abstract: Background: MRI is crucial for brain imaging but is highly susceptible to motion artifacts due to long acquisition times. This study introduces PI-MoCoNet, a physics-informed motion correction network that integrates spatial and k-space information to remove motion artifacts without explicit motion parameter estimation, enhancing image fidelity and diagnostic reliability. Materials and Methods: PI-MoCoNet consists of a motion detection network (U-net with spatial averaging) to identify corrupted k-space lines and a motion correction network (U-net with Swin Transformer blocks) to reconstruct motion-free images. The correction is guided by three loss functions: reconstruction (L1), perceptual (LPIPS), and data consistency (Ldc). Motion artifacts were simulated via rigid phase encoding perturbations and evaluated on IXI and MR-ART datasets against Pix2Pix, CycleGAN, and U-net using PSNR, SSIM, and NMSE. Results: PI-MoCoNet significantly improved image quality. On IXI, for minor artifacts, PSNR increased from 34.15 dB to 45.95 dB, SSIM from 0.87 to 1.00, and NMSE reduced from 0.55% to 0.04%. For moderate artifacts, PSNR improved from 30.23 dB to 42.16 dB, SSIM from 0.80 to 0.99, and NMSE from 1.32% to 0.09%. For heavy artifacts, PSNR rose from 27.99 dB to 36.01 dB, SSIM from 0.75 to 0.97, and NMSE decreased from 2.21% to 0.36%. On MR-ART, PI-MoCoNet achieved PSNR gains of ~10 dB and SSIM improvements of up to 0.20, with NMSE reductions of ~6%. Ablation studies confirmed the importance of data consistency and perceptual losses, yielding a 1 dB PSNR gain and 0.17% NMSE reduction. Conclusions: PI-MoCoNet effectively mitigates motion artifacts in brain MRI, outperforming existing methods. Its ability to integrate spatial and k-space information makes it a promising tool for clinical use in motion-prone settings. Code: https://github.com/mosaf/PI-MoCoNet.git.",
        "arxiv_id": "2502.09296",
        "ARXIVID": "2502.09296",
        "COMMENT": "Does not match any specific criterion but is related to spatial understanding in the context of MRI motion correction.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.09471": {
        "authors": [
            "Yi Yu",
            "Xue Yang",
            "Yansheng Li",
            "Zhenjun Han",
            "Feipeng Da",
            "Junchi Yan"
        ],
        "title": "Wholly-WOOD: Wholly Leveraging Diversified-quality Labels for Weakly-supervised Oriented Object Detection",
        "abstract": "arXiv:2502.09471v1 Announce Type: new  Abstract: Accurately estimating the orientation of visual objects with compact rotated bounding boxes (RBoxes) has become a prominent demand, which challenges existing object detection paradigms that only use horizontal bounding boxes (HBoxes). To equip the detectors with orientation awareness, supervised regression/classification modules have been introduced at the high cost of rotation annotation. Meanwhile, some existing datasets with oriented objects are already annotated with horizontal boxes or even single points. It becomes attractive yet remains open for effectively utilizing weaker single point and horizontal annotations to train an oriented object detector (OOD). We develop Wholly-WOOD, a weakly-supervised OOD framework, capable of wholly leveraging various labeling forms (Points, HBoxes, RBoxes, and their combination) in a unified fashion. By only using HBox for training, our Wholly-WOOD achieves performance very close to that of the RBox-trained counterpart on remote sensing and other areas, significantly reducing the tedious efforts on labor-intensive annotation for oriented objects. The source codes are available at https://github.com/VisionXLab/whollywood (PyTorch-based) and https://github.com/VisionXLab/whollywood-jittor (Jittor-based).",
        "arxiv_id": "2502.09471",
        "ARXIVID": "2502.09471",
        "COMMENT": "Does not match any specific criterion but is related to object detection, which is tangentially relevant to spatial understanding.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.08769": {
        "authors": [
            "Timoth\\'ee Darcet",
            "Federico Baldassarre",
            "Maxime Oquab",
            "Julien Mairal",
            "Piotr Bojanowski"
        ],
        "title": "Cluster and Predict Latents Patches for Improved Masked Image Modeling",
        "abstract": "arXiv:2502.08769v1 Announce Type: new  Abstract: Masked Image Modeling (MIM) offers a promising approach to self-supervised representation learning, however existing MIM models still lag behind the state-of-the-art. In this paper, we systematically analyze target representations, loss functions, and architectures, to introduce CAPI - a novel pure-MIM framework that relies on the prediction of latent clusterings. Our approach leverages a clustering-based loss, which is stable to train, and exhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8% accuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes, substantially outperforming previous MIM methods and approaching the performance of the current state-of-the-art, DINOv2. We release all our code and models.",
        "arxiv_id": "2502.08769",
        "ARXIVID": "2502.08769",
        "COMMENT": "Relevant to general interest in computer vision and self-supervised learning but does not directly match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.09356": {
        "authors": [
            "Gabriel Tseng",
            "Anthony Fuller",
            "Marlena Reil",
            "Henry Herzog",
            "Patrick Beukema",
            "Favyen Bastani",
            "James R. Green",
            "Evan Shelhamer",
            "Hannah Kerner",
            "David Rolnick"
        ],
        "title": "Galileo: Learning Global and Local Features in Pretrained Remote Sensing Models",
        "abstract": "arXiv:2502.09356v1 Announce Type: new  Abstract: From crop mapping to flood detection, machine learning in remote sensing has a wide range of societally beneficial applications. The commonalities between remote sensing data in these applications present an opportunity for pretrained machine learning models tailored to remote sensing to reduce the labeled data and effort required to solve individual tasks. However, such models must be: (i) flexible enough to ingest input data of varying sensor modalities and shapes (i.e., of varying spatial and temporal dimensions), and (ii) able to model Earth surface phenomena of varying scales and types. To solve this gap, we present Galileo, a family of pretrained remote sensing models designed to flexibly process multimodal remote sensing data. We also introduce a novel and highly effective self-supervised learning approach to learn both large- and small-scale features, a challenge not addressed by previous models. Our Galileo models obtain state-of-the-art results across diverse remote sensing tasks.",
        "arxiv_id": "2502.09356",
        "ARXIVID": "2502.09356",
        "COMMENT": "Relevant to general interest in vision foundation models but does not directly match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.09573": {
        "authors": [
            "Mark Beliaev",
            "Victor Yang",
            "Madhura Raju",
            "Jiachen Sun",
            "Xinghai Hu"
        ],
        "title": "Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt Engineering",
        "abstract": "arXiv:2502.09573v1 Announce Type: new  Abstract: In this study, we tackle industry challenges in video content classification by exploring and optimizing GPT-based models for zero-shot classification across seven critical categories of video quality. We contribute a novel approach to improving GPT's performance through prompt optimization and policy refinement, demonstrating that simplifying complex policies significantly reduces false negatives. Additionally, we introduce a new decomposition-aggregation-based prompt engineering technique, which outperforms traditional single-prompt methods. These experiments, conducted on real industry problems, show that thoughtful prompt design can substantially enhance GPT's performance without additional finetuning, offering an effective and scalable solution for improving video classification systems across various domains in industry.",
        "arxiv_id": "2502.09573",
        "ARXIVID": "2502.09573",
        "COMMENT": "This paper explores prompt engineering for GPT-based models in video understanding, which does not directly match any of the criteria but is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.09282": {
        "authors": [
            "Swadhin Das",
            "Raksha Sharma"
        ],
        "title": "FE-LWS: Refined Image-Text Representations via Decoder Stacking and Fused Encodings for Remote Sensing Image Captioning",
        "abstract": "arXiv:2502.09282v1 Announce Type: new  Abstract: Remote sensing image captioning aims to generate descriptive text from remote sensing images, typically employing an encoder-decoder framework. In this setup, a convolutional neural network (CNN) extracts feature representations from the input image, which then guide the decoder in a sequence-to-sequence caption generation process. Although much research has focused on refining the decoder, the quality of image representations from the encoder remains crucial for accurate captioning. This paper introduces a novel approach that integrates features from two distinct CNN based encoders, capturing complementary information to enhance caption generation. Additionally, we propose a weighted averaging technique to combine the outputs of all GRUs in the stacked decoder. Furthermore, a comparison-based beam search strategy is incorporated to refine caption selection. The results demonstrate that our fusion-based approach, along with the enhanced stacked decoder, significantly outperforms both the transformer-based state-of-the-art model and other LSTM-based baselines.",
        "arxiv_id": "2502.09282",
        "ARXIVID": "2502.09282",
        "COMMENT": "Does not match any specific criterion but is related to image-text representations, which is tangentially relevant to vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.09100": {
        "authors": [
            "Hanmeng Liu",
            "Zhizhang Fu",
            "Mengru Ding",
            "Ruoxi Ning",
            "Chaoli Zhang",
            "Xiaozhang Liu",
            "Yue Zhang"
        ],
        "title": "Logical Reasoning in Large Language Models: A Survey",
        "abstract": "arXiv:2502.09100v1 Announce Type: new  Abstract: With the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, their ability to perform rigorous logical reasoning remains an open question. This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of AI research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. We analyze existing capabilities across different reasoning paradigms - deductive, inductive, abductive, and analogical - and assess strategies to enhance reasoning performance, including data-centric tuning, reinforcement learning, decoding strategies, and neuro-symbolic approaches. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems.",
        "arxiv_id": "2502.09100",
        "ARXIVID": "2502.09100",
        "COMMENT": "Relevant to general interest in reasoning within large language models but does not directly match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.08674": {
        "authors": [
            "Dongliang Zhou",
            "Haijun Zhang",
            "Qun Li",
            "Jianghong Ma",
            "Xiaofei Xu"
        ],
        "title": "COutfitGAN: Learning to Synthesize Compatible Outfits Supervised by Silhouette Masks and Fashion Styles",
        "abstract": "arXiv:2502.08674v1 Announce Type: new  Abstract: How to recommend outfits has gained considerable attention in both academia and industry in recent years. Many studies have been carried out regarding fashion compatibility learning, to determine whether the fashion items in an outfit are compatible or not. These methods mainly focus on evaluating the compatibility of existing outfits and rarely consider applying such knowledge to 'design' new fashion items. We propose the new task of generating complementary and compatible fashion items based on an arbitrary number of given fashion items. In particular, given some fashion items that can make up an outfit, the aim of this paper is to synthesize photo-realistic images of other, complementary, fashion items that are compatible with the given ones. To achieve this, we propose an outfit generation framework, referred to as COutfitGAN, which includes a pyramid style extractor, an outfit generator, a UNet-based real/fake discriminator, and a collocation discriminator. To train and evaluate this framework, we collected a large-scale fashion outfit dataset with over 200K outfits and 800K fashion items from the Internet. Extensive experiments show that COutfitGAN outperforms other baselines in terms of similarity, authenticity, and compatibility measurements.",
        "arxiv_id": "2502.08674",
        "ARXIVID": "2502.08674",
        "COMMENT": "Relevant to generative modeling in multi-modal learning but does not directly match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.09211": {
        "authors": [
            "Jakob Johannes Bauer (ETH Zuerich",
            "Switzerland)",
            "Thomas Eiter (TU Wien",
            "Austria)",
            "Nelson Higuera Ruiz (TU Wien",
            "Austria)",
            "Johannes Oetsch (Jonkoping University",
            "Sweden)"
        ],
        "title": "Visual Graph Question Answering with ASP and LLMs for Language Parsing",
        "abstract": "arXiv:2502.09211v1 Announce Type: new  Abstract: Visual Question Answering (VQA) is a challenging problem that requires to process multimodal input. Answer-Set Programming (ASP) has shown great potential in this regard to add interpretability and explainability to modular VQA architectures. In this work, we address the problem of how to integrate ASP with modules for vision and natural language processing to solve a new and demanding VQA variant that is concerned with images of graphs (not graphs in symbolic form). Images containing graph-based structures are an ubiquitous and popular form of visualisation. Here, we deal with the particular problem of graphs inspired by transit networks, and we introduce a novel dataset that amends an existing one by adding images of graphs that resemble metro lines. Our modular neuro-symbolic approach combines optical graph recognition for graph parsing, a pretrained optical character recognition neural network for parsing labels, Large Language Models (LLMs) for language processing, and ASP for reasoning. This method serves as a first baseline and achieves an overall average accuracy of 73% on the dataset. Our evaluation provides further evidence of the potential of modular neuro-symbolic systems, in particular with pretrained models that do not involve any further training and logic programming for reasoning, to solve complex VQA tasks.",
        "arxiv_id": "2502.09211",
        "ARXIVID": "2502.09211",
        "COMMENT": "Relevant to general interest in vision-language models but does not directly match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.09269": {
        "authors": [
            "Yiwei Liu",
            "Ziyi Wu",
            "Liang Zhong",
            "Linyi Wen",
            "Yuankai Wu"
        ],
        "title": "Memory-based Ensemble Learning in CMR Semantic Segmentation",
        "abstract": "arXiv:2502.09269v1 Announce Type: new  Abstract: Existing models typically segment either the entire 3D frame or 2D slices independently to derive clinical functional metrics from ventricular segmentation in cardiac cine sequences. While performing well overall, they struggle at the end slices. To address this, we leverage spatial continuity to extract global uncertainty from segmentation variance and use it as memory in our ensemble learning method, Streaming, for classifier weighting, balancing overall and end-slice performance. Additionally, we introduce the End Coefficient (EC) to quantify end-slice accuracy. Experiments on ACDC and M\\&Ms datasets show that our framework achieves near-state-of-the-art Dice Similarity Coefficient (DSC) and outperforms all models on end-slice performance, improving patient-specific segmentation accuracy.",
        "arxiv_id": "2502.09269",
        "ARXIVID": "2502.09269",
        "COMMENT": "Does not match any specific criterion but is tangentially related to spatial understanding in medical imaging, which might be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 3
    }
}