{
    "2506.00123": {
        "authors": [
            "Gen Luo",
            "Ganlin Yang",
            "Ziyang Gong",
            "Guanzhou Chen",
            "Haonan Duan",
            "Erfei Cui",
            "Ronglei Tong",
            "Zhi Hou",
            "Tianyi Zhang",
            "Zhe Chen",
            "Shenglong Ye",
            "Lewei Lu",
            "Jingbo Wang",
            "Wenhai Wang",
            "Jifeng Dai",
            "Yu Qiao",
            "Rongrong Ji",
            "Xizhou Zhu"
        ],
        "title": "Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces",
        "abstract": "arXiv:2506.00123v1 Announce Type: new  Abstract: The remarkable progress of Multimodal Large Language Models (MLLMs) has attracted increasing attention to extend them to physical entities like legged robot. This typically requires MLLMs to not only grasp multimodal understanding abilities, but also integrate visual-spatial reasoning and physical interaction capabilities. Nevertheless,existing methods struggle to unify these capabilities due to their fundamental differences.In this paper, we present the Visual Embodied Brain (VeBrain), a unified framework for perception, reasoning, and control in real world. VeBrain reformulates robotic control into common text-based MLLM tasks in the 2D visual space, thus unifying the objectives and mapping spaces of different tasks. Then, a novel robotic adapter is proposed to convert textual control signals from MLLMs to motion policies of real robots. From the data perspective, we further introduce VeBrain-600k, a high-quality instruction dataset encompassing various capabilities of VeBrain. In VeBrain-600k, we take hundreds of hours to collect, curate and annotate the data, and adopt multimodal chain-of-thought(CoT) to mix the different capabilities into a single conversation. Extensive experiments on 13 multimodal benchmarks and 5 spatial intelligence benchmarks demonstrate the superior performance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to legged robots and robotic arms, VeBrain shows strong adaptability, flexibility, and compositional capabilities compared to existing methods. For example, compared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by +5.6%, but also excels in legged robot tasks with +50% average gains.",
        "arxiv_id": "2506.00123",
        "ARXIVID": "2506.00123",
        "COMMENT": "Matches criteria 1 and 3. Proposes a unified framework for spatial reasoning and control in embodied AI, with a novel dataset and benchmarks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2506.01078": {
        "authors": [
            "Yufei Zhan",
            "Ziheng Wu",
            "Yousong Zhu",
            "Rongkun Xue",
            "Ruipu Luo",
            "Zhenghao Chen",
            "Can Zhang",
            "Yifan Li",
            "Zhentao He",
            "Zheming Yang",
            "Ming Tang",
            "Minghui Qiu",
            "Jinqiao Wang"
        ],
        "title": "GThinker: Towards General Multimodal Reasoning via Cue-Guided Rethinking",
        "abstract": "arXiv:2506.01078v1 Announce Type: new  Abstract: Despite notable advancements in multimodal reasoning, leading Multimodal Large Language Models (MLLMs) still underperform on vision-centric multimodal reasoning tasks in general scenarios. This shortfall stems from their predominant reliance on logic- and knowledge-based slow thinking strategies, while effective for domains like math and science, fail to integrate visual information effectively during reasoning. Consequently, these models often fail to adequately ground visual cues, resulting in suboptimal performance in tasks that require multiple plausible visual interpretations and inferences. To address this, we present GThinker (General Thinker), a novel reasoning MLLM excelling in multimodal reasoning across general scenarios, mathematics, and science. GThinker introduces Cue-Rethinking, a flexible reasoning pattern that grounds inferences in visual cues and iteratively reinterprets these cues to resolve inconsistencies. Building on this pattern, we further propose a two-stage training pipeline, including pattern-guided cold start and incentive reinforcement learning, designed to enable multimodal reasoning capabilities across domains. Furthermore, to support the training, we construct GThinker-11K, comprising 7K high-quality, iteratively-annotated reasoning paths and 4K curated reinforcement learning samples, filling the data gap toward general multimodal reasoning. Extensive experiments demonstrate that GThinker achieves 81.5% on the challenging comprehensive multimodal reasoning benchmark M$^3$CoT, surpassing the latest O4-mini model. It also shows an average improvement of 2.1% on general scenario multimodal reasoning benchmarks, while maintaining on-par performance in mathematical reasoning compared to counterpart advanced reasoning models. The code, model, and data will be released soon at https://github.com/jefferyZhan/GThinker.",
        "arxiv_id": "2506.01078",
        "ARXIVID": "2506.01078",
        "COMMENT": "Matches criterion 2 as it introduces a new multimodal large language model (GThinker) with novel reasoning capabilities.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2506.01103": {
        "authors": [
            "Junyi Chen",
            "Haoyi Zhu",
            "Xianglong He",
            "Yifan Wang",
            "Jianjun Zhou",
            "Wenzheng Chang",
            "Yang Zhou",
            "Zizun Li",
            "Zhoujie Fu",
            "Jiangmiao Pang",
            "Tong He"
        ],
        "title": "DeepVerse: 4D Autoregressive Video Generation as a World Model",
        "abstract": "arXiv:2506.01103v1 Announce Type: new  Abstract: World models serve as essential building blocks toward Artificial General Intelligence (AGI), enabling intelligent agents to predict future states and plan actions by simulating complex physical interactions. However, existing interactive models primarily predict visual observations, thereby neglecting crucial hidden states like geometric structures and spatial coherence. This leads to rapid error accumulation and temporal inconsistency. To address these limitations, we introduce DeepVerse, a novel 4D interactive world model explicitly incorporating geometric predictions from previous timesteps into current predictions conditioned on actions. Experiments demonstrate that by incorporating explicit geometric constraints, DeepVerse captures richer spatio-temporal relationships and underlying physical dynamics. This capability significantly reduces drift and enhances temporal consistency, enabling the model to reliably generate extended future sequences and achieve substantial improvements in prediction accuracy, visual realism, and scene rationality. Furthermore, our method provides an effective solution for geometry-aware memory retrieval, effectively preserving long-term spatial consistency. We validate the effectiveness of DeepVerse across diverse scenarios, establishing its capacity for high-fidelity, long-horizon predictions grounded in geometry-aware dynamics.",
        "arxiv_id": "2506.01103",
        "ARXIVID": "2506.01103",
        "COMMENT": "Matches criterion 1 and 3 as it introduces a novel 4D interactive world model for spatial understanding and embodied AI with geometry-aware dynamics.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2506.01551": {
        "authors": [
            "Bingqian Lin",
            "Yunshuang Nie",
            "Khun Loun Zai",
            "Ziming Wei",
            "Mingfei Han",
            "Rongtao Xu",
            "Minzhe Niu",
            "Jianhua Han",
            "Liang Lin",
            "Cewu Lu",
            "Xiaodan Liang"
        ],
        "title": "EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation",
        "abstract": "arXiv:2506.01551v1 Announce Type: new  Abstract: Building Vision-Language Navigation (VLN) agents which can navigate following natural language instructions is a long-standing goal in human-robot interaction applications. Recent studies have revealed the potential of training open-source Large Language Models (LLMs) to unleash LLMs' reasoning ability for improving navigation, and simultaneously mitigate the domain gap between LLMs' training corpus and the VLN task. However, these approaches primarily adopt direct input-output mapping paradigms, causing the mapping learning difficult and the navigational decisions unexplainable. Chain-of-Thought (CoT) training is a promising way to improve both navigational decision accuracy and interpretability, while the complexity of the navigation task makes the perfect CoT labels unavailable and may lead to overfitting through pure CoT supervised fine-tuning. In this paper, we propose a novel sElf-improving embodied reasoning framework for boosting LLM-based vision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two stages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model with formalized CoT labels to both activate the model's navigational reasoning capabilities and increase the reasoning speed; (2) Self-Reflective Post-Training, where the model is iteratively trained with its own reasoning outputs as self-enriched CoT labels to enhance the supervision diversity. A self-reflective auxiliary task is also introduced to encourage learning correct reasoning patterns by contrasting with wrong ones. Experimental results on the popular VLN benchmarks demonstrate the superiority of EvolveNav over previous LLM-based VLN approaches. Code is available at https://github.com/expectorlin/EvolveNav.",
        "arxiv_id": "2506.01551",
        "ARXIVID": "2506.01551",
        "COMMENT": "Matches criterion 3 as it proposes a novel self-improving framework for vision-language navigation, focusing on embodied reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.00320": {
        "authors": [
            "Xiao Yu",
            "Baolin Peng",
            "Ruize Xu",
            "Michel Galley",
            "Hao Cheng",
            "Suman Nath",
            "Jianfeng Gao",
            "Zhou Yu"
        ],
        "title": "Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents",
        "abstract": "arXiv:2506.00320v1 Announce Type: new  Abstract: Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent's world modeling ability via objectives such as state prediction or critique generation, and then improve the agent's action via policy training. We evaluate our methods on OSWorld, and demonstrate that Dyna-Think improves the agent's in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.",
        "arxiv_id": "2506.00320",
        "ARXIVID": "2506.00320",
        "COMMENT": "Matches criterion 1 and 3 as it proposes a new framework (Dyna-Think) for reasoning, acting, and world model simulation in AI agents, and evaluates it on a new benchmark (OSWorld).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2506.01943": {
        "authors": [
            "Xiao Fu",
            "Xintao Wang",
            "Xian Liu",
            "Jianhong Bai",
            "Runsen Xu",
            "Pengfei Wan",
            "Di Zhang",
            "Dahua Lin"
        ],
        "title": "Learning Video Generation for Robotic Manipulation with Collaborative Trajectory Control",
        "abstract": "arXiv:2506.01943v1 Announce Type: new  Abstract: Recent advances in video diffusion models have demonstrated strong potential for generating robotic decision-making data, with trajectory conditions further enabling fine-grained control. However, existing trajectory-based methods primarily focus on individual object motion and struggle to capture multi-object interaction crucial in complex robotic manipulation. This limitation arises from multi-feature entanglement in overlapping regions, which leads to degraded visual fidelity. To address this, we present RoboMaster, a novel framework that models inter-object dynamics through a collaborative trajectory formulation. Unlike prior methods that decompose objects, our core is to decompose the interaction process into three sub-stages: pre-interaction, interaction, and post-interaction. Each stage is modeled using the feature of the dominant object, specifically the robotic arm in the pre- and post-interaction phases and the manipulated object during interaction, thereby mitigating the drawback of multi-object feature fusion present during interaction in prior work. To further ensure subject semantic consistency throughout the video, we incorporate appearance- and shape-aware latent representations for objects. Extensive experiments on the challenging Bridge V2 dataset, as well as in-the-wild evaluation, demonstrate that our method outperforms existing approaches, establishing new state-of-the-art performance in trajectory-controlled video generation for robotic manipulation.",
        "arxiv_id": "2506.01943",
        "ARXIVID": "2506.01943",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework (RoboMaster) for robotic manipulation with a focus on multi-object interaction, addressing limitations in prior work.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.01031": {
        "authors": [
            "Yanyuan Qiao",
            "Haodong Hong",
            "Wenqi Lyu",
            "Dong An",
            "Siqi Zhang",
            "Yutong Xie",
            "Xinyu Wang",
            "Qi Wu"
        ],
        "title": "NavBench: Probing Multimodal Large Language Models for Embodied Navigation",
        "abstract": "arXiv:2506.01031v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong generalization in vision-language tasks, yet their ability to understand and act within embodied environments remains underexplored. We present NavBench, a benchmark to evaluate the embodied navigation capabilities of MLLMs under zero-shot settings. NavBench consists of two components: (1) navigation comprehension, assessed through three cognitively grounded tasks including global instruction alignment, temporal progress estimation, and local observation-action reasoning, covering 3,200 question-answer pairs; and (2) step-by-step execution in 432 episodes across 72 indoor scenes, stratified by spatial, cognitive, and execution complexity. To support real-world deployment, we introduce a pipeline that converts MLLMs' outputs into robotic actions. We evaluate both proprietary and open-source models, finding that GPT-4o performs well across tasks, while lighter open-source models succeed in simpler cases. Results also show that models with higher comprehension scores tend to achieve better execution performance. Providing map-based context improves decision accuracy, especially in medium-difficulty scenarios. However, most models struggle with temporal understanding, particularly in estimating progress during navigation, which may pose a key challenge.",
        "arxiv_id": "2506.01031",
        "ARXIVID": "2506.01031",
        "COMMENT": "Matches criterion 3. Introduces a benchmark for evaluating MLLMs in embodied navigation, focusing on novel tasks and real-world deployment.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.01738": {
        "authors": [
            "Jinhong Wang",
            "Shuo Tong",
            "Jian liu",
            "Dongqi Tang",
            "Jintai Chen",
            "Haochao Ying",
            "Hongxia Xu",
            "Danny Chen",
            "Jian Wu"
        ],
        "title": "STORM: Benchmarking Visual Rating of MLLMs with a Comprehensive Ordinal Regression Dataset",
        "abstract": "arXiv:2506.01738v1 Announce Type: new  Abstract: Visual rating is an essential capability of artificial intelligence (AI) for multi-dimensional quantification of visual content, primarily applied in ordinal regression (OR) tasks such as image quality assessment, facial age estimation, and medical image grading. However, current multi-modal large language models (MLLMs) under-perform in such visual rating ability while also suffering the lack of relevant datasets and benchmarks. In this work, we collect and present STORM, a data collection and benchmark for Stimulating Trustworthy Ordinal Regression Ability of MLLMs for universal visual rating. STORM encompasses 14 ordinal regression datasets across five common visual rating domains, comprising 655K image-level pairs and the corresponding carefully curated VQAs. Importantly, we also propose a coarse-to-fine processing pipeline that dynamically considers label candidates and provides interpretable thoughts, providing MLLMs with a general and trustworthy ordinal thinking paradigm. This benchmark aims to evaluate the all-in-one and zero-shot performance of MLLMs in scenarios requiring understanding of the essential common ordinal relationships of rating labels. Extensive experiments demonstrate the effectiveness of our framework and shed light on better fine-tuning strategies. The STORM dataset, benchmark, and pre-trained models are available on the following webpage to support further research in this area. Datasets and codes are released on the project page: https://storm-bench.github.io/.",
        "arxiv_id": "2506.01738",
        "ARXIVID": "2506.01738",
        "COMMENT": "Matches criterion 2 as it introduces a benchmark for MLLMs with ordinal regression tasks, focusing on visual rating capabilities.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.01380": {
        "authors": [
            "Xinle Cheng",
            "Tianyu He",
            "Jiayi Xu",
            "Junliang Guo",
            "Di He",
            "Jiang Bian"
        ],
        "title": "Playing with Transformer at 30+ FPS via Next-Frame Diffusion",
        "abstract": "arXiv:2506.01380v1 Announce Type: new  Abstract: Autoregressive video models offer distinct advantages over bidirectional diffusion models in creating interactive video content and supporting streaming applications with arbitrary duration. In this work, we present Next-Frame Diffusion (NFD), an autoregressive diffusion transformer that incorporates block-wise causal attention, enabling iterative sampling and efficient inference via parallel token generation within each frame. Nonetheless, achieving real-time video generation remains a significant challenge for such models, primarily due to the high computational cost associated with diffusion sampling and the hardware inefficiencies inherent to autoregressive generation. To address this, we introduce two innovations: (1) We extend consistency distillation to the video domain and adapt it specifically for video models, enabling efficient inference with few sampling steps; (2) To fully leverage parallel computation, motivated by the observation that adjacent frames often share the identical action input, we propose speculative sampling. In this approach, the model generates next few frames using current action input, and discard speculatively generated frames if the input action differs. Experiments on a large-scale action-conditioned video generation benchmark demonstrate that NFD beats autoregressive baselines in terms of both visual quality and sampling efficiency. We, for the first time, achieves autoregressive video generation at over 30 Frames Per Second (FPS) on an A100 GPU using a 310M model.",
        "arxiv_id": "2506.01380",
        "ARXIVID": "2506.01380",
        "COMMENT": "Matches criterion 3 as it proposes a novel autoregressive video generation method with efficient sampling, which could be relevant for embodied AI benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.01608": {
        "authors": [
            "Andy Bonnetto",
            "Haozhe Qi",
            "Franklin Leong",
            "Matea Tashkovska",
            "Mahdi Rad",
            "Solaiman Shokur",
            "Friedhelm Hummel",
            "Silvestro Micera",
            "Marc Pollefeys",
            "Alexander Mathis"
        ],
        "title": "EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models",
        "abstract": "arXiv:2506.01608v1 Announce Type: new  Abstract: Understanding behavior requires datasets that capture humans while carrying out complex tasks. The kitchen is an excellent environment for assessing human motor and cognitive function, as many complex actions are naturally exhibited in kitchens from chopping to cleaning. Here, we introduce the EPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion capture platform inside a kitchen environment. Nine static RGB-D cameras, inertial measurement units (IMUs) and one head-mounted HoloLens~2 headset were used to capture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset is a multi-view action dataset with synchronized exocentric, egocentric, depth, IMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjects cooking four different recipes. Action sequences were densely annotated with 33.78 action segments per minute. Leveraging this multi-modal dataset, we propose four benchmarks to advance behavior understanding and modeling through 1) a vision-language benchmark, 2) a semantic text-to-motion generation benchmark, 3) a multi-modal action recognition benchmark, 4) a pose-based action segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset to pave the way for better methods as well as insights to understand the nature of ecologically-valid human behavior. Code and data are available at https://github.com/amathislab/EPFL-Smart-Kitchen",
        "arxiv_id": "2506.01608",
        "ARXIVID": "2506.01608",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for understanding human behavior in a kitchen environment, with multiple modalities and tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.01933": {
        "authors": [
            "Wenyan Cong",
            "Yiqing Liang",
            "Yancheng Zhang",
            "Ziyi Yang",
            "Yan Wang",
            "Boris Ivanovic",
            "Marco Pavone",
            "Chen Chen",
            "Zhangyang Wang",
            "Zhiwen Fan"
        ],
        "title": "E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models",
        "abstract": "arXiv:2506.01933v1 Announce Type: new  Abstract: Spatial intelligence, encompassing 3D reconstruction, perception, and reasoning, is fundamental to applications such as robotics, aerial imaging, and extended reality. A key enabler is the real-time, accurate estimation of core 3D attributes (camera parameters, point clouds, depth maps, and 3D point tracks) from unstructured or streaming imagery. Inspired by the success of large foundation models in language and 2D vision, a new class of end-to-end 3D geometric foundation models (GFMs) has emerged, directly predicting dense 3D representations in a single feed-forward pass, eliminating the need for slow or unavailable precomputed camera parameters. Since late 2023, the field has exploded with diverse variants, but systematic evaluation is lacking. In this work, we present the first comprehensive benchmark for 3D GFMs, covering five core tasks: sparse-view depth estimation, video depth estimation, 3D reconstruction, multi-view pose estimation, novel view synthesis, and spanning both standard and challenging out-of-distribution datasets. Our standardized toolkit automates dataset handling, evaluation protocols, and metric computation to ensure fair, reproducible comparisons. We evaluate 16 state-of-the-art GFMs, revealing their strengths and limitations across tasks and domains, and derive key insights to guide future model scaling and optimization. All code, evaluation scripts, and processed data will be publicly released to accelerate research in 3D spatial intelligence.",
        "arxiv_id": "2506.01933",
        "ARXIVID": "2506.01933",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for 3D geometric foundation models, focusing on spatial intelligence and systematic evaluation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.00600": {
        "authors": [
            "Xianghui Ze",
            "Beiyi Zhu",
            "Zhenbo Song",
            "Jianfeng Lu",
            "Yujiao Shi"
        ],
        "title": "SatDreamer360: Geometry Consistent Street-View Video Generation from Satellite Imagery",
        "abstract": "arXiv:2506.00600v1 Announce Type: new  Abstract: Generating continuous ground-level video from satellite imagery is a challenging task with significant potential for applications in simulation, autonomous navigation, and digital twin cities. Existing approaches primarily focus on synthesizing individual ground-view images, often relying on auxiliary inputs like height maps or handcrafted projections, and fall short in producing temporally consistent sequences. In this paper, we propose {SatDreamer360}, a novel framework that generates geometrically and temporally consistent ground-view video from a single satellite image and a predefined trajectory. To bridge the large viewpoint gap, we introduce a compact tri-plane representation that encodes scene geometry directly from the satellite image. A ray-based pixel attention mechanism retrieves view-dependent features from the tri-plane, enabling accurate cross-view correspondence without requiring additional geometric priors. To ensure multi-frame consistency, we propose an epipolar-constrained temporal attention module that aligns features across frames using the known relative poses along the trajectory. To support evaluation, we introduce {VIGOR++}, a large-scale dataset for cross-view video generation, with dense trajectory annotations and high-quality ground-view sequences. Extensive experiments demonstrate that SatDreamer360 achieves superior performance in fidelity, coherence, and geometric alignment across diverse urban scenes.",
        "arxiv_id": "2506.00600",
        "ARXIVID": "2506.00600",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework (SatDreamer360) for generating ground-level video from satellite imagery, and also provides a new benchmark dataset (VIGOR++).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2506.00258": {
        "authors": [
            "Qianqi Yan",
            "Hongquan Li",
            "Shan Jiang",
            "Yang Zhao",
            "Xinze Guan",
            "Ching-Chen Kuo",
            "Xin Eric Wang"
        ],
        "title": "Hidden in Plain Sight: Probing Implicit Reasoning in Multimodal Language Models",
        "abstract": "arXiv:2506.00258v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) are increasingly deployed in open-ended, real-world environments where inputs are messy, underspecified, and not always trustworthy. Unlike curated benchmarks, these settings frequently involve instructions that refer to missing objects or contradictory facts, rely on ambiguous references, or request infeasible actions. In such cases, success hinges not on task execution alone, but on a model's ability to detect when something is silently wrong. This paper presents a systematic analysis of how current MLLMs handle such implicit reasoning scenarios: cases where the flaw is not explicitly stated but must be inferred from context. Using a curated diagnostic suite spanning four categories of real-world failure modes, we evaluate six MLLMs, including o3 and GPT-4o, and find that models frequently fail to surface hidden issues, even when they possess the necessary perceptual and reasoning skills. Explicit prompting reveals that the underlying capabilities exist but are often suppressed in favor of user compliance. We further show that simple inference-time interventions, such as cautious persona prompting and, in particular, requiring a clarifying question, can dramatically recover performance. Our findings highlight a persistent gap between reasoning competence and behavioral compliance in current MLLMs and suggest practical strategies for making these models more trustworthy in underconstrained environments.",
        "arxiv_id": "2506.00258",
        "ARXIVID": "2506.00258",
        "COMMENT": "Matches criterion 2 as it evaluates reasoning capabilities in multimodal large language models and provides insights into their limitations.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.01663": {
        "authors": [
            "Xuan Yu",
            "Dayan Guan",
            "Michael Ying Yang",
            "Yanfeng Gu"
        ],
        "title": "Zoom-Refine: Boosting High-Resolution Multimodal Understanding via Localized Zoom and Self-Refinement",
        "abstract": "arXiv:2506.01663v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLM) often struggle to interpret high-resolution images accurately, where fine-grained details are crucial for complex visual understanding. We introduce Zoom-Refine, a novel training-free method that enhances MLLM capabilities to address this issue. Zoom-Refine operates through a synergistic process of \\textit{Localized Zoom} and \\textit{Self-Refinement}. In the \\textit{Localized Zoom} step, Zoom-Refine leverages the MLLM to provide a preliminary response to an input query and identifies the most task-relevant image region by predicting its bounding box coordinates. During the \\textit{Self-Refinement} step, Zoom-Refine then integrates fine-grained details from the high-resolution crop (identified by \\textit{Localized Zoom}) with its initial reasoning to re-evaluate and refine its preliminary response. Our method harnesses the MLLM's inherent capabilities for spatial localization, contextual reasoning and comparative analysis without requiring additional training or external experts. Comprehensive experiments demonstrate the efficacy of Zoom-Refine on two challenging high-resolution multimodal benchmarks. Code is available at \\href{https://github.com/xavier-yu114/Zoom-Refine}{\\color{magenta}github.com/xavier-yu114/Zoom-Refine}",
        "arxiv_id": "2506.01663",
        "ARXIVID": "2506.01663",
        "COMMENT": "Matches criterion 2 as it proposes a novel method to enhance multimodal large language models (MLLMs) for high-resolution image understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.01676": {
        "authors": [
            "Chong Li",
            "Chenglin Zhu",
            "Tao Zhang",
            "Mingan Lin",
            "Zenan Zhou",
            "Jian Xie"
        ],
        "title": "K12Vista: Exploring the Boundaries of MLLMs in K-12 Education",
        "abstract": "arXiv:2506.01676v1 Announce Type: new  Abstract: Multimodal large language models have demonstrated remarkable reasoning capabilities in various visual tasks. However, their abilities in K12 scenarios are still systematically underexplored. Previous studies suffer from various limitations including narrow subject coverage, insufficient data scale, lack of diversity in question types, and naive answer-centric evaluation method, resulting in insufficient exploration of model capabilities. To address these gaps, we propose K12Vista, the most comprehensive multimodal benchmark for Chinese K12 subject knowledge understanding and reasoning to date, featuring 33,000 questions across five core subjects from primary to high school and three question types. Moreover, beyond the final outcome, we are also concerned with the correctness of MLLMs' reasoning processes. For this purpose, we meticulously compiles errors from MLLMs' reasoning processes and leverage an automated data pipeline to construct K12-PEM-800K, the largest process evaluation dataset offering detailed step-by-step judgement annotations for MLLMs' reasoning. Subsequently, we developed K12-PEM, an advanced process evaluation model that integrates an overall assessment of both the reasoning process and answer correctness. Moreover, we also introduce K12-PEBench, the first high-quality, human-annotated benchmark specifically designed for evaluating abilities of reasoning process evaluation.Extensive experiments reveal that current MLLMs exhibit significant flaws when reasoning within K12Vista, providing critical insights for the development of more capable MLLMs.We open our resources at https://github.com/lichongod/K12Vista.",
        "arxiv_id": "2506.01676",
        "ARXIVID": "2506.01676",
        "COMMENT": "Matches criterion 2 as it explores the capabilities of multimodal large language models (MLLMs) in K-12 education and introduces a new benchmark (K12Vista).",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.01299": {
        "authors": [
            "Jinmei Liu",
            "Fuhong Liu",
            "Jianye Hao",
            "Bo Wang",
            "Huaxiong Li",
            "Chunlin Chen",
            "Zhi Wang"
        ],
        "title": "Scalable In-Context Q-Learning",
        "abstract": "arXiv:2506.01299v1 Announce Type: new  Abstract: Recent advancements in language models have demonstrated remarkable in-context learning abilities, prompting the exploration of in-context reinforcement learning (ICRL) to extend the promise to decision domains. Due to involving more complex dynamics and temporal correlations, existing ICRL approaches may face challenges in learning from suboptimal trajectories and achieving precise in-context inference. In the paper, we propose \\textbf{S}calable \\textbf{I}n-\\textbf{C}ontext \\textbf{Q}-\\textbf{L}earning (\\textbf{SICQL}), an innovative framework that harnesses dynamic programming and world modeling to steer ICRL toward efficient reward maximization and task generalization, while retaining the scalability and stability of supervised pretraining. We design a prompt-based multi-head transformer architecture that simultaneously predicts optimal policies and in-context value functions using separate heads. We pretrain a generalized world model to capture task-relevant information, enabling the construction of a compact prompt that facilitates fast and precise in-context inference. During training, we perform iterative policy improvement by fitting a state value function to an upper-expectile of the Q-function, and distill the in-context value functions into policy extraction using advantage-weighted regression. Extensive experiments across a range of discrete and continuous environments show consistent performance gains over various types of baselines, especially when learning from suboptimal data. Our code is available at https://github.com/NJU-RL/SICQL",
        "arxiv_id": "2506.01299",
        "ARXIVID": "2506.01299",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework (SICQL) for in-context Q-learning, which is relevant to embodied AI and decision-making tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2506.01546": {
        "authors": [
            "Xiaodong Wang",
            "Zhirong Wu",
            "Peixi Peng"
        ],
        "title": "LongDWM: Cross-Granularity Distillation for Building a Long-Term Driving World Model",
        "abstract": "arXiv:2506.01546v1 Announce Type: new  Abstract: Driving world models are used to simulate futures by video generation based on the condition of the current state and actions. However, current models often suffer serious error accumulations when predicting the long-term future, which limits the practical application. Recent studies utilize the Diffusion Transformer (DiT) as the backbone of driving world models to improve learning flexibility. However, these models are always trained on short video clips (high fps and short duration), and multiple roll-out generations struggle to produce consistent and reasonable long videos due to the training-inference gap. To this end, we propose several solutions to build a simple yet effective long-term driving world model. First, we hierarchically decouple world model learning into large motion learning and bidirectional continuous motion learning. Then, considering the continuity of driving scenes, we propose a simple distillation method where fine-grained video flows are self-supervised signals for coarse-grained flows. The distillation is designed to improve the coherence of infinite video generation. The coarse-grained and fine-grained modules are coordinated to generate long-term and temporally coherent videos. In the public benchmark NuScenes, compared with the state-of-the-art front-view model, our model improves FVD by $27\\%$ and reduces inference time by $85\\%$ for the video task of generating 110+ frames. More videos (including 90s duration) are available at https://Wang-Xiaodong1899.github.io/longdwm/.",
        "arxiv_id": "2506.01546",
        "ARXIVID": "2506.01546",
        "COMMENT": "Matches criterion 3 as it proposes a new method for building long-term driving world models with hierarchical decoupling and distillation techniques.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2506.01758": {
        "authors": [
            "Tao Yang",
            "Ruibin Li",
            "Yangming Shi",
            "Yuqi Zhang",
            "Qide Dong",
            "Haoran Cheng",
            "Weiguo Feng",
            "Shilei Wen",
            "Bingyue Peng",
            "Lei Zhang"
        ],
        "title": "Many-for-Many: Unify the Training of Multiple Video and Image Generation and Manipulation Tasks",
        "abstract": "arXiv:2506.01758v1 Announce Type: new  Abstract: Diffusion models have shown impressive performance in many visual generation and manipulation tasks. Many existing methods focus on training a model for a specific task, especially, text-to-video (T2V) generation, while many other works focus on finetuning the pretrained T2V model for image-to-video (I2V), video-to-video (V2V), image and video manipulation tasks, etc. However, training a strong T2V foundation model requires a large amount of high-quality annotations, which is very costly. In addition, many existing models can perform only one or several tasks. In this work, we introduce a unified framework, namely many-for-many, which leverages the available training data from many different visual generation and manipulation tasks to train a single model for those different tasks. Specifically, we design a lightweight adapter to unify the different conditions in different tasks, then employ a joint image-video learning strategy to progressively train the model from scratch. Our joint learning leads to a unified visual generation and manipulation model with improved video generation performance. In addition, we introduce depth maps as a condition to help our model better perceive the 3D space in visual generation. Two versions of our model are trained with different model sizes (8B and 2B), each of which can perform more than 10 different tasks. In particular, our 8B model demonstrates highly competitive performance in video generation tasks compared to open-source and even commercial engines. Our models and source codes are available at https://github.com/leeruibin/MfM.git.",
        "arxiv_id": "2506.01758",
        "ARXIVID": "2506.01758",
        "COMMENT": "Matches criterion 2 as it introduces a unified framework for multiple video and image generation tasks, leveraging diffusion models.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2506.01689": {
        "authors": [
            "Shuting Wang",
            "Yunqi Liu",
            "Zixin Yang",
            "Ning Hu",
            "Zhicheng Dou",
            "Chenyan Xiong"
        ],
        "title": "Respond Beyond Language: A Benchmark for Video Generation in Response to Realistic User Intents",
        "abstract": "arXiv:2506.01689v1 Announce Type: new  Abstract: Querying generative AI models, e.g., large language models (LLMs), has become a prevalent method for information acquisition. However, existing query-answer datasets primarily focus on textual responses, making it challenging to address complex user queries that require visual demonstrations or explanations for better understanding. To bridge this gap, we construct a benchmark, RealVideoQuest, designed to evaluate the abilities of text-to-video (T2V) models in answering real-world, visually grounded queries. It identifies 7.5K real user queries with video response intents from Chatbot-Arena and builds 4.5K high-quality query-video pairs through a multistage video retrieval and refinement process. We further develop a multi-angle evaluation system to assess the quality of generated video answers. Experiments indicate that current T2V models struggle with effectively addressing real user queries, pointing to key challenges and future research opportunities in multimodal AI.",
        "arxiv_id": "2506.01689",
        "ARXIVID": "2506.01689",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (RealVideoQuest) for evaluating text-to-video models in response to realistic user intents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.00568": {
        "authors": [
            "Ke Niu",
            "Zhuofan Chen",
            "Haiyang Yu",
            "Yuwen Chen",
            "Teng Fu",
            "Mengyang Zhao",
            "Bin Li",
            "Xiangyang Xue"
        ],
        "title": "CReFT-CAD: Boosting Orthographic Projection Reasoning for CAD via Reinforcement Fine-Tuning",
        "abstract": "arXiv:2506.00568v1 Announce Type: new  Abstract: Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing. Orthographic projection reasoning underpins the entire CAD workflow, encompassing design, manufacturing, and simulation. However, prevailing deep-learning approaches employ standard 3D reconstruction pipelines as an alternative, which often introduce imprecise dimensions and limit the parametric editability required for CAD workflows. Recently, some researchers adopt vision-language models (VLMs), particularly supervised fine-tuning (SFT), to tackle CAD-related challenges. SFT shows promise but often devolves into pattern memorization, yielding poor out-of-distribution performance on complex reasoning tasks. To address these gaps, we introduce CReFT-CAD, a two-stage fine-tuning paradigm that first employs a curriculum-driven reinforcement learning stage with difficulty-aware rewards to build reasoning ability steadily, and then applies supervised post-tuning to hone instruction following and semantic extraction. Complementing this, we release TriView2CAD, the first large-scale, open-source benchmark for orthographic projection reasoning, comprising 200,000 synthetic and 3,000 real-world orthographic projections with precise dimension annotations and six interoperable data modalities. We benchmark leading VLMs on orthographic projection reasoning and demonstrate that CReFT-CAD substantially improves reasoning accuracy and out-of-distribution generalizability in real-world scenarios, offering valuable insights for advancing CAD reasoning research.",
        "arxiv_id": "2506.00568",
        "ARXIVID": "2506.00568",
        "COMMENT": "Matches criterion 3 as it introduces a new fine-tuning paradigm and benchmark for CAD reasoning, focusing on orthographic projection reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.01275": {
        "authors": [
            "Artemis Panagopoulou",
            "Le Xue",
            "Honglu Zhou",
            "silvio savarese",
            "Ran Xu",
            "Caiming Xiong",
            "Chris Callison-Burch",
            "Mark Yatskar",
            "Juan Carlos Niebles"
        ],
        "title": "Contra4: Evaluating Contrastive Cross-Modal Reasoning in Audio, Video, Image, and 3D",
        "abstract": "arXiv:2506.01275v1 Announce Type: new  Abstract: Real-world decision-making often begins with identifying which modality contains the most relevant information for a given query. While recent multimodal models have made impressive progress in processing diverse inputs, it remains unclear whether they can reason contrastively across multiple modalities to select the one that best satisfies a natural language prompt. We argue this capability is foundational, especially in retrieval-augmented and decision-time contexts, where systems must evaluate multiple signals and identify which one conveys the relevant information. To evaluate this skill, we introduce Contra4, a dataset for contrastive cross-modal reasoning across four modalities: image, audio, video, and 3D. Each example presents a natural language question alongside multiple candidate modality instances, and the model must select the one that semantically aligns with the prompt. Contra4 combines human-annotated captions with a mixture-of-models round-trip-consistency filter to ensure high-quality supervision, resulting in 174k training examples and a manually verified test set of 2.3k samples. While task-specific fine-tuning improves performance by 56% relative to baseline, state-of-the-art models still achieve only 56% accuracy overall and 42% in four-modality settings, underscoring a significant limitation in current multimodal models.",
        "arxiv_id": "2506.01275",
        "ARXIVID": "2506.01275",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (Contra4) for contrastive cross-modal reasoning, which is relevant to embodied AI and multimodal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.01704": {
        "authors": [
            "Jiongnan Liu",
            "Zhicheng Dou",
            "Ning Hu",
            "Chenyan Xiong"
        ],
        "title": "Generate, Not Recommend: Personalized Multimodal Content Generation",
        "abstract": "arXiv:2506.01704v1 Announce Type: new  Abstract: To address the challenge of information overload from massive web contents, recommender systems are widely applied to retrieve and present personalized results for users. However, recommendation tasks are inherently constrained to filtering existing items and lack the ability to generate novel concepts, limiting their capacity to fully satisfy user demands and preferences. In this paper, we propose a new paradigm that goes beyond content filtering and selecting: directly generating personalized items in a multimodal form, such as images, tailored to individual users. To accomplish this, we leverage any-to-any Large Multimodal Models (LMMs) and train them in both supervised fine-tuning and online reinforcement learning strategy to equip them with the ability to yield tailored next items for users. Experiments on two benchmark datasets and user study confirm the efficacy of the proposed method. Notably, the generated images not only align well with users' historical preferences but also exhibit relevance to their potential future interests.",
        "arxiv_id": "2506.01704",
        "ARXIVID": "2506.01704",
        "COMMENT": "Matches criterion 2 as it discusses leveraging large multimodal models (LMMs) for personalized multimodal content generation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.00807": {
        "authors": [
            "Jiahui Zhou",
            "Dan Li",
            "Lin Li",
            "Zhuomin Chen",
            "Shunyu Wu",
            "Haozheng Ye",
            "Jian Lou",
            "Costas J. Spanos"
        ],
        "title": "Enhancing LLM Reasoning for Time Series Classification by Tailored Thinking and Fused Decision",
        "abstract": "arXiv:2506.00807v1 Announce Type: new  Abstract: The reasoning capabilities of large language models (LLMs) have significantly advanced their performance by enabling in-depth understanding of diverse tasks. With growing interest in applying LLMs to the time series domain, this has proven nontrivial, as evidenced by the limited efficacy of straightforwardly adapting text-domain reasoning techniques. Although recent work has shown promise in several time series tasks, further leveraging advancements in LLM reasoning remains under-explored for time series classification (TSC) tasks, despite their prevalence and significance in many real-world applications. In this paper, we propose ReasonTSC, a novel framework designed to effectively leverage LLM reasoning for time series classification through both a multi-turn reasoning and a fused decision-making strategy tailored to TSC. Rather than straightforwardly applying existing reasoning techniques or relying solely on LLMs' built-in reasoning capabilities, ReasonTSC first steers the model to think over the essential characteristics of time series data. Next, it integrates predictions and confidence scores from plug-in classifiers, e.g., domain-specific time series models, as in-context examples. Finally, ReasonTSC guides the LLM through a structured reasoning process: it evaluates the initial assessment, backtracks to consider alternative hypotheses, and compares their merits before arriving at a final classification. Extensive experiments and systematic ablation studies demonstrate that ReasonTSC consistently outperforms both existing time series reasoning baselines and plug-in models, and is even capable of identifying and correcting plug-in models' false predictions.",
        "arxiv_id": "2506.00807",
        "ARXIVID": "2506.00807",
        "COMMENT": "Matches criterion 2 as it proposes a novel framework (ReasonTSC) for leveraging LLM reasoning in time series classification, which is a novel application of LLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.01301": {
        "authors": [
            "Chunhui Zhang",
            "Zhongyu Ouyang",
            "Kwonjoon Lee",
            "Nakul Agarwal",
            "Sean Dae Houlihan",
            "Soroush Vosoughi",
            "Shao-Yuan Lo"
        ],
        "title": "Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner",
        "abstract": "arXiv:2506.01301v1 Announce Type: new  Abstract: Theory-of-Mind (ToM) enables humans to infer mental states-such as beliefs, desires, and intentions-forming the foundation of social cognition. However, existing computational ToM methods rely on structured workflows with ToM-specific priors or deep model fine-tuning, which struggle with scalability in multimodal environments and fail to generalize as task complexity increases. To address these limitations, we propose a scalable Bayesian ToM planner that decomposes ToM reasoning into stepwise Bayesian updates. Our framework introduces weak-to-strong control, allowing smaller language models (LMs) to specialize in ToM-specific likelihood estimation and transfer their reasoning behaviors to larger LMs (7B to 405B) for integration with social and world knowledge. This synergistic approach aligns large-model inference of human mental states with Bayesian principles. Extensive experiments show that our method achieves a 4.6% accuracy improvement over state-of-the-art techniques on multimodal ToM benchmarks, including challenging unseen scenarios, thereby establishing a new standard for modeling human mental states in complex environments.",
        "arxiv_id": "2506.01301",
        "ARXIVID": "2506.01301",
        "COMMENT": "Matches criterion 2 as it focuses on multimodal reasoning and introduces a scalable Bayesian planner for Theory-of-Mind tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2506.01586": {
        "authors": [
            "Zhuohang Dang",
            "Minnan Luo",
            "Chengyou Jia",
            "Hangwei Qian",
            "Xiaojun Chang",
            "Ivor W. Tsang"
        ],
        "title": "Multi-Modal Dataset Distillation in the Wild",
        "abstract": "arXiv:2506.01586v1 Announce Type: new  Abstract: Recent multi-modal models have shown remarkable versatility in real-world applications. However, their rapid development encounters two critical data challenges. First, the training process requires large-scale datasets, leading to substantial storage and computational costs. Second, these data are typically web-crawled with inevitable noise, i.e., partially mismatched pairs, severely degrading model performance. To these ends, we propose Multi-modal dataset Distillation in the Wild, i.e., MDW, the first framework to distill noisy multi-modal datasets into compact clean ones for effective and efficient model training. Specifically, MDW introduces learnable fine-grained correspondences during distillation and adaptively optimizes distilled data to emphasize correspondence-discriminative regions, thereby enhancing distilled data's information density and efficacy. Moreover, to capture robust cross-modal correspondence prior knowledge from real data, MDW proposes dual-track collaborative learning to avoid the risky data noise, alleviating information loss with certifiable noise tolerance. Extensive experiments validate MDW's theoretical and empirical efficacy with remarkable scalability, surpassing prior methods by over 15% across various compression ratios, highlighting its appealing practicality for applications with diverse efficacy and resource needs.",
        "arxiv_id": "2506.01586",
        "ARXIVID": "2506.01586",
        "COMMENT": "Matches criterion 2 as it focuses on multi-modal dataset distillation, which is relevant to multi-modal large language models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.01487": {
        "authors": [
            "Yi Yang",
            "Yuren Cong",
            "Hao Cheng",
            "Bodo Rosenhahn",
            "Michael Ying Yang"
        ],
        "title": "FDSG: Forecasting Dynamic Scene Graphs",
        "abstract": "arXiv:2506.01487v1 Announce Type: new  Abstract: Dynamic scene graph generation extends scene graph generation from images to videos by modeling entity relationships and their temporal evolution. However, existing methods either generate scene graphs from observed frames without explicitly modeling temporal dynamics, or predict only relationships while assuming static entity labels and locations. These limitations hinder effective extrapolation of both entity and relationship dynamics, restricting video scene understanding. We propose Forecasting Dynamic Scene Graphs (FDSG), a novel framework that predicts future entity labels, bounding boxes, and relationships, for unobserved frames, while also generating scene graphs for observed frames. Our scene graph forecast module leverages query decomposition and neural stochastic differential equations to model entity and relationship dynamics. A temporal aggregation module further refines predictions by integrating forecasted and observed information via cross-attention. To benchmark FDSG, we introduce Scene Graph Forecasting, a new task for full future scene graph prediction. Experiments on Action Genome show that FDSG outperforms state-of-the-art methods on dynamic scene graph generation, scene graph anticipation, and scene graph forecasting. Codes will be released upon publication.",
        "arxiv_id": "2506.01487",
        "ARXIVID": "2506.01487",
        "COMMENT": "Matches criterion 3. Proposes a novel framework for dynamic scene graph forecasting, addressing limitations in video scene understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.00742": {
        "authors": [
            "Zeqi Gu",
            "Yin Cui",
            "Zhaoshuo Li",
            "Fangyin Wei",
            "Yunhao Ge",
            "Jinwei Gu",
            "Ming-Yu Liu",
            "Abe Davis",
            "Yifan Ding"
        ],
        "title": "ArtiScene: Language-Driven Artistic 3D Scene Generation Through Image Intermediary",
        "abstract": "arXiv:2506.00742v1 Announce Type: new  Abstract: Designing 3D scenes is traditionally a challenging task that demands both artistic expertise and proficiency with complex software. Recent advances in text-to-3D generation have greatly simplified this process by letting users create scenes based on simple text descriptions. However, as these methods generally require extra training or in-context learning, their performance is often hindered by the limited availability of high-quality 3D data. In contrast, modern text-to-image models learned from web-scale images can generate scenes with diverse, reliable spatial layouts and consistent, visually appealing styles. Our key insight is that instead of learning directly from 3D scenes, we can leverage generated 2D images as an intermediary to guide 3D synthesis. In light of this, we introduce ArtiScene, a training-free automated pipeline for scene design that integrates the flexibility of free-form text-to-image generation with the diversity and reliability of 2D intermediary layouts.   First, we generate 2D images from a scene description, then extract the shape and appearance of objects to create 3D models. These models are assembled into the final scene using geometry, position, and pose information derived from the same intermediary image. Being generalizable to a wide range of scenes and styles, ArtiScene outperforms state-of-the-art benchmarks by a large margin in layout and aesthetic quality by quantitative metrics. It also averages a 74.89% winning rate in extensive user studies and 95.07% in GPT-4o evaluation. Project page: https://artiscene-cvpr.github.io/",
        "arxiv_id": "2506.00742",
        "ARXIVID": "2506.00742",
        "COMMENT": "Matches criterion 4. Focuses on vision foundation models and their application to artistic 3D scene generation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.01391": {
        "authors": [
            "Zhong Zhang",
            "Yaxi Lu",
            "Yikun Fu",
            "Yupeng Huo",
            "Shenzhi Yang",
            "Yesai Wu",
            "Han Si",
            "Xin Cong",
            "Haotian Chen",
            "Yankai Lin",
            "Jie Xie",
            "Wei Zhou",
            "Wang Xu",
            "Yuanheng Zhang",
            "Zhou Su",
            "Zhongwu Zhai",
            "Xiaoming Liu",
            "Yudong Mei",
            "Jianming Xu",
            "Hongyan Tian",
            "Chongyi Wang",
            "Chi Chen",
            "Yuan Yao",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "title": "AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning",
        "abstract": "arXiv:2506.01391v1 Announce Type: new  Abstract: The recent progress of large language model agents has opened new possibilities for automating tasks through graphical user interfaces (GUIs), especially in mobile environments where intelligent interaction can greatly enhance usability. However, practical deployment of such agents remains constrained by several key challenges. Existing training data is often noisy and lack semantic diversity, which hinders the learning of precise grounding and planning. Models trained purely by imitation tend to overfit to seen interface patterns and fail to generalize in unfamiliar scenarios. Moreover, most prior work focuses on English interfaces while overlooks the growing diversity of non-English applications such as those in the Chinese mobile ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent built for robust and efficient on-device GUI interaction. Our training pipeline includes grounding-aware pre-training to enhance perception, supervised fine-tuning on high-quality Chinese and English trajectories to imitate human-like actions, and reinforcement fine-tuning with GRPO to improve reasoning capability. We also introduce a compact action space that reduces output length and supports low-latency execution on mobile devices. AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks and a new Chinese GUI benchmark called CAGUI, reaching $96.9\\%$ Type-Match and $91.3\\%$ Exact-Match. To facilitate reproducibility and further research, we publicly release all code, model checkpoint, and evaluation data.",
        "arxiv_id": "2506.01391",
        "ARXIVID": "2506.01391",
        "COMMENT": "Matches criterion 3 as it discusses building a new benchmark and methods for GUI agents with reinforcement fine-tuning.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2506.00558": {
        "authors": [
            "Adrian Azzarelli",
            "Ge Gao",
            "Ho Man Kwan",
            "Fan Zhang",
            "Nantheera Anantrasirichai",
            "Ollie Moolan-Feroze",
            "David Bull"
        ],
        "title": "ViVo: A Dataset for Volumetric VideoReconstruction and Compression",
        "abstract": "arXiv:2506.00558v1 Announce Type: new  Abstract: As research on neural volumetric video reconstruction and compression flourishes, there is a need for diverse and realistic datasets, which can be used to develop and validate reconstruction and compression models. However, existing volumetric video datasets lack diverse content in terms of both semantic and low-level features that are commonly present in real-world production pipelines. In this context, we propose a new dataset, ViVo, for VolumetrIc VideO reconstruction and compression. The dataset is faithful to real-world volumetric video production and is the first dataset to extend the definition of diversity to include both human-centric characteristics (skin, hair, etc.) and dynamic visual phenomena (transparent, reflective, liquid, etc.). Each video sequence in this database contains raw data including fourteen multi-view RGB and depth video pairs, synchronized at 30FPS with per-frame calibration and audio data, and their associated 2-D foreground masks and 3-D point clouds. To demonstrate the use of this database, we have benchmarked three state-of-the-art (SotA) 3-D reconstruction methods and two volumetric video compression algorithms. The obtained results evidence the challenging nature of the proposed dataset and the limitations of existing datasets for both volumetric video reconstruction and compression tasks, highlighting the need to develop more effective algorithms for these applications. The database and the associated results are available at https://vivo-bvicr.github.io/",
        "arxiv_id": "2506.00558",
        "ARXIVID": "2506.00558",
        "COMMENT": "Matches criterion 3 as it introduces a new dataset for volumetric video reconstruction and compression, addressing gaps in existing datasets.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.00993": {
        "authors": [
            "Yunzhu Zhang",
            "Yu Lu",
            "Tianyi Wang",
            "Fengyun Rao",
            "Yi Yang",
            "Linchao Zhu"
        ],
        "title": "FlexSelect: Flexible Token Selection for Efficient Long Video Understanding",
        "abstract": "arXiv:2506.00993v1 Announce Type: new  Abstract: Long-form video understanding poses a significant challenge for video large language models (VideoLLMs) due to prohibitively high computational and memory demands. In this paper, we propose FlexSelect, a flexible and efficient token selection strategy for processing long videos. FlexSelect identifies and retains the most semantically relevant content by leveraging cross-modal attention patterns from a reference transformer layer. It comprises two key components: (1) a training-free token ranking pipeline that leverages faithful cross-modal attention weights to estimate each video token's importance, and (2) a rank-supervised lightweight selector that is trained to replicate these rankings and filter redundant tokens. This generic approach can be seamlessly integrated into various VideoLLM architectures, such as LLaVA-Video, InternVL and Qwen-VL, serving as a plug-and-play module to extend their temporal context length. Empirically, FlexSelect delivers strong gains across multiple long-video benchmarks including VideoMME, MLVU, LongVB, and LVBench. Moreover, it achieves significant speed-ups (for example, up to 9 times on a LLaVA-Video-7B model), highlighting FlexSelect's promise for efficient long-form video understanding. Project page available at: https://yunzhuzhang0918.github.io/flex_select",
        "arxiv_id": "2506.00993",
        "ARXIVID": "2506.00993",
        "COMMENT": "Matches criterion 2 as it proposes a token selection strategy for VideoLLMs, improving efficiency in long video understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2506.01366": {
        "authors": [
            "Cong Guan",
            "Osamu Yoshie"
        ],
        "title": "CLIP-driven rain perception: Adaptive deraining with pattern-aware network routing and mask-guided cross-attention",
        "abstract": "arXiv:2506.01366v1 Announce Type: new  Abstract: Existing deraining models process all rainy images within a single network. However, different rain patterns have significant variations, which makes it challenging for a single network to handle diverse types of raindrops and streaks. To address this limitation, we propose a novel CLIP-driven rain perception network (CLIP-RPN) that leverages CLIP to automatically perceive rain patterns by computing visual-language matching scores and adaptively routing to sub-networks to handle different rain patterns, such as varying raindrop densities, streak orientations, and rainfall intensity. CLIP-RPN establishes semantic-aware rain pattern recognition through CLIP's cross-modal visual-language alignment capabilities, enabling automatic identification of precipitation characteristics across different rain scenarios. This rain pattern awareness drives an adaptive subnetwork routing mechanism where specialized processing branches are dynamically activated based on the detected rain type, significantly enhancing the model's capacity to handle diverse rainfall conditions. Furthermore, within sub-networks of CLIP-RPN, we introduce a mask-guided cross-attention mechanism (MGCA) that predicts precise rain masks at multi-scale to facilitate contextual interactions between rainy regions and clean background areas by cross-attention. We also introduces a dynamic loss scheduling mechanism (DLS) to adaptively adjust the gradients for the optimization process of CLIP-RPN. Compared with the commonly used $l_1$ or $l_2$ loss, DLS is more compatible with the inherent dynamics of the network training process, thus achieving enhanced outcomes. Our method achieves state-of-the-art performance across multiple datasets, particularly excelling in complex mixed datasets.",
        "arxiv_id": "2506.01366",
        "ARXIVID": "2506.01366",
        "COMMENT": "Matches criterion 4 as it introduces a novel deraining model leveraging CLIP for vision-language alignment.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.00679": {
        "authors": [
            "Yunguan Fu",
            "Weixi Yi",
            "Charlotte Manisty",
            "Anish N Bhuva",
            "Thomas A Treibel",
            "James C Moon",
            "Matthew J Clarkson",
            "Rhodri Huw Davies",
            "Yipeng Hu"
        ],
        "title": "CineMA: A Foundation Model for Cine Cardiac MRI",
        "abstract": "arXiv:2506.00679v1 Announce Type: new  Abstract: Cardiac magnetic resonance (CMR) is a key investigation in clinical cardiovascular medicine and has been used extensively in population research. However, extracting clinically important measurements such as ejection fraction for diagnosing cardiovascular diseases remains time-consuming and subjective. We developed CineMA, a foundation AI model automating these tasks with limited labels. CineMA is a self-supervised autoencoder model trained on 74,916 cine CMR studies to reconstruct images from masked inputs. After fine-tuning, it was evaluated across eight datasets on 23 tasks from four categories: ventricle and myocardium segmentation, left and right ventricle ejection fraction calculation, disease detection and classification, and landmark localisation. CineMA is the first foundation model for cine CMR to match or outperform convolutional neural networks (CNNs). CineMA demonstrated greater label efficiency than CNNs, achieving comparable or better performance with fewer annotations. This reduces the burden of clinician labelling and supports replacing task-specific training with fine-tuning foundation models in future cardiac imaging applications. Models and code for pre-training and fine-tuning are available at https://github.com/mathpluscode/CineMA, democratising access to high-performance models that otherwise require substantial computational resources, promoting reproducibility and accelerating clinical translation.",
        "arxiv_id": "2506.00679",
        "ARXIVID": "2506.00679",
        "COMMENT": "Matches criterion 4 as it introduces a vision foundation model (CineMA) for cardiac MRI applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.01795": {
        "authors": [
            "Yu-Lin Shih",
            "Wei-En Tai",
            "Cheng Sun",
            "Yu-Chiang Frank Wang",
            "Hwann-Tzong Chen"
        ],
        "title": "R2SM: Referring and Reasoning for Selective Masks",
        "abstract": "arXiv:2506.01795v1 Announce Type: new  Abstract: We introduce a new task, Referring and Reasoning for Selective Masks (R2SM), which extends text-guided segmentation by incorporating mask-type selection driven by user intent. This task challenges vision-language models to determine whether to generate a modal (visible) or amodal (complete) segmentation mask based solely on natural language prompts. To support the R2SM task, we present the R2SM dataset, constructed by augmenting annotations of COCOA-cls, D2SA, and MUVA. The R2SM dataset consists of both modal and amodal text queries, each paired with the corresponding ground-truth mask, enabling model finetuning and evaluation for the ability to segment images as per user intent. Specifically, the task requires the model to interpret whether a given prompt refers to only the visible part of an object or to its complete shape, including occluded regions, and then produce the appropriate segmentation. For example, if a prompt explicitly requests the whole shape of a partially hidden object, the model is expected to output an amodal mask that completes the occluded parts. In contrast, prompts without explicit mention of hidden regions should generate standard modal masks. The R2SM benchmark provides a challenging and insightful testbed for advancing research in multimodal reasoning and intent-aware segmentation.",
        "arxiv_id": "2506.01795",
        "ARXIVID": "2506.01795",
        "COMMENT": "Matches criterion 2. Introduces a new task and dataset for vision-language models, focusing on segmentation based on user intent.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.01293": {
        "authors": [
            "Yichi Zhang",
            "Zhuo Chen",
            "Lingbing Guo",
            "Yajing Xu",
            "Min Zhang",
            "Wen Zhang",
            "Huajun Chen"
        ],
        "title": "Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New Perspective for MLLM Evaluation",
        "abstract": "arXiv:2506.01293v1 Announce Type: new  Abstract: Multi-modal large language models (MLLMs) incorporate heterogeneous modalities into LLMs, enabling a comprehensive understanding of diverse scenarios and objects. Despite the proliferation of evaluation benchmarks and leaderboards for MLLMs, they predominantly overlook the critical capacity of MLLMs to comprehend world knowledge with structured abstractions that appear in visual form. To address this gap, we propose a novel evaluation paradigm and devise M3STR, an innovative benchmark grounded in the Multi-Modal Map for STRuctured understanding. This benchmark leverages multi-modal knowledge graphs to synthesize images encapsulating subgraph architectures enriched with multi-modal entities. M3STR necessitates that MLLMs not only recognize the multi-modal entities within the visual inputs but also decipher intricate relational topologies among them. We delineate the benchmark's statistical profiles and automated construction pipeline, accompanied by an extensive empirical analysis of 26 state-of-the-art MLLMs. Our findings reveal persistent deficiencies in processing abstractive visual information with structured knowledge, thereby charting a pivotal trajectory for advancing MLLMs' holistic reasoning capacities. Our code and data are released at https://github.com/zjukg/M3STR",
        "arxiv_id": "2506.01293",
        "ARXIVID": "2506.01293",
        "COMMENT": "Matches criterion 2. Proposes a new evaluation paradigm for MLLMs, focusing on structured knowledge in visual form.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.01923": {
        "authors": [
            "Amin Karimi Monsefi",
            "Mridul Khurana",
            "Rajiv Ramnath",
            "Anuj Karpatne",
            "Wei-Lun Chao",
            "Cheng Zhang"
        ],
        "title": "TaxaDiffusion: Progressively Trained Diffusion Model for Fine-Grained Species Generation",
        "abstract": "arXiv:2506.01923v1 Announce Type: new  Abstract: We propose TaxaDiffusion, a taxonomy-informed training framework for diffusion models to generate fine-grained animal images with high morphological and identity accuracy. Unlike standard approaches that treat each species as an independent category, TaxaDiffusion incorporates domain knowledge that many species exhibit strong visual similarities, with distinctions often residing in subtle variations of shape, pattern, and color. To exploit these relationships, TaxaDiffusion progressively trains conditioned diffusion models across different taxonomic levels -- starting from broad classifications such as Class and Order, refining through Family and Genus, and ultimately distinguishing at the Species level. This hierarchical learning strategy first captures coarse-grained morphological traits shared by species with common ancestors, facilitating knowledge transfer before refining fine-grained differences for species-level distinction. As a result, TaxaDiffusion enables accurate generation even with limited training samples per species. Extensive experiments on three fine-grained animal datasets demonstrate that outperforms existing approaches, achieving superior fidelity in fine-grained animal image generation. Project page: https://amink8.github.io/TaxaDiffusion/",
        "arxiv_id": "2506.01923",
        "ARXIVID": "2506.01923",
        "COMMENT": "Matches criterion 4 as it focuses on a novel application of diffusion models for fine-grained image generation, which is related to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.01300": {
        "authors": [
            "Yiyang Zhou",
            "Yangfan He",
            "Yaofeng Su",
            "Siwei Han",
            "Joel Jang",
            "Gedas Bertasius",
            "Mohit Bansal",
            "Huaxiu Yao"
        ],
        "title": "ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding",
        "abstract": "arXiv:2506.01300v1 Announce Type: new  Abstract: Video understanding is fundamental to tasks such as action recognition, video reasoning, and robotic control. Early video understanding methods based on large vision-language models (LVLMs) typically adopt a single-pass reasoning paradigm without dynamic feedback, limiting the model's capacity to self-correct and adapt in complex scenarios. Recent efforts have attempted to address this limitation by incorporating reward models and reinforcement learning to enhance reasoning, or by employing tool-agent frameworks. However, these approaches face several challenges, including high annotation costs, reward signals that fail to capture real-time reasoning states, and low inference efficiency. To overcome these issues, we propose ReAgent-V, a novel agentic video understanding framework that integrates efficient frame selection with real-time reward generation during inference. These reward signals not only guide iterative answer refinement through a multi-perspective reflection mechanism-adjusting predictions from conservative, neutral, and aggressive viewpoints-but also enable automatic filtering of high-quality data for supervised fine-tuning (SFT), direct preference optimization (DPO), and group relative policy optimization (GRPO). ReAgent-V is lightweight, modular, and extensible, supporting flexible tool integration tailored to diverse tasks. Extensive experiments on 12 datasets across three core applications-video understanding, video reasoning enhancement, and vision-language-action model alignment-demonstrate significant gains in generalization and reasoning, with improvements of up to 6.9%, 2.1%, and 9.8%, respectively, highlighting the effectiveness and versatility of the proposed framework.",
        "arxiv_id": "2506.01300",
        "ARXIVID": "2506.01300",
        "COMMENT": "Matches criterion 2 as it proposes a novel framework for video understanding using large vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.00530": {
        "authors": [
            "Tianhui Liu",
            "Jie Feng",
            "Hetian Pang",
            "Xin Zhang",
            "Tianjian Ouyang",
            "Zhiyuan Zhang",
            "Yong Li"
        ],
        "title": "CityLens: Benchmarking Large Language-Vision Models for Urban Socioeconomic Sensing",
        "abstract": "arXiv:2506.00530v1 Announce Type: new  Abstract: Understanding urban socioeconomic conditions through visual data is a challenging yet essential task for sustainable urban development and policy planning. In this work, we introduce $\\textbf{CityLens}$, a comprehensive benchmark designed to evaluate the capabilities of large language-vision models (LLVMs) in predicting socioeconomic indicators from satellite and street view imagery. We construct a multi-modal dataset covering a total of 17 globally distributed cities, spanning 6 key domains: economy, education, crime, transport, health, and environment, reflecting the multifaceted nature of urban life. Based on this dataset, we define 11 prediction tasks and utilize three evaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation, and Feature-Based Regression. We benchmark 17 state-of-the-art LLVMs across these tasks. Our results reveal that while LLVMs demonstrate promising perceptual and reasoning capabilities, they still exhibit limitations in predicting urban socioeconomic indicators. CityLens provides a unified framework for diagnosing these limitations and guiding future efforts in using LLVMs to understand and predict urban socioeconomic patterns. Our codes and datasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens.",
        "arxiv_id": "2506.00530",
        "ARXIVID": "2506.00530",
        "COMMENT": "Matches criterion 4 as it benchmarks large language-vision models for urban socioeconomic sensing.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.00618": {
        "authors": [
            "Jingyi Yang",
            "Shuai Shao",
            "Dongrui Liu",
            "Jing Shao"
        ],
        "title": "RiOSWorld: Benchmarking the Risk of Multimodal Compter-Use Agents",
        "abstract": "arXiv:2506.00618v1 Announce Type: new  Abstract: With the rapid development of multimodal large language models (MLLMs), they are increasingly deployed as autonomous computer-use agents capable of accomplishing complex computer tasks. However, a pressing issue arises: Can the safety risk principles designed and aligned for general MLLMs in dialogue scenarios be effectively transferred to real-world computer-use scenarios? Existing research on evaluating the safety risks of MLLM-based computer-use agents suffers from several limitations: it either lacks realistic interactive environments, or narrowly focuses on one or a few specific risk types. These limitations ignore the complexity, variability, and diversity of real-world environments, thereby restricting comprehensive risk evaluation for computer-use agents. To this end, we introduce \\textbf{RiOSWorld}, a benchmark designed to evaluate the potential risks of MLLM-based agents during real-world computer manipulations. Our benchmark includes 492 risky tasks spanning various computer applications, involving web, social media, multimedia, os, email, and office software. We categorize these risks into two major classes based on their risk source: (i) User-originated risks and (ii) Environmental risks. For the evaluation, we evaluate safety risks from two perspectives: (i) Risk goal intention and (ii) Risk goal completion. Extensive experiments with multimodal agents on \\textbf{RiOSWorld} demonstrate that current computer-use agents confront significant safety risks in real-world scenarios. Our findings highlight the necessity and urgency of safety alignment for computer-use agents in real-world computer manipulation, providing valuable insights for developing trustworthy computer-use agents. Our benchmark is publicly available at https://yjyddq.github.io/RiOSWorld.github.io/.",
        "arxiv_id": "2506.00618",
        "ARXIVID": "2506.00618",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for evaluating risks in multimodal agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.01716": {
        "authors": [
            "Yifei Zhou",
            "Sergey Levine",
            "Jason Weston",
            "Xian Li",
            "Sainbayar Sukhbaatar"
        ],
        "title": "Self-Challenging Language Model Agents",
        "abstract": "arXiv:2506.01716v1 Announce Type: new  Abstract: Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.",
        "arxiv_id": "2506.01716",
        "ARXIVID": "2506.01716",
        "COMMENT": "Matches criterion 2 as it discusses a novel framework for training language model agents with self-generated tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.00830": {
        "authors": [
            "Zhengcong Fei",
            "Hao Jiang",
            "Di Qiu",
            "Baoxuan Gu",
            "Youqiang Zhang",
            "Jiahua Wang",
            "Jialin Bai",
            "Debang Li",
            "Mingyuan Fan",
            "Guibin Chen",
            "Yahui Zhou"
        ],
        "title": "SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video Diffusion Transformers",
        "abstract": "arXiv:2506.00830v1 Announce Type: new  Abstract: The generation and editing of audio-conditioned talking portraits guided by multimodal inputs, including text, images, and videos, remains under explored. In this paper, we present SkyReels-Audio, a unified framework for synthesizing high-fidelity and temporally coherent talking portrait videos. Built upon pretrained video diffusion transformers, our framework supports infinite-length generation and editing, while enabling diverse and controllable conditioning through multimodal inputs. We employ a hybrid curriculum learning strategy to progressively align audio with facial motion, enabling fine-grained multimodal control over long video sequences. To enhance local facial coherence, we introduce a facial mask loss and an audio-guided classifier-free guidance mechanism. A sliding-window denoising approach further fuses latent representations across temporal segments, ensuring visual fidelity and temporal consistency across extended durations and diverse identities. More importantly, we construct a dedicated data pipeline for curating high-quality triplets consisting of synchronized audio, video, and textual descriptions. Comprehensive benchmark evaluations show that SkyReels-Audio achieves superior performance in lip-sync accuracy, identity consistency, and realistic facial dynamics, particularly under complex and challenging conditions.",
        "arxiv_id": "2506.00830",
        "ARXIVID": "2506.00830",
        "COMMENT": "Matches criterion 2 as it discusses a multimodal framework for synthesizing talking portrait videos using video diffusion transformers.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.01725": {
        "authors": [
            "Desen Meng",
            "Rui Huang",
            "Zhilin Dai",
            "Xinhao Li",
            "Yifan Xu",
            "Jun Zhang",
            "Zhenpeng Huang",
            "Meng Zhang",
            "Lingshu Zhang",
            "Yi Liu",
            "Limin Wang"
        ],
        "title": "VideoCap-R1: Enhancing MLLMs for Video Captioning via Structured Thinking",
        "abstract": "arXiv:2506.01725v1 Announce Type: new  Abstract: While recent advances in reinforcement learning have significantly enhanced reasoning capabilities in large language models (LLMs), these techniques remain underexplored in multi-modal LLMs for video captioning. This paper presents the first systematic investigation of GRPO-based RL post-training for video MLLMs, with the goal of enhancing video MLLMs' capability of describing actions in videos. Specifically, we develop the VideoCap-R1, which is prompted to first perform structured thinking that analyzes video subjects with their attributes and actions before generating complete captions, supported by two specialized reward mechanisms: a LLM-free think scorer evaluating the structured thinking quality and a LLM-assisted caption scorer assessing the output quality. The RL training framework effectively establishes the connection between structured reasoning and comprehensive description generation, enabling the model to produce captions with more accurate actions. Our experiments demonstrate that VideoCap-R1 achieves substantial improvements over the Qwen2VL-7B baseline using limited samples (1.5k) across multiple video caption benchmarks (DREAM1K: +4.4 event F1, VDC: +4.2 Acc, CAREBENCH: +3.1 action F1, +6.9 object F1) while consistently outperforming the SFT-trained counterparts, confirming GRPO's superiority in enhancing MLLMs' captioning capabilities.",
        "arxiv_id": "2506.01725",
        "ARXIVID": "2506.01725",
        "COMMENT": "Matches criterion 2 as it enhances MLLMs for video captioning, focusing on structured reasoning and reinforcement learning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.01085": {
        "authors": [
            "Shivam Chandhok",
            "Qian Yang",
            "Oscar Manas",
            "Kanishk Jain",
            "Leonid Sigal",
            "Aishwarya Agrawal"
        ],
        "title": "Learning What Matters: Prioritized Concept Learning via Relative Error-driven Sample Selection",
        "abstract": "arXiv:2506.01085v1 Announce Type: new  Abstract: Instruction tuning has been central to the success of recent vision-language models (VLMs), but it remains expensive-requiring large-scale datasets, high-quality annotations, and large compute budgets. We propose PRioritized cOncept learninG via Relative Error-driven Sample Selection (PROGRESS), a data- and compute-efficient framework that enables VLMs to dynamically select what to learn next based on their evolving needs during training. At each stage, the model tracks its learning progress across skills and selects the most informative samples-those it has not already mastered and that are not too difficult to learn at the current stage of training. This strategy effectively controls skill acquisition and the order in which skills are learned. Specifically, we sample from skills showing the highest learning progress, prioritizing those with the most rapid improvement. Unlike prior methods, PROGRESS requires no upfront answer annotations, queries answers only on a need basis, avoids reliance on additional supervision from auxiliary VLMs, and does not require compute-heavy gradient computations for data selection. Experiments across multiple instruction-tuning datasets of varying scales demonstrate that PROGRESS consistently outperforms state-of-the-art baselines with much less data and supervision. Additionally, we show strong cross-architecture generalization and transferability to larger models, validating PROGRESS as a scalable solution for efficient learning.",
        "arxiv_id": "2506.01085",
        "ARXIVID": "2506.01085",
        "COMMENT": "Matches criterion 2 as it proposes a novel framework for instruction tuning in vision-language models, focusing on efficient learning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.01119": {
        "authors": [
            "Hong Nguyen",
            "Dung Tran",
            "Hieu Hoang",
            "Phong Nguyen",
            "Shrikanth Narayanan"
        ],
        "title": "MOOSE: Pay Attention to Temporal Dynamics for Video Understanding via Optical Flows",
        "abstract": "arXiv:2506.01119v1 Announce Type: new  Abstract: Many motion-centric video analysis tasks, such as atomic actions, detecting atypical motor behavior in individuals with autism, or analyzing articulatory motion in real-time MRI of human speech, require efficient and interpretable temporal modeling. Capturing temporal dynamics is a central challenge in video analysis, often requiring significant computational resources and fine-grained annotations that are not widely available. This paper presents MOOSE (Motion Flow Over Spatial Space), a novel temporally-centric video encoder explicitly integrating optical flow with spatial embeddings to model temporal information efficiently, inspired by human perception of motion. Unlike prior models, MOOSE takes advantage of rich, widely available pre-trained visual and optical flow encoders instead of training video models from scratch. This significantly reduces computational complexity while enhancing temporal interpretability. Our primary contributions includes (1) proposing a computationally efficient temporally-centric architecture for video understanding (2) demonstrating enhanced interpretability in modeling temporal dynamics; and (3) achieving state-of-the-art performance on diverse benchmarks, including clinical, medical, and standard action recognition datasets, confirming the broad applicability and effectiveness of our approach.",
        "arxiv_id": "2506.01119",
        "ARXIVID": "2506.01119",
        "COMMENT": "Matches criterion 1 as it proposes a novel method for temporal dynamics in video understanding, which is related to spatial intelligence in embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.00991": {
        "authors": [
            "Xiaorong Zhu",
            "Ziheng Jia",
            "Jiarui Wang",
            "Xiangyu Zhao",
            "Haodong Duan",
            "Xiongkuo Min",
            "Jia Wang",
            "Zicheng Zhang",
            "Guangtao Zhai"
        ],
        "title": "GOBench: Benchmarking Geometric Optics Generation and Understanding of MLLMs",
        "abstract": "arXiv:2506.00991v1 Announce Type: new  Abstract: The rapid evolution of Multi-modality Large Language Models (MLLMs) is driving significant advancements in visual understanding and generation. Nevertheless, a comprehensive assessment of their capabilities, concerning the fine-grained physical principles especially in geometric optics, remains underexplored. To address this gap, we introduce GOBench, the first benchmark to systematically evaluate MLLMs' ability across two tasks: 1) Generating Optically Authentic Imagery and 2) Understanding Underlying Optical Phenomena. We curates high-quality prompts of geometric optical scenarios and use MLLMs to construct GOBench-Gen-1k dataset.We then organize subjective experiments to assess the generated imagery based on Optical Authenticity, Aesthetic Quality, and Instruction Fidelity, revealing MLLMs' generation flaws that violate optical principles. For the understanding task, we apply crafted evaluation instructions to test optical understanding ability of eleven prominent MLLMs. The experimental results demonstrate that current models face significant challenges in both optical generation and understanding. The top-performing generative model, GPT-4o-Image, cannot perfectly complete all generation tasks, and the best-performing MLLM model, Gemini-2.5Pro, attains a mere 37.35\\% accuracy in optical understanding.",
        "arxiv_id": "2506.00991",
        "ARXIVID": "2506.00991",
        "COMMENT": "Matches criterion 2 as it introduces GOBench, a benchmark for evaluating MLLMs' capabilities in geometric optics, focusing on visual understanding and generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.00956": {
        "authors": [
            "Geonu Lee",
            "Yujeong Oh",
            "Geonhui Jang",
            "Soyoung Lee",
            "Jeonghyo Song",
            "Sungmin Cha",
            "YoungJoon Yoo"
        ],
        "title": "Continual-MEGA: A Large-scale Benchmark for Generalizable Continual Anomaly Detection",
        "abstract": "arXiv:2506.00956v1 Announce Type: new  Abstract: In this paper, we introduce a new benchmark for continual learning in anomaly detection, aimed at better reflecting real-world deployment scenarios. Our benchmark, Continual-MEGA, includes a large and diverse dataset that significantly expands existing evaluation settings by combining carefully curated existing datasets with our newly proposed dataset, ContinualAD. In addition to standard continual learning with expanded quantity, we propose a novel scenario that measures zero-shot generalization to unseen classes, those not observed during continual adaptation. This setting poses a new problem setting that continual adaptation also enhances zero-shot performance. We also present a unified baseline algorithm that improves robustness in few-shot detection and maintains strong generalization. Through extensive evaluations, we report three key findings: (1) existing methods show substantial room for improvement, particularly in pixel-level defect localization; (2) our proposed method consistently outperforms prior approaches; and (3) the newly introduced ContinualAD dataset enhances the performance of strong anomaly detection models. We release the benchmark and code in https://github.com/Continual-Mega/Continual-Mega.",
        "arxiv_id": "2506.00956",
        "ARXIVID": "2506.00956",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for continual learning in anomaly detection, focusing on novel scenarios like zero-shot generalization.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.01015": {
        "authors": [
            "Yuyuan Liu",
            "Yuanhong Chen",
            "Chong Wang",
            "Junlin Han",
            "Junde Wu",
            "Can Peng",
            "Jingkun Chen",
            "Yu Tian",
            "Gustavo Carneiro"
        ],
        "title": "AuralSAM2: Enabling SAM2 Hear Through Pyramid Audio-Visual Feature Prompting",
        "abstract": "arXiv:2506.01015v1 Announce Type: new  Abstract: Segment Anything Model 2 (SAM2) exhibits strong generalisation for promptable segmentation in video clips; however, its integration with the audio modality remains underexplored. Existing approaches mainly follow two directions: (1) injecting adapters into the image encoder to receive audio signals, which incurs efficiency costs during prompt engineering, and (2) leveraging additional foundation models to generate visual prompts for the sounding objects, which are often imprecisely localised, leading to misguidance in SAM2. Moreover, these methods overlook the rich semantic interplay between hierarchical visual features and other modalities, resulting in suboptimal cross-modal fusion. In this work, we propose AuralSAM2, comprising the novel AuralFuser module, which externally attaches to SAM2 to integrate features from different modalities and generate feature-level prompts, guiding SAM2's decoder in segmenting sounding targets. Such integration is facilitated by a feature pyramid, further refining semantic understanding and enhancing object awareness in multimodal scenarios. Additionally, the audio-guided contrastive learning is introduced to explicitly align audio and visual representations and to also mitigate biases caused by dominant visual patterns. Results on public benchmarks show that our approach achieves remarkable improvements over the previous methods in the field. Code is available at https://github.com/yyliu01/AuralSAM2.",
        "arxiv_id": "2506.01015",
        "ARXIVID": "2506.01015",
        "COMMENT": "Matches criterion 2 as it extends SAM2 with audio-visual feature prompting for segmentation tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.00238": {
        "authors": [
            "Ehsan Karimi",
            "Maryam Rahnemoonfar"
        ],
        "title": "ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping for Natural Disaster Damage Assessment",
        "abstract": "arXiv:2506.00238v1 Announce Type: new  Abstract: Natural disasters usually affect vast areas and devastate infrastructures. Performing a timely and efficient response is crucial to minimize the impact on affected communities, and data-driven approaches are the best choice. Visual question answering (VQA) models help management teams to achieve in-depth understanding of damages. However, recently published models do not possess the ability to answer open-ended questions and only select the best answer among a predefined list of answers. If we want to ask questions with new additional possible answers that do not exist in the predefined list, the model needs to be fin-tuned/retrained on a new collected and annotated dataset, which is a time-consuming procedure. In recent years, large-scale Vision-Language Models (VLMs) have earned significant attention. These models are trained on extensive datasets and demonstrate strong performance on both unimodal and multimodal vision/language downstream tasks, often without the need for fine-tuning. In this paper, we propose a VLM-based zero-shot VQA (ZeShot-VQA) method, and investigate the performance of on post-disaster FloodNet dataset. Since the proposed method takes advantage of zero-shot learning, it can be applied on new datasets without fine-tuning. In addition, ZeShot-VQA is able to process and generate answers that has been not seen during the training procedure, which demonstrates its flexibility.",
        "arxiv_id": "2506.00238",
        "ARXIVID": "2506.00238",
        "COMMENT": "Matches criterion 2 as it proposes a zero-shot VQA framework leveraging vision-language models for disaster damage assessment.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.00930": {
        "authors": [
            "Yongqi Li",
            "Shen Zhou",
            "Xiaohu Li",
            "Xin Miao",
            "Jintao Wen",
            "Mayi Xu",
            "Jianhao Chen",
            "Birong Pan",
            "Hankun Kang",
            "Yuanyuan Zhu",
            "Ming Zhong",
            "Tieyun Qian"
        ],
        "title": "Aligning VLM Assistants with Personalized Situated Cognition",
        "abstract": "arXiv:2506.00930v1 Announce Type: new  Abstract: Vision-language models (VLMs) aligned with general human objectives, such as being harmless and hallucination-free, have become valuable assistants of humans in managing visual tasks. However, people with diversified backgrounds have different cognition even in the same situation. Consequently, they may have personalized expectations for VLM assistants. This highlights the urgent need to align VLM assistants with personalized situated cognition for real-world assistance. To study this problem, we first simplify it by characterizing individuals based on the sociological concept of Role-Set. Then, we propose to evaluate the individuals' actions to examine whether the personalized alignment is achieved. Further, we construct a benchmark named PCogAlignBench, which includes 18k instances and 20 individuals with different Role-Sets. Finally, we present a framework called PCogAlign, which constructs a cognition-aware and action-based reward model for personalized alignment. Experimental results and human evaluations demonstrate the reliability of the PCogAlignBench and the effectiveness of our proposed PCogAlign. We will open-source the constructed benchmark and code at https://github.com/NLPGM/PCogAlign.",
        "arxiv_id": "2506.00930",
        "ARXIVID": "2506.00930",
        "COMMENT": "Matches criterion 2 as it focuses on aligning vision-language models (VLMs) with personalized cognition, introducing a new benchmark.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.00835": {
        "authors": [
            "Jisheng Dang",
            "Yizhou Zhang",
            "Hao Ye",
            "Teng Wang",
            "Siming Chen",
            "Huicheng Zheng",
            "Yulan Guo",
            "Jianhuang Lai",
            "Bin Hu"
        ],
        "title": "SynPO: Synergizing Descriptiveness and Preference Optimization for Video Detailed Captioning",
        "abstract": "arXiv:2506.00835v1 Announce Type: new  Abstract: Fine-grained video captioning aims to generate detailed, temporally coherent descriptions of video content. However, existing methods struggle to capture subtle video dynamics and rich detailed information. In this paper, we leverage preference learning to enhance the performance of vision-language models in fine-grained video captioning, while mitigating several limitations inherent to direct preference optimization (DPO). First, we propose a pipeline for constructing preference pairs that leverages the intrinsic properties of VLMs along with partial assistance from large language models, achieving an optimal balance between cost and data quality. Second, we propose Synergistic Preference Optimization (SynPO), a novel optimization method offering significant advantages over DPO and its variants. SynPO prevents negative preferences from dominating the optimization, explicitly preserves the model's language capability to avoid deviation of the optimization objective, and improves training efficiency by eliminating the need for the reference model. We extensively evaluate SynPO not only on video captioning benchmarks (e.g., VDC, VDD, VATEX) but also across well-established NLP tasks, including general language understanding and preference evaluation, using diverse pretrained models. Results demonstrate that SynPO consistently outperforms DPO variants while achieving 20\\% improvement in training efficiency. Code is available at https://github.com/longmalongma/SynPO",
        "arxiv_id": "2506.00835",
        "ARXIVID": "2506.00835",
        "COMMENT": "Matches criterion 2 as it discusses improvements to vision-language models (VLMs) for fine-grained video captioning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2506.00886": {
        "authors": [
            "Hongru Wang",
            "Cheng Qian",
            "Manling Li",
            "Jiahao Qiu",
            "Boyang Xue",
            "Mengdi Wang",
            "Heng Ji",
            "Kam-Fai Wong"
        ],
        "title": "Toward a Theory of Agents as Tool-Use Decision-Makers",
        "abstract": "arXiv:2506.00886v1 Announce Type: new  Abstract: As Large Language Models (LLMs) evolve into increasingly autonomous agents, fundamental questions about their epistemic foundations remain unresolved: What defines an agent? How should it make decisions? And what objectives should guide its behavior? In this position paper, we argue that true autonomy requires agents to be grounded in a coherent epistemic framework that governs what they know, what they need to know, and how to acquire that knowledge efficiently. We propose a unified theory that treats internal reasoning and external actions as equivalent epistemic tools, enabling agents to systematically coordinate introspection and interaction. Building on this framework, we advocate for aligning an agent's tool use decision-making boundary with its knowledge boundary, thereby minimizing unnecessary tool use and maximizing epistemic efficiency. This perspective shifts the design of agents from mere action executors to knowledge-driven intelligence systems, offering a principled path toward building foundation agents capable of adaptive, efficient, and goal-directed behavior.",
        "arxiv_id": "2506.00886",
        "ARXIVID": "2506.00886",
        "COMMENT": "Matches criterion 2 as it discusses a theoretical framework for tool-use decision-making in large language model agents.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2506.00333": {
        "authors": [
            "Mingxuan Liu",
            "Tyler L. Hayes",
            "Massimiliano Mancini",
            "Elisa Ricci",
            "Riccardo Volpi",
            "Gabriela Csurka"
        ],
        "title": "Test-time Vocabulary Adaptation for Language-driven Object Detection",
        "abstract": "arXiv:2506.00333v1 Announce Type: new  Abstract: Open-vocabulary object detection models allow users to freely specify a class vocabulary in natural language at test time, guiding the detection of desired objects. However, vocabularies can be overly broad or even mis-specified, hampering the overall performance of the detector. In this work, we propose a plug-and-play Vocabulary Adapter (VocAda) to refine the user-defined vocabulary, automatically tailoring it to categories that are relevant for a given image. VocAda does not require any training, it operates at inference time in three steps: i) it uses an image captionner to describe visible objects, ii) it parses nouns from those captions, and iii) it selects relevant classes from the user-defined vocabulary, discarding irrelevant ones. Experiments on COCO and Objects365 with three state-of-the-art detectors show that VocAda consistently improves performance, proving its versatility. The code is open source.",
        "arxiv_id": "2506.00333",
        "ARXIVID": "2506.00333",
        "COMMENT": "Matches criterion 4 as it focuses on vision-language models and their application to open-vocabulary object detection.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2506.00996": {
        "authors": [
            "Kinam Kim",
            "Junha Hyung",
            "Jaegul Choo"
        ],
        "title": "Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models",
        "abstract": "arXiv:2506.00996v1 Announce Type: new  Abstract: Recent advances in text-to-video diffusion models have enabled high-quality video synthesis, but controllable generation remains challenging, particularly under limited data and compute. Existing fine-tuning methods for conditional generation often rely on external encoders or architectural modifications, which demand large datasets and are typically restricted to spatially aligned conditioning, limiting flexibility and scalability. In this work, we introduce Temporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach for adapting pretrained video diffusion models to diverse conditional generation tasks. Our key idea is to concatenate condition and target frames along the temporal axis and insert intermediate buffer frames with progressively increasing noise levels. These buffer frames enable smooth transitions, aligning the fine-tuning process with the pretrained model's temporal dynamics. TIC-FT requires no architectural changes and achieves strong performance with as few as 10-30 training samples. We validate our method across a range of tasks, including image-to-video and video-to-video generation, using large-scale base models such as CogVideoX-5B and Wan-14B. Extensive experiments show that TIC-FT outperforms existing baselines in both condition fidelity and visual quality, while remaining highly efficient in both training and inference. For additional results, visit https://kinam0252.github.io/TIC-FT/",
        "arxiv_id": "2506.00996",
        "ARXIVID": "2506.00996",
        "COMMENT": "Does not match any specific criteria. Focuses on fine-tuning video diffusion models, which is not directly related to spatial intelligence, VLLMs, MLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.00227": {
        "authors": [
            "Anthony Gosselin",
            "Ge Ya Luo",
            "Luis Lara",
            "Florian Golemo",
            "Derek Nowrouzezahrai",
            "Liam Paull",
            "Alexia Jolicoeur-Martineau",
            "Christopher Pal"
        ],
        "title": "Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes",
        "abstract": "arXiv:2506.00227v1 Announce Type: new  Abstract: Video diffusion techniques have advanced significantly in recent years; however, they struggle to generate realistic imagery of car crashes due to the scarcity of accident events in most driving datasets. Improving traffic safety requires realistic and controllable accident simulations. To tackle the problem, we propose Ctrl-Crash, a controllable car crash video generation model that conditions on signals such as bounding boxes, crash types, and an initial image frame. Our approach enables counterfactual scenario generation where minor variations in input can lead to dramatically different crash outcomes. To support fine-grained control at inference time, we leverage classifier-free guidance with independently tunable scales for each conditioning signal. Ctrl-Crash achieves state-of-the-art performance across quantitative video quality metrics (e.g., FVD and JEDi) and qualitative measurements based on a human-evaluation of physical realism and video quality compared to prior diffusion-based methods.",
        "arxiv_id": "2506.00227",
        "ARXIVID": "2506.00227",
        "COMMENT": "Does not match any specific criteria. Focuses on controllable video generation for car crashes, which is not directly related to spatial intelligence, VLLMs, MLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.00874": {
        "authors": [
            "Yue Zhou",
            "Xinan He",
            "KaiQing Lin",
            "Bin Fan",
            "Feng Ding",
            "Bin Li"
        ],
        "title": "Breaking Latent Prior Bias in Detectors for Generalizable AIGC Image Detection",
        "abstract": "arXiv:2506.00874v1 Announce Type: new  Abstract: Current AIGC detectors often achieve near-perfect accuracy on images produced by the same generator used for training but struggle to generalize to outputs from unseen generators. We trace this failure in part to latent prior bias: detectors learn shortcuts tied to patterns stemming from the initial noise vector rather than learning robust generative artifacts. To address this, we propose On-Manifold Adversarial Training (OMAT): by optimizing the initial latent noise of diffusion models under fixed conditioning, we generate on-manifold adversarial examples that remain on the generator's output manifold-unlike pixel-space attacks, which introduce off-manifold perturbations that the generator itself cannot reproduce and that can obscure the true discriminative artifacts. To test against state-of-the-art generative models, we introduce GenImage++, a test-only benchmark of outputs from advanced generators (Flux.1, SD3) with extended prompts and diverse styles. We apply our adversarial-training paradigm to ResNet50 and CLIP baselines and evaluate across existing AIGC forensic benchmarks and recent challenge datasets. Extensive experiments show that adversarially trained detectors significantly improve cross-generator performance without any network redesign. Our findings on latent-prior bias offer valuable insights for future dataset construction and detector evaluation, guiding the development of more robust and generalizable AIGC forensic methodologies.",
        "arxiv_id": "2506.00874",
        "ARXIVID": "2506.00874",
        "COMMENT": "Does not match any specific criteria but discusses generalizable AIGC image detection, which is tangentially relevant to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.01692": {
        "authors": [
            "Sylee Dandekar",
            "Shripad Deshmukh",
            "Frank Chiu",
            "W. Bradley Knox",
            "Scott Niekum"
        ],
        "title": "A Descriptive and Normative Theory of Human Beliefs in RLHF",
        "abstract": "arXiv:2506.01692v1 Announce Type: new  Abstract: Human preferences in RLHF are typically modeled as a function of the human's reward function or corresponding optimal state-action values. In this work, we propose that human beliefs about the capabilities of the agent being trained also play a key role in preference generation. We examine two questions related to this hypothesis, one descriptive and one normative, respectively: Do human labelers' beliefs about agent capabilities affect the preferences that they provide? And what is the ideal set of beliefs about an agent -- and resulting preferences -- for humans to have? We propose a new preference model that incorporates human beliefs and provide a normative theory that bounds the error on the final learned policy based on the \\textit{mismatch} between the human's beliefs and an idealized set of beliefs. We then confirm via a human study that beliefs about agent capabilities do, in fact, significantly affect preferences and can be influenced through simple interventions. Additionally, we empirically show through synthetic experiments that it is often suboptimal for human preference labelers to assume agent optimality. Collectively, these results theoretically and empirically demonstrate how reducing the mismatch between human beliefs and agent capabilities can lead to more performant RLHF and point toward new best practices for RLHF practitioners.",
        "arxiv_id": "2506.01692",
        "ARXIVID": "2506.01692",
        "COMMENT": "Does not match any specific criteria but discusses human beliefs in RLHF, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2506.01701": {
        "authors": [
            "Haoru Tan",
            "Sitong Wu",
            "Wei Huang",
            "Shizhen Zhao",
            "Xiaojuan Qi"
        ],
        "title": "Data Pruning by Information Maximization",
        "abstract": "arXiv:2506.01701v1 Announce Type: new  Abstract: In this paper, we present InfoMax, a novel data pruning method, also known as coreset selection, designed to maximize the information content of selected samples while minimizing redundancy. By doing so, InfoMax enhances the overall informativeness of the coreset. The information of individual samples is measured by importance scores, which capture their influence or difficulty in model learning. To quantify redundancy, we use pairwise sample similarities, based on the premise that similar samples contribute similarly to the learning process. We formalize the coreset selection problem as a discrete quadratic programming (DQP) task, with the objective of maximizing the total information content, represented as the sum of individual sample contributions minus the redundancies introduced by similar samples within the coreset. To ensure practical scalability, we introduce an efficient gradient-based solver, complemented by sparsification techniques applied to the similarity matrix and dataset partitioning strategies. This enables InfoMax to seamlessly scale to datasets with millions of samples. Extensive experiments demonstrate the superior performance of InfoMax in various data pruning tasks, including image classification, vision-language pre-training, and instruction tuning for large language models.",
        "arxiv_id": "2506.01701",
        "ARXIVID": "2506.01701",
        "COMMENT": "Does not match any specific criteria. Focuses on data pruning for various tasks, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.01466": {
        "authors": [
            "Shuyu Yang",
            "Yilun Wang",
            "Yaxiong Wang",
            "Li Zhu",
            "Zhedong Zheng"
        ],
        "title": "Towards Scalable Video Anomaly Retrieval: A Synthetic Video-Text Benchmark",
        "abstract": "arXiv:2506.01466v1 Announce Type: new  Abstract: Video anomaly retrieval aims to localize anomalous events in videos using natural language queries to facilitate public safety. However, existing datasets suffer from severe limitations: (1) data scarcity due to the long-tail nature of real-world anomalies, and (2) privacy constraints that impede large-scale collection. To address the aforementioned issues in one go, we introduce SVTA (Synthetic Video-Text Anomaly benchmark), the first large-scale dataset for cross-modal anomaly retrieval, leveraging generative models to overcome data availability challenges. Specifically, we collect and generate video descriptions via the off-the-shelf LLM (Large Language Model) covering 68 anomaly categories, e.g., throwing, stealing, and shooting. These descriptions encompass common long-tail events. We adopt these texts to guide the video generative model to produce diverse and high-quality videos. Finally, our SVTA involves 41,315 videos (1.36M frames) with paired captions, covering 30 normal activities, e.g., standing, walking, and sports, and 68 anomalous events, e.g., falling, fighting, theft, explosions, and natural disasters. We adopt three widely-used video-text retrieval baselines to comprehensively test our SVTA, revealing SVTA's challenging nature and its effectiveness in evaluating a robust cross-modal retrieval method. SVTA eliminates privacy risks associated with real-world anomaly collection while maintaining realistic scenarios. The dataset demo is available at: [https://svta-mm.github.io/SVTA.github.io/].",
        "arxiv_id": "2506.01466",
        "ARXIVID": "2506.01466",
        "COMMENT": "Does not match any specific criteria. Focuses on synthetic video-text benchmarks for anomaly retrieval, which is not directly related to spatial intelligence, VLLMs, MLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.01413": {
        "authors": [
            "Yulei Qin",
            "Gang Li",
            "Zongyi Li",
            "Zihan Xu",
            "Yuchen Shi",
            "Zhekai Lin",
            "Xiao Cui",
            "Ke Li",
            "Xing Sun"
        ],
        "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models",
        "abstract": "arXiv:2506.01413v1 Announce Type: new  Abstract: Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at https://github.com/yuleiqin/RAIF.",
        "arxiv_id": "2506.01413",
        "ARXIVID": "2506.01413",
        "COMMENT": "Does not match any specific criteria. Focuses on reasoning and instruction-following in LLMs, not spatial intelligence, VLLMs, MLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.01926": {
        "authors": [
            "Joey Skaf",
            "Luis Ibanez-Lissen",
            "Robert McCarthy",
            "Connor Watts",
            "Vasil Georgiv",
            "Hannes Whittingham",
            "Lorena Gonzalez-Manzano",
            "David Lindner",
            "Cameron Tice",
            "Edward James Young",
            "Puria Radmard"
        ],
        "title": "Large language models can learn and generalize steganographic chain-of-thought under process supervision",
        "abstract": "arXiv:2506.01926v1 Announce Type: new  Abstract: Chain-of-thought (CoT) reasoning not only enhances large language model performance but also provides critical insights into decision-making processes, marking it as a useful tool for monitoring model intent and planning. By proactively preventing models from acting on CoT indicating misaligned or harmful intent, CoT monitoring can be used to reduce risks associated with deploying models. However, developers may be incentivized to train away the appearance of harmful intent from CoT traces, by either customer preferences or regulatory requirements. Recent works have shown that banning mention of a specific example of reward hacking, which may be done either to make CoT presentable to users or as a naive attempt to prevent the behavior, causes obfuscation of the undesired reasoning traces but the persistence of the undesired behavior. Such obfuscation threatens the reliability of CoT monitoring. However, obfuscation of reasoning can be due to its internalization to latent space computation, or its encoding within the CoT. Here, we provide an extension to these results. First, we show that penalizing the use of specific strings within load-bearing reasoning traces causes models to substitute alternative strings. Crucially, this does not alter the underlying method by which the model performs the task, demonstrating that the model can learn to steganographically encode its reasoning. We further demonstrate that models can generalize an encoding scheme. When the penalized strings belong to an overarching class, the model learns not only to substitute strings seen in training, but also develops a general encoding scheme for all members of the class which it can apply to held-out testing strings.",
        "arxiv_id": "2506.01926",
        "ARXIVID": "2506.01926",
        "COMMENT": "Does not match any specific criterion but discusses chain-of-thought reasoning in LLMs, which may be of tangential interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.00607": {
        "authors": [
            "JungWoo Chae",
            "Jiyoon Kim",
            "Sangheum Hwang"
        ],
        "title": "Parallel Rescaling: Rebalancing Consistency Guidance for Personalized Diffusion Models",
        "abstract": "arXiv:2506.00607v1 Announce Type: new  Abstract: Personalizing diffusion models to specific users or concepts remains challenging, particularly when only a few reference images are available. Existing methods such as DreamBooth and Textual Inversion often overfit to limited data, causing misalignment between generated images and text prompts when attempting to balance identity fidelity with prompt adherence. While Direct Consistency Optimization (DCO) with its consistency-guided sampling partially alleviates this issue, it still struggles with complex or stylized prompts. In this paper, we propose a parallel rescaling technique for personalized diffusion models. Our approach explicitly decomposes the consistency guidance signal into parallel and orthogonal components relative to classifier free guidance (CFG). By rescaling the parallel component, we minimize disruptive interference with CFG while preserving the subject's identity. Unlike prior personalization methods, our technique does not require additional training data or expensive annotations. Extensive experiments show improved prompt alignment and visual fidelity compared to baseline methods, even on challenging stylized prompts. These findings highlight the potential of parallel rescaled guidance to yield more stable and accurate personalization for diverse user inputs.",
        "arxiv_id": "2506.00607",
        "ARXIVID": "2506.00607",
        "COMMENT": "Does not match any specific criterion but focuses on improving diffusion models for personalization, which may be of tangential interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.00780": {
        "authors": [
            "Jingyu Liu",
            "Jingquan Peng",
            "xiaopeng Wu",
            "Xubin Li",
            "Tiezheng Ge",
            "Bo Zheng",
            "Yong Liu"
        ],
        "title": "Do not Abstain! Identify and Solve the Uncertainty",
        "abstract": "arXiv:2506.00780v1 Announce Type: new  Abstract: Despite the widespread application of Large Language Models (LLMs) across various domains, they frequently exhibit overconfidence when encountering uncertain scenarios, yet existing solutions primarily rely on evasive responses (e.g., \"I don't know\") overlooks the opportunity of identifying and addressing the uncertainty to generate more satisfactory responses. To systematically investigate and improve LLMs' ability of recognizing and addressing the source of uncertainty, we introduce \\textbf{ConfuseBench}, a benchmark mainly focus on three types of uncertainty: document scarcity, limited capability, and query ambiguity. Experiments with ConfuseBench reveal that current LLMs struggle to accurately identify the root cause of uncertainty and solve it. They prefer to attribute uncertainty to query ambiguity while overlooking capability limitations, especially for those weaker models. To tackle this challenge, we first generate context-aware inquiries that highlight the confusing aspect of the original query. Then we judge the source of uncertainty based on the uniqueness of the inquiry's answer. Further we use an on-policy training method, InteractDPO to generate better inquiries. Experimental results demonstrate the efficacy of our approach.",
        "arxiv_id": "2506.00780",
        "ARXIVID": "2506.00780",
        "COMMENT": "Does not match any specific criterion but introduces a benchmark (ConfuseBench) for uncertainty in LLMs, which may be of tangential interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.00242": {
        "authors": [
            "Shuai Feng",
            "Wei-Chuang Chan",
            "Srishti Chouhan",
            "Junior Francisco Garcia Ayala",
            "Srujananjali Medicherla",
            "Kyle Clark",
            "Mingwei Shi"
        ],
        "title": "Whispers of Many Shores: Cultural Alignment through Collaborative Cultural Expertise",
        "abstract": "arXiv:2506.00242v1 Announce Type: new  Abstract: The integration of large language models (LLMs) into global applications necessitates effective cultural alignment for meaningful and culturally-sensitive interactions. Current LLMs often lack the nuanced understanding required for diverse cultural contexts, and adapting them typically involves costly full fine-tuning. To address this, we introduce a novel soft prompt fine-tuning framework that enables efficient and modular cultural alignment. Our method utilizes vectorized prompt tuning to dynamically route queries to a committee of culturally specialized 'expert' LLM configurations, created by optimizing soft prompt embeddings without altering the base model's parameters. Extensive experiments demonstrate that our framework significantly enhances cultural sensitivity and adaptability, improving alignment scores from 0.208 to 0.820, offering a robust solution for culturally-aware LLM deployment. This research paves the way for subsequent investigations into enhanced cultural coverage and dynamic expert adaptation, crucial for realizing autonomous AI with deeply nuanced understanding in a globally interconnected world.",
        "arxiv_id": "2506.00242",
        "ARXIVID": "2506.00242",
        "COMMENT": "Does not match any specific criteria but discusses cultural alignment in LLMs, which is tangentially relevant to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.01273": {
        "authors": [
            "Fernando Granado",
            "Roberto Lotufo",
            "Jayr Pereira"
        ],
        "title": "RAISE: Reasoning Agent for Interactive SQL Exploration",
        "abstract": "arXiv:2506.01273v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) have propelled research in natural language interfaces to databases. However, most state-of-the-art text-to- SQL systems still depend on complex, multi-stage pipelines. This work proposes a novel agentic framework that unifies schema linking, query generation, and itera- tive refinement within a single, end-to-end component. By leveraging the intrinsic reasoning abilities of LLMs, our method emulates how humans answer questions when working with unfamiliar databases: understanding the data by formulating hypotheses, running dynamic queries to validate them, reasoning over the results, and revising outputs based on observed results. Crucially, our approach intro- duces a new strategy for scaling test-time computation in text-to-SQL: we scale the depth of interactive database exploration and reflection. This shift enables the model to allocate computation dynamically to better understand the data, especially useful in ambiguous and underspecified scenarios. Our experiments show that it improved the Execution Accuracy (EX) from 44.8% to 56.5% on the challenging BIRD dataset using DeepSeek-R1-Distill-Llama-70B. Fur- thermore, when equipped with steps to add more diversity to the answers, our agent achieves a Best-of-N accuracy of 81.8% with 8 rounds of candidate gener- ation, rivaling the 82.79% achieved by the top-ranked published solution, while reducing engineering complexity. These findings position our unified framework as a promising alternative for building natural language interfaces to databases.",
        "arxiv_id": "2506.01273",
        "ARXIVID": "2506.01273",
        "COMMENT": "Does not match any specific criteria but is related to natural language interfaces and reasoning, which is tangentially relevant to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.01921": {
        "authors": [
            "Minghao Liu",
            "Zhitao He",
            "Zhiyuan Fan",
            "Qingyun Wang",
            "Yi R. Fung"
        ],
        "title": "MedEBench: Revisiting Text-instructed Image Editing",
        "abstract": "arXiv:2506.01921v1 Announce Type: new  Abstract: Text-guided image editing has seen rapid progress in natural image domains, but its adaptation to medical imaging remains limited and lacks standardized evaluation. Clinically, such editing holds promise for simulating surgical outcomes, creating personalized teaching materials, and enhancing patient communication. To bridge this gap, we introduce \\textbf{MedEBench}, a comprehensive benchmark for evaluating text-guided medical image editing. It consists of 1,182 clinically sourced image-prompt triplets spanning 70 tasks across 13 anatomical regions. MedEBench offers three key contributions: (1) a clinically relevant evaluation framework covering Editing Accuracy, Contextual Preservation, and Visual Quality, supported by detailed descriptions of expected change and ROI (Region of Interest) masks; (2) a systematic comparison of seven state-of-the-art models, revealing common failure patterns; and (3) a failure analysis protocol based on attention grounding, using IoU between attention maps and ROIs to identify mislocalization. MedEBench provides a solid foundation for developing and evaluating reliable, clinically meaningful medical image editing systems.",
        "arxiv_id": "2506.01921",
        "ARXIVID": "2506.01921",
        "COMMENT": "Does not match any specific criteria. Focuses on text-instructed image editing in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.01935": {
        "authors": [
            "Sai Tanmay Reddy Chakkera",
            "Aggelina Chatziagapi",
            "Md Moniruzzaman",
            "Chen-Ping Yu",
            "Yi-Hsuan Tsai",
            "Dimitris Samaras"
        ],
        "title": "Low-Rank Head Avatar Personalization with Registers",
        "abstract": "arXiv:2506.01935v1 Announce Type: new  Abstract: We introduce a novel method for low-rank personalization of a generic model for head avatar generation. Prior work proposes generic models that achieve high-quality face animation by leveraging large-scale datasets of multiple identities. However, such generic models usually fail to synthesize unique identity-specific details, since they learn a general domain prior. To adapt to specific subjects, we find that it is still challenging to capture high-frequency facial details via popular solutions like low-rank adaptation (LoRA). This motivates us to propose a specific architecture, a Register Module, that enhances the performance of LoRA, while requiring only a small number of parameters to adapt to an unseen identity. Our module is applied to intermediate features of a pre-trained model, storing and re-purposing information in a learnable 3D feature space. To demonstrate the efficacy of our personalization method, we collect a dataset of talking videos of individuals with distinctive facial details, such as wrinkles and tattoos. Our approach faithfully captures unseen faces, outperforming existing methods quantitatively and qualitatively. We will release the code, models, and dataset to the public.",
        "arxiv_id": "2506.01935",
        "ARXIVID": "2506.01935",
        "COMMENT": "Does not match any specific criteria. Focuses on low-rank personalization for head avatar generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.00708": {
        "authors": [
            "Yongkang Xiao",
            "Sinian Zhang",
            "Yi Dai",
            "Huixue Zhou",
            "Jue Hou",
            "Jie Ding",
            "Rui Zhang"
        ],
        "title": "DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains",
        "abstract": "arXiv:2506.00708v1 Announce Type: new  Abstract: Knowledge graph completion (KGC) aims to predict missing triples in knowledge graphs (KGs) by leveraging existing triples and textual information. Recently, generative large language models (LLMs) have been increasingly employed for graph tasks. However, current approaches typically encode graph context in textual form, which fails to fully exploit the potential of LLMs for perceiving and reasoning about graph structures. To address this limitation, we propose DrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion). DrKGC employs a flexible lightweight model training strategy to learn structural embeddings and logical rules within the KG. It then leverages a novel bottom-up graph retrieval method to extract a subgraph for each query guided by the learned rules. Finally, a graph convolutional network (GCN) adapter uses the retrieved subgraph to enhance the structural embeddings, which are then integrated into the prompt for effective LLM fine-tuning. Experimental results on two general domain benchmark datasets and two biomedical datasets demonstrate the superior performance of DrKGC. Furthermore, a realistic case study in the biomedical domain highlights its interpretability and practical utility.",
        "arxiv_id": "2506.00708",
        "ARXIVID": "2506.00708",
        "COMMENT": "Does not match any specific criteria. Focuses on knowledge graph completion using LLMs and graph retrieval.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.01048": {
        "authors": [
            "Wei Song",
            "Zhenya Huang",
            "Cheng Cheng",
            "Weibo Gao",
            "Bihan Xu",
            "GuanHao Zhao",
            "Fei Wang",
            "Runze Wu"
        ],
        "title": "IRT-Router: Effective and Interpretable Multi-LLM Routing via Item Response Theory",
        "abstract": "arXiv:2506.01048v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated exceptional performance across a wide range of natural language tasks. However, selecting the optimal LLM to respond to a user query often necessitates a delicate balance between performance and cost. While powerful models deliver better results, they come at a high cost, whereas smaller models are more cost-effective but less capable. To address this trade-off, we propose IRT-Router, a multi-LLM routing framework that efficiently routes user queries to the most suitable LLM. Inspired by Item Response Theory (IRT), a psychological measurement methodology, IRT-Router explicitly models the relationship between LLM capabilities and user query attributes. This not only enables accurate prediction of response performance but also provides interpretable insights, such as LLM abilities and query difficulty. Additionally, we design an online query warm-up technique based on semantic similarity, further enhancing the online generalization capability of IRT-Router. Extensive experiments on 20 LLMs and 12 datasets demonstrate that IRT-Router outperforms most baseline methods in terms of effectiveness and interpretability. Its superior performance in cold-start scenarios further confirms the reliability and practicality of IRT-Router in real-world applications. Code is available at https://github.com/Mercidaiha/IRT-Router.",
        "arxiv_id": "2506.01048",
        "ARXIVID": "2506.01048",
        "COMMENT": "Does not match any specific criteria. Focuses on multi-LLM routing and interpretability using Item Response Theory.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.00794": {
        "authors": [
            "Jiaxin Wen",
            "Chenglei Si",
            "Yueh-han Chen",
            "He He",
            "Shi Feng"
        ],
        "title": "Predicting Empirical AI Research Outcomes with Language Models",
        "abstract": "arXiv:2506.00794v1 Announce Type: new  Abstract: Many promising-looking ideas in AI research fail to deliver, but their validation takes substantial human labor and compute. Predicting an idea's chance of success is thus crucial for accelerating empirical AI research, a skill that even expert researchers can only acquire through substantial experience. We build the first benchmark for this task and compare LMs with human experts. Concretely, given two research ideas (e.g., two jailbreaking methods), we aim to predict which will perform better on a set of benchmarks. We scrape ideas and experimental results from conference papers, yielding 1,585 human-verified idea pairs published after our base model's cut-off date for testing, and 6,000 pairs for training. We then develop a system that combines a fine-tuned GPT-4.1 with a paper retrieval agent, and we recruit 25 human experts to compare with. In the NLP domain, our system beats human experts by a large margin (64.4% v.s. 48.9%). On the full test set, our system achieves 77% accuracy, while off-the-shelf frontier LMs like o3 perform no better than random guessing, even with the same retrieval augmentation. We verify that our system does not exploit superficial features like idea complexity through extensive human-written and LM-designed robustness tests. Finally, we evaluate our system on unpublished novel ideas, including ideas generated by an AI ideation agent. Our system achieves 63.6% accuracy, demonstrating its potential as a reward model for improving idea generation models. Altogether, our results outline a promising new direction for LMs to accelerate empirical AI research.",
        "arxiv_id": "2506.00794",
        "ARXIVID": "2506.00794",
        "COMMENT": "Does not match any specific criteria but is tangentially related to vision-language models through its focus on predicting research outcomes using language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.00523": {
        "authors": [
            "Xingtong Ge",
            "Xin Zhang",
            "Tongda Xu",
            "Yi Zhang",
            "Xinjie Zhang",
            "Yan Wang",
            "Jun Zhang"
        ],
        "title": "SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image Distillation",
        "abstract": "arXiv:2506.00523v1 Announce Type: new  Abstract: The Distribution Matching Distillation (DMD) has been successfully applied to text-to-image diffusion models such as Stable Diffusion (SD) 1.5. However, vanilla DMD suffers from convergence difficulties on large-scale flow-based text-to-image models, such as SD 3.5 and FLUX. In this paper, we first analyze the issues when applying vanilla DMD on large-scale models. Then, to overcome the scalability challenge, we propose implicit distribution alignment (IDA) to regularize the distance between the generator and fake distribution. Furthermore, we propose intra-segment guidance (ISG) to relocate the timestep importance distribution from the teacher model. With IDA alone, DMD converges for SD 3.5; employing both IDA and ISG, DMD converges for SD 3.5 and FLUX.1 dev. Along with other improvements such as scaled up discriminator models, our final model, dubbed \\textbf{SenseFlow}, achieves superior performance in distillation for both diffusion based text-to-image models such as SDXL, and flow-matching models such as SD 3.5 Large and FLUX. The source code will be avaliable at https://github.com/XingtongGe/SenseFlow.",
        "arxiv_id": "2506.00523",
        "ARXIVID": "2506.00523",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling in multi-modal learning through its focus on text-to-image distillation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.01096": {
        "authors": [
            "Yihao Liu",
            "Shuocheng Li",
            "Lang Cao",
            "Yuhang Xie",
            "Mengyu Zhou",
            "Haoyu Dong",
            "Xiaojun Ma",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "title": "SuperRL: Reinforcement Learning with Supervision to Boost Language Model Reasoning",
        "abstract": "arXiv:2506.01096v1 Announce Type: new  Abstract: Large language models are increasingly used for complex reasoning tasks where high-quality offline data such as expert-annotated solutions and distilled reasoning traces are often available. However, in environments with sparse rewards, reinforcement learning struggles to sample successful trajectories, leading to inefficient learning. At the same time, these offline trajectories that represent correct reasoning paths are not utilized by standard on-policy reinforcement learning methods. To address this limitation, we propose SuperRL, a unified training framework that adaptively incorporates offline supervision into reinforcement learning. SuperRL introduces an Adaptive Switch to detect sparse reward conditions and activates a Hybrid Actor when necessary. The Hybrid Actor integrates policy gradient and supervised learning objectives at the loss level, enabling the model to benefit from accurate offline reasoning signals while maintaining the exploratory capacity of reinforcement learning. Experiments on a range of reasoning benchmarks show that SuperRL consistently outperforms standard reinforcement learning by improving sample efficiency, generalization, and robustness under sparse rewards.",
        "arxiv_id": "2506.01096",
        "ARXIVID": "2506.01096",
        "COMMENT": "Does not match any specific criteria but proposes a reinforcement learning framework for reasoning tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.01201": {
        "authors": [
            "Tianqin Li",
            "Junru Zhao",
            "Dunhan Jiang",
            "Shenghao Wu",
            "Alan Ramirez",
            "Tai Sing Lee"
        ],
        "title": "Perceptual Inductive Bias Is What You Need Before Contrastive Learning",
        "abstract": "arXiv:2506.01201v1 Announce Type: new  Abstract: David Marr's seminal theory of human perception stipulates that visual processing is a multi-stage process, prioritizing the derivation of boundary and surface properties before forming semantic object representations. In contrast, contrastive representation learning frameworks typically bypass this explicit multi-stage approach, defining their objective as the direct learning of a semantic representation space for objects. While effective in general contexts, this approach sacrifices the inductive biases of vision, leading to slower convergence speed and learning shortcut resulting in texture bias. In this work, we demonstrate that leveraging Marr's multi-stage theory-by first constructing boundary and surface-level representations using perceptual constructs from early visual processing stages and subsequently training for object semantics-leads to 2x faster convergence on ResNet18, improved final representations on semantic segmentation, depth estimation, and object recognition, and enhanced robustness and out-of-distribution capability. Together, we propose a pretraining stage before the general contrastive representation pretraining to further enhance the final representation quality and reduce the overall convergence time via inductive bias from human vision systems.",
        "arxiv_id": "2506.01201",
        "ARXIVID": "2506.01201",
        "COMMENT": "Does not match any specific criteria but explores perceptual inductive bias in contrastive learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.00765": {
        "authors": [
            "Shengkun Wang",
            "Yanshen Sun",
            "Fanglan Chen",
            "Linhan Wang",
            "Naren Ramakrishnan",
            "Chang-Tien Lu",
            "Yinlin Chen"
        ],
        "title": "HouseTS: A Large-Scale, Multimodal Spatiotemporal U.S. Housing Dataset",
        "abstract": "arXiv:2506.00765v1 Announce Type: new  Abstract: Accurate house-price forecasting is essential for investors, planners, and researchers. However, reproducible benchmarks with sufficient spatiotemporal depth and contextual richness for long horizon prediction remain scarce. To address this, we introduce HouseTS a large scale, multimodal dataset covering monthly house prices from March 2012 to December 2023 across 6,000 ZIP codes in 30 major U.S. metropolitan areas. The dataset includes over 890K records, enriched with points of Interest (POI), socioeconomic indicators, and detailed real estate metrics. To establish standardized performance baselines, we evaluate 14 models, spanning classical statistical approaches, deep neural networks (DNNs), and pretrained time-series foundation models. We further demonstrate the value of HouseTS in a multimodal case study, where a vision language model extracts structured textual descriptions of geographic change from time stamped satellite imagery. This enables interpretable, grounded insights into urban evolution. HouseTS is hosted on Kaggle, while all preprocessing pipelines, benchmark code, and documentation are openly maintained on GitHub to ensure full reproducibility and easy adoption.",
        "arxiv_id": "2506.00765",
        "ARXIVID": "2506.00765",
        "COMMENT": "Does not match any specific criteria but introduces a multimodal spatiotemporal dataset.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.01130": {
        "authors": [
            "Yiliang Chen",
            "Zhixi Li",
            "Cheng Xu",
            "Alex Qinyang Liu",
            "Xuemiao Xu",
            "Jeremy Yuen-Chun Teoh",
            "Shengfeng He",
            "Jing Qin"
        ],
        "title": "ProstaTD: A Large-scale Multi-source Dataset for Structured Surgical Triplet Detection",
        "abstract": "arXiv:2506.01130v1 Announce Type: new  Abstract: Surgical triplet detection has emerged as a pivotal task in surgical video analysis, with significant implications for performance assessment and the training of novice surgeons. However, existing datasets such as CholecT50 exhibit critical limitations: they lack precise spatial bounding box annotations, provide inconsistent and clinically ungrounded temporal labels, and rely on a single data source, which limits model generalizability.To address these shortcomings, we introduce ProstaTD, a large-scale, multi-institutional dataset for surgical triplet detection, developed from the technically demanding domain of robot-assisted prostatectomy. ProstaTD offers clinically defined temporal boundaries and high-precision bounding box annotations for each structured triplet action. The dataset comprises 60,529 video frames and 165,567 annotated triplet instances, collected from 21 surgeries performed across multiple institutions, reflecting a broad range of surgical practices and intraoperative conditions. The annotation process was conducted under rigorous medical supervision and involved more than 50 contributors, including practicing surgeons and medically trained annotators, through multiple iterative phases of labeling and verification. ProstaTD is the largest and most diverse surgical triplet dataset to date, providing a robust foundation for fair benchmarking, the development of reliable surgical AI systems, and scalable tools for procedural training.",
        "arxiv_id": "2506.01130",
        "ARXIVID": "2506.01130",
        "COMMENT": "Does not match any specific criteria but introduces a new dataset for surgical video analysis.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.01025": {
        "authors": [
            "Xudong Ma",
            "Nantheera Anantrasirichai",
            "Stefanos Bolomytis",
            "Alin Achim"
        ],
        "title": "Modality Translation and Registration of MR and Ultrasound Images Using Diffusion Models",
        "abstract": "arXiv:2506.01025v1 Announce Type: new  Abstract: Multimodal MR-US registration is critical for prostate cancer diagnosis. However, this task remains challenging due to significant modality discrepancies. Existing methods often fail to align critical boundaries while being overly sensitive to irrelevant details. To address this, we propose an anatomically coherent modality translation (ACMT) network based on a hierarchical feature disentanglement design. We leverage shallow-layer features for texture consistency and deep-layer features for boundary preservation. Unlike conventional modality translation methods that convert one modality into another, our ACMT introduces the customized design of an intermediate pseudo modality. Both MR and US images are translated toward this intermediate domain, effectively addressing the bottlenecks faced by traditional translation methods in the downstream registration task. Experiments demonstrate that our method mitigates modality-specific discrepancies while preserving crucial anatomical boundaries for accurate registration. Quantitative evaluations show superior modality similarity compared to state-of-the-art modality translation methods. Furthermore, downstream registration experiments confirm that our translated images achieve the best alignment performance, highlighting the robustness of our framework for multi-modal prostate image registration.",
        "arxiv_id": "2506.01025",
        "ARXIVID": "2506.01025",
        "COMMENT": "Does not match any specific criterion but is relevant to multi-modal learning applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.01539": {
        "authors": [
            "Tianjiao Zhang",
            "Fei Zhang",
            "Jiangchao Yao",
            "Ya Zhang",
            "Yanfeng Wang"
        ],
        "title": "G4Seg: Generation for Inexact Segmentation Refinement with Diffusion Models",
        "abstract": "arXiv:2506.01539v1 Announce Type: new  Abstract: This paper considers the problem of utilizing a large-scale text-to-image diffusion model to tackle the challenging Inexact Segmentation (IS) task. Unlike traditional approaches that rely heavily on discriminative-model-based paradigms or dense visual representations derived from internal attention mechanisms, our method focuses on the intrinsic generative priors in Stable Diffusion~(SD). Specifically, we exploit the pattern discrepancies between original images and mask-conditional generated images to facilitate a coarse-to-fine segmentation refinement by establishing a semantic correspondence alignment and updating the foreground probability. Comprehensive quantitative and qualitative experiments validate the effectiveness and superiority of our plug-and-play design, underscoring the potential of leveraging generation discrepancies to model dense representations and encouraging further exploration of generative approaches for solving discriminative tasks.",
        "arxiv_id": "2506.01539",
        "ARXIVID": "2506.01539",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.01471": {
        "authors": [
            "Yiping Li",
            "Ronald de Jong",
            "Sahar Nasirihaghighi",
            "Tim Jaspers",
            "Romy van Jaarsveld",
            "Gino Kuiper",
            "Richard van Hillegersberg",
            "Fons van der Sommen",
            "Jelle Ruurda",
            "Marcel Breeuwer",
            "Yasmina Al Khalil"
        ],
        "title": "SemiVT-Surge: Semi-Supervised Video Transformer for Surgical Phase Recognition",
        "abstract": "arXiv:2506.01471v1 Announce Type: new  Abstract: Accurate surgical phase recognition is crucial for computer-assisted interventions and surgical video analysis. Annotating long surgical videos is labor-intensive, driving research toward leveraging unlabeled data for strong performance with minimal annotations. Although self-supervised learning has gained popularity by enabling large-scale pretraining followed by fine-tuning on small labeled subsets, semi-supervised approaches remain largely underexplored in the surgical domain. In this work, we propose a video transformer-based model with a robust pseudo-labeling framework. Our method incorporates temporal consistency regularization for unlabeled data and contrastive learning with class prototypes, which leverages both labeled data and pseudo-labels to refine the feature space. Through extensive experiments on the private RAMIE (Robot-Assisted Minimally Invasive Esophagectomy) dataset and the public Cholec80 dataset, we demonstrate the effectiveness of our approach. By incorporating unlabeled data, we achieve state-of-the-art performance on RAMIE with a 4.9% accuracy increase and obtain comparable results to full supervision while using only 1/4 of the labeled data on Cholec80. Our findings establish a strong benchmark for semi-supervised surgical phase recognition, paving the way for future research in this domain.",
        "arxiv_id": "2506.01471",
        "ARXIVID": "2506.01471",
        "COMMENT": "Does not match any specific criteria but is related to semi-supervised learning in surgical video analysis, which is tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.00406": {
        "authors": [
            "Huahui Yi",
            "Wei Xu",
            "Ziyuan Qin",
            "Xi Chen",
            "Xiaohu Wu",
            "Kang Li",
            "Qicheng Lao"
        ],
        "title": "iDPA: Instance Decoupled Prompt Attention for Incremental Medical Object Detection",
        "abstract": "arXiv:2506.00406v1 Announce Type: new  Abstract: Existing prompt-based approaches have demonstrated impressive performance in continual learning, leveraging pre-trained large-scale models for classification tasks; however, the tight coupling between foreground-background information and the coupled attention between prompts and image-text tokens present significant challenges in incremental medical object detection tasks, due to the conceptual gap between medical and natural domains. To overcome these challenges, we introduce the \\method~framework, which comprises two main components: 1) Instance-level Prompt Generation (\\ipg), which decouples fine-grained instance-level knowledge from images and generates prompts that focus on dense predictions, and 2) Decoupled Prompt Attention (\\dpa), which decouples the original prompt attention, enabling a more direct and efficient transfer of prompt information while reducing memory usage and mitigating catastrophic forgetting. We collect 13 clinical, cross-modal, multi-organ, and multi-category datasets, referred to as \\dataset, and experiments demonstrate that \\method~outperforms existing SOTA methods, with FAP improvements of 5.44\\%, 4.83\\%, 12.88\\%, and 4.59\\% in full data, 1-shot, 10-shot, and 50-shot settings, respectively.",
        "arxiv_id": "2506.00406",
        "ARXIVID": "2506.00406",
        "COMMENT": "Does not match any specific criteria but is related to incremental learning in medical object detection, which is tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.00625": {
        "authors": [
            "Mengke Li",
            "Zhikai Hu",
            "Yang Lu",
            "Weichao Lan",
            "Yiu-ming Cheung",
            "Hui Huang"
        ],
        "title": "Long-Tailed Visual Recognition via Permutation-Invariant Head-to-Tail Feature Fusion",
        "abstract": "arXiv:2506.00625v1 Announce Type: new  Abstract: The imbalanced distribution of long-tailed data presents a significant challenge for deep learning models, causing them to prioritize head classes while neglecting tail classes. Two key factors contributing to low recognition accuracy are the deformed representation space and a biased classifier, stemming from insufficient semantic information in tail classes. To address these issues, we propose permutation-invariant and head-to-tail feature fusion (PI-H2T), a highly adaptable method. PI-H2T enhances the representation space through permutation-invariant representation fusion (PIF), yielding more clustered features and automatic class margins. Additionally, it adjusts the biased classifier by transferring semantic information from head to tail classes via head-to-tail fusion (H2TF), improving tail class diversity. Theoretical analysis and experiments show that PI-H2T optimizes both the representation space and decision boundaries. Its plug-and-play design ensures seamless integration into existing methods, providing a straightforward path to further performance improvements. Extensive experiments on long-tailed benchmarks confirm the effectiveness of PI-H2T.",
        "arxiv_id": "2506.00625",
        "ARXIVID": "2506.00625",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and machine learning, focusing on long-tailed visual recognition.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2506.00496": {
        "authors": [
            "Ashutosh Gupta",
            "Thomas A. Henzinger",
            "Konstantin Kueffner",
            "Kaushik Mallik",
            "David Pape"
        ],
        "title": "Monitoring Robustness and Individual Fairness",
        "abstract": "arXiv:2506.00496v1 Announce Type: new  Abstract: Input-output robustness appears in various different forms in the literature, such as robustness of AI models to adversarial or semantic perturbations and individual fairness of AI models that make decisions about humans.   We propose runtime monitoring of input-output robustness of deployed, black-box AI models, where the goal is to design monitors that would observe one long execution sequence of the model, and would raise an alarm whenever it is detected that two similar inputs from the past led to dissimilar outputs.   This way, monitoring will complement existing offline ``robustification'' approaches to increase the trustworthiness of AI decision-makers.   We show that the monitoring problem can be cast as the fixed-radius nearest neighbor (FRNN) search problem, which, despite being well-studied, lacks suitable online solutions.   We present our tool Clemont, which offers a number of lightweight monitors, some of which use upgraded online variants of existing FRNN algorithms, and one uses a novel algorithm based on binary decision diagrams -- a data-structure commonly used in software and hardware verification.   We have also developed an efficient parallelization technique that can substantially cut down the computation time of monitors for which the distance between input-output pairs is measured using the $L_\\infty$ norm.   Using standard benchmarks from the literature of adversarial and semantic robustness and individual fairness, we perform a comparative study of different monitors in \\tool, and demonstrate their effectiveness in correctly detecting robustness violations at runtime.",
        "arxiv_id": "2506.00496",
        "ARXIVID": "2506.00496",
        "COMMENT": "Does not match any specific criterion but introduces a tool for monitoring robustness and fairness in AI models, which may be of tangential interest.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.01388": {
        "authors": [
            "Yihao Ding",
            "Soyeon Caren Han",
            "Yan Li",
            "Josiah Poon"
        ],
        "title": "VRD-IU: Lessons from Visually Rich Document Intelligence and Understanding",
        "abstract": "arXiv:2506.01388v1 Announce Type: new  Abstract: Visually Rich Document Understanding (VRDU) has emerged as a critical field in document intelligence, enabling automated extraction of key information from complex documents across domains such as medical, financial, and educational applications. However, form-like documents pose unique challenges due to their complex layouts, multi-stakeholder involvement, and high structural variability. Addressing these issues, the VRD-IU Competition was introduced, focusing on extracting and localizing key information from multi-format forms within the Form-NLU dataset, which includes digital, printed, and handwritten documents. This paper presents insights from the competition, which featured two tracks: Track A, emphasizing entity-based key information retrieval, and Track B, targeting end-to-end key information localization from raw document images. With over 20 participating teams, the competition showcased various state-of-the-art methodologies, including hierarchical decomposition, transformer-based retrieval, multimodal feature fusion, and advanced object detection techniques. The top-performing models set new benchmarks in VRDU, providing valuable insights into document intelligence.",
        "arxiv_id": "2506.01388",
        "ARXIVID": "2506.01388",
        "COMMENT": "Does not match any specific criterion but is tangentially related to document intelligence, which may align with general interest in multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.01214": {
        "authors": [
            "Ali Zia",
            "Renuka Sharma",
            "Abdelwahed Khamis",
            "Xuesong Li",
            "Muhammad Husnain",
            "Numan Shafi",
            "Saeed Anwar",
            "Sabine Schmoelzl",
            "Eric Stone",
            "Lars Petersson",
            "Vivien Rolland"
        ],
        "title": "A Review on Coarse to Fine-Grained Animal Action Recognition",
        "abstract": "arXiv:2506.01214v1 Announce Type: new  Abstract: This review provides an in-depth exploration of the field of animal action recognition, focusing on coarse-grained (CG) and fine-grained (FG) techniques. The primary aim is to examine the current state of research in animal behaviour recognition and to elucidate the unique challenges associated with recognising subtle animal actions in outdoor environments. These challenges differ significantly from those encountered in human action recognition due to factors such as non-rigid body structures, frequent occlusions, and the lack of large-scale, annotated datasets. The review begins by discussing the evolution of human action recognition, a more established field, highlighting how it progressed from broad, coarse actions in controlled settings to the demand for fine-grained recognition in dynamic environments. This shift is particularly relevant for animal action recognition, where behavioural variability and environmental complexity present unique challenges that human-centric models cannot fully address. The review then underscores the critical differences between human and animal action recognition, with an emphasis on high intra-species variability, unstructured datasets, and the natural complexity of animal habitats. Techniques like spatio-temporal deep learning frameworks (e.g., SlowFast) are evaluated for their effectiveness in animal behaviour analysis, along with the limitations of existing datasets. By assessing the strengths and weaknesses of current methodologies and introducing a recently-published dataset, the review outlines future directions for advancing fine-grained action recognition, aiming to improve accuracy and generalisability in behaviour analysis across species.",
        "arxiv_id": "2506.01214",
        "ARXIVID": "2506.01214",
        "COMMENT": "Does not match any specific criteria but is related to action recognition in computer vision, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.01947": {
        "authors": [
            "Marcos V. Conde",
            "Radu Timofte",
            "Radu Berdan",
            "Beril Besbinar",
            "Daisuke Iso",
            "Pengzhou Ji",
            "Xiong Dun",
            "Zeying Fan",
            "Chen Wu",
            "Zhansheng Wang",
            "Pengbo Zhang",
            "Jiazi Huang",
            "Qinglin Liu",
            "Wei Yu",
            "Shengping Zhang",
            "Xiangyang Ji",
            "Kyungsik Kim",
            "Minkyung Kim",
            "Hwalmin Lee",
            "Hekun Ma",
            "Huan Zheng",
            "Yanyan Wei",
            "Zhao Zhang",
            "Jing Fang",
            "Meilin Gao",
            "Xiang Yu",
            "Shangbin Xie",
            "Mengyuan Sun",
            "Huanjing Yue",
            "Jingyu Yang Huize Cheng",
            "Shaomeng Zhang",
            "Zhaoyang Zhang",
            "Haoxiang Liang"
        ],
        "title": "RAW Image Reconstruction from RGB on Smartphones. NTIRE 2025 Challenge Report",
        "abstract": "arXiv:2506.01947v1 Announce Type: new  Abstract: Numerous low-level vision tasks operate in the RAW domain due to its linear properties, bit depth, and sensor designs. Despite this, RAW image datasets are scarce and more expensive to collect than the already large and public sRGB datasets. For this reason, many approaches try to generate realistic RAW images using sensor information and sRGB images. This paper covers the second challenge on RAW Reconstruction from sRGB (Reverse ISP). We aim to recover RAW sensor images from smartphones given the corresponding sRGB images without metadata and, by doing this, ``reverse\" the ISP transformation. Over 150 participants joined this NTIRE 2025 challenge and submitted efficient models. The proposed methods and benchmark establish the state-of-the-art for generating realistic RAW data.",
        "arxiv_id": "2506.01947",
        "ARXIVID": "2506.01947",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.00820": {
        "authors": [
            "Jiatong Li",
            "Libo Zhu",
            "Haotong Qin",
            "Jingkai Wang",
            "Linghe Kong",
            "Guihai Chen",
            "Yulun Zhang",
            "Xiaokang Yang"
        ],
        "title": "QuantFace: Low-Bit Post-Training Quantization for One-Step Diffusion Face Restoration",
        "abstract": "arXiv:2506.00820v1 Announce Type: new  Abstract: Diffusion models have been achieving remarkable performance in face restoration. However, the heavy computations of diffusion models make it difficult to deploy them on devices like smartphones. In this work, we propose QuantFace, a novel low-bit quantization for one-step diffusion face restoration models, where the full-precision (\\ie, 32-bit) weights and activations are quantized to 4$\\sim$6-bit. We first analyze the data distribution within activations and find that they are highly variant. To preserve the original data information, we employ rotation-scaling channel balancing. Furthermore, we propose Quantization-Distillation Low-Rank Adaptation (QD-LoRA) that jointly optimizes for quantization and distillation performance. Finally, we propose an adaptive bit-width allocation strategy. We formulate such a strategy as an integer programming problem, which combines quantization error and perceptual metrics to find a satisfactory resource allocation. Extensive experiments on the synthetic and real-world datasets demonstrate the effectiveness of QuantFace under 6-bit and 4-bit. QuantFace achieves significant advantages over recent leading low-bit quantization methods for face restoration. The code is available at https://github.com/jiatongli2024/QuantFace.",
        "arxiv_id": "2506.00820",
        "ARXIVID": "2506.00820",
        "COMMENT": "Does not match any specific criteria. Focuses on low-bit quantization for diffusion models in face restoration.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.00754": {
        "authors": [
            "Benjamin Civjan",
            "Bo Chen",
            "Ruixiao Zhang",
            "Klara Nahrstedt"
        ],
        "title": "EcoLens: Leveraging Multi-Objective Bayesian Optimization for Energy-Efficient Video Processing on Edge Devices",
        "abstract": "arXiv:2506.00754v1 Announce Type: new  Abstract: Video processing for real-time analytics in resource-constrained environments presents a significant challenge in balancing energy consumption and video semantics. This paper addresses the problem of energy-efficient video processing by proposing a system that dynamically optimizes processing configurations to minimize energy usage on the edge, while preserving essential video features for deep learning inference. We first gather an extensive offline profile of various configurations consisting of device CPU frequencies, frame filtering features, difference thresholds, and video bitrates, to establish apriori knowledge of their impact on energy consumption and inference accuracy. Leveraging this insight, we introduce an online system that employs multi-objective Bayesian optimization to intelligently explore and adapt configurations in real time. Our approach continuously refines processing settings to meet a target inference accuracy with minimal edge device energy expenditure. Experimental results demonstrate the system's effectiveness in reducing video processing energy use while maintaining high analytical performance, offering a practical solution for smart devices and edge computing applications.",
        "arxiv_id": "2506.00754",
        "ARXIVID": "2506.00754",
        "COMMENT": "Does not match any specific criteria but is tangentially related to embodied AI through its focus on energy-efficient video processing on edge devices.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.00974": {
        "authors": [
            "Zahra Dehghanian",
            "Pouya Ardekhani",
            "Amir Vahedi",
            "Hamid Beigy",
            "Hamid R. Rabiee"
        ],
        "title": "Camera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions",
        "abstract": "arXiv:2506.00974v1 Announce Type: new  Abstract: Camera trajectory generation is a cornerstone in computer graphics, robotics, virtual reality, and cinematography, enabling seamless and adaptive camera movements that enhance visual storytelling and immersive experiences. Despite its growing prominence, the field lacks a systematic and unified survey that consolidates essential knowledge and advancements in this domain. This paper addresses this gap by providing the first comprehensive review of the field, covering from foundational definitions to advanced methodologies. We introduce the different approaches to camera representation and present an in-depth review of available camera trajectory generation models, starting with rule-based approaches and progressing through optimization-based techniques, machine learning advancements, and hybrid methods that integrate multiple strategies. Additionally, we gather and analyze the metrics and datasets commonly used for evaluating camera trajectory systems, offering insights into how these tools measure performance, aesthetic quality, and practical applicability. Finally, we highlight existing limitations, critical gaps in current research, and promising opportunities for investment and innovation in the field. This paper not only serves as a foundational resource for researchers entering the field but also paves the way for advancing adaptive, efficient, and creative camera trajectory systems across diverse applications.",
        "arxiv_id": "2506.00974",
        "ARXIVID": "2506.00974",
        "COMMENT": "Does not match any specific criteria but is tangentially related to spatial intelligence and embodied AI through its focus on camera trajectory generation.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.00324": {
        "authors": [
            "Jisoo Jeong",
            "Hong Cai",
            "Jamie Menjay Lin",
            "Fatih Porikli"
        ],
        "title": "Improving Optical Flow and Stereo Depth Estimation by Leveraging Uncertainty-Based Learning Difficulties",
        "abstract": "arXiv:2506.00324v1 Announce Type: new  Abstract: Conventional training for optical flow and stereo depth models typically employs a uniform loss function across all pixels. However, this one-size-fits-all approach often overlooks the significant variations in learning difficulty among individual pixels and contextual regions. This paper investigates the uncertainty-based confidence maps which capture these spatially varying learning difficulties and introduces tailored solutions to address them. We first present the Difficulty Balancing (DB) loss, which utilizes an error-based confidence measure to encourage the network to focus more on challenging pixels and regions. Moreover, we identify that some difficult pixels and regions are affected by occlusions, resulting from the inherently ill-posed matching problem in the absence of real correspondences. To address this, we propose the Occlusion Avoiding (OA) loss, designed to guide the network into cycle consistency-based confident regions, where feature matching is more reliable. By combining the DB and OA losses, we effectively manage various types of challenging pixels and regions during training. Experiments on both optical flow and stereo depth tasks consistently demonstrate significant performance improvements when applying our proposed combination of the DB and OA losses.",
        "arxiv_id": "2506.00324",
        "ARXIVID": "2506.00324",
        "COMMENT": "Does not match any specific criteria but focuses on improving optical flow and stereo depth estimation.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.00836": {
        "authors": [
            "Baolu Li",
            "Hongkai Yu",
            "Huiming Sun",
            "Jin Ma",
            "Yuewei Lin",
            "Lu Ma",
            "Yonghua Du"
        ],
        "title": "Advancing from Automated to Autonomous Beamline by Leveraging Computer Vision",
        "abstract": "arXiv:2506.00836v1 Announce Type: new  Abstract: The synchrotron light source, a cutting-edge large-scale user facility, requires autonomous synchrotron beamline operations, a crucial technique that should enable experiments to be conducted automatically, reliably, and safely with minimum human intervention. However, current state-of-the-art synchrotron beamlines still heavily rely on human safety oversight. To bridge the gap between automated and autonomous operation, a computer vision-based system is proposed, integrating deep learning and multiview cameras for real-time collision detection. The system utilizes equipment segmentation, tracking, and geometric analysis to assess potential collisions with transfer learning that enhances robustness. In addition, an interactive annotation module has been developed to improve the adaptability to new object classes. Experiments on a real beamline dataset demonstrate high accuracy, real-time performance, and strong potential for autonomous synchrotron beamline operations.",
        "arxiv_id": "2506.00836",
        "ARXIVID": "2506.00836",
        "COMMENT": "Does not match any specific criteria but focuses on computer vision for autonomous systems.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.01636": {
        "authors": [
            "Yi Liao",
            "Ugochukwu Ejike Akpudo",
            "Jue Zhang",
            "Yongsheng Gao",
            "Jun Zhou",
            "Wenyi Zeng",
            "Weichuan Zhang"
        ],
        "title": "Visual Explanation via Similar Feature Activation for Metric Learning",
        "abstract": "arXiv:2506.01636v1 Announce Type: new  Abstract: Visual explanation maps enhance the trustworthiness of decisions made by deep learning models and offer valuable guidance for developing new algorithms in image recognition tasks. Class activation maps (CAM) and their variants (e.g., Grad-CAM and Relevance-CAM) have been extensively employed to explore the interpretability of softmax-based convolutional neural networks, which require a fully connected layer as the classifier for decision-making. However, these methods cannot be directly applied to metric learning models, as such models lack a fully connected layer functioning as a classifier. To address this limitation, we propose a novel visual explanation method termed Similar Feature Activation Map (SFAM). This method introduces the channel-wise contribution importance score (CIS) to measure feature importance, derived from the similarity measurement between two image embeddings. The explanation map is constructed by linearly combining the proposed importance weights with the feature map from a CNN model. Quantitative and qualitative experiments show that SFAM provides highly promising interpretable visual explanations for CNN models using Euclidean distance or cosine similarity as the similarity metric.",
        "arxiv_id": "2506.01636",
        "ARXIVID": "2506.01636",
        "COMMENT": "Does not match any specific criteria but is related to interpretability in vision models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.01372": {
        "authors": [
            "Minjun Zhu",
            "Qiujie Xie",
            "Yixuan Weng",
            "Jian Wu",
            "Zhen Lin",
            "Linyi Yang",
            "Yue Zhang"
        ],
        "title": "AI Scientists Fail Without Strong Implementation Capability",
        "abstract": "arXiv:2506.01372v1 Announce Type: new  Abstract: The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the entire scientific workflow from idea generation to experiment implementation. Recent AI Scientist studies demonstrate sufficient capabilities for independent scientific discovery, with the generated research reports gaining acceptance at the ICLR 2025 workshop and ACL 2025, arguing that a human-level AI Scientist, capable of uncovering phenomena previously unknown to humans, may be imminent. Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools. Based on extensive quantitative evidence from existing benchmarks in complex engineering tasks and a systematic evaluation assess 28 research papers generated by five advanced AI Scientist systems, we argue that \\textbf{the fundamental bottleneck for AI Scientists lies in their capability to execute the requisite verification procedures.} Current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers. To better illustrate the root cause of this \\textbf{implementation gap}, we provide an in-depth discussion on the fundamental limitations of AI Scientist. This position paper aims to call for the participants in the community to bridge the implementation gap.",
        "arxiv_id": "2506.01372",
        "ARXIVID": "2506.01372",
        "COMMENT": "Does not match any specific criterion but discusses limitations of AI scientists, which is tangentially related to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.01102": {
        "authors": [
            "Julia Lee Romero",
            "Kyle Min",
            "Subarna Tripathi",
            "Morteza Karimzadeh"
        ],
        "title": "Keystep Recognition using Graph Neural Networks",
        "abstract": "arXiv:2506.01102v1 Announce Type: new  Abstract: We pose keystep recognition as a node classification task, and propose a flexible graph-learning framework for fine-grained keystep recognition that is able to effectively leverage long-term dependencies in egocentric videos. Our approach, termed GLEVR, consists of constructing a graph where each video clip of the egocentric video corresponds to a node. The constructed graphs are sparse and computationally efficient, outperforming existing larger models substantially. We further leverage alignment between egocentric and exocentric videos during training for improved inference on egocentric videos, as well as adding automatic captioning as an additional modality. We consider each clip of each exocentric video (if available) or video captions as additional nodes during training. We examine several strategies to define connections across these nodes. We perform extensive experiments on the Ego-Exo4D dataset and show that our proposed flexible graph-based framework notably outperforms existing methods.",
        "arxiv_id": "2506.01102",
        "ARXIVID": "2506.01102",
        "COMMENT": "Does not match any specific criterion but discusses graph neural networks for keystep recognition, which is tangentially related to spatial understanding.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.00605": {
        "authors": [
            "Ruiming Min",
            "Minghao Liu"
        ],
        "title": "ABCDEFGH: An Adaptation-Based Convolutional Neural Network-CycleGAN Disease-Courses Evolution Framework Using Generative Models in Health Education",
        "abstract": "arXiv:2506.00605v1 Announce Type: new  Abstract: With the advancement of modern medicine and the development of technologies such as MRI, CT, and cellular analysis, it has become increasingly critical for clinicians to accurately interpret various diagnostic images. However, modern medical education often faces challenges due to limited access to high-quality teaching materials, stemming from privacy concerns and a shortage of educational resources (Balogh et al., 2015). In this context, image data generated by machine learning models, particularly generative models, presents a promising solution. These models can create diverse and comparable imaging datasets without compromising patient privacy, thereby supporting modern medical education. In this study, we explore the use of convolutional neural networks (CNNs) and CycleGAN (Zhu et al., 2017) for generating synthetic medical images. The source code is available at https://github.com/mliuby/COMP4211-Project.",
        "arxiv_id": "2506.00605",
        "ARXIVID": "2506.00605",
        "COMMENT": "Does not match any specific criterion but discusses generative models in medical imaging, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.00652": {
        "authors": [
            "Yu Huang",
            "Junhao Chen",
            "Qi Zheng",
            "Hanqian Li",
            "Shuliang Liu",
            "Xuming Hu"
        ],
        "title": "Video Signature: In-generation Watermarking for Latent Video Diffusion Models",
        "abstract": "arXiv:2506.00652v1 Announce Type: new  Abstract: The rapid development of Artificial Intelligence Generated Content (AIGC) has led to significant progress in video generation but also raises serious concerns about intellectual property protection and reliable content tracing. Watermarking is a widely adopted solution to this issue, but existing methods for video generation mainly follow a post-generation paradigm, which introduces additional computational overhead and often fails to effectively balance the trade-off between video quality and watermark extraction. To address these issues, we propose Video Signature (VIDSIG), an in-generation watermarking method for latent video diffusion models, which enables implicit and adaptive watermark integration during generation. Specifically, we achieve this by partially fine-tuning the latent decoder, where Perturbation-Aware Suppression (PAS) pre-identifies and freezes perceptually sensitive layers to preserve visual quality. Beyond spatial fidelity, we further enhance temporal consistency by introducing a lightweight Temporal Alignment module that guides the decoder to generate coherent frame sequences during fine-tuning. Experimental results show that VIDSIG achieves the best overall performance in watermark extraction, visual quality, and generation efficiency. It also demonstrates strong robustness against both spatial and temporal tampering, highlighting its practicality in real-world scenarios.",
        "arxiv_id": "2506.00652",
        "ARXIVID": "2506.00652",
        "COMMENT": "Does not match any specific criterion but discusses watermarking in video diffusion models, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.01394": {
        "authors": [
            "Jie Liang",
            "Radu Timofte",
            "Qiaosi Yi",
            "Zhengqiang Zhang",
            "Shuaizheng Liu",
            "Lingchen Sun",
            "Rongyuan Wu",
            "Xindong Zhang",
            "Hui Zeng",
            "Lei Zhang"
        ],
        "title": "NTIRE 2025 the 2nd Restore Any Image Model (RAIM) in the Wild Challenge",
        "abstract": "arXiv:2506.01394v1 Announce Type: new  Abstract: In this paper, we present a comprehensive overview of the NTIRE 2025 challenge on the 2nd Restore Any Image Model (RAIM) in the Wild. This challenge established a new benchmark for real-world image restoration, featuring diverse scenarios with and without reference ground truth. Participants were tasked with restoring real-captured images suffering from complex and unknown degradations, where both perceptual quality and fidelity were critically evaluated. The challenge comprised two tracks: (1) the low-light joint denoising and demosaicing (JDD) task, and (2) the image detail enhancement/generation task. Each track included two sub-tasks. The first sub-task involved paired data with available ground truth, enabling quantitative evaluation. The second sub-task dealt with real-world yet unpaired images, emphasizing restoration efficiency and subjective quality assessed through a comprehensive user study. In total, the challenge attracted nearly 300 registrations, with 51 teams submitting more than 600 results. The top-performing methods advanced the state of the art in image restoration and received unanimous recognition from all 20+ expert judges. The datasets used in Track 1 and Track 2 are available at https://drive.google.com/drive/folders/1Mgqve-yNcE26IIieI8lMIf-25VvZRs_J and https://drive.google.com/drive/folders/1UB7nnzLwqDZOwDmD9aT8J0KVg2ag4Qae, respectively. The official challenge pages for Track 1 and Track 2 can be found at https://codalab.lisn.upsaclay.fr/competitions/21334#learn_the_details and https://codalab.lisn.upsaclay.fr/competitions/21623#learn_the_details.",
        "arxiv_id": "2506.01394",
        "ARXIVID": "2506.01394",
        "COMMENT": "Does not match any specific criteria but discusses a benchmark for image restoration, which is tangentially relevant to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2506.01061": {
        "authors": [
            "Dahyeon Kye",
            "Changhyun Roh",
            "Sukhun Ko",
            "Chanho Eom",
            "Jihyong Oh"
        ],
        "title": "AceVFI: A Comprehensive Survey of Advances in Video Frame Interpolation",
        "abstract": "arXiv:2506.01061v1 Announce Type: new  Abstract: Video Frame Interpolation (VFI) is a fundamental Low-Level Vision (LLV) task that synthesizes intermediate frames between existing ones while maintaining spatial and temporal coherence. VFI techniques have evolved from classical motion compensation-based approach to deep learning-based approach, including kernel-, flow-, hybrid-, phase-, GAN-, Transformer-, Mamba-, and more recently diffusion model-based approach. We introduce AceVFI, the most comprehensive survey on VFI to date, covering over 250+ papers across these approaches. We systematically organize and describe VFI methodologies, detailing the core principles, design assumptions, and technical characteristics of each approach. We categorize the learning paradigm of VFI methods namely, Center-Time Frame Interpolation (CTFI) and Arbitrary-Time Frame Interpolation (ATFI). We analyze key challenges of VFI such as large motion, occlusion, lighting variation, and non-linear motion. In addition, we review standard datasets, loss functions, evaluation metrics. We examine applications of VFI including event-based, cartoon, medical image VFI and joint VFI with other LLV tasks. We conclude by outlining promising future research directions to support continued progress in the field. This survey aims to serve as a unified reference for both newcomers and experts seeking a deep understanding of modern VFI landscapes.",
        "arxiv_id": "2506.01061",
        "ARXIVID": "2506.01061",
        "COMMENT": "Does not match any specific criterion but is a survey paper on video frame interpolation, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 3
    }
}