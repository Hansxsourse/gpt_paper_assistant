{
    "2508.21542": {
        "authors": [
            "Ziwei Liao",
            "Mohamed Sayed",
            "Steven L. Waslander",
            "Sara Vicente",
            "Daniyar Turmukhambetov",
            "Michael Firman"
        ],
        "title": "Complete Gaussian Splats from a Single Image with Denoising Diffusion Models",
        "abstract": "arXiv:2508.21542v1 Announce Type: new  Abstract: Gaussian splatting typically requires dense observations of the scene and can fail to reconstruct occluded and unobserved areas. We propose a latent diffusion model to reconstruct a complete 3D scene with Gaussian splats, including the occluded parts, from only a single image during inference. Completing the unobserved surfaces of a scene is challenging due to the ambiguity of the plausible surfaces. Conventional methods use a regression-based formulation to predict a single \"mode\" for occluded and out-of-frustum surfaces, leading to blurriness, implausibility, and failure to capture multiple possible explanations. Thus, they often address this problem partially, focusing either on objects isolated from the background, reconstructing only visible surfaces, or failing to extrapolate far from the input views. In contrast, we propose a generative formulation to learn a distribution of 3D representations of Gaussian splats conditioned on a single input image. To address the lack of ground-truth training data, we propose a Variational AutoReconstructor to learn a latent space only from 2D images in a self-supervised manner, over which a diffusion model is trained. Our method generates faithful reconstructions and diverse samples with the ability to complete the occluded surfaces for high-quality 360-degree renderings.",
        "arxiv_id": "2508.21542",
        "ARXIVID": "2508.21542",
        "COMMENT": "The paper does not match any specific criteria closely. It proposes a diffusion model for 3D scene reconstruction from a single image, but does not address multi-task vision tasks or unified generation and segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2508.21257": {
        "authors": [
            "Hsuan-I Ho",
            "Chen Guo",
            "Po-Chen Wu",
            "Ivan Shugurov",
            "Chengcheng Tang",
            "Abhay Mittal",
            "Sizhe An",
            "Manuel Kaufmann",
            "Linguang Zhang"
        ],
        "title": "PHD: Personalized 3D Human Body Fitting with Point Diffusion",
        "abstract": "arXiv:2508.21257v1 Announce Type: new  Abstract: We introduce PHD, a novel approach for personalized 3D human mesh recovery (HMR) and body fitting that leverages user-specific shape information to improve pose estimation accuracy from videos. Traditional HMR methods are designed to be user-agnostic and optimized for generalization. While these methods often refine poses using constraints derived from the 2D image to improve alignment, this process compromises 3D accuracy by failing to jointly account for person-specific body shapes and the plausibility of 3D poses. In contrast, our pipeline decouples this process by first calibrating the user's body shape and then employing a personalized pose fitting process conditioned on that shape. To achieve this, we develop a body shape-conditioned 3D pose prior, implemented as a Point Diffusion Transformer, which iteratively guides the pose fitting via a Point Distillation Sampling loss. This learned 3D pose prior effectively mitigates errors arising from an over-reliance on 2D constraints. Consequently, our approach improves not only pelvis-aligned pose accuracy but also absolute pose accuracy -- an important metric often overlooked by prior work. Furthermore, our method is highly data-efficient, requiring only synthetic data for training, and serves as a versatile plug-and-play module that can be seamlessly integrated with existing 3D pose estimators to enhance their performance. Project page: https://phd-pose.github.io/",
        "arxiv_id": "2508.21257",
        "ARXIVID": "2508.21257",
        "COMMENT": "The paper does not match any specific criteria closely. It focuses on personalized 3D human body fitting using a diffusion model, but does not propose a multi-task diffusion model for vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2508.21809": {
        "authors": [
            "Jasper Uijlings",
            "Xingyi Zhou",
            "Xiuye Gu",
            "Arsha Nagrani",
            "Anurag Arnab",
            "Alireza Fathi",
            "David Ross",
            "Cordelia Schmid"
        ],
        "title": "VoCap: Video Object Captioning and Segmentation from Any Prompt",
        "abstract": "arXiv:2508.21809v1 Announce Type: new  Abstract: Understanding objects in videos in terms of fine-grained localization masks and detailed semantic properties is a fundamental task in video understanding. In this paper, we propose VoCap, a flexible video model that consumes a video and a prompt of various modalities (text, box or mask), and produces a spatio-temporal masklet with a corresponding object-centric caption. As such our model addresses simultaneously the tasks of promptable video object segmentation, referring expression segmentation, and object captioning. Since obtaining data for this task is tedious and expensive, we propose to annotate an existing large-scale segmentation dataset (SAV) with pseudo object captions. We do so by preprocessing videos with their ground-truth masks to highlight the object of interest and feed this to a large Vision Language Model (VLM). For an unbiased evaluation, we collect manual annotations on the validation set. We call the resulting dataset SAV-Caption. We train our VoCap model at scale on a SAV-Caption together with a mix of other image and video datasets. Our model yields state-of-the-art results on referring expression video object segmentation, is competitive on semi-supervised video object segmentation, and establishes a benchmark for video object captioning. Our dataset will be made available at https://github.com/google-deepmind/vocap.",
        "arxiv_id": "2508.21809",
        "ARXIVID": "2508.21809",
        "COMMENT": "The paper does not match any specific criteria closely. It focuses on video object captioning and segmentation from prompts, but does not propose a unified model for generation and segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}