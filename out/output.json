{
    "2503.11651": {
        "authors": [
            "Jianyuan Wang",
            "Minghao Chen",
            "Nikita Karaev",
            "Andrea Vedaldi",
            "Christian Rupprecht",
            "David Novotny"
        ],
        "title": "VGGT: Visual Geometry Grounded Transformer",
        "abstract": "arXiv:2503.11651v1 Announce Type: new  Abstract: We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt.",
        "arxiv_id": "2503.11651",
        "ARXIVID": "2503.11651",
        "COMMENT": "Matches criterion 4 as it introduces a vision foundation model (VGGT) with applications in 3D computer vision tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.11218": {
        "authors": [
            "Andong Lu",
            "Mai Wen",
            "Jinhu Wang",
            "Yuanzhi Guo",
            "Chenglong Li",
            "Jin Tang",
            "Bin Luo"
        ],
        "title": "Towards General Multimodal Visual Tracking",
        "abstract": "arXiv:2503.11218v1 Announce Type: new  Abstract: Existing multimodal tracking studies focus on bi-modal scenarios such as RGB-Thermal, RGB-Event, and RGB-Language. Although promising tracking performance is achieved through leveraging complementary cues from different sources, it remains challenging in complex scenes due to the limitations of bi-modal scenarios. In this work, we introduce a general multimodal visual tracking task that fully exploits the advantages of four modalities, including RGB, thermal infrared, event, and language, for robust tracking under challenging conditions. To provide a comprehensive evaluation platform for general multimodal visual tracking, we construct QuadTrack600, a large-scale, high-quality benchmark comprising 600 video sequences (totaling 384.7K high-resolution (640x480) frame groups). In each frame group, all four modalities are spatially aligned and meticulously annotated with bounding boxes, while 21 sequence-level challenge attributes are provided for detailed performance analysis. Despite quad-modal data provides richer information, the differences in information quantity among modalities and the computational burden from four modalities are two challenging issues in fusing four modalities. To handle these issues, we propose a novel approach called QuadFusion, which incorporates an efficient Multiscale Fusion Mamba with four different scanning scales to achieve sufficient interactions of the four modalities while overcoming the exponential computational burden, for general multimodal visual tracking. Extensive experiments on the QuadTrack600 dataset and three bi-modal tracking datasets, including LasHeR, VisEvent, and TNL2K, validate the effectiveness of our QuadFusion.",
        "arxiv_id": "2503.11218",
        "ARXIVID": "2503.11218",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (QuadTrack600) and a novel multimodal fusion method for visual tracking.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2503.11006": {
        "authors": [
            "Yifan Xie",
            "Binkai Ou",
            "Fei Ma",
            "Yaohua Liu"
        ],
        "title": "Observation-Graph Interaction and Key-Detail Guidance for Vision and Language Navigation",
        "abstract": "arXiv:2503.11006v1 Announce Type: new  Abstract: Vision and Language Navigation (VLN) requires an agent to navigate through environments following natural language instructions. However, existing methods often struggle with effectively integrating visual observations and instruction details during navigation, leading to suboptimal path planning and limited success rates. In this paper, we propose OIKG (Observation-graph Interaction and Key-detail Guidance), a novel framework that addresses these limitations through two key components: (1) an observation-graph interaction module that decouples angular and visual information while strengthening edge representations in the navigation space, and (2) a key-detail guidance module that dynamically extracts and utilizes fine-grained location and object information from instructions. By enabling more precise cross-modal alignment and dynamic instruction interpretation, our approach significantly improves the agent's ability to follow complex navigation instructions. Extensive experiments on the R2R and RxR datasets demonstrate that OIKG achieves state-of-the-art performance across multiple evaluation metrics, validating the effectiveness of our method in enhancing navigation precision through better observation-instruction alignment.",
        "arxiv_id": "2503.11006",
        "ARXIVID": "2503.11006",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework for Vision and Language Navigation (VLN) with new methods for observation-instruction alignment, which is a key aspect of embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.11495": {
        "authors": [
            "Zixu Cheng",
            "Jian Hu",
            "Ziquan Liu",
            "Chenyang Si",
            "Wei Li",
            "Shaogang Gong"
        ],
        "title": "V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning",
        "abstract": "arXiv:2503.11495v1 Announce Type: new  Abstract: Human processes video reasoning in a sequential spatio-temporal reasoning logic, we first identify the relevant frames (\"when\") and then analyse the spatial relationships (\"where\") between key objects, and finally leverage these relationships to draw inferences (\"what\"). However, can Video Large Language Models (Video-LLMs) also \"reason through a sequential spatio-temporal logic\" in videos? Existing Video-LLM benchmarks primarily focus on assessing object presence, neglecting relational reasoning. Consequently, it is difficult to measure whether a model truly comprehends object interactions (actions/events) in videos or merely relies on pre-trained \"memory\" of co-occurrences as biases in generating answers. In this work, we introduce a Video Spatio-Temporal Reasoning (V-STaR) benchmark to address these shortcomings. The key idea is to decompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR) task that simultaneously evaluates what objects are present, when events occur, and where they are located while capturing the underlying Chain-of-thought (CoT) logic. To support this evaluation, we construct a dataset to elicit the spatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine CoT questions generated by a semi-automated GPT-4-powered pipeline, embedding explicit reasoning chains to mimic human cognition. Experiments from 14 Video-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and the needs for robust and consistent spatio-temporal reasoning.",
        "arxiv_id": "2503.11495",
        "ARXIVID": "2503.11495",
        "COMMENT": "Matches criterion 2 as it introduces a benchmark for Video-LLMs focusing on spatio-temporal reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.11371": {
        "authors": [
            "Zengyu Wan",
            "Wei Zhai",
            "Yang Cao",
            "Zhengjun Zha"
        ],
        "title": "EMoTive: Event-guided Trajectory Modeling for 3D Motion Estimation",
        "abstract": "arXiv:2503.11371v1 Announce Type: new  Abstract: Visual 3D motion estimation aims to infer the motion of 2D pixels in 3D space based on visual cues. The key challenge arises from depth variation induced spatio-temporal motion inconsistencies, disrupting the assumptions of local spatial or temporal motion smoothness in previous motion estimation frameworks. In contrast, event cameras offer new possibilities for 3D motion estimation through continuous adaptive pixel-level responses to scene changes. This paper presents EMoTive, a novel event-based framework that models spatio-temporal trajectories via event-guided non-uniform parametric curves, effectively characterizing locally heterogeneous spatio-temporal motion. Specifically, we first introduce Event Kymograph - an event projection method that leverages a continuous temporal projection kernel and decouples spatial observations to encode fine-grained temporal evolution explicitly. For motion representation, we introduce a density-aware adaptation mechanism to fuse spatial and temporal features under event guidance, coupled with a non-uniform rational curve parameterization framework to adaptively model heterogeneous trajectories. The final 3D motion estimation is achieved through multi-temporal sampling of parametric trajectories, yielding optical flow and depth motion fields. To facilitate evaluation, we introduce CarlaEvent3D, a multi-dynamic synthetic dataset for comprehensive validation. Extensive experiments on both this dataset and a real-world benchmark demonstrate the effectiveness of the proposed method.",
        "arxiv_id": "2503.11371",
        "ARXIVID": "2503.11371",
        "COMMENT": "Matches criterion 1 as it introduces a novel event-based framework for 3D motion estimation with spatial understanding improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.11073": {
        "authors": [
            "Hongyang Wei",
            "Shuaizheng Liu",
            "Chun Yuan",
            "Lei Zhang"
        ],
        "title": "Perceive, Understand and Restore: Real-World Image Super-Resolution with Autoregressive Multimodal Generative Models",
        "abstract": "arXiv:2503.11073v1 Announce Type: new  Abstract: By leveraging the generative priors from pre-trained text-to-image diffusion models, significant progress has been made in real-world image super-resolution (Real-ISR). However, these methods tend to generate inaccurate and unnatural reconstructions in complex and/or heavily degraded scenes, primarily due to their limited perception and understanding capability of the input low-quality image. To address these limitations, we propose, for the first time to our knowledge, to adapt the pre-trained autoregressive multimodal model such as Lumina-mGPT into a robust Real-ISR model, namely PURE, which Perceives and Understands the input low-quality image, then REstores its high-quality counterpart. Specifically, we implement instruction tuning on Lumina-mGPT to perceive the image degradation level and the relationships between previously generated image tokens and the next token, understand the image content by generating image semantic descriptions, and consequently restore the image by generating high-quality image tokens autoregressively with the collected information. In addition, we reveal that the image token entropy reflects the image structure and present a entropy-based Top-k sampling strategy to optimize the local structure of the image during inference. Experimental results demonstrate that PURE preserves image content while generating realistic details, especially in complex scenes with multiple objects, showcasing the potential of autoregressive multimodal generative models for robust Real-ISR. The model and code will be available at https://github.com/nonwhy/PURE.",
        "arxiv_id": "2503.11073",
        "ARXIVID": "2503.11073",
        "COMMENT": "Matches criterion 2 as it adapts a pre-trained autoregressive multimodal model for real-world image super-resolution.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.11315": {
        "authors": [
            "Jeong Hun Yeo",
            "Hyeongseop Rha",
            "Se Jin Park",
            "Yong Man Ro"
        ],
        "title": "MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens",
        "abstract": "arXiv:2503.11315v1 Announce Type: new  Abstract: Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in noisy environments by combining auditory and visual information. However, recent Large Language Model (LLM) based AVSR systems incur high computational costs due to the high temporal resolution of audio-visual speech processed by LLMs. In this work, we introduce an efficient multimodal speech LLM framework that minimizes token length while preserving essential linguistic content. Our approach employs an early av-fusion module for streamlined feature integration, an audio-visual speech Q-Former that dynamically allocates tokens based on input duration, and a refined query allocation strategy with a speech rate predictor to adjust token allocation according to speaking speed of each audio sample. Extensive experiments on the LRS3 dataset show that our method achieves state-of-the-art performance with a WER of 0.74% while using only 3.5 tokens per second. Moreover, our approach not only reduces token usage by 86% compared to the previous multimodal speech LLM framework, but also improves computational efficiency by reducing FLOPs by 35.7%.",
        "arxiv_id": "2503.11315",
        "ARXIVID": "2503.11315",
        "COMMENT": "Matches criterion 2 as it introduces a new multimodal LLM framework for audio-visual speech recognition.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.11117": {
        "authors": [
            "Kaixuan Jiang",
            "Yang Liu",
            "Weixing Chen",
            "Jingzhou Luo",
            "Ziliang Chen",
            "Ling Pan",
            "Guanbin Li",
            "Liang Lin"
        ],
        "title": "Beyond the Destination: A Novel Benchmark for Exploration-Aware Embodied Question Answering",
        "abstract": "arXiv:2503.11117v1 Announce Type: new  Abstract: Embodied Question Answering (EQA) is a challenging task in embodied intelligence that requires agents to dynamically explore 3D environments, actively gather visual information, and perform multi-step reasoning to answer questions. However, current EQA approaches suffer from critical limitations in exploration efficiency, dataset design, and evaluation metrics. Moreover, existing datasets often introduce biases or prior knowledge, leading to disembodied reasoning, while frontier-based exploration strategies struggle in cluttered environments and fail to ensure fine-grained exploration of task-relevant areas. To address these challenges, we construct the EXPloration-awaRe Embodied queStion anSwering Benchmark (EXPRESS-Bench), the largest dataset designed specifically to evaluate both exploration and reasoning capabilities. EXPRESS-Bench consists of 777 exploration trajectories and 2,044 question-trajectory pairs. To improve exploration efficiency, we propose Fine-EQA, a hybrid exploration model that integrates frontier-based and goal-oriented navigation to guide agents toward task-relevant regions more effectively. Additionally, we introduce a novel evaluation metric, Exploration-Answer Consistency (EAC), which ensures faithful assessment by measuring the alignment between answer grounding and exploration reliability. Extensive experimental comparisons with state-of-the-art EQA models demonstrate the effectiveness of our EXPRESS-Bench in advancing embodied exploration and question reasoning.",
        "arxiv_id": "2503.11117",
        "ARXIVID": "2503.11117",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (EXPRESS-Bench) for embodied question answering with a focus on exploration and reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.10777": {
        "authors": [
            "Zhang Zhang",
            "Chao Sun",
            "Chao Yue",
            "Da Wen",
            "Yujie Chen",
            "Tianze Wang",
            "Jianghao Leng"
        ],
        "title": "HeightFormer: Learning Height Prediction in Voxel Features for Roadside Vision Centric 3D Object Detection via Transformer",
        "abstract": "arXiv:2503.10777v1 Announce Type: new  Abstract: Roadside vision centric 3D object detection has received increasing attention in recent years. It expands the perception range of autonomous vehicles, enhances the road safety. Previous methods focused on predicting per-pixel height rather than depth, making significant gains in roadside visual perception. While it is limited by the perspective property of near-large and far-small on image features, making it difficult for network to understand real dimension of objects in the 3D world. BEV features and voxel features present the real distribution of objects in 3D world compared to the image features. However, BEV features tend to lose details due to the lack of explicit height information, and voxel features are computationally expensive. Inspired by this insight, an efficient framework learning height prediction in voxel features via transformer is proposed, dubbed HeightFormer. It groups the voxel features into local height sequences, and utilize attention mechanism to obtain height distribution prediction. Subsequently, the local height sequences are reassembled to generate accurate 3D features. The proposed method is applied to two large-scale roadside benchmarks, DAIR-V2X-I and Rope3D. Extensive experiments are performed and the HeightFormer outperforms the state-of-the-art methods in roadside vision centric 3D object detection task.",
        "arxiv_id": "2503.10777",
        "ARXIVID": "2503.10777",
        "COMMENT": "Matches criterion 1 as it proposes a novel method for height prediction in voxel features for 3D object detection, enhancing spatial understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.10781": {
        "authors": [
            "Evangelos Kazakos",
            "Cordelia Schmid",
            "Josef Sivic"
        ],
        "title": "Large-scale Pre-training for Grounded Video Caption Generation",
        "abstract": "arXiv:2503.10781v1 Announce Type: new  Abstract: We propose a novel approach for captioning and object grounding in video, where the objects in the caption are grounded in the video via temporally dense bounding boxes. We introduce the following contributions. First, we present a large-scale automatic annotation method that aggregates captions grounded with bounding boxes across individual frames into temporally dense and consistent bounding box annotations. We apply this approach on the HowTo100M dataset to construct a large-scale pre-training dataset, named HowToGround1M. We also introduce a Grounded Video Caption Generation model, dubbed GROVE, and pre-train the model on HowToGround1M. Second, we introduce a new dataset, called iGround, of 3500 videos with manually annotated captions and dense spatio-temporally grounded bounding boxes. This allows us to measure progress on this challenging problem, as well as to fine-tune our model on this small-scale but high-quality data. Third, we demonstrate that our approach achieves state-of-the-art results on the proposed iGround dataset compared to a number of baselines, as well as on the VidSTG and ActivityNet-Entities datasets. We perform extensive ablations that demonstrate the importance of pre-training using our automatically annotated HowToGround1M dataset followed by fine-tuning on the manually annotated iGround dataset and validate the key technical contributions of our model.",
        "arxiv_id": "2503.10781",
        "ARXIVID": "2503.10781",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (iGround) and a novel method (GROVE) for grounded video caption generation.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2503.11496": {
        "authors": [
            "Shaofeng Liang",
            "Runwei Guan",
            "Wangwang Lian",
            "Daizong Liu",
            "Xiaolou Sun",
            "Dongming Wu",
            "Yutao Yue",
            "Weiping Ding",
            "Hui Xiong"
        ],
        "title": "Cognitive Disentanglement for Referring Multi-Object Tracking",
        "abstract": "arXiv:2503.11496v1 Announce Type: new  Abstract: As a significant application of multi-source information fusion in intelligent transportation perception systems, Referring Multi-Object Tracking (RMOT) involves localizing and tracking specific objects in video sequences based on language references. However, existing RMOT approaches often treat language descriptions as holistic embeddings and struggle to effectively integrate the rich semantic information contained in language expressions with visual features. This limitation is especially apparent in complex scenes requiring comprehensive understanding of both static object attributes and spatial motion information. In this paper, we propose a Cognitive Disentanglement for Referring Multi-Object Tracking (CDRMT) framework that addresses these challenges. It adapts the \"what\" and \"where\" pathways from human visual processing system to RMOT tasks. Specifically, our framework comprises three collaborative components: (1)The Bidirectional Interactive Fusion module first establishes cross-modal connections while preserving modality-specific characteristics; (2) Building upon this foundation, the Progressive Semantic-Decoupled Query Learning mechanism hierarchically injects complementary information into object queries, progressively refining object understanding from coarse to fine-grained semantic levels; (3) Finally, the Structural Consensus Constraint enforces bidirectional semantic consistency between visual features and language descriptions, ensuring that tracked objects faithfully reflect the referring expression. Extensive experiments on different benchmark datasets demonstrate that CDRMT achieves substantial improvements over state-of-the-art methods, with average gains of 6.0% in HOTA score on Refer-KITTI and 3.2% on Refer-KITTI-V2. Our approach advances the state-of-the-art in RMOT while simultaneously providing new insights into multi-source information fusion.",
        "arxiv_id": "2503.11496",
        "ARXIVID": "2503.11496",
        "COMMENT": "Matches criterion 1 as it proposes a novel framework for spatial understanding in multi-object tracking using cognitive disentanglement.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.11094": {
        "authors": [
            "Weichen Zhan",
            "Zile Zhou",
            "Zhiheng Zheng",
            "Chen Gao",
            "Jinqiang Cui",
            "Yong Li",
            "Xinlei Chen",
            "Xiao-Ping Zhang"
        ],
        "title": "Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space",
        "abstract": "arXiv:2503.11094v1 Announce Type: new  Abstract: Spatial reasoning is a fundamental capability of embodied agents and has garnered widespread attention in the field of multimodal large language models (MLLMs). In this work, we propose a novel benchmark, Open3DVQA, to comprehensively evaluate the spatial reasoning capacities of current state-of-the-art (SOTA) foundation models in open 3D space. Open3DVQA consists of 9k VQA samples, collected using an efficient semi-automated tool in a high-fidelity urban simulator. We evaluate several SOTA MLLMs across various aspects of spatial reasoning, such as relative and absolute spatial relationships, situational reasoning, and object-centric spatial attributes. Our results reveal that: 1) MLLMs perform better at answering questions regarding relative spatial relationships than absolute spatial relationships, 2) MLLMs demonstrate similar spatial reasoning abilities for both egocentric and allocentric perspectives, and 3) Fine-tuning large models significantly improves their performance across different spatial reasoning tasks. We believe that our open-source data collection tools and in-depth analyses will inspire further research on MLLM spatial reasoning capabilities. The benchmark is available at https://github.com/WeichenZh/Open3DVQA.",
        "arxiv_id": "2503.11094",
        "ARXIVID": "2503.11094",
        "COMMENT": "Matches criteria 1 and 3. Proposes a new benchmark for spatial reasoning in multimodal large language models, which is relevant to embodied AI and spatial intelligence.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.11245": {
        "authors": [
            "Ziwei Shi",
            "Xiaoran Zhang",
            "Yan Xia",
            "Yu Zang",
            "Siqi Shen",
            "Cheng Wang"
        ],
        "title": "L2RSI: Cross-view LiDAR-based Place Recognition for Large-scale Urban Scenes via Remote Sensing Imagery",
        "abstract": "arXiv:2503.11245v1 Announce Type: new  Abstract: We tackle the challenge of LiDAR-based place recognition, which traditionally depends on costly and time-consuming prior 3D maps. To overcome this, we first construct XA-L&RSI dataset, which encompasses approximately $110,000$ remote sensing submaps and $13,000$ LiDAR point cloud submaps captured in urban scenes, and propose a novel method, L2RSI, for cross-view LiDAR place recognition using high-resolution Remote Sensing Imagery. This approach enables large-scale localization capabilities at a reduced cost by leveraging readily available overhead images as map proxies. L2RSI addresses the dual challenges of cross-view and cross-modal place recognition by learning feature alignment between point cloud submaps and remote sensing submaps in the semantic domain. Additionally, we introduce a novel probability propagation method based on a dynamic Gaussian mixture model to refine position predictions, effectively leveraging temporal and spatial information. This approach enables large-scale retrieval and cross-scene generalization without fine-tuning. Extensive experiments on XA-L&RSI demonstrate that, within a $100km^2$ retrieval range, L2RSI accurately localizes $95.08\\%$ of point cloud submaps within a $30m$ radius for top-$1$ retrieved location. We provide a video to more vividly display the place recognition results of L2RSI at https://shizw695.github.io/L2RSI/.",
        "arxiv_id": "2503.11245",
        "ARXIVID": "2503.11245",
        "COMMENT": "Matches criterion 3 as it introduces a novel method and dataset for LiDAR-based place recognition, focusing on cross-view and cross-modal challenges.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.11652": {
        "authors": [
            "Hiroyasu Akada",
            "Jian Wang",
            "Vladislav Golyanik",
            "Christian Theobalt"
        ],
        "title": "Bring Your Rear Cameras for Egocentric 3D Human Pose Estimation",
        "abstract": "arXiv:2503.11652v1 Announce Type: new  Abstract: Egocentric 3D human pose estimation has been actively studied using cameras installed in front of a head-mounted device (HMD). While frontal placement is the optimal and the only option for some tasks, such as hand tracking, it remains unclear if the same holds for full-body tracking due to self-occlusion and limited field-of-view coverage. Notably, even the state-of-the-art methods often fail to estimate accurate 3D poses in many scenarios, such as when HMD users tilt their heads upward (a common motion in human activities). A key limitation of existing HMD designs is their neglect of the back of the body, despite its potential to provide crucial 3D reconstruction cues. Hence, this paper investigates the usefulness of rear cameras in the HMD design for full-body tracking. We also show that simply adding rear views to the frontal inputs is not optimal for existing methods due to their dependence on individual 2D joint detectors without effective multi-view integration. To address this issue, we propose a new transformer-based method that refines 2D joint heatmap estimation with multi-view information and heatmap uncertainty, thereby improving 3D pose tracking. Moreover, we introduce two new large-scale datasets, Ego4View-Syn and Ego4View-RW, for a rear-view evaluation. Our experiments show that the new camera configurations with back views provide superior support for 3D pose tracking compared to only frontal placements. The proposed method achieves significant improvement over the current state of the art (>10% on MPJPE). We will release the source code, trained models, and new datasets on our project page https://4dqv.mpi-inf.mpg.de/EgoRear/.",
        "arxiv_id": "2503.11652",
        "ARXIVID": "2503.11652",
        "COMMENT": "Matches criterion 3 as it introduces a new method and datasets for egocentric 3D human pose estimation, focusing on a novel angle by incorporating rear cameras.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.10905": {
        "authors": [
            "Zhuoyan Xu",
            "Khoi Duc Nguyen",
            "Preeti Mukherjee",
            "Saurabh Bagchi",
            "Somali Chaterji",
            "Yingyu Liang",
            "Yin Li"
        ],
        "title": "Learning to Inference Adaptively for Multimodal Large Language Models",
        "abstract": "arXiv:2503.10905v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have shown impressive capabilities in reasoning, yet come with substantial computational cost, limiting their deployment in resource-constrained settings. Despite recent efforts on improving the efficiency of MLLMs, prior solutions fall short in responding to varying runtime conditions, in particular changing resource availability (e.g., contention due to the execution of other programs on the device). To bridge this gap, we introduce AdaLLaVA, an adaptive inference framework that learns to dynamically reconfigure operations in an MLLM during inference, accounting for the input data and a latency budget. We conduct extensive experiments across benchmarks involving question-answering, reasoning, and hallucination. Our results show that AdaLLaVA effectively adheres to input latency budget, achieving varying accuracy and latency tradeoffs at runtime. Further, we demonstrate that AdaLLaVA adapts to both input latency and content, can be integrated with token selection for enhanced efficiency, and generalizes across MLLMs.Our project webpage with code release is at https://zhuoyan-xu.github.io/ada-llava/.",
        "arxiv_id": "2503.10905",
        "ARXIVID": "2503.10905",
        "COMMENT": "Matches criterion 2 as it proposes an adaptive inference framework for MLLMs, focusing on efficiency and dynamic reconfiguration.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.10886": {
        "authors": [
            "Nathaniel Lesperance",
            "Sujeevan Ratnasingham",
            "Graham W. Taylor"
        ],
        "title": "Taxonomic Reasoning for Rare Arthropods: Combining Dense Image Captioning and RAG for Interpretable Classification",
        "abstract": "arXiv:2503.10886v1 Announce Type: new  Abstract: In the context of pressing climate change challenges and the significant biodiversity loss among arthropods, automated taxonomic classification from organismal images is a subject of intense research. However, traditional AI pipelines based on deep neural visual architectures such as CNNs or ViTs face limitations such as degraded performance on the long-tail of classes and the inability to reason about their predictions. We integrate image captioning and retrieval-augmented generation (RAG) with large language models (LLMs) to enhance biodiversity monitoring, showing particular promise for characterizing rare and unknown arthropod species. While a naive Vision-Language Model (VLM) excels in classifying images of common species, the RAG model enables classification of rarer taxa by matching explicit textual descriptions of taxonomic features to contextual biodiversity text data from external sources. The RAG model shows promise in reducing overconfidence and enhancing accuracy relative to naive LLMs, suggesting its viability in capturing the nuances of taxonomic hierarchy, particularly at the challenging family and genus levels. Our findings highlight the potential for modern vision-language AI pipelines to support biodiversity conservation initiatives, emphasizing the role of comprehensive data curation and collaboration with citizen science platforms to improve species identification, unknown species characterization and ultimately inform conservation strategies.",
        "arxiv_id": "2503.10886",
        "ARXIVID": "2503.10886",
        "COMMENT": "Matches criterion 2 as it integrates vision-language models (VLMs) with retrieval-augmented generation (RAG) for taxonomic reasoning, showcasing a novel application of VLLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.11513": {
        "authors": [
            "Ziqin Zhou",
            "Yifan Yang",
            "Yuqing Yang",
            "Tianyu He",
            "Houwen Peng",
            "Kai Qiu",
            "Qi Dai",
            "Lili Qiu",
            "Chong Luo",
            "Lingqiao Liu"
        ],
        "title": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models",
        "abstract": "arXiv:2503.11513v1 Announce Type: new  Abstract: Text-to-video generation poses significant challenges due to the inherent complexity of video data, which spans both temporal and spatial dimensions. It introduces additional redundancy, abrupt variations, and a domain gap between language and vision tokens while generation. Addressing these challenges requires an effective video tokenizer that can efficiently encode video data while preserving essential semantic and spatiotemporal information, serving as a critical bridge between text and vision. Inspired by the observation in VQ-VAE-2 and workflows of traditional animation, we propose HiTVideo for text-to-video generation with hierarchical tokenizers. It utilizes a 3D causal VAE with a multi-layer discrete token framework, encoding video content into hierarchically structured codebooks. Higher layers capture semantic information with higher compression, while lower layers focus on fine-grained spatiotemporal details, striking a balance between compression efficiency and reconstruction quality. Our approach efficiently encodes longer video sequences (e.g., 8 seconds, 64 frames), reducing bits per pixel (bpp) by approximately 70\\% compared to baseline tokenizers, while maintaining competitive reconstruction quality. We explore the trade-offs between compression and reconstruction, while emphasizing the advantages of high-compressed semantic tokens in text-to-video tasks. HiTVideo aims to address the potential limitations of existing video tokenizers in text-to-video generation tasks, striving for higher compression ratios and simplify LLMs modeling under language guidance, offering a scalable and promising framework for advancing text to video generation. Demo page: https://ziqinzhou66.github.io/project/HiTVideo.",
        "arxiv_id": "2503.11513",
        "ARXIVID": "2503.11513",
        "COMMENT": "Matches criterion 2 as it proposes a hierarchical tokenizer for text-to-video generation, which is relevant to multi-modal large language models.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2503.11093": {
        "authors": [
            "Yuan Liu",
            "Saihui Hou",
            "Saijie Hou",
            "Jiabao Du",
            "Shibei Meng",
            "Yongzhen Huang"
        ],
        "title": "OmniDiff: A Comprehensive Benchmark for Fine-grained Image Difference Captioning",
        "abstract": "arXiv:2503.11093v1 Announce Type: new  Abstract: Image Difference Captioning (IDC) aims to generate natural language descriptions of subtle differences between image pairs, requiring both precise visual change localization and coherent semantic expression. Despite recent advancements, existing datasets often lack breadth and depth, limiting their applicability in complex and dynamic environments: (1) from a breadth perspective, current datasets are constrained to limited variations of objects in specific scenes, and (2) from a depth perspective, prior benchmarks often provide overly simplistic descriptions. To address these challenges, we introduce OmniDiff, a comprehensive dataset comprising 324 diverse scenarios-spanning real-world complex environments and 3D synthetic settings-with fine-grained human annotations averaging 60 words in length and covering 12 distinct change types. Building on this foundation, we propose M$^3$Diff, a MultiModal large language model enhanced by a plug-and-play Multi-scale Differential Perception (MDP) module. This module improves the model's ability to accurately identify and describe inter-image differences while maintaining the foundational model's generalization capabilities. With the addition of the OmniDiff dataset, M$^3$Diff achieves state-of-the-art performance across multiple benchmarks, including Spot-the-Diff, IEdit, CLEVR-Change, CLEVR-DC, and OmniDiff, demonstrating significant improvements in cross-scenario difference recognition accuracy compared to existing methods. The dataset, code, and models will be made publicly available to support further research.",
        "arxiv_id": "2503.11093",
        "ARXIVID": "2503.11093",
        "COMMENT": "Matches criterion 4 as it introduces a new dataset and method for fine-grained image difference captioning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.11122": {
        "authors": [
            "Hongbin Lin",
            "Zilu Guo",
            "Yifan Zhang",
            "Shuaicheng Niu",
            "Yafeng Li",
            "Ruimao Zhang",
            "Shuguang Cui",
            "Zhen Li"
        ],
        "title": "DriveGEN: Generalized and Robust 3D Detection in Driving via Controllable Text-to-Image Diffusion Generation",
        "abstract": "arXiv:2503.11122v1 Announce Type: new  Abstract: In autonomous driving, vision-centric 3D detection aims to identify 3D objects from images. However, high data collection costs and diverse real-world scenarios limit the scale of training data. Once distribution shifts occur between training and test data, existing methods often suffer from performance degradation, known as Out-of-Distribution (OOD) problems. To address this, controllable Text-to-Image (T2I) diffusion offers a potential solution for training data enhancement, which is required to generate diverse OOD scenarios with precise 3D object geometry. Nevertheless, existing controllable T2I approaches are restricted by the limited scale of training data or struggle to preserve all annotated 3D objects. In this paper, we present DriveGEN, a method designed to improve the robustness of 3D detectors in Driving via Training-Free Controllable Text-to-Image Diffusion Generation. Without extra diffusion model training, DriveGEN consistently preserves objects with precise 3D geometry across diverse OOD generations, consisting of 2 stages: 1) Self-Prototype Extraction: We empirically find that self-attention features are semantic-aware but require accurate region selection for 3D objects. Thus, we extract precise object features via layouts to capture 3D object geometry, termed self-prototypes. 2) Prototype-Guided Diffusion: To preserve objects across various OOD scenarios, we perform semantic-aware feature alignment and shallow feature alignment during denoising. Extensive experiments demonstrate the effectiveness of DriveGEN in improving 3D detection. The code is available at https://github.com/Hongbin98/DriveGEN.",
        "arxiv_id": "2503.11122",
        "ARXIVID": "2503.11122",
        "COMMENT": "Matches criterion 4 as it applies text-to-image diffusion models to improve robustness in 3D detection for autonomous driving.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.11609": {
        "authors": [
            "Matteo Farina",
            "Massimiliano Mancini",
            "Giovanni Iacca",
            "Elisa Ricci"
        ],
        "title": "Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages",
        "abstract": "arXiv:2503.11609v1 Announce Type: new  Abstract: An old-school recipe for training a classifier is to (i) learn a good feature extractor and (ii) optimize a linear layer atop. When only a handful of samples are available per category, as in Few-Shot Adaptation (FSA), data are insufficient to fit a large number of parameters, rendering the above impractical. This is especially true with large pre-trained Vision-Language Models (VLMs), which motivated successful research at the intersection of Parameter-Efficient Fine-tuning (PEFT) and FSA. In this work, we start by analyzing the learning dynamics of PEFT techniques when trained on few-shot data from only a subset of categories, referred to as the ``base'' classes. We show that such dynamics naturally splits into two distinct phases: (i) task-level feature extraction and (ii) specialization to the available concepts. To accommodate this dynamic, we then depart from prompt- or adapter-based methods and tackle FSA differently. Specifically, given a fixed computational budget, we split it to (i) learn a task-specific feature extractor via PEFT and (ii) train a linear classifier on top. We call this scheme Two-Stage Few-Shot Adaptation (2SFS). Differently from established methods, our scheme enables a novel form of selective inference at a category level, i.e., at test time, only novel categories are embedded by the adapted text encoder, while embeddings of base categories are available within the classifier. Results with fixed hyperparameters across two settings, three backbones, and eleven datasets, show that 2SFS matches or surpasses the state-of-the-art, while established methods degrade significantly across settings.",
        "arxiv_id": "2503.11609",
        "ARXIVID": "2503.11609",
        "COMMENT": "Matches criterion 2 as it focuses on Few-Shot Adaptation of Vision-Language Models with a novel two-stage approach.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.11647": {
        "authors": [
            "Jianhong Bai",
            "Menghan Xia",
            "Xiao Fu",
            "Xintao Wang",
            "Lianrui Mu",
            "Jinwen Cao",
            "Zuozhu Liu",
            "Haoji Hu",
            "Xiang Bai",
            "Pengfei Wan",
            "Di Zhang"
        ],
        "title": "ReCamMaster: Camera-Controlled Generative Rendering from A Single Video",
        "abstract": "arXiv:2503.11647v1 Announce Type: new  Abstract: Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-frame appearance and dynamic synchronization. To address this, we present ReCamMaster, a camera-controlled generative video re-rendering framework that reproduces the dynamic scene of an input video at novel camera trajectories. The core innovation lies in harnessing the generative capabilities of pre-trained text-to-video models through a simple yet powerful video conditioning mechanism -- its capability often overlooked in current research. To overcome the scarcity of qualified training data, we construct a comprehensive multi-camera synchronized video dataset using Unreal Engine 5, which is carefully curated to follow real-world filming characteristics, covering diverse scenes and camera movements. It helps the model generalize to in-the-wild videos. Lastly, we further improve the robustness to diverse inputs through a meticulously designed training strategy. Extensive experiments tell that our method substantially outperforms existing state-of-the-art approaches and strong baselines. Our method also finds promising applications in video stabilization, super-resolution, and outpainting. Project page: https://jianhongbai.github.io/ReCamMaster/",
        "arxiv_id": "2503.11647",
        "ARXIVID": "2503.11647",
        "COMMENT": "Matches criterion 3 as it introduces a new framework for camera-controlled generative video re-rendering and builds a novel dataset using Unreal Engine 5.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.11576": {
        "authors": [
            "Ahmed Nassar",
            "Andres Marafioti",
            "Matteo Omenetti",
            "Maksym Lysak",
            "Nikolaos Livathinos",
            "Christoph Auer",
            "Lucas Morin",
            "Rafael Teixeira de Lima",
            "Yusik Kim",
            "A. Said Gurbuz",
            "Michele Dolfi",
            "Miquel Farr\\'e",
            "Peter W. J. Staar"
        ],
        "title": "SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion",
        "abstract": "arXiv:2503.11576v1 Announce Type: new  Abstract: We introduce SmolDocling, an ultra-compact vision-language model targeting end-to-end document conversion. Our model comprehensively processes entire pages by generating DocTags, a new universal markup format that captures all page elements in their full context with location. Unlike existing approaches that rely on large foundational models, or ensemble solutions that rely on handcrafted pipelines of multiple specialized models, SmolDocling offers an end-to-end conversion for accurately capturing content, structure and spatial location of document elements in a 256M parameters vision-language model. SmolDocling exhibits robust performance in correctly reproducing document features such as code listings, tables, equations, charts, lists, and more across a diverse range of document types including business documents, academic papers, technical reports, patents, and forms -- significantly extending beyond the commonly observed focus on scientific papers. Additionally, we contribute novel publicly sourced datasets for charts, tables, equations, and code recognition. Experimental results demonstrate that SmolDocling competes with other Vision Language Models that are up to 27 times larger in size, while reducing computational requirements substantially. The model is currently available, datasets will be publicly available soon.",
        "arxiv_id": "2503.11576",
        "ARXIVID": "2503.11576",
        "COMMENT": "Matches criterion 2 as it introduces an ultra-compact vision-language model for document conversion.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.11557": {
        "authors": [
            "Jing Bi",
            "Junjia Guo",
            "Susan Liang",
            "Guangyu Sun",
            "Luchuan Song",
            "Yunlong Tang",
            "Jinxi He",
            "Jiarui Wu",
            "Ali Vosoughi",
            "Chen Chen",
            "Chenliang Xu"
        ],
        "title": "VERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning Fidelity",
        "abstract": "arXiv:2503.11557v1 Announce Type: new  Abstract: Visual reasoning is central to human cognition, enabling individuals to interpret and abstractly understand their environment. Although recent Multimodal Large Language Models (MLLMs) have demonstrated impressive performance across language and vision-language tasks, existing benchmarks primarily measure recognition-based skills and inadequately assess true visual reasoning capabilities. To bridge this critical gap, we introduce VERIFY, a benchmark explicitly designed to isolate and rigorously evaluate the visual reasoning capabilities of state-of-the-art MLLMs. VERIFY compels models to reason primarily from visual information, providing minimal textual context to reduce reliance on domain-specific knowledge and linguistic biases. Each problem is accompanied by a human-annotated reasoning path, making it the first to provide in-depth evaluation of model decision-making processes. Additionally, we propose novel metrics that assess visual reasoning fidelity beyond mere accuracy, highlighting critical imbalances in current model reasoning patterns. Our comprehensive benchmarking of leading MLLMs uncovers significant limitations, underscoring the need for a balanced and holistic approach to both perception and reasoning. For more teaser and testing, visit our project page (https://verify-eqh.pages.dev/).",
        "arxiv_id": "2503.11557",
        "ARXIVID": "2503.11557",
        "COMMENT": "Matches criterion 2 as it evaluates and benchmarks MLLMs for visual reasoning, providing insights into their reasoning fidelity.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.11219": {
        "authors": [
            "Yansheng Li",
            "Yuning Wu",
            "Gong Cheng",
            "Chao Tao",
            "Bo Dang",
            "Yu Wang",
            "Jiahao Zhang",
            "Chuge Zhang",
            "Yiting Liu",
            "Xu Tang",
            "Jiayi Ma",
            "Yongjun Zhang"
        ],
        "title": "MEET: A Million-Scale Dataset for Fine-Grained Geospatial Scene Classification with Zoom-Free Remote Sensing Imagery",
        "abstract": "arXiv:2503.11219v1 Announce Type: new  Abstract: Accurate fine-grained geospatial scene classification using remote sensing imagery is essential for a wide range of applications. However, existing approaches often rely on manually zooming remote sensing images at different scales to create typical scene samples. This approach fails to adequately support the fixed-resolution image interpretation requirements in real-world scenarios. To address this limitation, we introduce the Million-scale finE-grained geospatial scEne classification dataseT (MEET), which contains over 1.03 million zoom-free remote sensing scene samples, manually annotated into 80 fine-grained categories. In MEET, each scene sample follows a scene-inscene layout, where the central scene serves as the reference, and auxiliary scenes provide crucial spatial context for finegrained classification. Moreover, to tackle the emerging challenge of scene-in-scene classification, we present the Context-Aware Transformer (CAT), a model specifically designed for this task, which adaptively fuses spatial context to accurately classify the scene samples. CAT adaptively fuses spatial context to accurately classify the scene samples by learning attentional features that capture the relationships between the center and auxiliary scenes. Based on MEET, we establish a comprehensive benchmark for fine-grained geospatial scene classification, evaluating CAT against 11 competitive baselines. The results demonstrate that CAT significantly outperforms these baselines, achieving a 1.88% higher balanced accuracy (BA) with the Swin-Large backbone, and a notable 7.87% improvement with the Swin-Huge backbone. Further experiments validate the effectiveness of each module in CAT and show the practical applicability of CAT in the urban functional zone mapping. The source code and dataset will be publicly available at https://jerrywyn.github.io/project/MEET.html.",
        "arxiv_id": "2503.11219",
        "ARXIVID": "2503.11219",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset (MEET) for geospatial scene classification.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2503.11172": {
        "authors": [
            "Zhen Tan",
            "Xieyuanli Chen",
            "Jinpu Zhang",
            "Lei Feng",
            "Dewen Hu"
        ],
        "title": "Uncertainty-Aware Normal-Guided Gaussian Splatting for Surface Reconstruction from Sparse Image Sequences",
        "abstract": "arXiv:2503.11172v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has achieved impressive rendering performance in novel view synthesis. However, its efficacy diminishes considerably in sparse image sequences, where inherent data sparsity amplifies geometric uncertainty during optimization. This often leads to convergence at suboptimal local minima, resulting in noticeable structural artifacts in the reconstructed scenes.To mitigate these issues, we propose Uncertainty-aware Normal-Guided Gaussian Splatting (UNG-GS), a novel framework featuring an explicit Spatial Uncertainty Field (SUF) to quantify geometric uncertainty within the 3DGS pipeline. UNG-GS enables high-fidelity rendering and achieves high-precision reconstruction without relying on priors. Specifically, we first integrate Gaussian-based probabilistic modeling into the training of 3DGS to optimize the SUF, providing the model with adaptive error tolerance. An uncertainty-aware depth rendering strategy is then employed to weight depth contributions based on the SUF, effectively reducing noise while preserving fine details. Furthermore, an uncertainty-guided normal refinement method adjusts the influence of neighboring depth values in normal estimation, promoting robust results. Extensive experiments demonstrate that UNG-GS significantly outperforms state-of-the-art methods in both sparse and dense sequences. The code will be open-source.",
        "arxiv_id": "2503.11172",
        "ARXIVID": "2503.11172",
        "COMMENT": "Matches criterion 1. Proposes a novel method for spatial understanding in 3D Gaussian Splatting, which aligns with spatial intelligence improvements.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.11400": {
        "authors": [
            "Tin Stribor Sohn",
            "Philipp Reis",
            "Maximilian Dillitzer",
            "Johannes Bach",
            "Jason J. Corso",
            "Eric Sax"
        ],
        "title": "A Framework for a Capability-driven Evaluation of Scenario Understanding for Multimodal Large Language Models in Autonomous Driving",
        "abstract": "arXiv:2503.11400v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) hold the potential to enhance autonomous driving by combining domain-independent world knowledge with context-specific language guidance. Their integration into autonomous driving systems shows promising results in isolated proof-of-concept applications, while their performance is evaluated on selective singular aspects of perception, reasoning, or planning. To leverage their full potential a systematic framework for evaluating MLLMs in the context of autonomous driving is required. This paper proposes a holistic framework for a capability-driven evaluation of MLLMs in autonomous driving. The framework structures scenario understanding along the four core capability dimensions semantic, spatial, temporal, and physical. They are derived from the general requirements of autonomous driving systems, human driver cognition, and language-based reasoning. It further organises the domain into context layers, processing modalities, and downstream tasks such as language-based interaction and decision-making. To illustrate the framework's applicability, two exemplary traffic scenarios are analysed, grounding the proposed dimensions in realistic driving situations. The framework provides a foundation for the structured evaluation of MLLMs' potential for scenario understanding in autonomous driving.",
        "arxiv_id": "2503.11400",
        "ARXIVID": "2503.11400",
        "COMMENT": "Matches criterion 3. Proposes a framework for evaluating multimodal large language models in autonomous driving, which aligns with embodied AI and novel benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.10704": {
        "authors": [
            "Jing Wang",
            "Fengzhuo Zhang",
            "Xiaoli Li",
            "Vincent Y. F. Tan",
            "Tianyu Pang",
            "Chao Du",
            "Aixin Sun",
            "Zhuoran Yang"
        ],
        "title": "Error Analyses of Auto-Regressive Video Diffusion Models: A Unified Framework",
        "abstract": "arXiv:2503.10704v1 Announce Type: new  Abstract: A variety of Auto-Regressive Video Diffusion Models (ARVDM) have achieved remarkable successes in generating realistic long-form videos. However, theoretical analyses of these models remain scant. In this work, we develop theoretical underpinnings for these models and use our insights to improve the performance of existing models. We first develop Meta-ARVDM, a unified framework of ARVDMs that subsumes most existing methods. Using Meta-ARVDM, we analyze the KL-divergence between the videos generated by Meta-ARVDM and the true videos. Our analysis uncovers two important phenomena inherent to ARVDM -- error accumulation and memory bottleneck. By deriving an information-theoretic impossibility result, we show that the memory bottleneck phenomenon cannot be avoided. To mitigate the memory bottleneck, we design various network structures to explicitly use more past frames. We also achieve a significantly improved trade-off between the mitigation of the memory bottleneck and the inference efficiency by compressing the frames. Experimental results on DMLab and Minecraft validate the efficacy of our methods. Our experiments also demonstrate a Pareto-frontier between the error accumulation and memory bottleneck across different methods.",
        "arxiv_id": "2503.10704",
        "ARXIVID": "2503.10704",
        "COMMENT": "Matches criterion 3 as it provides a unified framework and theoretical analysis for auto-regressive video diffusion models, focusing on error accumulation and memory bottleneck.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.11038": {
        "authors": [
            "Mingjie Wei",
            "Xuemei Xie",
            "Guangming Shi"
        ],
        "title": "ACMo: Attribute Controllable Motion Generation",
        "abstract": "arXiv:2503.11038v1 Announce Type: new  Abstract: Attributes such as style, fine-grained text, and trajectory are specific conditions for describing motion. However, existing methods often lack precise user control over motion attributes and suffer from limited generalizability to unseen motions. This work introduces an Attribute Controllable Motion generation architecture, to address these challenges via decouple any conditions and control them separately. Firstly, we explored the Attribute Diffusion Model to imporve text-to-motion performance via decouple text and motion learning, as the controllable model relies heavily on the pre-trained model. Then, we introduce Motion Adpater to quickly finetune previously unseen motion patterns. Its motion prompts inputs achieve multimodal text-to-motion generation that captures user-specified styles. Finally, we propose a LLM Planner to bridge the gap between unseen attributes and dataset-specific texts via local knowledage for user-friendly interaction. Our approach introduces the capability for motion prompts for stylize generation, enabling fine-grained and user-friendly attribute control while providing performance comparable to state-of-the-art methods. Project page: https://mjwei3d.github.io/ACMo/",
        "arxiv_id": "2503.11038",
        "ARXIVID": "2503.11038",
        "COMMENT": "Matches criterion 2 as it focuses on attribute-controllable motion generation using a large language model planner, relevant to multi-modal large language models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.11205": {
        "authors": [
            "Leqi Shen",
            "Tao He",
            "Guoqiang Gong",
            "Fan Yang",
            "Yifeng Zhang",
            "Pengzhang Liu",
            "Sicheng Zhao",
            "Guiguang Ding"
        ],
        "title": "LLaVA-MLB: Mitigating and Leveraging Attention Bias for Training-Free Video LLMs",
        "abstract": "arXiv:2503.11205v1 Announce Type: new  Abstract: Training-free video large language models (LLMs) leverage pretrained Image LLMs to process video content without the need for further training. A key challenge in such approaches is the difficulty of retaining essential visual and temporal information, constrained by the token limits in Image LLMs. To address this, we propose a two-stage method for selecting query-relevant tokens based on the LLM attention scores: compressing the video sequence and then expanding the sequence. However, during the compression stage, Image LLMs often exhibit a positional attention bias in video sequences, where attention is overly concentrated on later frames, causing early-frame information to be underutilized. To alleviate this attention bias during sequence compression, we propose Gridded Attention Pooling for preserving spatiotemporal structure. Additionally, we introduce Visual Summarization Tail to effectively utilize this bias, facilitating overall video understanding during sequence expansion. In this way, our method effectively Mitigates and Leverages attention Bias (LLaVA-MLB), enabling the frozen Image LLM for detailed video understanding. Experiments on several benchmarks demonstrate that our approach outperforms state-of-the-art methods, achieving superior performance in both efficiency and accuracy. Our code will be released.",
        "arxiv_id": "2503.11205",
        "ARXIVID": "2503.11205",
        "COMMENT": "Matches criterion 2 as it introduces a method for training-free video large language models (VLLMs) with novel attention mechanisms.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.10872": {
        "authors": [
            "Xiangyu Yin",
            "Yi Qi",
            "Jinwei Hu",
            "Zhen Chen",
            "Yi Dong",
            "Xingyu Zhao",
            "Xiaowei Huang",
            "Wenjie Ruan"
        ],
        "title": "TAIJI: Textual Anchoring for Immunizing Jailbreak Images in Vision Language Models",
        "abstract": "arXiv:2503.10872v1 Announce Type: new  Abstract: Vision Language Models (VLMs) have demonstrated impressive inference capabilities, but remain vulnerable to jailbreak attacks that can induce harmful or unethical responses. Existing defence methods are predominantly white-box approaches that require access to model parameters and extensive modifications, making them costly and impractical for many real-world scenarios. Although some black-box defences have been proposed, they often impose input constraints or require multiple queries, limiting their effectiveness in safety-critical tasks such as autonomous driving. To address these challenges, we propose a novel black-box defence framework called \\textbf{T}extual \\textbf{A}nchoring for \\textbf{I}mmunizing \\textbf{J}ailbreak \\textbf{I}mages (\\textbf{TAIJI}). TAIJI leverages key phrase-based textual anchoring to enhance the model's ability to assess and mitigate the harmful content embedded within both visual and textual prompts. Unlike existing methods, TAIJI operates effectively with a single query during inference, while preserving the VLM's performance on benign tasks. Extensive experiments demonstrate that TAIJI significantly enhances the safety and reliability of VLMs, providing a practical and efficient solution for real-world deployment.",
        "arxiv_id": "2503.10872",
        "ARXIVID": "2503.10872",
        "COMMENT": "Matches criterion 2 as it proposes a novel defense framework for Vision Language Models (VLMs) against jailbreak attacks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.11044": {
        "authors": [
            "Hasan Iqbal",
            "Nazmul Karim",
            "Umar Khalid",
            "Azib Farooq",
            "Zichun Zhong",
            "Jing Hua",
            "Chen Chen"
        ],
        "title": "PSF-4D: A Progressive Sampling Framework for View Consistent 4D Editing",
        "abstract": "arXiv:2503.11044v1 Announce Type: new  Abstract: Instruction-guided generative models, especially those using text-to-image (T2I) and text-to-video (T2V) diffusion frameworks, have advanced the field of content editing in recent years. To extend these capabilities to 4D scene, we introduce a progressive sampling framework for 4D editing (PSF-4D) that ensures temporal and multi-view consistency by intuitively controlling the noise initialization during forward diffusion. For temporal coherence, we design a correlated Gaussian noise structure that links frames over time, allowing each frame to depend meaningfully on prior frames. Additionally, to ensure spatial consistency across views, we implement a cross-view noise model, which uses shared and independent noise components to balance commonalities and distinct details among different views. To further enhance spatial coherence, PSF-4D incorporates view-consistent iterative refinement, embedding view-aware information into the denoising process to ensure aligned edits across frames and views. Our approach enables high-quality 4D editing without relying on external models, addressing key challenges in previous methods. Through extensive evaluation on multiple benchmarks and multiple editing aspects (e.g., style transfer, multi-attribute editing, object removal, local editing, etc.), we show the effectiveness of our proposed method. Experimental results demonstrate that our proposed method outperforms state-of-the-art 4D editing methods in diverse benchmarks.",
        "arxiv_id": "2503.11044",
        "ARXIVID": "2503.11044",
        "COMMENT": "Matches criterion 4 as it introduces a framework for 4D editing with temporal and spatial consistency, relevant to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.10696": {
        "authors": [
            "Yefei He",
            "Yuanyu He",
            "Shaoxuan He",
            "Feng Chen",
            "Hong Zhou",
            "Kaipeng Zhang",
            "Bohan Zhuang"
        ],
        "title": "Neighboring Autoregressive Modeling for Efficient Visual Generation",
        "abstract": "arXiv:2503.10696v1 Announce Type: new  Abstract: Visual autoregressive models typically adhere to a raster-order ``next-token prediction\" paradigm, which overlooks the spatial and temporal locality inherent in visual content. Specifically, visual tokens exhibit significantly stronger correlations with their spatially or temporally adjacent tokens compared to those that are distant. In this paper, we propose Neighboring Autoregressive Modeling (NAR), a novel paradigm that formulates autoregressive visual generation as a progressive outpainting procedure, following a near-to-far ``next-neighbor prediction\" mechanism. Starting from an initial token, the remaining tokens are decoded in ascending order of their Manhattan distance from the initial token in the spatial-temporal space, progressively expanding the boundary of the decoded region. To enable parallel prediction of multiple adjacent tokens in the spatial-temporal space, we introduce a set of dimension-oriented decoding heads, each predicting the next token along a mutually orthogonal dimension. During inference, all tokens adjacent to the decoded tokens are processed in parallel, substantially reducing the model forward steps for generation. Experiments on ImageNet$256\\times 256$ and UCF101 demonstrate that NAR achieves 2.4$\\times$ and 8.6$\\times$ higher throughput respectively, while obtaining superior FID/FVD scores for both image and video generation tasks compared to the PAR-4X approach. When evaluating on text-to-image generation benchmark GenEval, NAR with 0.8B parameters outperforms Chameleon-7B while using merely 0.4 of the training data. Code is available at https://github.com/ThisisBillhe/NAR.",
        "arxiv_id": "2503.10696",
        "ARXIVID": "2503.10696",
        "COMMENT": "Matches criterion 4 as it introduces a novel autoregressive modeling paradigm for visual generation, which is relevant to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.11481": {
        "authors": [
            "Seyed Mohammad Hadi Hosseini",
            "Amir Mohammad Izadi",
            "Ali Abdollahi",
            "Armin Saghafian",
            "Mahdieh Soleymani Baghshah"
        ],
        "title": "T2I-FineEval: Fine-Grained Compositional Metric for Text-to-Image Evaluation",
        "abstract": "arXiv:2503.11481v1 Announce Type: new  Abstract: Although recent text-to-image generative models have achieved impressive performance, they still often struggle with capturing the compositional complexities of prompts including attribute binding, and spatial relationships between different entities. This misalignment is not revealed by common evaluation metrics such as CLIPScore. Recent works have proposed evaluation metrics that utilize Visual Question Answering (VQA) by decomposing prompts into questions about the generated image for more robust compositional evaluation. Although these methods align better with human evaluations, they still fail to fully cover the compositionality within the image. To address this, we propose a novel metric that breaks down images into components, and texts into fine-grained questions about the generated image for evaluation. Our method outperforms previous state-of-the-art metrics, demonstrating its effectiveness in evaluating text-to-image generative models. Code is available at https://github.com/hadi-hosseini/ T2I-FineEval.",
        "arxiv_id": "2503.11481",
        "ARXIVID": "2503.11481",
        "COMMENT": "Matches criterion 4 as it focuses on a novel evaluation metric for text-to-image generative models, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.11342": {
        "authors": [
            "Yibing Weng",
            "Yu Gu",
            "Fuji Ren"
        ],
        "title": "Road Rage Reasoning with Vision-language Models (VLMs): Task Definition and Evaluation Dataset",
        "abstract": "arXiv:2503.11342v1 Announce Type: new  Abstract: Road rage, triggered by driving-related stimuli such as traffic congestion and aggressive driving, poses a significant threat to road safety. Previous research on road rage regulation has primarily focused on response suppression, lacking proactive prevention capabilities. With the advent of Vision-Language Models (VLMs), it has become possible to reason about trigger events visually and then engage in dialog-based comforting before drivers' anger escalates. To this end, we propose the road rage reasoning task, along with a finely annotated test dataset and evaluation metrics, to assess the capabilities of current mainstream VLMs in scene understanding, event recognition, and road rage reasoning. The results indicate that current VLMs exhibit significant shortcomings in scene understanding within the visual modality, as well as in comprehending the spatial relationships between objects in the textual modality. Improving VLMs' performance in these areas will greatly benefit downstream tasks like antecedent-focused road rage regulation.",
        "arxiv_id": "2503.11342",
        "ARXIVID": "2503.11342",
        "COMMENT": "Matches criterion 2 as it evaluates Vision-Language Models (VLMs) on road rage reasoning tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2503.11345": {
        "authors": [
            "Di Li",
            "Jie Feng",
            "Jiahao Chen",
            "Weisheng Dong",
            "Guanbin Li",
            "Guangming Shi",
            "Licheng Jiao"
        ],
        "title": "EgoSplat: Open-Vocabulary Egocentric Scene Understanding with Language Embedded 3D Gaussian Splatting",
        "abstract": "arXiv:2503.11345v1 Announce Type: new  Abstract: Egocentric scenes exhibit frequent occlusions, varied viewpoints, and dynamic interactions compared to typical scene understanding tasks. Occlusions and varied viewpoints can lead to multi-view semantic inconsistencies, while dynamic objects may act as transient distractors, introducing artifacts into semantic feature modeling. To address these challenges, we propose EgoSplat, a language-embedded 3D Gaussian Splatting framework for open-vocabulary egocentric scene understanding. A multi-view consistent instance feature aggregation method is designed to leverage the segmentation and tracking capabilities of SAM2 to selectively aggregate complementary features across views for each instance, ensuring precise semantic representation of scenes. Additionally, an instance-aware spatial-temporal transient prediction module is constructed to improve spatial integrity and temporal continuity in predictions by incorporating spatial-temporal associations across multi-view instances, effectively reducing artifacts in the semantic reconstruction of egocentric scenes. EgoSplat achieves state-of-the-art performance in both localization and segmentation tasks on two datasets, outperforming existing methods with a 8.2% improvement in localization accuracy and a 3.7% improvement in segmentation mIoU on the ADT dataset, and setting a new benchmark in open-vocabulary egocentric scene understanding. The code will be made publicly available.",
        "arxiv_id": "2503.11345",
        "ARXIVID": "2503.11345",
        "COMMENT": "Matches criterion 3 as it focuses on egocentric scene understanding with a novel 3D Gaussian Splatting framework.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.10745": {
        "authors": [
            "Ayush Jain",
            "Alexander Swerdlow",
            "Yuzhou Wang",
            "Sergio Arnaud",
            "Ada Martin",
            "Alexander Sax",
            "Franziska Meier",
            "Katerina Fragkiadaki"
        ],
        "title": "Unifying 2D and 3D Vision-Language Understanding",
        "abstract": "arXiv:2503.10745v1 Announce Type: new  Abstract: Progress in 3D vision-language learning has been hindered by the scarcity of large-scale 3D datasets. We introduce UniVLG, a unified architecture for 2D and 3D vision-language understanding that bridges the gap between existing 2D-centric models and the rich 3D sensory data available in embodied systems. Our approach initializes most model weights from pre-trained 2D models and trains on both 2D and 3D vision-language data. We propose a novel language-conditioned mask decoder shared across 2D and 3D modalities to ground objects effectively in both RGB and RGB-D images, outperforming box-based approaches. To further reduce the domain gap between 2D and 3D, we incorporate 2D-to-3D lifting strategies, enabling UniVLG to utilize 2D data to enhance 3D performance. With these innovations, our model achieves state-of-the-art performance across multiple 3D vision-language grounding tasks, demonstrating the potential of transferring advances from 2D vision-language learning to the data-constrained 3D domain. Furthermore, co-training on both 2D and 3D data enhances performance across modalities without sacrificing 2D capabilities. By removing the reliance on 3D mesh reconstruction and ground-truth object proposals, UniVLG sets a new standard for realistic, embodied-aligned evaluation. Code and additional visualizations are available at $\\href{https://univlg.github.io}{univlg.github.io}$.",
        "arxiv_id": "2503.10745",
        "ARXIVID": "2503.10745",
        "COMMENT": "Matches criterion 3 as it focuses on 3D vision-language understanding and proposes a unified architecture for 2D and 3D data.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.11368": {
        "authors": [
            "Xiaokang Wei",
            "Bowen Zhang",
            "Xianghui Yang",
            "Yuxuan Wang",
            "Chunchao Guo",
            "Xi Zhao",
            "Yan Luximon"
        ],
        "title": "PBR3DGen: A VLM-guided Mesh Generation with High-quality PBR Texture",
        "abstract": "arXiv:2503.11368v1 Announce Type: new  Abstract: Generating high-quality physically based rendering (PBR) materials is important to achieve realistic rendering in the downstream tasks, yet it remains challenging due to the intertwined effects of materials and lighting. While existing methods have made breakthroughs by incorporating material decomposition in the 3D generation pipeline, they tend to bake highlights into albedo and ignore spatially varying properties of metallicity and roughness. In this work, we present PBR3DGen, a two-stage mesh generation method with high-quality PBR materials that integrates the novel multi-view PBR material estimation model and a 3D PBR mesh reconstruction model. Specifically, PBR3DGen leverages vision language models (VLM) to guide multi-view diffusion, precisely capturing the spatial distribution and inherent attributes of reflective-metalness material. Additionally, we incorporate view-dependent illumination-aware conditions as pixel-aware priors to enhance spatially varying material properties. Furthermore, our reconstruction model reconstructs high-quality mesh with PBR materials. Experimental results demonstrate that PBR3DGen significantly outperforms existing methods, achieving new state-of-the-art results for PBR estimation and mesh generation. More results and visualization can be found on our project page: https://pbr3dgen1218.github.io/.",
        "arxiv_id": "2503.11368",
        "ARXIVID": "2503.11368",
        "COMMENT": "Matches criterion 2 as it discusses a vision-language model (VLM) for high-quality PBR material generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.11187": {
        "authors": [
            "Leqi Shen",
            "Guoqiang Gong",
            "Tao He",
            "Yifeng Zhang",
            "Pengzhang Liu",
            "Sicheng Zhao",
            "Guiguang Ding"
        ],
        "title": "FastVID: Dynamic Density Pruning for Fast Video Large Language Models",
        "abstract": "arXiv:2503.11187v1 Announce Type: new  Abstract: Video Large Language Models have shown impressive capabilities in video comprehension, yet their practical deployment is hindered by substantial inference costs caused by redundant video tokens. Existing pruning techniques fail to fully exploit the spatiotemporal redundancy inherent in video data. To bridge this gap, we perform a systematic analysis of video redundancy from two perspectives: temporal context and visual context. Leveraging this insight, we propose Dynamic Density Pruning for Fast Video LLMs termed FastVID. Specifically, FastVID dynamically partitions videos into temporally ordered segments to preserve temporal structure and applies a density-based token pruning strategy to maintain essential visual information. Our method significantly reduces computational overhead while maintaining temporal and visual integrity. Extensive evaluations show that FastVID achieves state-of-the-art performance across various short- and long-video benchmarks on leading Video LLMs, including LLaVA-OneVision and LLaVA-Video. Notably, FastVID effectively prunes 90% of video tokens while retaining 98.0% of LLaVA-OneVision's original performance. The code is available at https://github.com/LunarShen/FastVID.",
        "arxiv_id": "2503.11187",
        "ARXIVID": "2503.11187",
        "COMMENT": "Matches criterion 2 as it introduces a novel pruning method for video large language models (VLLMs), improving efficiency while maintaining performance.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.11097": {
        "authors": [
            "Wenbang Deng",
            "Xieyuanli Chen",
            "Qinghua Yu",
            "Yunze He",
            "Junhao Xiao",
            "Huimin Lu"
        ],
        "title": "A Novel Decomposed Feature-Oriented Framework for Open-Set Semantic Segmentation on LiDAR Data",
        "abstract": "arXiv:2503.11097v1 Announce Type: new  Abstract: Semantic segmentation is a key technique that enables mobile robots to understand and navigate surrounding environments autonomously. However, most existing works focus on segmenting known objects, overlooking the identification of unknown classes, which is common in real-world applications. In this paper, we propose a feature-oriented framework for open-set semantic segmentation on LiDAR data, capable of identifying unknown objects while retaining the ability to classify known ones. We design a decomposed dual-decoder network to simultaneously perform closed-set semantic segmentation and generate distinctive features for unknown objects. The network is trained with multi-objective loss functions to capture the characteristics of known and unknown objects. Using the extracted features, we introduce an anomaly detection mechanism to identify unknown objects. By integrating the results of close-set semantic segmentation and anomaly detection, we achieve effective feature-driven LiDAR open-set semantic segmentation. Evaluations on both SemanticKITTI and nuScenes datasets demonstrate that our proposed framework significantly outperforms state-of-the-art methods. The source code will be made publicly available at https://github.com/nubot-nudt/DOSS.",
        "arxiv_id": "2503.11097",
        "ARXIVID": "2503.11097",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework for open-set semantic segmentation on LiDAR data, addressing previously overlooked challenges in identifying unknown objects.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.11062": {
        "authors": [
            "Wenhao Jiang",
            "Duo Li",
            "Menghan Hu",
            "Chao Ma",
            "Ke Wang",
            "Zhipeng Zhang"
        ],
        "title": "Active Learning from Scene Embeddings for End-to-End Autonomous Driving",
        "abstract": "arXiv:2503.11062v1 Announce Type: new  Abstract: In the field of autonomous driving, end-to-end deep learning models show great potential by learning driving decisions directly from sensor data. However, training these models requires large amounts of labeled data, which is time-consuming and expensive. Considering that the real-world driving data exhibits a long-tailed distribution where simple scenarios constitute a majority part of the data, we are thus inspired to identify the most challenging scenarios within it. Subsequently, we can efficiently improve the performance of the model by training with the selected data of the highest value. Prior research has focused on the selection of valuable data by empirically designed strategies. However, manually designed methods suffer from being less generalizable to new data distributions. Observing that the BEV (Bird's Eye View) features in end-to-end models contain all the information required to represent the scenario, we propose an active learning framework that relies on these vectorized scene-level features, called SEAD. The framework selects initial data based on driving-environmental information and incremental data based on BEV features. Experiments show that we only need 30\\% of the nuScenes training data to achieve performance close to what can be achieved with the full dataset. The source code will be released.",
        "arxiv_id": "2503.11062",
        "ARXIVID": "2503.11062",
        "COMMENT": "Matches criterion 3 as it proposes a novel active learning framework for autonomous driving using BEV features, which is a new method for embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.10937": {
        "authors": [
            "Haoyu Zhang",
            "Raghavendra Ramachandra",
            "Kiran Raja",
            "Christoph Busch"
        ],
        "title": "ChatGPT Encounters Morphing Attack Detection: Zero-Shot MAD with Multi-Modal Large Language Models and General Vision Models",
        "abstract": "arXiv:2503.10937v1 Announce Type: new  Abstract: Face Recognition Systems (FRS) are increasingly vulnerable to face-morphing attacks, prompting the development of Morphing Attack Detection (MAD) algorithms. However, a key challenge in MAD lies in its limited generalizability to unseen data and its lack of explainability-critical for practical application environments such as enrolment stations and automated border control systems. Recognizing that most existing MAD algorithms rely on supervised learning paradigms, this work explores a novel approach to MAD using zero-shot learning leveraged on Large Language Models (LLMs). We propose two types of zero-shot MAD algorithms: one leveraging general vision models and the other utilizing multimodal LLMs. For general vision models, we address the MAD task by computing the mean support embedding of an independent support set without using morphed images. For the LLM-based approach, we employ the state-of-the-art GPT-4 Turbo API with carefully crafted prompts. To evaluate the feasibility of zero-shot MAD and the effectiveness of the proposed methods, we constructed a print-scan morph dataset featuring various unseen morphing algorithms, simulating challenging real-world application scenarios. Experimental results demonstrated notable detection accuracy, validating the applicability of zero-shot learning for MAD tasks. Additionally, our investigation into LLM-based MAD revealed that multimodal LLMs, such as ChatGPT, exhibit remarkable generalizability to untrained MAD tasks. Furthermore, they possess a unique ability to provide explanations and guidance, which can enhance transparency and usability for end-users in practical applications.",
        "arxiv_id": "2503.10937",
        "ARXIVID": "2503.10937",
        "COMMENT": "Matches criterion 2. Explores the use of multimodal large language models (MLLMs) for morphing attack detection, which aligns with interest in VLLMs/MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.10705": {
        "authors": [
            "Haoyuan Gao",
            "Zicong Zhang",
            "Yuqi Wei",
            "Linglan Zhao",
            "Guilin Li",
            "Yexin Li",
            "Linghe Kong",
            "Weiran Huang"
        ],
        "title": "Enhanced Continual Learning of Vision-Language Models with Model Fusion",
        "abstract": "arXiv:2503.10705v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) represent a breakthrough in artificial intelligence by integrating visual and textual modalities to achieve impressive zero-shot capabilities. However, VLMs are susceptible to catastrophic forgetting when sequentially fine-tuned on multiple downstream tasks. Existing continual learning methods for VLMs often rely heavily on additional reference datasets, compromise zero-shot performance, or are limited to parameter-efficient fine-tuning scenarios. In this paper, we propose Continual Decoupling-Unifying (ConDU), a novel approach, by introducing model fusion into continual learning for VLMs. ConDU maintains a unified model along with task triggers and prototype sets, employing an iterative process of decoupling task-specific models for previous tasks and unifying them with the model for the newly learned task. Additionally, we introduce an inference strategy for zero-shot scenarios by aggregating predictions from multiple decoupled task-specific models. Extensive experiments across various settings show that ConDU achieves up to a 2\\% improvement in average performance across all seen tasks compared to state-of-the-art baselines, while also enhancing zero-shot capabilities relative to the original VLM.",
        "arxiv_id": "2503.10705",
        "ARXIVID": "2503.10705",
        "COMMENT": "Matches criterion 2 as it discusses continual learning for vision-language models, which is relevant to multi-modal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2503.11519": {
        "authors": [
            "Hao Cheng",
            "Erjia Xiao",
            "Yichi Wang",
            "Kaidi Xu",
            "Mengshu Sun",
            "Jindong Gu",
            "Renjing Xu"
        ],
        "title": "Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models",
        "abstract": "arXiv:2503.11519v1 Announce Type: new  Abstract: Current Cross-Modality Generation Models (GMs) demonstrate remarkable capabilities in various generative tasks. Given the ubiquity and information richness of vision modality inputs in real-world scenarios, Cross-vision, encompassing Vision-Language Perception (VLP) and Image-to-Image (I2I), tasks have attracted significant attention. Large Vision Language Models (LVLMs) and I2I GMs are employed to handle VLP and I2I tasks, respectively. Previous research indicates that printing typographic words into input images significantly induces LVLMs and I2I GMs to generate disruptive outputs semantically related to those words. Additionally, visual prompts, as a more sophisticated form of typography, are also revealed to pose security risks to various applications of VLP tasks when injected into images. In this paper, we comprehensively investigate the performance impact induced by Typographic Visual Prompt Injection (TVPI) in various LVLMs and I2I GMs. To better observe performance modifications and characteristics of this threat, we also introduce the TVPI Dataset. Through extensive explorations, we deepen the understanding of the underlying causes of the TVPI threat in various GMs and offer valuable insights into its potential origins.",
        "arxiv_id": "2503.11519",
        "ARXIVID": "2503.11519",
        "COMMENT": "Matches criterion 2 as it explores threats in cross-modality generation models, including vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2503.11030": {
        "authors": [
            "Ming Deng",
            "Sijin Sun",
            "Zihao Li",
            "Xiaochuan Hu",
            "Xing Wu"
        ],
        "title": "FMNet: Frequency-Assisted Mamba-Like Linear Attention Network for Camouflaged Object Detection",
        "abstract": "arXiv:2503.11030v1 Announce Type: new  Abstract: Camouflaged Object Detection (COD) is challenging due to the strong similarity between camouflaged objects and their surroundings, which complicates identification. Existing methods mainly rely on spatial local features, failing to capture global information, while Transformers increase computational costs.To address this, the Frequency-Assisted Mamba-Like Linear Attention Network (FMNet) is proposed, which leverages frequency-domain learning to efficiently capture global features and mitigate ambiguity between objects and the background. FMNet introduces the Multi-Scale Frequency-Assisted Mamba-Like Linear Attention (MFM) module, integrating frequency and spatial features through a multi-scale structure to handle scale variations while reducing computational complexity. Additionally, the Pyramidal Frequency Attention Extraction (PFAE) module and the Frequency Reverse Decoder (FRD) enhance semantics and reconstruct features. Experimental results demonstrate that FMNet outperforms existing methods on multiple COD datasets, showcasing its advantages in both performance and efficiency. Code available at https://anonymous.4open.science/r/FMNet-3CE5.",
        "arxiv_id": "2503.11030",
        "ARXIVID": "2503.11030",
        "COMMENT": "Does not match any specific criteria. Focuses on camouflaged object detection using frequency-domain learning, which is outside the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.10697": {
        "authors": [
            "Kaifeng Zou",
            "Xiaoyi Feng",
            "Peng Wang",
            "Tao Huang",
            "Zizhou Huang",
            "Zhang Haihang",
            "Yuntao Zou",
            "Dagang Li"
        ],
        "title": "Zero-Shot Subject-Centric Generation for Creative Application Using Entropy Fusion",
        "abstract": "arXiv:2503.10697v1 Announce Type: new  Abstract: Generative models are widely used in visual content creation. However, current text-to-image models often face challenges in practical applications-such as textile pattern design and meme generation-due to the presence of unwanted elements that are difficult to separate with existing methods. Meanwhile, subject-reference generation has emerged as a key research trend, highlighting the need for techniques that can produce clean, high-quality subject images while effectively removing extraneous components. To address this challenge, we introduce a framework for reliable subject-centric image generation. In this work, we propose an entropy-based feature-weighted fusion method to merge the informative cross-attention features obtained from each sampling step of the pretrained text-to-image model FLUX, enabling a precise mask prediction and subject-centric generation. Additionally, we have developed an agent framework based on Large Language Models (LLMs) that translates users' casual inputs into more descriptive prompts, leading to highly detailed image generation. Simultaneously, the agents extract primary elements of prompts to guide the entropy-based feature fusion, ensuring focused primary element generation without extraneous components. Experimental results and user studies demonstrate our methods generates high-quality subject-centric images, outperform existing methods or other possible pipelines, highlighting the effectiveness of our approach.",
        "arxiv_id": "2503.10697",
        "ARXIVID": "2503.10697",
        "COMMENT": "Does not match any specific criteria. Focuses on subject-centric image generation and entropy fusion, which are tangential to the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.10678": {
        "authors": [
            "Lehan Yang",
            "Jincen Song",
            "Tianlong Wang",
            "Daiqing Qi",
            "Weili Shi",
            "Yuheng Liu",
            "Sheng Li"
        ],
        "title": "VRMDiff: Text-Guided Video Referring Matting Generation of Diffusion",
        "abstract": "arXiv:2503.10678v1 Announce Type: new  Abstract: We propose a new task, video referring matting, which obtains the alpha matte of a specified instance by inputting a referring caption. We treat the dense prediction task of matting as video generation, leveraging the text-to-video alignment prior of video diffusion models to generate alpha mattes that are temporally coherent and closely related to the corresponding semantic instances. Moreover, we propose a new Latent-Constructive loss to further distinguish different instances, enabling more controllable interactive matting. Additionally, we introduce a large-scale video referring matting dataset with 10,000 videos. To the best of our knowledge, this is the first dataset that concurrently contains captions, videos, and instance-level alpha mattes. Extensive experiments demonstrate the effectiveness of our method. The dataset and code are available at https://github.com/Hansxsourse/VRMDiff.",
        "arxiv_id": "2503.10678",
        "ARXIVID": "2503.10678",
        "COMMENT": "Does not match any specific criteria. Focuses on video referring matting and text-to-video alignment, which are tangential to the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.11078": {
        "authors": [
            "Taehwan Lee",
            "Kyeongkook Seo",
            "Jaejun Yoo",
            "Sung Whan Yoon"
        ],
        "title": "Understanding Flatness in Generative Models: Its Role and Benefits",
        "abstract": "arXiv:2503.11078v1 Announce Type: new  Abstract: Flat minima, known to enhance generalization and robustness in supervised learning, remain largely unexplored in generative models. In this work, we systematically investigate the role of loss surface flatness in generative models, both theoretically and empirically, with a particular focus on diffusion models. We establish a theoretical claim that flatter minima improve robustness against perturbations in target prior distributions, leading to benefits such as reduced exposure bias -- where errors in noise estimation accumulate over iterations -- and significantly improved resilience to model quantization, preserving generative performance even under strong quantization constraints. We further observe that Sharpness-Aware Minimization (SAM), which explicitly controls the degree of flatness, effectively enhances flatness in diffusion models, whereas other well-known methods such as Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA), which promote flatness indirectly via ensembling, are less effective. Through extensive experiments on CIFAR-10, LSUN Tower, and FFHQ, we demonstrate that flat minima in diffusion models indeed improves not only generative performance but also robustness.",
        "arxiv_id": "2503.11078",
        "ARXIVID": "2503.11078",
        "COMMENT": "Does not match any specific criterion but discusses flatness in generative models, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.11601": {
        "authors": [
            "Xuanqi Zhang",
            "Jieun Lee",
            "Chris Joslin",
            "Wonsook Lee"
        ],
        "title": "Advancing 3D Gaussian Splatting Editing with Complementary and Consensus Information",
        "abstract": "arXiv:2503.11601v1 Announce Type: new  Abstract: We present a novel framework for enhancing the visual fidelity and consistency of text-guided 3D Gaussian Splatting (3DGS) editing. Existing editing approaches face two critical challenges: inconsistent geometric reconstructions across multiple viewpoints, particularly in challenging camera positions, and ineffective utilization of depth information during image manipulation, resulting in over-texture artifacts and degraded object boundaries. To address these limitations, we introduce: 1) A complementary information mutual learning network that enhances depth map estimation from 3DGS, enabling precise depth-conditioned 3D editing while preserving geometric structures. 2) A wavelet consensus attention mechanism that effectively aligns latent codes during the diffusion denoising process, ensuring multi-view consistency in the edited results. Through extensive experimentation, our method demonstrates superior performance in rendering quality and view consistency compared to state-of-the-art approaches. The results validate our framework as an effective solution for text-guided editing of 3D scenes.",
        "arxiv_id": "2503.11601",
        "ARXIVID": "2503.11601",
        "COMMENT": "Does not match any specific criterion but is related to 3D scene editing, which is tangentially relevant to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.11544": {
        "authors": [
            "Parsa Rahimi",
            "Damien Teney",
            "Sebastien Marcel"
        ],
        "title": "AugGen: Synthetic Augmentation Can Improve Discriminative Models",
        "abstract": "arXiv:2503.11544v1 Announce Type: new  Abstract: The increasing dependence on large-scale datasets in machine learning introduces significant privacy and ethical challenges. Synthetic data generation offers a promising solution; however, most current methods rely on external datasets or pre-trained models, which add complexity and escalate resource demands. In this work, we introduce a novel self-contained synthetic augmentation technique that strategically samples from a conditional generative model trained exclusively on the target dataset. This approach eliminates the need for auxiliary data sources. Applied to face recognition datasets, our method achieves 1--12\\% performance improvements on the IJB-C and IJB-B benchmarks. It outperforms models trained solely on real data and exceeds the performance of state-of-the-art synthetic data generation baselines. Notably, these enhancements often surpass those achieved through architectural improvements, underscoring the significant impact of synthetic augmentation in data-scarce environments. These findings demonstrate that carefully integrated synthetic data not only addresses privacy and resource constraints but also substantially boosts model performance. Project page https://parsa-ra.github.io/auggen",
        "arxiv_id": "2503.11544",
        "ARXIVID": "2503.11544",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to generative modeling and synthetic data augmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.11017": {
        "authors": [
            "Jiaqi Jin",
            "Siwei Wang",
            "Zhibin Dong",
            "Xihong Yang",
            "Xinwang Liu",
            "En Zhu",
            "Kunlun He"
        ],
        "title": "Deep Incomplete Multi-view Clustering with Distribution Dual-Consistency Recovery Guidance",
        "abstract": "arXiv:2503.11017v1 Announce Type: new  Abstract: Multi-view clustering leverages complementary representations from diverse sources to enhance performance. However, real-world data often suffer incomplete cases due to factors like privacy concerns and device malfunctions. A key challenge is effectively utilizing available instances to recover missing views. Existing methods frequently overlook the heterogeneity among views during recovery, leading to significant distribution discrepancies between recovered and true data. Additionally, many approaches focus on cross-view correlations, neglecting insights from intra-view reliable structure and cross-view clustering structure. To address these issues, we propose BURG, a novel method for incomplete multi-view clustering with distriBution dUal-consistency Recovery Guidance. We treat each sample as a distinct category and perform cross-view distribution transfer to predict the distribution space of missing views. To compensate for the lack of reliable category information, we design a dual-consistency guided recovery strategy that includes intra-view alignment guided by neighbor-aware consistency and cross-view alignment guided by prototypical consistency. Extensive experiments on benchmarks demonstrate the superiority of BURG in the incomplete multi-view scenario.",
        "arxiv_id": "2503.11017",
        "ARXIVID": "2503.11017",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to multi-view clustering and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.11328": {
        "authors": [
            "Ruiqian Li",
            "Siyuan Shen",
            "Suan Xia",
            "Ziheng Wang",
            "Xingyue Peng",
            "Chengxuan Song",
            "Yingsheng Zhu",
            "Tao Wu",
            "Shiying Li",
            "Jingyi Yu"
        ],
        "title": "TransiT: Transient Transformer for Non-line-of-sight Videography",
        "abstract": "arXiv:2503.11328v1 Announce Type: new  Abstract: High quality and high speed videography using Non-Line-of-Sight (NLOS) imaging benefit autonomous navigation, collision prevention, and post-disaster search and rescue tasks. Current solutions have to balance between the frame rate and image quality. High frame rates, for example, can be achieved by reducing either per-point scanning time or scanning density, but at the cost of lowering the information density at individual frames. Fast scanning process further reduces the signal-to-noise ratio and different scanning systems exhibit different distortion characteristics. In this work, we design and employ a new Transient Transformer architecture called TransiT to achieve real-time NLOS recovery under fast scans. TransiT directly compresses the temporal dimension of input transients to extract features, reducing computation costs and meeting high frame rate requirements. It further adopts a feature fusion mechanism as well as employs a spatial-temporal Transformer to help capture features of NLOS transient videos. Moreover, TransiT applies transfer learning to bridge the gap between synthetic and real-measured data. In real experiments, TransiT manages to reconstruct from sparse transients of $16 \\times 16$ measured at an exposure time of 0.4 ms per point to NLOS videos at a $64 \\times 64$ resolution at 10 frames per second. We will make our code and dataset available to the community.",
        "arxiv_id": "2503.11328",
        "ARXIVID": "2503.11328",
        "COMMENT": "This paper does not match any specific criteria but focuses on NLOS videography using a novel transformer architecture.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.11335": {
        "authors": [
            "Moein Sorkhei",
            "Emir Konuk",
            "Kevin Smith",
            "Christos Matsoukas"
        ],
        "title": "APLA: A Simple Adaptation Method for Vision Transformers",
        "abstract": "arXiv:2503.11335v1 Announce Type: new  Abstract: Existing adaptation techniques typically require architectural modifications or added parameters, leading to high computational costs and complexity. We introduce Attention Projection Layer Adaptation (APLA), a simple approach to adapt vision transformers (ViTs) without altering the architecture or adding parameters. Through a systematic analysis, we find that the layer immediately after the attention mechanism is crucial for adaptation. By updating only this projection layer, or even just a random subset of this layer's weights, APLA achieves state-of-the-art performance while reducing GPU memory usage by up to 52.63% and training time by up to 43.0%, with no extra cost at inference. Across 46 datasets covering a variety of tasks including scene classification, medical imaging, satellite imaging, and fine-grained classification, APLA consistently outperforms 17 other leading adaptation methods, including full fine-tuning, on classification, segmentation, and detection tasks. The code is available at https://github.com/MoeinSorkhei/APLA.",
        "arxiv_id": "2503.11335",
        "ARXIVID": "2503.11335",
        "COMMENT": "This paper does not match any specific criteria but introduces a novel adaptation method for vision transformers.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.11167": {
        "authors": [
            "Haonan Wang",
            "Qixiang Zhang",
            "Lehan Wang",
            "Xuanqi Huang",
            "Xiaomeng Li"
        ],
        "title": "Neurons: Emulating the Human Visual Cortex Improves Fidelity and Interpretability in fMRI-to-Video Reconstruction",
        "abstract": "arXiv:2503.11167v1 Announce Type: new  Abstract: Decoding visual stimuli from neural activity is essential for understanding the human brain. While fMRI methods have successfully reconstructed static images, fMRI-to-video reconstruction faces challenges due to the need for capturing spatiotemporal dynamics like motion and scene transitions. Recent approaches have improved semantic and perceptual alignment but struggle to integrate coarse fMRI data with detailed visual features. Inspired by the hierarchical organization of the visual system, we propose NEURONS, a novel framework that decouples learning into four correlated sub-tasks: key object segmentation, concept recognition, scene description, and blurry video reconstruction. This approach simulates the visual cortex's functional specialization, allowing the model to capture diverse video content. In the inference stage, NEURONS generates robust conditioning signals for a pre-trained text-to-video diffusion model to reconstruct the videos. Extensive experiments demonstrate that NEURONS outperforms state-of-the-art baselines, achieving solid improvements in video consistency (26.6%) and semantic-level accuracy (19.1%). Notably, NEURONS shows a strong functional correlation with the visual cortex, highlighting its potential for brain-computer interfaces and clinical applications. Code and model weights will be available at: https://github.com/xmed-lab/NEURONS.",
        "arxiv_id": "2503.11167",
        "ARXIVID": "2503.11167",
        "COMMENT": "This paper does not match any specific criteria but is related to brain-computer interfaces and video reconstruction.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.11213": {
        "authors": [
            "Fengchen He",
            "Dayang Zhao",
            "Hao Xu",
            "Tingwei Quan",
            "Shaoqun Zeng"
        ],
        "title": "Simulating Dual-Pixel Images From Ray Tracing For Depth Estimation",
        "abstract": "arXiv:2503.11213v1 Announce Type: new  Abstract: Many studies utilize dual-pixel (DP) sensor phase characteristics for various applications, such as depth estimation and deblurring. However, since the DP image features are entirely determined by the camera hardware, DP-depth paired datasets are very scarce, especially when performing depth estimation on customized cameras. To overcome this, studies simulate DP images using ideal optical system models. However, these simulations often violate real optical propagation laws,leading to poor generalization to real DP data. To address this, we investigate the domain gap between simulated and real DP data, and propose solutions using the Simulating DP images from ray tracing (Sdirt) scheme. The Sdirt generates realistic DP images via ray tracing and integrates them into the depth estimation training pipeline. Experimental results show that models trained with Sdirt-simulated images generalize better to real DP data.",
        "arxiv_id": "2503.11213",
        "ARXIVID": "2503.11213",
        "COMMENT": "Does not match any specific criteria. Focuses on simulating dual-pixel images for depth estimation, which is outside the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.10941": {
        "authors": [
            "Piyush Gupta",
            "Sangjae Bae",
            "David Isele"
        ],
        "title": "Graph-Grounded LLMs: Leveraging Graphical Function Calling to Minimize LLM Hallucinations",
        "abstract": "arXiv:2503.10941v1 Announce Type: new  Abstract: The adoption of Large Language Models (LLMs) is rapidly expanding across various tasks that involve inherent graphical structures. Graphs are integral to a wide range of applications, including motion planning for autonomous vehicles, social networks, scene understanding, and knowledge graphs. Many problems, even those not initially perceived as graph-based, can be effectively addressed through graph theory. However, when applied to these tasks, LLMs often encounter challenges, such as hallucinations and mathematical inaccuracies. To overcome these limitations, we propose Graph-Grounded LLMs, a system that improves LLM performance on graph-related tasks by integrating a graph library through function calls. By grounding LLMs in this manner, we demonstrate significant reductions in hallucinations and improved mathematical accuracy in solving graph-based problems, as evidenced by the performance on the NLGraph benchmark. Finally, we showcase a disaster rescue application where the Graph-Grounded LLM acts as a decision-support system.",
        "arxiv_id": "2503.10941",
        "ARXIVID": "2503.10941",
        "COMMENT": "Does not match any specific criteria. Focuses on graph-grounded LLMs for reducing hallucinations in graph-related tasks, which is outside the specified areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.11194": {
        "authors": [
            "Qiuxia Lin",
            "Kerui Gu",
            "Linlin Yang",
            "Angela Yao"
        ],
        "title": "Online Test-time Adaptation for 3D Human Pose Estimation: A Practical Perspective with Estimated 2D Poses",
        "abstract": "arXiv:2503.11194v1 Announce Type: new  Abstract: Online test-time adaptation for 3D human pose estimation is used for video streams that differ from training data. Ground truth 2D poses are used for adaptation, but only estimated 2D poses are available in practice. This paper addresses adapting models to streaming videos with estimated 2D poses. Comparing adaptations reveals the challenge of limiting estimation errors while preserving accurate pose information. To this end, we propose adaptive aggregation, a two-stage optimization, and local augmentation for handling varying levels of estimated pose error. First, we perform adaptive aggregation across videos to initialize the model state with labeled representative samples. Within each video, we use a two-stage optimization to benefit from 2D fitting while minimizing the impact of erroneous updates. Second, we employ local augmentation, using adjacent confident samples to update the model before adapting to the current non-confident sample. Our method surpasses state-of-the-art by a large margin, advancing adaptation towards more practical settings of using estimated 2D poses.",
        "arxiv_id": "2503.11194",
        "ARXIVID": "2503.11194",
        "COMMENT": "Does not match any specific criterion but focuses on test-time adaptation for 3D human pose estimation, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.10740": {
        "authors": [
            "Jeimin Jeon",
            "Youngmin Oh",
            "Junghyup Lee",
            "Donghyeon Baek",
            "Dohyung Kim",
            "Chanho Eom",
            "Bumsub Ham"
        ],
        "title": "Subnet-Aware Dynamic Supernet Training for Neural Architecture Search",
        "abstract": "arXiv:2503.10740v1 Announce Type: new  Abstract: N-shot neural architecture search (NAS) exploits a supernet containing all candidate subnets for a given search space. The subnets are typically trained with a static training strategy (e.g., using the same learning rate (LR) scheduler and optimizer for all subnets). This, however, does not consider that individual subnets have distinct characteristics, leading to two problems: (1) The supernet training is biased towards the low-complexity subnets (unfairness); (2) the momentum update in the supernet is noisy (noisy momentum). We present a dynamic supernet training technique to address these problems by adjusting the training strategy adaptive to the subnets. Specifically, we introduce a complexity-aware LR scheduler (CaLR) that controls the decay ratio of LR adaptive to the complexities of subnets, which alleviates the unfairness problem. We also present a momentum separation technique (MS). It groups the subnets with similar structural characteristics and uses a separate momentum for each group, avoiding the noisy momentum problem. Our approach can be applicable to various N-shot NAS methods with marginal cost, while improving the search performance drastically. We validate the effectiveness of our approach on various search spaces (e.g., NAS-Bench-201, Mobilenet spaces) and datasets (e.g., CIFAR-10/100, ImageNet).",
        "arxiv_id": "2503.10740",
        "ARXIVID": "2503.10740",
        "COMMENT": "Does not match any specific criterion but focuses on neural architecture search, which is tangentially related to machine learning methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.11571": {
        "authors": [
            "Tianrui Pan",
            "Lin Liu",
            "Jie Liu",
            "Xiaopeng Zhang",
            "Jie Tang",
            "Gangshan Wu",
            "Qi Tian"
        ],
        "title": "RASA: Replace Anyone, Say Anything -- A Training-Free Framework for Audio-Driven and Universal Portrait Video Editing",
        "abstract": "arXiv:2503.11571v1 Announce Type: new  Abstract: Portrait video editing focuses on modifying specific attributes of portrait videos, guided by audio or video streams. Previous methods typically either concentrate on lip-region reenactment or require training specialized models to extract keypoints for motion transfer to a new identity. In this paper, we introduce a training-free universal portrait video editing framework that provides a versatile and adaptable editing strategy. This framework supports portrait appearance editing conditioned on the changed first reference frame, as well as lip editing conditioned on varied speech, or a combination of both. It is based on a Unified Animation Control (UAC) mechanism with source inversion latents to edit the entire portrait, including visual-driven shape control, audio-driven speaking control, and inter-frame temporal control. Furthermore, our method can be adapted to different scenarios by adjusting the initial reference frame, enabling detailed editing of portrait videos with specific head rotations and facial expressions. This comprehensive approach ensures a holistic and flexible solution for portrait video editing. The experimental results show that our model can achieve more accurate and synchronized lip movements for the lip editing task, as well as more flexible motion transfer for the appearance editing task. Demo is available at https://alice01010101.github.io/RASA/.",
        "arxiv_id": "2503.11571",
        "ARXIVID": "2503.11571",
        "COMMENT": "Does not match any specific criterion but focuses on portrait video editing, which is tangentially related to vision applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.11074": {
        "authors": [
            "Xueyang Zhou",
            "Guiyao Tie",
            "Guowen Zhang",
            "Weidong Wang",
            "Zhigang Zuo",
            "Di Wu",
            "Duanfeng Chu",
            "Pan Zhou",
            "Lichao Sun",
            "Neil Zhenqiang Gong"
        ],
        "title": "Large Reasoning Models in Agent Scenarios: Exploring the Necessity of Reasoning Capabilities",
        "abstract": "arXiv:2503.11074v1 Announce Type: new  Abstract: The rise of Large Reasoning Models (LRMs) signifies a paradigm shift toward advanced computational reasoning. Yet, this progress disrupts traditional agent frameworks, traditionally anchored by execution-oriented Large Language Models (LLMs). To explore this transformation, we propose the LaRMA framework, encompassing nine tasks across Tool Usage, Plan Design, and Problem Solving, assessed with three top LLMs (e.g., Claude3.5-sonnet) and five leading LRMs (e.g., DeepSeek-R1). Our findings address four research questions: LRMs surpass LLMs in reasoning-intensive tasks like Plan Design, leveraging iterative reflection for superior outcomes; LLMs excel in execution-driven tasks such as Tool Usage, prioritizing efficiency; hybrid LLM-LRM configurations, pairing LLMs as actors with LRMs as reflectors, optimize agent performance by blending execution speed with reasoning depth; and LRMs' enhanced reasoning incurs higher computational costs, prolonged processing, and behavioral challenges, including overthinking and fact-ignoring tendencies. This study fosters deeper inquiry into LRMs' balance of deep thinking and overthinking, laying a critical foundation for future agent design advancements.",
        "arxiv_id": "2503.11074",
        "ARXIVID": "2503.11074",
        "COMMENT": "Does not match any specific criterion but discusses reasoning models in agent scenarios, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.11088": {
        "authors": [
            "Yifan Liu",
            "Xun Xu",
            "Shijie Li",
            "Jingyi Liao",
            "Xulei Yang"
        ],
        "title": "Multi-View Industrial Anomaly Detection with Epipolar Constrained Cross-View Fusion",
        "abstract": "arXiv:2503.11088v1 Announce Type: new  Abstract: Multi-camera systems provide richer contextual information for industrial anomaly detection. However, traditional methods process each view independently, disregarding the complementary information across viewpoints. Existing multi-view anomaly detection approaches typically employ data-driven cross-view attention for feature fusion but fail to leverage the unique geometric properties of multi-camera setups. In this work, we introduce an epipolar geometry-constrained attention module to guide cross-view fusion, ensuring more effective information aggregation. To further enhance the potential of cross-view attention, we propose a pretraining strategy inspired by memory bank-based anomaly detection. This approach encourages normal feature representations to form multiple local clusters and incorporate multi-view aware negative sample synthesis to regularize pretraining. We demonstrate that our epipolar guided multi-view anomaly detection framework outperforms existing methods on the state-of-the-art multi-view anomaly detection dataset.",
        "arxiv_id": "2503.11088",
        "ARXIVID": "2503.11088",
        "COMMENT": "Does not match any specific criterion but is related to computer vision and anomaly detection, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.11412": {
        "authors": [
            "Shiyuan Yang",
            "Zheng Gu",
            "Liang Hou",
            "Xin Tao",
            "Pengfei Wan",
            "Xiaodong Chen",
            "Jing Liao"
        ],
        "title": "MTV-Inpaint: Multi-Task Long Video Inpainting",
        "abstract": "arXiv:2503.11412v1 Announce Type: new  Abstract: Video inpainting involves modifying local regions within a video, ensuring spatial and temporal consistency. Most existing methods focus primarily on scene completion (i.e., filling missing regions) and lack the capability to insert new objects into a scene in a controllable manner. Fortunately, recent advancements in text-to-video (T2V) diffusion models pave the way for text-guided video inpainting. However, directly adapting T2V models for inpainting remains limited in unifying completion and insertion tasks, lacks input controllability, and struggles with long videos, thereby restricting their applicability and flexibility. To address these challenges, we propose MTV-Inpaint, a unified multi-task video inpainting framework capable of handling both traditional scene completion and novel object insertion tasks. To unify these distinct tasks, we design a dual-branch spatial attention mechanism in the T2V diffusion U-Net, enabling seamless integration of scene completion and object insertion within a single framework. In addition to textual guidance, MTV-Inpaint supports multimodal control by integrating various image inpainting models through our proposed image-to-video (I2V) inpainting mode. Additionally, we propose a two-stage pipeline that combines keyframe inpainting with in-between frame propagation, enabling MTV-Inpaint to effectively handle long videos with hundreds of frames. Extensive experiments demonstrate that MTV-Inpaint achieves state-of-the-art performance in both scene completion and object insertion tasks. Furthermore, it demonstrates versatility in derived applications such as multi-modal inpainting, object editing, removal, image object brush, and the ability to handle long videos. Project page: https://mtv-inpaint.github.io/.",
        "arxiv_id": "2503.11412",
        "ARXIVID": "2503.11412",
        "COMMENT": "Does not match any specific criteria. Focuses on video inpainting with text-to-video diffusion models, which is tangential to the interest area but not directly relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.11321": {
        "authors": [
            "Lingyu Zhu",
            "Xiangrui Zeng",
            "Bolin Chen",
            "Peilin Chen",
            "Yung-Hui Li",
            "Shiqi Wang"
        ],
        "title": "Leveraging Diffusion Knowledge for Generative Image Compression with Fractal Frequency-Aware Band Learning",
        "abstract": "arXiv:2503.11321v1 Announce Type: new  Abstract: By optimizing the rate-distortion-realism trade-off, generative image compression approaches produce detailed, realistic images instead of the only sharp-looking reconstructions produced by rate-distortion-optimized models. In this paper, we propose a novel deep learning-based generative image compression method injected with diffusion knowledge, obtaining the capacity to recover more realistic textures in practical scenarios. Efforts are made from three perspectives to navigate the rate-distortion-realism trade-off in the generative image compression task. First, recognizing the strong connection between image texture and frequency-domain characteristics, we design a Fractal Frequency-Aware Band Image Compression (FFAB-IC) network to effectively capture the directional frequency components inherent in natural images. This network integrates commonly used fractal band feature operations within a neural non-linear mapping design, enhancing its ability to retain essential given information and filter out unnecessary details. Then, to improve the visual quality of image reconstruction under limited bandwidth, we integrate diffusion knowledge into the encoder and implement diffusion iterations into the decoder process, thus effectively recovering lost texture details. Finally, to fully leverage the spatial and frequency intensity information, we incorporate frequency- and content-aware regularization terms to regularize the training of the generative image compression network. Extensive experiments in quantitative and qualitative evaluations demonstrate the superiority of the proposed method, advancing the boundaries of achievable distortion-realism pairs, i.e., our method achieves better distortions at high realism and better realism at low distortion than ever before.",
        "arxiv_id": "2503.11321",
        "ARXIVID": "2503.11321",
        "COMMENT": "Does not match any specific criteria. Focuses on generative image compression with diffusion knowledge, which is tangential to the interest area but not directly relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.10959": {
        "authors": [
            "Akshat Ramachandran",
            "Mingyu Lee",
            "Huan Xu",
            "Souvik Kundu",
            "Tushar Krishna"
        ],
        "title": "OuroMamba: A Data-Free Quantization Framework for Vision Mamba Models",
        "abstract": "arXiv:2503.10959v1 Announce Type: new  Abstract: We present OuroMamba, the first data-free post-training quantization (DFQ) method for vision Mamba-based models (VMMs). We identify two key challenges in enabling DFQ for VMMs, (1) VMM's recurrent state transitions restricts capturing of long-range interactions and leads to semantically weak synthetic data, (2) VMM activations exhibit dynamic outlier variations across time-steps, rendering existing static PTQ techniques ineffective. To address these challenges, OuroMamba presents a two-stage framework: (1) OuroMamba-Gen to generate semantically rich and meaningful synthetic data. It applies contrastive learning on patch level VMM features generated through neighborhood interactions in the latent state space, (2) OuroMamba-Quant to employ mixed-precision quantization with lightweight dynamic outlier detection during inference. In specific, we present a thresholding based outlier channel selection strategy for activations that gets updated every time-step. Extensive experiments across vision and generative tasks show that our data-free OuroMamba surpasses existing data-driven PTQ techniques, achieving state-of-the-art performance across diverse quantization settings. Additionally, we implement efficient GPU kernels to achieve practical latency speedup of up to 2.36x. Code will be released soon.",
        "arxiv_id": "2503.10959",
        "ARXIVID": "2503.10959",
        "COMMENT": "Does not match any specific criteria. Focuses on data-free quantization for vision models, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.10687": {
        "authors": [
            "Khawar Islam",
            "Naveed Akhtar"
        ],
        "title": "Context-guided Responsible Data Augmentation with Diffusion Models",
        "abstract": "arXiv:2503.10687v1 Announce Type: new  Abstract: Generative diffusion models offer a natural choice for data augmentation when training complex vision models. However, ensuring reliability of their generative content as augmentation samples remains an open challenge. Despite a number of techniques utilizing generative images to strengthen model training, it remains unclear how to utilize the combination of natural and generative images as a rich supervisory signal for effective model induction. In this regard, we propose a text-to-image (T2I) data augmentation method, named DiffCoRe-Mix, that computes a set of generative counterparts for a training sample with an explicitly constrained diffusion model that leverages sample-based context and negative prompting for a reliable augmentation sample generation. To preserve key semantic axes, we also filter out undesired generative samples in our augmentation process. To that end, we propose a hard-cosine filtration in the embedding space of CLIP. Our approach systematically mixes the natural and generative images at pixel and patch levels. We extensively evaluate our technique on ImageNet-1K,Tiny ImageNet-200, CIFAR-100, Flowers102, CUB-Birds, Stanford Cars, and Caltech datasets, demonstrating a notable increase in performance across the board, achieving up to $\\sim 3\\%$ absolute gain for top-1 accuracy over the state-of-the-art methods, while showing comparable computational overhead. Our code is publicly available at https://github.com/khawar-islam/DiffCoRe-Mix",
        "arxiv_id": "2503.10687",
        "ARXIVID": "2503.10687",
        "COMMENT": "Does not match any specific criteria. Focuses on data augmentation using diffusion models, which is tangential to the interest area but not directly relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.11538": {
        "authors": [
            "Ayush Paliwal",
            "Oliver Schlenczek",
            "Birte Thiede",
            "Manuel Santos Pereira",
            "Katja Stieger",
            "Eberhard Bodenschatz",
            "Gholamhossein Bagheri",
            "Alexander Ecker"
        ],
        "title": "FLASH{\\mu}: Fast Localizing And Sizing of Holographic Microparticles",
        "abstract": "arXiv:2503.11538v1 Announce Type: new  Abstract: Reconstructing the 3D location and size of microparticles from diffraction images - holograms - is a computationally expensive inverse problem that has traditionally been solved using physics-based reconstruction methods. More recently, researchers have used machine learning methods to speed up the process. However, for small particles in large sample volumes the performance of these methods falls short of standard physics-based reconstruction methods. Here we designed a two-stage neural network architecture, FLASH$\\mu$, to detect small particles (6-100$\\mu$m) from holograms with large sample depths up to 20cm. Trained only on synthetic data with added physical noise, our method reliably detects particles of at least 9$\\mu$m diameter in real holograms, comparable to the standard reconstruction-based approaches while operating on smaller crops, at quarter of the original resolution and providing roughly a 600-fold speedup. In addition to introducing a novel approach to a non-local object detection or signal demixing problem, our work could enable low-cost, real-time holographic imaging setups.",
        "arxiv_id": "2503.11538",
        "ARXIVID": "2503.11538",
        "COMMENT": "This paper does not match any specific criteria but focuses on holographic microparticle detection using neural networks.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.11140": {
        "authors": [
            "Lexin Fang",
            "Yunyang Xu",
            "Xiang Ma",
            "Xuemei Li",
            "Caiming Zhang"
        ],
        "title": "Minding Fuzzy Regions: A Data-driven Alternating Learning Paradigm for Stable Lesion Segmentation",
        "abstract": "arXiv:2503.11140v1 Announce Type: new  Abstract: Deep learning has achieved significant advancements in medical image segmentation, but existing models still face challenges in accurately segmenting lesion regions. The main reason is that some lesion regions in medical images have unclear boundaries, irregular shapes, and small tissue density differences, leading to label ambiguity. However, the existing model treats all data equally without taking quality differences into account in the training process, resulting in noisy labels negatively impacting model training and unstable feature representations. In this paper, a data-driven alternating learning (DALE) paradigm is proposed to optimize the model's training process, achieving stable and high-precision segmentation. The paradigm focuses on two key points: (1) reducing the impact of noisy labels, and (2) calibrating unstable representations. To mitigate the negative impact of noisy labels, a loss consistency-based collaborative optimization method is proposed, and its effectiveness is theoretically demonstrated. Specifically, the label confidence parameters are introduced to dynamically adjust the influence of labels of different confidence levels during model training, thus reducing the influence of noise labels. To calibrate the learning bias of unstable representations, a distribution alignment method is proposed. This method restores the underlying distribution of unstable representations, thereby enhancing the discriminative capability of fuzzy region representations. Extensive experiments on various benchmarks and model backbones demonstrate the superiority of the DALE paradigm, achieving an average performance improvement of up to 7.16%.",
        "arxiv_id": "2503.11140",
        "ARXIVID": "2503.11140",
        "COMMENT": "This paper does not match any specific criteria but focuses on medical image segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.10912": {
        "authors": [
            "Linfeng Ye"
        ],
        "title": "JPEG Compliant Compression for Both Human and Machine, A Report",
        "abstract": "arXiv:2503.10912v1 Announce Type: new  Abstract: Deep Neural Networks (DNNs) have become an integral part of our daily lives, especially in vision-related applications. However, the conventional lossy image compression algorithms are primarily designed for the Human Vision System (HVS), which can non-trivially compromise the DNNs' validation accuracy after compression, as noted in \\cite{liu2018deepn}. Thus developing an image compression algorithm for both human and machine (DNNs) is on the horizon.   To address the challenge mentioned above, in this paper, we first formulate the image compression as a multi-objective optimization problem which take both human and machine prespectives into account, then we solve it by linear combination, and proposed a novel distortion measure for both human and machine, dubbed Human and Machine-Oriented Error (HMOE). After that, we develop Human And Machine Oriented Soft Decision Quantization (HMOSDQ) based on HMOE, a lossy image compression algorithm for both human and machine (DNNs), and fully complied with JPEG format. In order to evaluate the performance of HMOSDQ, finally we conduct the experiments for two pre-trained well-known DNN-based image classifiers named Alexnet \\cite{Alexnet} and VGG-16 \\cite{simonyan2014VGG} on two subsets of the ImageNet \\cite{deng2009imagenet} validation set: one subset included images with shorter side in the range of 496 to 512, while the other included images with shorter side in the range of 376 to 384. Our results demonstrate that HMOSDQ outperforms the default JPEG algorithm in terms of rate-accuracy and rate-distortion performance. For the Alexnet comparing with the default JPEG algorithm, HMOSDQ can improve the validation accuracy by more than $0.81\\%$ at $0.61$ BPP, or equivalently reduce the compression rate of default JPEG by $9.6\\times$ while maintaining the same validation accuracy.",
        "arxiv_id": "2503.10912",
        "ARXIVID": "2503.10912",
        "COMMENT": "This paper does not match any of the specific criteria but is related to image compression and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.10686": {
        "authors": [
            "Anzhe Cheng",
            "Chenzhong Yin",
            "Yu Chang",
            "Heng Ping",
            "Shixuan Li",
            "Shahin Nazarian",
            "Paul Bogdan"
        ],
        "title": "MaskAttn-UNet: A Mask Attention-Driven Framework for Universal Low-Resolution Image Segmentation",
        "abstract": "arXiv:2503.10686v1 Announce Type: new  Abstract: Low-resolution image segmentation is crucial in real-world applications such as robotics, augmented reality, and large-scale scene understanding, where high-resolution data is often unavailable due to computational constraints. To address this challenge, we propose MaskAttn-UNet, a novel segmentation framework that enhances the traditional U-Net architecture via a mask attention mechanism. Our model selectively emphasizes important regions while suppressing irrelevant backgrounds, thereby improving segmentation accuracy in cluttered and complex scenes. Unlike conventional U-Net variants, MaskAttn-UNet effectively balances local feature extraction with broader contextual awareness, making it particularly well-suited for low-resolution inputs. We evaluate our approach on three benchmark datasets with input images rescaled to 128x128 and demonstrate competitive performance across semantic, instance, and panoptic segmentation tasks. Our results show that MaskAttn-UNet achieves accuracy comparable to state-of-the-art methods at significantly lower computational cost than transformer-based models, making it an efficient and scalable solution for low-resolution segmentation in resource-constrained scenarios.",
        "arxiv_id": "2503.10686",
        "ARXIVID": "2503.10686",
        "COMMENT": "Does not match any specific criterion but focuses on low-resolution image segmentation, which is tangentially related to vision applications.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}