{
    "2508.13584": {
        "authors": [
            "Ruixin Zhang",
            "Jiaqing Fan",
            "Yifan Liao",
            "Qian Qiao",
            "Fanzhang Li"
        ],
        "title": "Temporal-Conditional Referring Video Object Segmentation with Noise-Free Text-to-Video Diffusion Model",
        "abstract": "arXiv:2508.13584v1 Announce Type: new  Abstract: Referring Video Object Segmentation (RVOS) aims to segment specific objects in a video according to textual descriptions. We observe that recent RVOS approaches often place excessive emphasis on feature extraction and temporal modeling, while relatively neglecting the design of the segmentation head. In fact, there remains considerable room for improvement in segmentation head design. To address this, we propose a Temporal-Conditional Referring Video Object Segmentation model, which innovatively integrates existing segmentation methods to effectively enhance boundary segmentation capability. Furthermore, our model leverages a text-to-video diffusion model for feature extraction. On top of this, we remove the traditional noise prediction module to avoid the randomness of noise from degrading segmentation accuracy, thereby simplifying the model while improving performance. Finally, to overcome the limited feature extraction capability of the VAE, we design a Temporal Context Mask Refinement (TCMR) module, which significantly improves segmentation quality without introducing complex designs. We evaluate our method on four public RVOS benchmarks, where it consistently achieves state-of-the-art performance.",
        "arxiv_id": "2508.13584",
        "ARXIVID": "2508.13584",
        "COMMENT": "Matches criteria 1 closely as it discusses a model that integrates segmentation methods and uses a text-to-video diffusion model for feature extraction, which aligns with unified image/video generation and segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.13628": {
        "authors": [
            "Ao Chen",
            "Lihe Ding",
            "Tianfan Xue"
        ],
        "title": "DiffIER: Optimizing Diffusion Models with Iterative Error Reduction",
        "abstract": "arXiv:2508.13628v1 Announce Type: new  Abstract: Diffusion models have demonstrated remarkable capabilities in generating high-quality samples and enhancing performance across diverse domains through Classifier-Free Guidance (CFG). However, the quality of generated samples is highly sensitive to the selection of the guidance weight. In this work, we identify a critical ``training-inference gap'' and we argue that it is the presence of this gap that undermines the performance of conditional generation and renders outputs highly sensitive to the guidance weight. We quantify this gap by measuring the accumulated error during the inference stage and establish a correlation between the selection of guidance weight and minimizing this gap. Furthermore, to mitigate this gap, we propose DiffIER, an optimization-based method for high-quality generation. We demonstrate that the accumulated error can be effectively reduced by an iterative error minimization at each step during inference. By introducing this novel plug-and-play optimization framework, we enable the optimization of errors at every single inference step and enhance generation quality. Empirical results demonstrate that our proposed method outperforms baseline approaches in conditional generation tasks. Furthermore, the method achieves consistent success in text-to-image generation, image super-resolution, and text-to-speech generation, underscoring its versatility and potential for broad applications in future research.",
        "arxiv_id": "2508.13628",
        "ARXIVID": "2508.13628",
        "COMMENT": "Does not match any specific criteria. Focuses on optimizing diffusion models for generation quality, but does not propose a multi-task framework or unified architecture.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}