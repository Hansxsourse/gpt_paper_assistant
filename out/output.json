{
    "2503.10437": {
        "authors": [
            "Wanhua Li",
            "Renping Zhou",
            "Jiawei Zhou",
            "Yingwei Song",
            "Johannes Herter",
            "Minghan Qin",
            "Gao Huang",
            "Hanspeter Pfister"
        ],
        "title": "4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large Language Models",
        "abstract": "arXiv:2503.10437v1 Announce Type: new  Abstract: Learning 4D language fields to enable time-sensitive, open-ended language queries in dynamic scenes is essential for many real-world applications. While LangSplat successfully grounds CLIP features into 3D Gaussian representations, achieving precision and efficiency in 3D static scenes, it lacks the ability to handle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot capture temporal dynamics in videos. Real-world environments are inherently dynamic, with object semantics evolving over time. Building a precise 4D language field necessitates obtaining pixel-aligned, object-wise video features, which current vision models struggle to achieve. To address these challenges, we propose 4D LangSplat, which learns 4D language fields to handle time-agnostic or time-sensitive open-vocabulary queries in dynamic scenes efficiently. 4D LangSplat bypasses learning the language field from vision features and instead learns directly from text generated from object-wise video captions via Multimodal Large Language Models (MLLMs). Specifically, we propose a multimodal object-wise video prompting method, consisting of visual and text prompts that guide MLLMs to generate detailed, temporally consistent, high-quality captions for objects throughout a video. These captions are encoded using a Large Language Model into high-quality sentence embeddings, which then serve as pixel-aligned, object-specific feature supervision, facilitating open-vocabulary text queries through shared embedding spaces. Recognizing that objects in 4D scenes exhibit smooth transitions across states, we further propose a status deformable network to model these continuous changes over time effectively. Our results across multiple benchmarks demonstrate that 4D LangSplat attains precise and efficient results for both time-sensitive and time-agnostic open-vocabulary queries.",
        "arxiv_id": "2503.10437",
        "ARXIVID": "2503.10437",
        "COMMENT": "Matches criterion 2 as it introduces a novel multimodal large language model (4D LangSplat) for dynamic 4D language fields.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2503.10631": {
        "authors": [
            "Jiaming Liu",
            "Hao Chen",
            "Pengju An",
            "Zhuoyang Liu",
            "Renrui Zhang",
            "Chenyang Gu",
            "Xiaoqi Li",
            "Ziyu Guo",
            "Sixiang Chen",
            "Mengzhen Liu",
            "Chengkai Hou",
            "Mengdi Zhao",
            "KC alex Zhou",
            "Pheng-Ann Heng",
            "Shanghang Zhang"
        ],
        "title": "HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model",
        "abstract": "arXiv:2503.10631v1 Announce Type: new  Abstract: Recent advancements in vision-language models (VLMs) for common-sense reasoning have led to the development of vision-language-action (VLA) models, enabling robots to perform generalized manipulation. Although existing autoregressive VLA methods leverage large-scale pretrained knowledge, they disrupt the continuity of actions. Meanwhile, some VLA methods incorporate an additional diffusion head to predict continuous actions, relying solely on VLM-extracted features, which limits their reasoning capabilities. In this paper, we introduce HybridVLA, a unified framework that seamlessly integrates the strengths of both autoregressive and diffusion policies within a single large language model, rather than simply connecting them. To bridge the generation gap, a collaborative training recipe is proposed that injects the diffusion modeling directly into the next-token prediction. With this recipe, we find that these two forms of action prediction not only reinforce each other but also exhibit varying performance across different tasks. Therefore, we design a collaborative action ensemble mechanism that adaptively fuses these two predictions, leading to more robust control. In experiments, HybridVLA outperforms previous state-of-the-art VLA methods across various simulation and real-world tasks, including both single-arm and dual-arm robots, while demonstrating stable manipulation in previously unseen configurations.",
        "arxiv_id": "2503.10631",
        "ARXIVID": "2503.10631",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a novel framework for spatial understanding and action prediction in embodied agents, and focuses on new methods for embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.10307": {
        "authors": [
            "Georgy Ponimatkin",
            "Martin C\\'ifka",
            "Tom\\'a\\v{s} Sou\\v{c}ek",
            "M\\'ed\\'eric Fourmy",
            "Yann Labb\\'e",
            "Vladimir Petrik",
            "Josef Sivic"
        ],
        "title": "6D Object Pose Tracking in Internet Videos for Robotic Manipulation",
        "abstract": "arXiv:2503.10307v1 Announce Type: new  Abstract: We seek to extract a temporally consistent 6D pose trajectory of a manipulated object from an Internet instructional video. This is a challenging set-up for current 6D pose estimation methods due to uncontrolled capturing conditions, subtle but dynamic object motions, and the fact that the exact mesh of the manipulated object is not known. To address these challenges, we present the following contributions. First, we develop a new method that estimates the 6D pose of any object in the input image without prior knowledge of the object itself. The method proceeds by (i) retrieving a CAD model similar to the depicted object from a large-scale model database, (ii) 6D aligning the retrieved CAD model with the input image, and (iii) grounding the absolute scale of the object with respect to the scene. Second, we extract smooth 6D object trajectories from Internet videos by carefully tracking the detected objects across video frames. The extracted object trajectories are then retargeted via trajectory optimization into the configuration space of a robotic manipulator. Third, we thoroughly evaluate and ablate our 6D pose estimation method on YCB-V and HOPE-Video datasets as well as a new dataset of instructional videos manually annotated with approximate 6D object trajectories. We demonstrate significant improvements over existing state-of-the-art RGB 6D pose estimation methods. Finally, we show that the 6D object motion estimated from Internet videos can be transferred to a 7-axis robotic manipulator both in a virtual simulator as well as in a real world set-up. We also successfully apply our method to egocentric videos taken from the EPIC-KITCHENS dataset, demonstrating potential for Embodied AI applications.",
        "arxiv_id": "2503.10307",
        "ARXIVID": "2503.10307",
        "COMMENT": "Matches criterion 3 as it focuses on 6D object pose tracking in Internet videos for robotic manipulation, introducing a novel method and dataset.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.10410": {
        "authors": [
            "Yuwen Du",
            "Anning Hu",
            "Zichen Chao",
            "Yifan Lu",
            "Junhao Ge",
            "Genjia Liu",
            "Weitao Wu",
            "Lanjun Wang",
            "Siheng Chen"
        ],
        "title": "RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground Simulation",
        "abstract": "arXiv:2503.10410v1 Announce Type: new  Abstract: Roadside Collaborative Perception refers to a system where multiple roadside units collaborate to pool their perceptual data, assisting vehicles in enhancing their environmental awareness. Existing roadside perception methods concentrate on model design but overlook data issues like calibration errors, sparse information, and multi-view consistency, leading to poor performance on recent published datasets. To significantly enhance roadside collaborative perception and address critical data issues, we present the first simulation framework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable of generating diverse, multi-view consistent simulated roadside data through dynamic foreground editing and full-scene style transfer of a single image. RoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures accurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View Occlusion-Aware Sampler (MOAS) determines the placement of diverse digital assets within 3D space; (3) DepthSAM innovatively models foreground-background relationships from single-frame fixed-view images, ensuring multi-view consistency of foreground; and (4) Scalable Post-Processing Toolkit generates more realistic and enriched scenes through style transfer and other enhancements. RoCo-Sim significantly improves roadside 3D object detection, outperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on TUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception simulation. Code and pre-trained models will be released soon: https://github.com/duyuwen-duen/RoCo-Sim",
        "arxiv_id": "2503.10410",
        "ARXIVID": "2503.10410",
        "COMMENT": "Matches criterion 3 as it introduces a new simulation framework (RoCo-Sim) for roadside collaborative perception, addressing overlooked data issues.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.10042": {
        "authors": [
            "Ziyue Wang",
            "Yurui Dong",
            "Fuwen Luo",
            "Minyuan Ruan",
            "Zhili Cheng",
            "Chi Chen",
            "Peng Li",
            "Yang Liu"
        ],
        "title": "How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game",
        "abstract": "arXiv:2503.10042v1 Announce Type: new  Abstract: The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred interest in complex multimodal reasoning tasks in the real-world and virtual environment, which require coordinating multiple abilities, including visual perception, visual reasoning, spatial awareness, and target deduction. However, existing evaluations primarily assess the final task completion, often degrading assessments to isolated abilities such as visual grounding and visual question answering. Less attention is given to comprehensively and quantitatively analyzing reasoning process in multimodal environments, which is crucial for understanding model behaviors and underlying reasoning mechanisms beyond merely task success. To address this, we introduce MM-Escape, an extensible benchmark for investigating multimodal reasoning, inspired by real-world escape games. MM-Escape emphasizes intermediate model behaviors alongside final task completion. To achieve this, we develop EscapeCraft, a customizable and open environment that enables models to engage in free-form exploration for assessing multimodal reasoning. Extensive experiments show that MLLMs, regardless of scale, can successfully complete the simplest room escape tasks, with some exhibiting human-like exploration strategies. Yet, performance dramatically drops as task difficulty increases. Moreover, we observe that performance bottlenecks vary across models, revealing distinct failure modes and limitations in their multimodal reasoning abilities, such as repetitive trajectories without adaptive exploration, getting stuck in corners due to poor visual spatial awareness, and ineffective use of acquired props, such as the key. We hope our work sheds light on new challenges in multimodal reasoning, and uncovers potential improvements in MLLMs capabilities.",
        "arxiv_id": "2503.10042",
        "ARXIVID": "2503.10042",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (MM-Escape) and simulator (EscapeCraft) for multimodal reasoning in embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.09871": {
        "authors": [
            "Xiaowen Qiu",
            "Yian Wang",
            "Jiting Cai",
            "Zhehuan Chen",
            "Chunru Lin",
            "Tsun-Hsuan Wang",
            "Chuang Gan"
        ],
        "title": "LuciBot: Automated Robot Policy Learning from Generated Videos",
        "abstract": "arXiv:2503.09871v1 Announce Type: new  Abstract: Automatically generating training supervision for embodied tasks is crucial, as manual designing is tedious and not scalable. While prior works use large language models (LLMs) or vision-language models (VLMs) to generate rewards, these approaches are largely limited to simple tasks with well-defined rewards, such as pick-and-place. This limitation arises because LLMs struggle to interpret complex scenes compressed into text or code due to their restricted input modality, while VLM-based rewards, though better at visual perception, remain limited by their less expressive output modality. To address these challenges, we leverage the imagination capability of general-purpose video generation models. Given an initial simulation frame and a textual task description, the video generation model produces a video demonstrating task completion with correct semantics. We then extract rich supervisory signals from the generated video, including 6D object pose sequences, 2D segmentations, and estimated depth, to facilitate task learning in simulation. Our approach significantly improves supervision quality for complex embodied tasks, enabling large-scale training in simulators.",
        "arxiv_id": "2503.09871",
        "ARXIVID": "2503.09871",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for generating training supervision for embodied tasks using video generation models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.10391": {
        "authors": [
            "Yufan Deng",
            "Xun Guo",
            "Yizhi Wang",
            "Jacob Zhiyuan Fang",
            "Angtian Wang",
            "Shenghai Yuan",
            "Yiding Yang",
            "Bo Liu",
            "Haibin Huang",
            "Chongyang Ma"
        ],
        "title": "CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance",
        "abstract": "arXiv:2503.10391v1 Announce Type: new  Abstract: Video generation has witnessed remarkable progress with the advent of deep generative models, particularly diffusion models. While existing methods excel in generating high-quality videos from text prompts or single images, personalized multi-subject video generation remains a largely unexplored challenge. This task involves synthesizing videos that incorporate multiple distinct subjects, each defined by separate reference images, while ensuring temporal and spatial consistency. Current approaches primarily rely on mapping subject images to keywords in text prompts, which introduces ambiguity and limits their ability to model subject relationships effectively. In this paper, we propose CINEMA, a novel framework for coherent multi-subject video generation by leveraging Multimodal Large Language Model (MLLM). Our approach eliminates the need for explicit correspondences between subject images and text entities, mitigating ambiguity and reducing annotation effort. By leveraging MLLM to interpret subject relationships, our method facilitates scalability, enabling the use of large and diverse datasets for training. Furthermore, our framework can be conditioned on varying numbers of subjects, offering greater flexibility in personalized content creation. Through extensive evaluations, we demonstrate that our approach significantly improves subject consistency, and overall video coherence, paving the way for advanced applications in storytelling, interactive media, and personalized video generation.",
        "arxiv_id": "2503.10391",
        "ARXIVID": "2503.10391",
        "COMMENT": "Matches criterion 2 as it discusses a novel framework for multi-subject video generation using Multimodal Large Language Models (MLLMs).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.10501": {
        "authors": [
            "Xudong Tan",
            "Peng Ye",
            "Chongjun Tu",
            "Jianjian Cao",
            "Yaoxin Yang",
            "Lin Zhang",
            "Dongzhan Zhou",
            "Tao Chen"
        ],
        "title": "TokenCarve: Information-Preserving Visual Token Compression in Multimodal Large Language Models",
        "abstract": "arXiv:2503.10501v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) are becoming increasingly popular, while the high computational cost associated with multimodal data input, particularly from visual tokens, poses a significant challenge. Existing training-based token compression methods improve inference efficiency but require costly retraining, while training-free methods struggle to maintain performance when aggressively reducing token counts. In this study, we reveal that the performance degradation of MLLM closely correlates with the accelerated loss of information in the attention output matrix. This insight introduces a novel information-preserving perspective, making it possible to maintain performance even under extreme token compression. Based on this finding, we propose TokenCarve, a training-free, plug-and-play, two-stage token compression framework. The first stage employs an Information-Preservation-Guided Selection (IPGS) strategy to prune low-information tokens, while the second stage further leverages IPGS to guide token merging, minimizing information loss. Extensive experiments on 11 datasets and 2 model variants demonstrate the effectiveness of TokenCarve. It can even reduce the number of visual tokens to 22.2% of the original count, achieving a 1.23x speedup in inference, a 64% reduction in KV cache storage, and only a 1.54% drop in accuracy. Our code is available at https://github.com/ShawnTan86/TokenCarve.",
        "arxiv_id": "2503.10501",
        "ARXIVID": "2503.10501",
        "COMMENT": "Matches criterion 2 as it proposes a token compression framework for Multimodal Large Language Models (MLLMs).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.10602": {
        "authors": [
            "Jinhao Duan",
            "Fei Kong",
            "Hao Cheng",
            "James Diffenderfer",
            "Bhavya Kailkhura",
            "Lichao Sun",
            "Xiaofeng Zhu",
            "Xiaoshuang Shi",
            "Kaidi Xu"
        ],
        "title": "TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention",
        "abstract": "arXiv:2503.10602v1 Announce Type: new  Abstract: Object Hallucination (OH) has been acknowledged as one of the major trustworthy challenges in Large Vision-Language Models (LVLMs). Recent advancements in Large Language Models (LLMs) indicate that internal states, such as hidden states, encode the \"overall truthfulness\" of generated responses. However, it remains under-explored how internal states in LVLMs function and whether they could serve as \"per-token\" hallucination indicators, which is essential for mitigating OH. In this paper, we first conduct an in-depth exploration of LVLM internal states in relation to OH issues and discover that (1) LVLM internal states are high-specificity per-token indicators of hallucination behaviors. Moreover, (2) different LVLMs encode universal patterns of hallucinations in common latent subspaces, indicating that there exist \"generic truthful directions\" shared by various LVLMs. Based on these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt) that first learns the truthful direction of LVLM decoding and then applies truthful-guided inference-time intervention during LVLM decoding. We further propose ComnHallu to enhance both cross-LVLM and cross-data hallucination detection transferability by constructing and aligning hallucination latent subspaces. We evaluate TruthPrInt in extensive experimental settings, including in-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks. Experimental results indicate that TruthPrInt significantly outperforms state-of-the-art methods. Codes will be available at https://github.com/jinhaoduan/TruthPrInt.",
        "arxiv_id": "2503.10602",
        "ARXIVID": "2503.10602",
        "COMMENT": "Matches criterion 2 as it addresses object hallucination in Large Vision-Language Models (LVLMs) with a novel intervention method.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.10582": {
        "authors": [
            "Yiming Jia",
            "Jiachen Li",
            "Xiang Yue",
            "Bo Li",
            "Ping Nie",
            "Kai Zou",
            "Wenhu Chen"
        ],
        "title": "VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search",
        "abstract": "arXiv:2503.10582v1 Announce Type: new  Abstract: Vision-Language Models have made significant progress on many perception-focused tasks, however, their progress on reasoning-focused tasks seem to be limited due to the lack of high-quality and diverse training data. In this work, we aim to address the scarcity issue of reasoning-focused multimodal datasets. We propose VisualWebInstruct - a novel approach that leverages search engine to create a diverse, and high-quality dataset spanning multiple disciplines like math, physics, finance, chemistry, etc. Starting with meticulously selected 30,000 seed images, we employ Google Image search to identify websites containing similar images. We collect and process the HTMLs from over 700K unique URL sources. Through a pipeline of content extraction, filtering and synthesis, we build a dataset of approximately 900K question-answer pairs, with 40% being visual QA pairs and the rest as text QA pairs. Models fine-tuned on VisualWebInstruct demonstrate significant performance gains: (1) training from Llava-OV-mid shows 10-20% absolute point gains across benchmarks, (2) training from MAmmoTH-VL shows 5% absoluate gain. Our best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B parameter class on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath (55.7%). These remarkable results highlight the effectiveness of our dataset in enhancing VLMs' reasoning capabilities for complex multimodal tasks.",
        "arxiv_id": "2503.10582",
        "ARXIVID": "2503.10582",
        "COMMENT": "Matches criterion 2 as it discusses a new dataset and improvements for reasoning-focused tasks in Vision-Language Models (VLLMs).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.10195": {
        "authors": [
            "Hongze Sun",
            "Jun Wang",
            "Wuque Cai",
            "Duo Chen",
            "Qianqian Liao",
            "Jiayi He",
            "Yan Cui",
            "Dezhong Yao",
            "Daqing Guo"
        ],
        "title": "ST-FlowNet: An Efficient Spiking Neural Network for Event-Based Optical Flow Estimation",
        "abstract": "arXiv:2503.10195v1 Announce Type: new  Abstract: Spiking Neural Networks (SNNs) have emerged as a promising tool for event-based optical flow estimation tasks due to their ability to leverage spatio-temporal information and low-power capabilities. However, the performance of SNN models is often constrained, limiting their application in real-world scenarios. In this work, we address this gap by proposing a novel neural network architecture, ST-FlowNet, specifically tailored for optical flow estimation from event-based data. The ST-FlowNet architecture integrates ConvGRU modules to facilitate cross-modal feature augmentation and temporal alignment of the predicted optical flow, improving the network's ability to capture complex motion dynamics. Additionally, to overcome the challenges associated with training SNNs, we introduce a novel approach to derive SNN models from pre-trained artificial neural networks (ANNs) through ANN-to-SNN conversion or our proposed BISNN method. Notably, the BISNN method alleviates the complexities involved in biological parameter selection, further enhancing the robustness of SNNs in optical flow estimation tasks. Extensive evaluations on three benchmark event-based datasets demonstrate that the SNN-based ST-FlowNet model outperforms state-of-the-art methods, delivering superior performance in accurate optical flow estimation across a diverse range of dynamic visual scenes. Furthermore, the inherent energy efficiency of SNN models is highlighted, establishing a compelling advantage for their practical deployment. Overall, our work presents a novel framework for optical flow estimation using SNNs and event-based data, contributing to the advancement of neuromorphic vision applications.",
        "arxiv_id": "2503.10195",
        "ARXIVID": "2503.10195",
        "COMMENT": "Matches criterion 1 as it introduces a novel spiking neural network architecture for event-based optical flow estimation, improving spatial intelligence in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.09941": {
        "authors": [
            "Mu Chen",
            "Wenyu Chen",
            "Mingchuan Yang",
            "Yuan Zhang",
            "Tao Han",
            "Xinchi Li",
            "Yunlong Li",
            "Huaici Zhao"
        ],
        "title": "TGP: Two-modal occupancy prediction with 3D Gaussian and sparse points for 3D Environment Awareness",
        "abstract": "arXiv:2503.09941v1 Announce Type: new  Abstract: 3D semantic occupancy has rapidly become a research focus in the fields of robotics and autonomous driving environment perception due to its ability to provide more realistic geometric perception and its closer integration with downstream tasks. By performing occupancy prediction of the 3D space in the environment, the ability and robustness of scene understanding can be effectively improved. However, existing occupancy prediction tasks are primarily modeled using voxel or point cloud-based approaches: voxel-based network structures often suffer from the loss of spatial information due to the voxelization process, while point cloud-based methods, although better at retaining spatial location information, face limitations in representing volumetric structural details. To address this issue, we propose a dual-modal prediction method based on 3D Gaussian sets and sparse points, which balances both spatial location and volumetric structural information, achieving higher accuracy in semantic occupancy prediction. Specifically, our method adopts a Transformer-based architecture, taking 3D Gaussian sets, sparse points, and queries as inputs. Through the multi-layer structure of the Transformer, the enhanced queries and 3D Gaussian sets jointly contribute to the semantic occupancy prediction, and an adaptive fusion mechanism integrates the semantic outputs of both modalities to generate the final prediction results. Additionally, to further improve accuracy, we dynamically refine the point cloud at each layer, allowing for more precise location information during occupancy prediction. We conducted experiments on the Occ3DnuScenes dataset, and the experimental results demonstrate superior performance of the proposed method on IoU based metrics.",
        "arxiv_id": "2503.09941",
        "ARXIVID": "2503.09941",
        "COMMENT": "Matches criterion 1 as it proposes a novel dual-modal prediction method for 3D semantic occupancy, improving spatial understanding in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.09994": {
        "authors": [
            "Yunxiao Wang",
            "Meng Liu",
            "Rui Shao",
            "Haoyu Zhang",
            "Bin Wen",
            "Fan Yang",
            "Tingting Gao",
            "Di Zhang",
            "Liqiang Nie"
        ],
        "title": "TIME: Temporal-sensitive Multi-dimensional Instruction Tuning and Benchmarking for Video-LLMs",
        "abstract": "arXiv:2503.09994v1 Announce Type: new  Abstract: Video large language models have achieved remarkable performance in tasks such as video question answering, however, their temporal understanding remains suboptimal. To address this limitation, we curate a dedicated instruction fine-tuning dataset that focuses on enhancing temporal comprehension across five key dimensions. In order to reduce reliance on costly temporal annotations, we introduce a multi-task prompt fine-tuning approach that seamlessly integrates temporal-sensitive tasks into existing instruction datasets without requiring additional annotations. Furthermore, we develop a novel benchmark for temporal-sensitive video understanding that not only fills the gaps in dimension coverage left by existing benchmarks but also rigorously filters out potential shortcuts, ensuring a more accurate evaluation. Extensive experimental results demonstrate that our approach significantly enhances the temporal understanding of video-LLMs while avoiding reliance on shortcuts.",
        "arxiv_id": "2503.09994",
        "ARXIVID": "2503.09994",
        "COMMENT": "Matches criterion 2 as it focuses on improving temporal understanding in video large language models (VLLMs), which aligns with visual large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.10080": {
        "authors": [
            "Zhen Qu",
            "Xian Tao",
            "Xinyi Gong",
            "Shichen Qu",
            "Qiyu Chen",
            "Zhengtao Zhang",
            "Xingang Wang",
            "Guiguang Ding"
        ],
        "title": "Bayesian Prompt Flow Learning for Zero-Shot Anomaly Detection",
        "abstract": "arXiv:2503.10080v1 Announce Type: new  Abstract: Recently, vision-language models (e.g. CLIP) have demonstrated remarkable performance in zero-shot anomaly detection (ZSAD). By leveraging auxiliary data during training, these models can directly perform cross-category anomaly detection on target datasets, such as detecting defects on industrial product surfaces or identifying tumors in organ tissues. Existing approaches typically construct text prompts through either manual design or the optimization of learnable prompt vectors. However, these methods face several challenges: 1) handcrafted prompts require extensive expert knowledge and trial-and-error; 2) single-form learnable prompts struggle to capture complex anomaly semantics; and 3) an unconstrained prompt space limit generalization to unseen categories. To address these issues, we propose Bayesian Prompt Flow Learning (Bayes-PFL), which models the prompt space as a learnable probability distribution from a Bayesian perspective. Specifically, a prompt flow module is designed to learn both image-specific and image-agnostic distributions, which are jointly utilized to regularize the text prompt space and enhance the model's generalization on unseen categories. These learned distributions are then sampled to generate diverse text prompts, effectively covering the prompt space. Additionally, a residual cross-attention (RCA) module is introduced to better align dynamic text embeddings with fine-grained image features. Extensive experiments on 15 industrial and medical datasets demonstrate our method's superior performance.",
        "arxiv_id": "2503.10080",
        "ARXIVID": "2503.10080",
        "COMMENT": "Matches criterion 2 as it proposes a novel Bayesian Prompt Flow Learning method for zero-shot anomaly detection using vision-language models like CLIP.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.10500": {
        "authors": [
            "Jiali Yao",
            "Xinran Deng",
            "Xin Gu",
            "Mengrui Dai",
            "Bing Fan",
            "Zhipeng Zhang",
            "Yan Huang",
            "Heng Fan",
            "Libo Zhang"
        ],
        "title": "OmniSTVG: Toward Spatio-Temporal Omni-Object Video Grounding",
        "abstract": "arXiv:2503.10500v1 Announce Type: new  Abstract: In this paper, we propose spatio-temporal omni-object video grounding, dubbed OmniSTVG, a new STVG task that aims at localizing spatially and temporally all targets mentioned in the textual query from videos. Compared to classic STVG locating only a single target, OmniSTVG enables localization of not only an arbitrary number of text-referred targets but also their interacting counterparts in the query from the video, making it more flexible and practical in real scenarios for comprehensive understanding. In order to facilitate exploration of OmniSTVG, we introduce BOSTVG, a large-scale benchmark dedicated to OmniSTVG. Specifically, our BOSTVG consists of 10,018 videos with 10.2M frames and covers a wide selection of 287 classes from diverse scenarios. Each sequence in BOSTVG, paired with a free-form textual query, encompasses a varying number of targets ranging from 1 to 10. To ensure high quality, each video is manually annotated with meticulous inspection and refinement. To our best knowledge, BOSTVG is to date the first and the largest benchmark for OmniSTVG. To encourage future research, we introduce a simple yet effective approach, named OmniTube, which, drawing inspiration from Transformer-based STVG methods, is specially designed for OmniSTVG and demonstrates promising results. By releasing BOSTVG, we hope to go beyond classic STVG by locating every object appearing in the query for more comprehensive understanding, opening up a new direction for STVG. Our benchmark, model, and results will be released at https://github.com/JellyYao3000/OmniSTVG.",
        "arxiv_id": "2503.10500",
        "ARXIVID": "2503.10500",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (BOSTVG) for spatio-temporal omni-object video grounding, which is a novel angle in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.10200": {
        "authors": [
            "Boyu Chen",
            "Zhengrong Yue",
            "Siran Chen",
            "Zikang Wang",
            "Yang Liu",
            "Peng Li",
            "Yali Wang"
        ],
        "title": "LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration of MLLM Agents",
        "abstract": "arXiv:2503.10200v1 Announce Type: new  Abstract: Existing Multimodal Large Language Models (MLLMs) encounter significant challenges in modeling the temporal context within long videos. Currently, mainstream Agent-based methods use external tools (e.g., search engine, memory banks, OCR, retrieval models) to assist a single MLLM in answering long video questions. Despite such tool-based support, a solitary MLLM still offers only a partial understanding of long videos, resulting in limited performance. In order to better address long video tasks, we introduce LVAgent, the first framework enabling multi-round dynamic collaboration of MLLM agents in long video understanding. Our methodology consists of four key steps: 1. Selection: We pre-select appropriate agents from the model library to form optimal agent teams based on different tasks. 2. Perception: We design an effective retrieval scheme for long videos, improving the coverage of critical temporal segments while maintaining computational efficiency. 3. Action: Agents answer long video-related questions and exchange reasons. 4. Reflection: We evaluate the performance of each agent in each round of discussion and optimize the agent team for dynamic collaboration. The agents iteratively refine their answers by multi-round dynamical collaboration of MLLM agents. LVAgent is the first agent system method that outperforms all closed-source models (including GPT-4o) and open-source models (including InternVL-2.5 and Qwen2-VL) in the long video understanding tasks. Our LVAgent achieves an accuracy of 80% on four mainstream long video understanding tasks. Notably, on the LongVideoBench dataset, LVAgent improves accuracy by up to 14.3% compared with SOTA.",
        "arxiv_id": "2503.10200",
        "ARXIVID": "2503.10200",
        "COMMENT": "Matches criteria 2 as it introduces a novel framework for long video understanding using multimodal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.10291": {
        "authors": [
            "Weiyun Wang",
            "Zhangwei Gao",
            "Lianjie Chen",
            "Zhe Chen",
            "Jinguo Zhu",
            "Xiangyu Zhao",
            "Yangzhou Liu",
            "Yue Cao",
            "Shenglong Ye",
            "Xizhou Zhu",
            "Lewei Lu",
            "Haodong Duan",
            "Yu Qiao",
            "Jifeng Dai",
            "Wenhai Wang"
        ],
        "title": "VisualPRM: An Effective Process Reward Model for Multimodal Reasoning",
        "abstract": "arXiv:2503.10291v1 Announce Type: new  Abstract: We introduce VisualPRM, an advanced multimodal Process Reward Model (PRM) with 8B parameters, which improves the reasoning abilities of existing Multimodal Large Language Models (MLLMs) across different model scales and families with Best-of-N (BoN) evaluation strategies. Specifically, our model improves the reasoning performance of three types of MLLMs and four different model scales. Even when applied to the highly capable InternVL2.5-78B, it achieves a 5.9-point improvement across seven multimodal reasoning benchmarks. Experimental results show that our model exhibits superior performance compared to Outcome Reward Models and Self-Consistency during BoN evaluation. To facilitate the training of multimodal PRMs, we construct a multimodal process supervision dataset VisualPRM400K using an automated data pipeline. For the evaluation of multimodal PRMs, we propose VisualProcessBench, a benchmark with human-annotated step-wise correctness labels, to measure the abilities of PRMs to detect erroneous steps in multimodal reasoning tasks. We hope that our work can inspire more future research and contribute to the development of MLLMs. Our model, data, and benchmark are released in https://internvl.github.io/blog/2025-03-13-VisualPRM/.",
        "arxiv_id": "2503.10291",
        "ARXIVID": "2503.10291",
        "COMMENT": "Matches criteria 2 as it introduces a new multimodal large language model with improved reasoning capabilities.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.10639": {
        "authors": [
            "Rongyao Fang",
            "Chengqi Duan",
            "Kun Wang",
            "Linjiang Huang",
            "Hao Li",
            "Shilin Yan",
            "Hao Tian",
            "Xingyu Zeng",
            "Rui Zhao",
            "Jifeng Dai",
            "Xihui Liu",
            "Hongsheng Li"
        ],
        "title": "GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing",
        "abstract": "arXiv:2503.10639v1 Announce Type: new  Abstract: Current image generation and editing methods primarily process textual prompts as direct inputs without reasoning about visual composition and explicit operations. We present Generation Chain-of-Thought (GoT), a novel paradigm that enables generation and editing through an explicit language reasoning process before outputting images. This approach transforms conventional text-to-image generation and editing into a reasoning-guided framework that analyzes semantic relationships and spatial arrangements. We define the formulation of GoT and construct large-scale GoT datasets containing over 9M samples with detailed reasoning chains capturing semantic-spatial relationships. To leverage the advantages of GoT, we implement a unified framework that integrates Qwen2.5-VL for reasoning chain generation with an end-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance Module. Experiments show our GoT framework achieves excellent performance on both generation and editing tasks, with significant improvements over baselines. Additionally, our approach enables interactive visual generation, allowing users to explicitly modify reasoning steps for precise image adjustments. GoT pioneers a new direction for reasoning-driven visual generation and editing, producing images that better align with human intent. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/rongyaofang/GoT.",
        "arxiv_id": "2503.10639",
        "ARXIVID": "2503.10639",
        "COMMENT": "Matches criteria 2 as it introduces a novel reasoning-driven framework for visual generation and editing using a multimodal large language model.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.10265": {
        "authors": [
            "Chang Han Low",
            "Ziyue Wang",
            "Tianyi Zhang",
            "Zhitao Zeng",
            "Zhu Zhuo",
            "Evangelos B. Mazomenos",
            "Yueming Jin"
        ],
        "title": "SurgRAW: Multi-Agent Workflow with Chain-of-Thought Reasoning for Surgical Intelligence",
        "abstract": "arXiv:2503.10265v1 Announce Type: new  Abstract: Integration of Vision-Language Models (VLMs) in surgical intelligence is hindered by hallucinations, domain knowledge gaps, and limited understanding of task interdependencies within surgical scenes, undermining clinical reliability. While recent VLMs demonstrate strong general reasoning and thinking capabilities, they still lack the domain expertise and task-awareness required for precise surgical scene interpretation. Although Chain-of-Thought (CoT) can structure reasoning more effectively, current approaches rely on self-generated CoT steps, which often exacerbate inherent domain gaps and hallucinations. To overcome this, we present SurgRAW, a CoT-driven multi-agent framework that delivers transparent, interpretable insights for most tasks in robotic-assisted surgery. By employing specialized CoT prompts across five tasks: instrument recognition, action recognition, action prediction, patient data extraction, and outcome assessment, SurgRAW mitigates hallucinations through structured, domain-aware reasoning. Retrieval-Augmented Generation (RAG) is also integrated to external medical knowledge to bridge domain gaps and improve response reliability. Most importantly, a hierarchical agentic system ensures that CoT-embedded VLM agents collaborate effectively while understanding task interdependencies, with a panel discussion mechanism promotes logical consistency. To evaluate our method, we introduce SurgCoTBench, the first reasoning-based dataset with structured frame-level annotations. With comprehensive experiments, we demonstrate the effectiveness of proposed SurgRAW with 29.32% accuracy improvement over baseline VLMs on 12 robotic procedures, achieving the state-of-the-art performance and advancing explainable, trustworthy, and autonomous surgical assistance.",
        "arxiv_id": "2503.10265",
        "ARXIVID": "2503.10265",
        "COMMENT": "Matches criterion 2 as it focuses on Vision-Language Models (VLMs) and introduces a novel multi-agent framework for surgical intelligence.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.09938": {
        "authors": [
            "Sen Wang",
            "Dongliang Zhou",
            "Liang Xie",
            "Chao Xu",
            "Ye Yan",
            "Erwei Yin"
        ],
        "title": "PanoGen++: Domain-Adapted Text-Guided Panoramic Environment Generation for Vision-and-Language Navigation",
        "abstract": "arXiv:2503.09938v1 Announce Type: new  Abstract: Vision-and-language navigation (VLN) tasks require agents to navigate three-dimensional environments guided by natural language instructions, offering substantial potential for diverse applications. However, the scarcity of training data impedes progress in this field. This paper introduces PanoGen++, a novel framework that addresses this limitation by generating varied and pertinent panoramic environments for VLN tasks. PanoGen++ incorporates pre-trained diffusion models with domain-specific fine-tuning, employing parameter-efficient techniques such as low-rank adaptation to minimize computational costs. We investigate two settings for environment generation: masked image inpainting and recursive image outpainting. The former maximizes novel environment creation by inpainting masked regions based on textual descriptions, while the latter facilitates agents' learning of spatial relationships within panoramas. Empirical evaluations on room-to-room (R2R), room-for-room (R4R), and cooperative vision-and-dialog navigation (CVDN) datasets reveal significant performance enhancements: a 2.44% increase in success rate on the R2R test leaderboard, a 0.63% improvement on the R4R validation unseen set, and a 0.75-meter enhancement in goal progress on the CVDN validation unseen set. PanoGen++ augments the diversity and relevance of training environments, resulting in improved generalization and efficacy in VLN tasks.",
        "arxiv_id": "2503.09938",
        "ARXIVID": "2503.09938",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for generating panoramic environments for VLN tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.10634": {
        "authors": [
            "Yanming Zhang",
            "Jun-Kun Chen",
            "Jipeng Lyu",
            "Yu-Xiong Wang"
        ],
        "title": "V2Edit: Versatile Video Diffusion Editor for Videos and 3D Scenes",
        "abstract": "arXiv:2503.10634v1 Announce Type: new  Abstract: This paper introduces V$^2$Edit, a novel training-free framework for instruction-guided video and 3D scene editing. Addressing the critical challenge of balancing original content preservation with editing task fulfillment, our approach employs a progressive strategy that decomposes complex editing tasks into a sequence of simpler subtasks. Each subtask is controlled through three key synergistic mechanisms: the initial noise, noise added at each denoising step, and cross-attention maps between text prompts and video content. This ensures robust preservation of original video elements while effectively applying the desired edits. Beyond its native video editing capability, we extend V$^2$Edit to 3D scene editing via a \"render-edit-reconstruct\" process, enabling high-quality, 3D-consistent edits even for tasks involving substantial geometric changes such as object insertion. Extensive experiments demonstrate that our V$^2$Edit achieves high-quality and successful edits across various challenging video editing tasks and complex 3D scene editing tasks, thereby establishing state-of-the-art performance in both domains.",
        "arxiv_id": "2503.10634",
        "ARXIVID": "2503.10634",
        "COMMENT": "Matches criterion 4 as it introduces a novel framework for video and 3D scene editing using diffusion models, which is related to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.10225": {
        "authors": [
            "Zhixuan Li",
            "Hyunse Yoon",
            "Sanghoon Lee",
            "Weisi Lin"
        ],
        "title": "Unveiling the Invisible: Reasoning Complex Occlusions Amodally with AURA",
        "abstract": "arXiv:2503.10225v1 Announce Type: new  Abstract: Amodal segmentation aims to infer the complete shape of occluded objects, even when the occluded region's appearance is unavailable. However, current amodal segmentation methods lack the capability to interact with users through text input and struggle to understand or reason about implicit and complex purposes. While methods like LISA integrate multi-modal large language models (LLMs) with segmentation for reasoning tasks, they are limited to predicting only visible object regions and face challenges in handling complex occlusion scenarios. To address these limitations, we propose a novel task named amodal reasoning segmentation, aiming to predict the complete amodal shape of occluded objects while providing answers with elaborations based on user text input. We develop a generalizable dataset generation pipeline and introduce a new dataset focusing on daily life scenarios, encompassing diverse real-world occlusions. Furthermore, we present AURA (Amodal Understanding and Reasoning Assistant), a novel model with advanced global and spatial-level designs specifically tailored to handle complex occlusions. Extensive experiments validate AURA's effectiveness on the proposed dataset. The code, model, and dataset will be publicly released.",
        "arxiv_id": "2503.10225",
        "ARXIVID": "2503.10225",
        "COMMENT": "Matches criterion 2 as it integrates multi-modal large language models for amodal segmentation and reasoning.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2503.10586": {
        "authors": [
            "Chaoqun Wang",
            "Jie Yang",
            "Xiaobin Hong",
            "Ruimao Zhang"
        ],
        "title": "Unlock the Power of Unlabeled Data in Language Driving Model",
        "abstract": "arXiv:2503.10586v1 Announce Type: new  Abstract: Recent Vision-based Large Language Models~(VisionLLMs) for autonomous driving have seen rapid advancements. However, such promotion is extremely dependent on large-scale high-quality annotated data, which is costly and labor-intensive. To address this issue, we propose unlocking the value of abundant yet unlabeled data to improve the language-driving model in a semi-supervised learning manner. Specifically, we first introduce a series of template-based prompts to extract scene information, generating questions that create pseudo-answers for the unlabeled data based on a model trained with limited labeled data. Next, we propose a Self-Consistency Refinement method to improve the quality of these pseudo-annotations, which are later used for further training. By utilizing a pre-trained VisionLLM (e.g., InternVL), we build a strong Language Driving Model (LDM) for driving scene question-answering, outperforming previous state-of-the-art methods. Extensive experiments on the DriveLM benchmark show that our approach performs well with just 5% labeled data, achieving competitive performance against models trained with full datasets. In particular, our LDM achieves 44.85% performance with limited labeled data, increasing to 54.27% when using unlabeled data, while models trained with full datasets reach 60.68% on the DriveLM benchmark.",
        "arxiv_id": "2503.10586",
        "ARXIVID": "2503.10586",
        "COMMENT": "Matches criterion 2 as it focuses on VisionLLMs and semi-supervised learning for autonomous driving tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.10152": {
        "authors": [
            "Shenghao Fu",
            "Junkai Yan",
            "Qize Yang",
            "Xihan Wei",
            "Xiaohua Xie",
            "Wei-Shi Zheng"
        ],
        "title": "A Hierarchical Semantic Distillation Framework for Open-Vocabulary Object Detection",
        "abstract": "arXiv:2503.10152v1 Announce Type: new  Abstract: Open-vocabulary object detection (OVD) aims to detect objects beyond the training annotations, where detectors are usually aligned to a pre-trained vision-language model, eg, CLIP, to inherit its generalizable recognition ability so that detectors can recognize new or novel objects. However, previous works directly align the feature space with CLIP and fail to learn the semantic knowledge effectively. In this work, we propose a hierarchical semantic distillation framework named HD-OVD to construct a comprehensive distillation process, which exploits generalizable knowledge from the CLIP model in three aspects. In the first hierarchy of HD-OVD, the detector learns fine-grained instance-wise semantics from the CLIP image encoder by modeling relations among single objects in the visual space. Besides, we introduce text space novel-class-aware classification to help the detector assimilate the highly generalizable class-wise semantics from the CLIP text encoder, representing the second hierarchy. Lastly, abundant image-wise semantics containing multi-object and their contexts are also distilled by an image-wise contrastive distillation. Benefiting from the elaborated semantic distillation in triple hierarchies, our HD-OVD inherits generalizable recognition ability from CLIP in instance, class, and image levels. Thus, we boost the novel AP on the OV-COCO dataset to 46.4% with a ResNet50 backbone, which outperforms others by a clear margin. We also conduct extensive ablation studies to analyze how each component works.",
        "arxiv_id": "2503.10152",
        "ARXIVID": "2503.10152",
        "COMMENT": "Matches criterion 4 as it focuses on open-vocabulary object detection leveraging vision-language models like CLIP, which is related to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.10127": {
        "authors": [
            "Runze He",
            "Bo Cheng",
            "Yuhang Ma",
            "Qingxiang Jia",
            "Shanyuan Liu",
            "Ao Ma",
            "Xiaoyu Wu",
            "Liebucha Wu",
            "Dawei Leng",
            "Yuhui Yin"
        ],
        "title": "PlanGen: Towards Unified Layout Planning and Image Generation in Auto-Regressive Vision Language Models",
        "abstract": "arXiv:2503.10127v1 Announce Type: new  Abstract: In this paper, we propose a unified layout planning and image generation model, PlanGen, which can pre-plan spatial layout conditions before generating images. Unlike previous diffusion-based models that treat layout planning and layout-to-image as two separate models, PlanGen jointly models the two tasks into one autoregressive transformer using only next-token prediction. PlanGen integrates layout conditions into the model as context without requiring specialized encoding of local captions and bounding box coordinates, which provides significant advantages over the previous embed-and-pool operations on layout conditions, particularly when dealing with complex layouts. Unified prompting allows PlanGen to perform multitasking training related to layout, including layout planning, layout-to-image generation, image layout understanding, etc. In addition, PlanGen can be seamlessly expanded to layout-guided image manipulation thanks to the well-designed modeling, with teacher-forcing content manipulation policy and negative layout guidance. Extensive experiments verify the effectiveness of our PlanGen in multiple layoutrelated tasks, showing its great potential. Code is available at: https://360cvgroup.github.io/PlanGen.",
        "arxiv_id": "2503.10127",
        "ARXIVID": "2503.10127",
        "COMMENT": "Matches criteria 4 as it focuses on a unified layout planning and image generation model, which is related to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.10592": {
        "authors": [
            "Hao He",
            "Ceyuan Yang",
            "Shanchuan Lin",
            "Yinghao Xu",
            "Meng Wei",
            "Liangke Gui",
            "Qi Zhao",
            "Gordon Wetzstein",
            "Lu Jiang",
            "Hongsheng Li"
        ],
        "title": "CameraCtrl II: Dynamic Scene Exploration via Camera-controlled Video Diffusion Models",
        "abstract": "arXiv:2503.10592v1 Announce Type: new  Abstract: This paper introduces CameraCtrl II, a framework that enables large-scale dynamic scene exploration through a camera-controlled video diffusion model. Previous camera-conditioned video generative models suffer from diminished video dynamics and limited range of viewpoints when generating videos with large camera movement. We take an approach that progressively expands the generation of dynamic scenes -- first enhancing dynamic content within individual video clip, then extending this capability to create seamless explorations across broad viewpoint ranges. Specifically, we construct a dataset featuring a large degree of dynamics with camera parameter annotations for training while designing a lightweight camera injection module and training scheme to preserve dynamics of the pretrained models. Building on these improved single-clip techniques, we enable extended scene exploration by allowing users to iteratively specify camera trajectories for generating coherent video sequences. Experiments across diverse scenarios demonstrate that CameraCtrl Ii enables camera-controlled dynamic scene synthesis with substantially wider spatial exploration than previous approaches.",
        "arxiv_id": "2503.10592",
        "ARXIVID": "2503.10592",
        "COMMENT": "Matches criteria 4 as it focuses on vision foundation models and their application to dynamic scene exploration.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.09837": {
        "authors": [
            "Ahmad Mustafa Anis",
            "Hasnain Ali",
            "Saquib Sarfraz"
        ],
        "title": "On the Limitations of Vision-Language Models in Understanding Image Transforms",
        "abstract": "arXiv:2503.09837v1 Announce Type: new  Abstract: Vision Language Models (VLMs) have demonstrated significant potential in various downstream tasks, including Image/Video Generation, Visual Question Answering, Multimodal Chatbots, and Video Understanding. However, these models often struggle with basic image transformations. This paper investigates the image-level understanding of VLMs, specifically CLIP by OpenAI and SigLIP by Google. Our findings reveal that these models lack comprehension of multiple image-level augmentations. To facilitate this study, we created an augmented version of the Flickr8k dataset, pairing each image with a detailed description of the applied transformation. We further explore how this deficiency impacts downstream tasks, particularly in image editing, and evaluate the performance of state-of-the-art Image2Image models on simple transformations.",
        "arxiv_id": "2503.09837",
        "ARXIVID": "2503.09837",
        "COMMENT": "Matches criterion 2 as it evaluates limitations of vision-language models (VLMs) in understanding image transforms.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.09962": {
        "authors": [
            "Jiayu Jiang",
            "Changxing Ding",
            "Wentao Tan",
            "Junhong Wang",
            "Jin Tao",
            "Xiangmin Xu"
        ],
        "title": "Modeling Thousands of Human Annotators for Generalizable Text-to-Image Person Re-identification",
        "abstract": "arXiv:2503.09962v1 Announce Type: new  Abstract: Text-to-image person re-identification (ReID) aims to retrieve the images of an interested person based on textual descriptions. One main challenge for this task is the high cost in manually annotating large-scale databases, which affects the generalization ability of ReID models. Recent works handle this problem by leveraging Multi-modal Large Language Models (MLLMs) to describe pedestrian images automatically. However, the captions produced by MLLMs lack diversity in description styles. To address this issue, we propose a Human Annotator Modeling (HAM) approach to enable MLLMs to mimic the description styles of thousands of human annotators. Specifically, we first extract style features from human textual descriptions and perform clustering on them. This allows us to group textual descriptions with similar styles into the same cluster. Then, we employ a prompt to represent each of these clusters and apply prompt learning to mimic the description styles of different human annotators. Furthermore, we define a style feature space and perform uniform sampling in this space to obtain more diverse clustering prototypes, which further enriches the diversity of the MLLM-generated captions. Finally, we adopt HAM to automatically annotate a massive-scale database for text-to-image ReID. Extensive experiments on this database demonstrate that it significantly improves the generalization ability of ReID models.",
        "arxiv_id": "2503.09962",
        "ARXIVID": "2503.09962",
        "COMMENT": "Matches criterion 2 as it leverages multi-modal large language models for text-to-image person re-identification.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.10125": {
        "authors": [
            "Yi Wu",
            "Lingting Zhu",
            "Lei Liu",
            "Wandi Qiao",
            "Ziqiang Li",
            "Lequan Yu",
            "Bin Li"
        ],
        "title": "Proxy-Tuning: Tailoring Multimodal Autoregressive Models for Subject-Driven Image Generation",
        "abstract": "arXiv:2503.10125v1 Announce Type: new  Abstract: Multimodal autoregressive (AR) models, based on next-token prediction and transformer architecture, have demonstrated remarkable capabilities in various multimodal tasks including text-to-image (T2I) generation. Despite their strong performance in general T2I tasks, our research reveals that these models initially struggle with subject-driven image generation compared to dominant diffusion models. To address this limitation, we introduce Proxy-Tuning, leveraging diffusion models to enhance AR models' capabilities in subject-specific image generation. Our method reveals a striking weak-to-strong phenomenon: fine-tuned AR models consistently outperform their diffusion model supervisors in both subject fidelity and prompt adherence. We analyze this performance shift and identify scenarios where AR models excel, particularly in multi-subject compositions and contextual understanding. This work not only demonstrates impressive results in subject-driven AR image generation, but also unveils the potential of weak-to-strong generalization in the image generation domain, contributing to a deeper understanding of different architectures' strengths and limitations.",
        "arxiv_id": "2503.10125",
        "ARXIVID": "2503.10125",
        "COMMENT": "Matches criterion 2 as it discusses multimodal autoregressive models and their improvements for subject-driven image generation.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.09826": {
        "authors": [
            "Wenyi Lian",
            "Joakim Lindblad",
            "Patrick Micke",
            "Nata\\v{s}a Sladoje"
        ],
        "title": "Isolated Channel Vision Transformers: From Single-Channel Pretraining to Multi-Channel Finetuning",
        "abstract": "arXiv:2503.09826v1 Announce Type: new  Abstract: Vision Transformers (ViTs) have achieved remarkable success in standard RGB image processing tasks. However, applying ViTs to multi-channel imaging (MCI) data, e.g., for medical and remote sensing applications, remains a challenge. In particular, MCI data often consist of layers acquired from different modalities. Directly training ViTs on such data can obscure complementary information and impair the performance. In this paper, we introduce a simple yet effective pretraining framework for large-scale MCI datasets. Our method, named Isolated Channel ViT (IC-ViT), patchifies image channels individually and thereby enables pretraining for multimodal multi-channel tasks. We show that this channel-wise patchifying is a key technique for MCI processing. More importantly, one can pretrain the IC-ViT on single channels and finetune it on downstream multi-channel datasets. This pretraining framework captures dependencies between patches as well as channels and produces robust feature representation. Experiments on various tasks and benchmarks, including JUMP-CP and CHAMMI for cell microscopy imaging, and So2Sat-LCZ42 for satellite imaging, show that the proposed IC-ViT delivers 4-14 percentage points of performance improvement over existing channel-adaptive approaches. Further, its efficient training makes it a suitable candidate for large-scale pretraining of foundation models on heterogeneous data.",
        "arxiv_id": "2503.09826",
        "ARXIVID": "2503.09826",
        "COMMENT": "Relevant to vision foundation models (criterion 4) as it proposes a novel pretraining framework for multi-channel imaging using Vision Transformers.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.10630": {
        "authors": [
            "Hang Yin",
            "Xiuwei Xu",
            "Lingqing Zhao",
            "Ziwei Wang",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "title": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation",
        "abstract": "arXiv:2503.10630v1 Announce Type: new  Abstract: In this paper, we propose a general framework for universal zero-shot goal-oriented navigation. Existing zero-shot methods build inference framework upon large language models (LLM) for specific tasks, which differs a lot in overall pipeline and fails to generalize across different types of goal. Towards the aim of universal zero-shot navigation, we propose a uniform graph representation to unify different goals, including object category, instance image and text description. We also convert the observation of agent into an online maintained scene graph. With this consistent scene and goal representation, we preserve most structural information compared with pure text and are able to leverage LLM for explicit graph-based reasoning. Specifically, we conduct graph matching between the scene graph and goal graph at each time instant and propose different strategies to generate long-term goal of exploration according to different matching states. The agent first iteratively searches subgraph of goal when zero-matched. With partial matching, the agent then utilizes coordinate projection and anchor pair alignment to infer the goal location. Finally scene graph correction and goal verification are applied for perfect matching. We also present a blacklist mechanism to enable robust switch between stages. Extensive experiments on several benchmarks show that our UniGoal achieves state-of-the-art zero-shot performance on three studied navigation tasks with a single model, even outperforming task-specific zero-shot methods and supervised universal methods.",
        "arxiv_id": "2503.10630",
        "ARXIVID": "2503.10630",
        "COMMENT": "Matches criterion 3 as it proposes a new framework for embodied AI navigation with a novel graph-based reasoning approach.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.10392": {
        "authors": [
            "Fengxiang Wang",
            "Hongzhen Wang",
            "Yulin Wang",
            "Di Wang",
            "Mingshuo Chen",
            "Haiyan Zhao",
            "Yangang Sun",
            "Shuo Wang",
            "Long Lan",
            "Wenjing Yang",
            "Jing Zhang"
        ],
        "title": "RoMA: Scaling up Mamba-based Foundation Models for Remote Sensing",
        "abstract": "arXiv:2503.10392v1 Announce Type: new  Abstract: Recent advances in self-supervised learning for Vision Transformers (ViTs) have fueled breakthroughs in remote sensing (RS) foundation models. However, the quadratic complexity of self-attention poses a significant barrier to scalability, particularly for large models and high-resolution images. While the linear-complexity Mamba architecture offers a promising alternative, existing RS applications of Mamba remain limited to supervised tasks on small, domain-specific datasets. To address these challenges, we propose RoMA, a framework that enables scalable self-supervised pretraining of Mamba-based RS foundation models using large-scale, diverse, unlabeled data. RoMA enhances scalability for high-resolution images through a tailored auto-regressive learning strategy, incorporating two key innovations: 1) a rotation-aware pretraining mechanism combining adaptive cropping with angular embeddings to handle sparsely distributed objects with arbitrary orientations, and 2) multi-scale token prediction objectives that address the extreme variations in object scales inherent to RS imagery. Systematic empirical studies validate that Mamba adheres to RS data and parameter scaling laws, with performance scaling reliably as model and data size increase. Furthermore, experiments across scene classification, object detection, and semantic segmentation tasks demonstrate that RoMA-pretrained Mamba models consistently outperform ViT-based counterparts in both accuracy and computational efficiency. The source code and pretrained models will be released at https://github.com/MiliLab/RoMA.",
        "arxiv_id": "2503.10392",
        "ARXIVID": "2503.10392",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models for remote sensing applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.10579": {
        "authors": [
            "Chaoqun Wang",
            "Xiaobin Hong",
            "Wenzhong Li",
            "Ruimao Zhang"
        ],
        "title": "Semantic-Supervised Spatial-Temporal Fusion for LiDAR-based 3D Object Detection",
        "abstract": "arXiv:2503.10579v1 Announce Type: new  Abstract: LiDAR-based 3D object detection presents significant challenges due to the inherent sparsity of LiDAR points. A common solution involves long-term temporal LiDAR data to densify the inputs. However, efficiently leveraging spatial-temporal information remains an open problem. In this paper, we propose a novel Semantic-Supervised Spatial-Temporal Fusion (ST-Fusion) method, which introduces a novel fusion module to relieve the spatial misalignment caused by the object motion over time and a feature-level semantic supervision to sufficiently unlock the capacity of the proposed fusion module. Specifically, the ST-Fusion consists of a Spatial Aggregation (SA) module and a Temporal Merging (TM) module. The SA module employs a convolutional layer with progressively expanding receptive fields to aggregate the object features from the local regions to alleviate the spatial misalignment, the TM module dynamically extracts object features from the preceding frames based on the attention mechanism for a comprehensive sequential presentation. Besides, in the semantic supervision, we propose a Semantic Injection method to enrich the sparse LiDAR data via injecting the point-wise semantic labels, using it for training a teacher model and providing a reconstruction target at the feature level supervised by the proposed object-aware loss. Extensive experiments on various LiDAR-based detectors demonstrate the effectiveness and universality of our proposal, yielding an improvement of approximately +2.8% in NDS based on the nuScenes benchmark.",
        "arxiv_id": "2503.10579",
        "ARXIVID": "2503.10579",
        "COMMENT": "Matches criterion 1 as it proposes a novel spatial-temporal fusion method for LiDAR-based 3D object detection, which involves spatial understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.10616": {
        "authors": [
            "Jinyang Li",
            "En Yu",
            "Sijia Chen",
            "Wenbing Tao"
        ],
        "title": "OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with Transformer",
        "abstract": "arXiv:2503.10616v1 Announce Type: new  Abstract: Open-vocabulary multiple object tracking aims to generalize trackers to unseen categories during training, enabling their application across a variety of real-world scenarios. However, the existing open-vocabulary tracker is constrained by its framework structure, isolated frame-level perception, and insufficient modal interactions, which hinder its performance in open-vocabulary classification and tracking. In this paper, we propose OVTR (End-to-End Open-Vocabulary Multiple Object Tracking with TRansformer), the first end-to-end open-vocabulary tracker that models motion, appearance, and category simultaneously. To achieve stable classification and continuous tracking, we design the CIP (Category Information Propagation) strategy, which establishes multiple high-level category information priors for subsequent frames. Additionally, we introduce a dual-branch structure for generalization capability and deep multimodal interaction, and incorporate protective strategies in the decoder to enhance performance. Experimental results show that our method surpasses previous trackers on the open-vocabulary MOT benchmark while also achieving faster inference speeds and significantly reducing preprocessing requirements. Moreover, the experiment transferring the model to another dataset demonstrates its strong adaptability. Models and code are released at https://github.com/jinyanglii/OVTR.",
        "arxiv_id": "2503.10616",
        "ARXIVID": "2503.10616",
        "COMMENT": "Matches criterion 3 as it introduces an end-to-end open-vocabulary multiple object tracking framework, focusing on novel methods for tracking and classification.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.10212": {
        "authors": [
            "Teng Xu",
            "Taotao Zhou",
            "Youjia Wang",
            "Peng Yang",
            "Simin Tang",
            "Kuixiang Shao",
            "Zifeng Tang",
            "Yifei Liu",
            "Xinyuan Chen",
            "Hongshuang Wang",
            "Xiaohui Wang",
            "Huoqing Luo",
            "Jingya Wang",
            "Ji Hu",
            "Jingyi Yu"
        ],
        "title": "MouseGPT: A Large-scale Vision-Language Model for Mouse Behavior Analysis",
        "abstract": "arXiv:2503.10212v1 Announce Type: new  Abstract: Analyzing animal behavior is crucial in advancing neuroscience, yet quantifying and deciphering its intricate dynamics remains a significant challenge. Traditional machine vision approaches, despite their ability to detect spontaneous behaviors, fall short due to limited interpretability and reliance on manual labeling, which restricts the exploration of the full behavioral spectrum. Here, we introduce MouseGPT, a Vision-Language Model (VLM) that integrates visual cues with natural language to revolutionize mouse behavior analysis. Built upon our first-of-its-kind dataset - incorporating pose dynamics and open-vocabulary behavioral annotations across over 42 million frames of diverse psychiatric conditions - MouseGPT provides a novel, context-rich method for comprehensive behavior interpretation. Our holistic analysis framework enables detailed behavior profiling, clustering, and novel behavior discovery, offering deep insights without the need for labor - intensive manual annotation. Evaluations reveal that MouseGPT surpasses existing models in precision, adaptability, and descriptive richness, positioning it as a transformative tool for ethology and for unraveling complex behavioral dynamics in animal models.",
        "arxiv_id": "2503.10212",
        "ARXIVID": "2503.10212",
        "COMMENT": "Matches criterion 2 as it introduces a vision-language model (MouseGPT) for mouse behavior analysis, which is a novel application of VLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.10627": {
        "authors": [
            "Ziyu Guo",
            "Ray Zhang",
            "Hao Chen",
            "Jialin Gao",
            "Dongzhi Jiang",
            "Jiaze Wang",
            "Pheng-Ann Heng"
        ],
        "title": "SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems",
        "abstract": "arXiv:2503.10627v1 Announce Type: new  Abstract: The rapid advancement of Large Multi-modal Models (LMMs) has enabled their application in scientific problem-solving, yet their fine-grained capabilities remain under-explored. In this paper, we introduce SciVerse, a multi-modal scientific evaluation benchmark to thoroughly assess LMMs across 5,735 test instances in five distinct versions. We aim to investigate three key dimensions of LMMs: scientific knowledge comprehension, multi-modal content interpretation, and Chain-of-Thought (CoT) reasoning. To unveil whether LMMs possess sufficient scientific expertise, we first transform each problem into three versions containing different levels of knowledge required for solving, i.e., Knowledge-free, -lite, and -rich. Then, to explore how LMMs interpret multi-modal scientific content, we annotate another two versions, i.e., Vision-rich and -only, marking more question information from texts to diagrams. Comparing the results of different versions, SciVerse systematically examines the professional knowledge stock and visual perception skills of LMMs in scientific domains. In addition, to rigorously assess CoT reasoning, we propose a new scientific CoT evaluation strategy, conducting a step-wise assessment on knowledge and logical errors in model outputs. Our extensive evaluation of different LMMs on SciVerse reveals critical limitations in their scientific proficiency and provides new insights into future developments. Project page: https://sciverse-cuhk.github.io",
        "arxiv_id": "2503.10627",
        "ARXIVID": "2503.10627",
        "COMMENT": "Matches criterion 2 as it evaluates LMMs on scientific problems, focusing on multi-modal reasoning and knowledge comprehension.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.10621": {
        "authors": [
            "Ayesha Ishaq",
            "Jean Lahoud",
            "Ketan More",
            "Omkar Thawakar",
            "Ritesh Thawkar",
            "Dinura Dissanayake",
            "Noor Ahsan",
            "Yuhao Li",
            "Fahad Shahbaz Khan",
            "Hisham Cholakkal",
            "Ivan Laptev",
            "Rao Muhammad Anwer",
            "Salman Khan"
        ],
        "title": "DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model for Driving Scenario Understanding",
        "abstract": "arXiv:2503.10621v1 Announce Type: new  Abstract: While large multimodal models (LMMs) have demonstrated strong performance across various Visual Question Answering (VQA) tasks, certain challenges require complex multi-step reasoning to reach accurate answers. One particularly challenging task is autonomous driving, which demands thorough cognitive processing before decisions can be made. In this domain, a sequential and interpretive understanding of visual cues is essential for effective perception, prediction, and planning. Nevertheless, common VQA benchmarks often focus on the accuracy of the final answer while overlooking the reasoning process that enables the generation of accurate responses. Moreover, existing methods lack a comprehensive framework for evaluating step-by-step reasoning in realistic driving scenarios. To address this gap, we propose DriveLMM-o1, a new dataset and benchmark specifically designed to advance step-wise visual reasoning for autonomous driving. Our benchmark features over 18k VQA examples in the training set and more than 4k in the test set, covering diverse questions on perception, prediction, and planning, each enriched with step-by-step reasoning to ensure logical inference in autonomous driving scenarios. We further introduce a large multimodal model that is fine-tuned on our reasoning dataset, demonstrating robust performance in complex driving scenarios. In addition, we benchmark various open-source and closed-source methods on our proposed dataset, systematically comparing their reasoning capabilities for autonomous driving tasks. Our model achieves a +7.49% gain in final answer accuracy, along with a 3.62% improvement in reasoning score over the previous best open-source model. Our framework, dataset, and model are available at https://github.com/ayesha-ishaq/DriveLMM-o1.",
        "arxiv_id": "2503.10621",
        "ARXIVID": "2503.10621",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark and dataset for reasoning in autonomous driving scenarios, focusing on step-by-step reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.10596": {
        "authors": [
            "Rui Hu",
            "Lianghui Zhu",
            "Yuxuan Zhang",
            "Tianheng Cheng",
            "Lei Liu",
            "Heng Liu",
            "Longjin Ran",
            "Xiaoxin Chen",
            "Wenyu Liu",
            "Xinggang Wang"
        ],
        "title": "GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding",
        "abstract": "arXiv:2503.10596v1 Announce Type: new  Abstract: Pixel grounding, encompassing tasks such as Referring Expression Segmentation (RES), has garnered considerable attention due to its immense potential for bridging the gap between vision and language modalities. However, advancements in this domain are currently constrained by limitations inherent in existing datasets, including limited object categories, insufficient textual diversity, and a scarcity of high-quality annotations. To mitigate these limitations, we introduce GroundingSuite, which comprises: (1) an automated data annotation framework leveraging multiple Vision-Language Model (VLM) agents; (2) a large-scale training dataset encompassing 9.56 million diverse referring expressions and their corresponding segmentations; and (3) a meticulously curated evaluation benchmark consisting of 3,800 images. The GroundingSuite training dataset facilitates substantial performance improvements, enabling models trained on it to achieve state-of-the-art results. Specifically, a cIoU of 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the GroundingSuite annotation framework demonstrates superior efficiency compared to the current leading data annotation method, i.e., $4.5 \\times$ faster than the GLaMM.",
        "arxiv_id": "2503.10596",
        "ARXIVID": "2503.10596",
        "COMMENT": "Matches criterion 4 as it introduces a new dataset and benchmark for pixel grounding, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.09949": {
        "authors": [
            "Yuanxin Liu",
            "Rui Zhu",
            "Shuhuai Ren",
            "Jiacong Wang",
            "Haoyuan Guo",
            "Xu Sun",
            "Lu Jiang"
        ],
        "title": "UVE: Are MLLMs Unified Evaluators for AI-Generated Videos?",
        "abstract": "arXiv:2503.09949v1 Announce Type: new  Abstract: With the rapid growth of video generative models (VGMs), it is essential to develop reliable and comprehensive automatic metrics for AI-generated videos (AIGVs). Existing methods either use off-the-shelf models optimized for other tasks or rely on human assessment data to train specialized evaluators. These approaches are constrained to specific evaluation aspects and are difficult to scale with the increasing demands for finer-grained and more comprehensive evaluations. To address this issue, this work investigates the feasibility of using multimodal large language models (MLLMs) as a unified evaluator for AIGVs, leveraging their strong visual perception and language understanding capabilities. To evaluate the performance of automatic metrics in unified AIGV evaluation, we introduce a benchmark called UVE-Bench. UVE-Bench collects videos generated by state-of-the-art VGMs and provides pairwise human preference annotations across 15 evaluation aspects. Using UVE-Bench, we extensively evaluate 16 MLLMs. Our empirical results suggest that while advanced MLLMs (e.g., Qwen2VL-72B and InternVL2.5-78B) still lag behind human evaluators, they demonstrate promising ability in unified AIGV evaluation, significantly surpassing existing specialized evaluation methods. Additionally, we conduct an in-depth analysis of key design choices that impact the performance of MLLM-driven evaluators, offering valuable insights for future research on AIGV evaluation. The code is available at https://github.com/bytedance/UVE.",
        "arxiv_id": "2503.09949",
        "ARXIVID": "2503.09949",
        "COMMENT": "Matches criterion 2 as it evaluates MLLMs for video generation, focusing on their unified evaluation capabilities.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.10618": {
        "authors": [
            "Chen Chen",
            "Rui Qian",
            "Wenze Hu",
            "Tsu-Jui Fu",
            "Lezhi Li",
            "Bowen Zhang",
            "Alex Schwing",
            "Wei Liu",
            "Yinfei Yang"
        ],
        "title": "DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture Design in Text to Image Generation",
        "abstract": "arXiv:2503.10618v1 Announce Type: new  Abstract: In this work, we empirically study Diffusion Transformers (DiTs) for text-to-image generation, focusing on architectural choices, text-conditioning strategies, and training protocols. We evaluate a range of DiT-based architectures--including PixArt-style and MMDiT variants--and compare them with a standard DiT variant which directly processes concatenated text and noise inputs. Surprisingly, our findings reveal that the performance of standard DiT is comparable with those specialized models, while demonstrating superior parameter-efficiency, especially when scaled up. Leveraging the layer-wise parameter sharing strategy, we achieve a further reduction of 66% in model size compared to an MMDiT architecture, with minimal performance impact. Building on an in-depth analysis of critical components such as text encoders and Variational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With supervised and reward fine-tuning, DiT-Air achieves state-of-the-art performance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly competitive, surpassing most existing models despite its compact size.",
        "arxiv_id": "2503.10618",
        "ARXIVID": "2503.10618",
        "COMMENT": "Matches criterion 4 as it discusses architectural improvements in diffusion models for text-to-image generation, which aligns with vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.10000": {
        "authors": [
            "Shu Wang",
            "Yanbo Gao",
            "Shuai Li",
            "Chong Lv",
            "Xun Cai",
            "Chuankun Li",
            "Hui Yuan",
            "Jinglin Zhang"
        ],
        "title": "MetricGrids: Arbitrary Nonlinear Approximation with Elementary Metric Grids based Implicit Neural Representation",
        "abstract": "arXiv:2503.10000v1 Announce Type: new  Abstract: This paper presents MetricGrids, a novel grid-based neural representation that combines elementary metric grids in various metric spaces to approximate complex nonlinear signals. While grid-based representations are widely adopted for their efficiency and scalability, the existing feature grids with linear indexing for continuous-space points can only provide degenerate linear latent space representations, and such representations cannot be adequately compensated to represent complex nonlinear signals by the following compact decoder. To address this problem while keeping the simplicity of a regular grid structure, our approach builds upon the standard grid-based paradigm by constructing multiple elementary metric grids as high-order terms to approximate complex nonlinearities, following the Taylor expansion principle. Furthermore, we enhance model compactness with hash encoding based on different sparsities of the grids to prevent detrimental hash collisions, and a high-order extrapolation decoder to reduce explicit grid storage requirements. experimental results on both 2D and 3D reconstructions demonstrate the superior fitting and rendering accuracy of the proposed method across diverse signal types, validating its robustness and generalizability. Code is available at https://github.com/wangshu31/MetricGrids}{https://github.com/wangshu31/MetricGrids.",
        "arxiv_id": "2503.10000",
        "ARXIVID": "2503.10000",
        "COMMENT": "Does not match any specific criteria but proposes a novel grid-based neural representation, which might be tangentially interesting for spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.10365": {
        "authors": [
            "Elad Richardson",
            "Kfir Goldberg",
            "Yuval Alaluf",
            "Daniel Cohen-Or"
        ],
        "title": "Piece it Together: Part-Based Concepting with IP-Priors",
        "abstract": "arXiv:2503.10365v1 Announce Type: new  Abstract: Advanced generative models excel at synthesizing images but often rely on text-based conditioning. Visual designers, however, often work beyond language, directly drawing inspiration from existing visual elements. In many cases, these elements represent only fragments of a potential concept-such as an uniquely structured wing, or a specific hairstyle-serving as inspiration for the artist to explore how they can come together creatively into a coherent whole. Recognizing this need, we introduce a generative framework that seamlessly integrates a partial set of user-provided visual components into a coherent composition while simultaneously sampling the missing parts needed to generate a plausible and complete concept. Our approach builds on a strong and underexplored representation space, extracted from IP-Adapter+, on which we train IP-Prior, a lightweight flow-matching model that synthesizes coherent compositions based on domain-specific priors, enabling diverse and context-aware generations. Additionally, we present a LoRA-based fine-tuning strategy that significantly improves prompt adherence in IP-Adapter+ for a given task, addressing its common trade-off between reconstruction quality and prompt adherence.",
        "arxiv_id": "2503.10365",
        "ARXIVID": "2503.10365",
        "COMMENT": "Does not match any specific criteria but introduces a novel generative framework for visual composition, which could be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.10096": {
        "authors": [
            "Qiyuan Zhang",
            "Chenyu Wu",
            "Wenzhang Sun",
            "Huaize Liu",
            "Donglin Di",
            "Wei Chen",
            "Changqing Zou"
        ],
        "title": "Semantic Latent Motion for Portrait Video Generation",
        "abstract": "arXiv:2503.10096v1 Announce Type: new  Abstract: Recent advancements in portrait video generation have been noteworthy. However, existing methods rely heavily on human priors and pre-trained generation models, which may introduce unrealistic motion and lead to inefficient inference. To address these challenges, we propose Semantic Latent Motion (SeMo), a compact and expressive motion representation. Leveraging this representation, our approach achieve both high-quality visual results and efficient inference. SeMo follows an effective three-step framework: Abstraction, Reasoning, and Generation. First, in the Abstraction step, we use a carefully designed Mask Motion Encoder to compress the subject's motion state into a compact and abstract latent motion (1D token). Second, in the Reasoning step, long-term modeling and efficient reasoning are performed in this latent space to generate motion sequences. Finally, in the Generation step, the motion dynamics serve as conditional information to guide the generation model in synthesizing realistic transitions from reference frames to target frames. Thanks to the compact and descriptive nature of Semantic Latent Motion, our method enables real-time video generation with highly realistic motion. User studies demonstrate that our approach surpasses state-of-the-art models with an 81% win rate in realism. Extensive experiments further highlight its strong compression capability, reconstruction quality, and generative potential. Moreover, its fully self-supervised nature suggests promising applications in broader video generation tasks.",
        "arxiv_id": "2503.10096",
        "ARXIVID": "2503.10096",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling for video generation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.10568": {
        "authors": [
            "Haopeng Li",
            "Jinyue Yang",
            "Guoqi Li",
            "Huan Wang"
        ],
        "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
        "abstract": "arXiv:2503.10568v1 Announce Type: new  Abstract: We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose a novel guided decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mechanism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using a shared KV cache. On the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only 64 sampling steps, achieving over a 20-fold increase in throughput while reducing memory consumption by over 75% compared to representative recent autoregressive models at a similar scale.",
        "arxiv_id": "2503.10568",
        "ARXIVID": "2503.10568",
        "COMMENT": "Does not match any specific criteria but is relevant to the general interest area of generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.10111": {
        "authors": [
            "Zecheng Zhao",
            "Zhi Chen",
            "Zi Huang",
            "Shazia Sadiq",
            "Tong Chen"
        ],
        "title": "StableFusion: Continual Video Retrieval via Frame Adaptation",
        "abstract": "arXiv:2503.10111v1 Announce Type: new  Abstract: Text-to-Video Retrieval (TVR) aims to match videos with corresponding textual queries, yet the continual influx of new video content poses a significant challenge for maintaining system performance over time. In this work, we introduce the first benchmark for Continual Text-to-Video Retrieval (CTVR) to overcome these limitations. Our analysis reveals that current TVR methods based on pre-trained models struggle to retain plasticity when adapting to new tasks, while existing continual learning approaches experience catastrophic forgetting, resulting in semantic misalignment between historical queries and stored video features. To address these challenges, we propose StableFusion, a novel CTVR framework comprising two main components: the Frame Fusion Adapter (FFA), which captures temporal dynamics in video content while preserving model flexibility, and the Task-Aware Mixture-of-Experts (TAME), which maintains consistent semantic alignment between queries across tasks and the stored video features. Comprehensive evaluations on two benchmark datasets under various task settings demonstrate that StableFusion outperforms existing continual learning and TVR methods, achieving superior retrieval performance with minimal degradation on earlier tasks in the context of continuous video streams. Our code is available at: https://github.com/JasonCodeMaker/CTVR",
        "arxiv_id": "2503.10111",
        "ARXIVID": "2503.10111",
        "COMMENT": "Does not match any specific criterion but introduces a novel framework for continual video retrieval, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.10403": {
        "authors": [
            "Jingyu Guo",
            "Sensen Gao",
            "Jia-Wang Bian",
            "Wanhu Sun",
            "Heliang Zheng",
            "Rongfei Jia",
            "Mingming Gong"
        ],
        "title": "Hyper3D: Efficient 3D Representation via Hybrid Triplane and Octree Feature for Enhanced 3D Shape Variational Auto-Encoders",
        "abstract": "arXiv:2503.10403v1 Announce Type: new  Abstract: Recent 3D content generation pipelines often leverage Variational Autoencoders (VAEs) to encode shapes into compact latent representations, facilitating diffusion-based generation. Efficiently compressing 3D shapes while preserving intricate geometric details remains a key challenge. Existing 3D shape VAEs often employ uniform point sampling and 1D/2D latent representations, such as vector sets or triplanes, leading to significant geometric detail loss due to inadequate surface coverage and the absence of explicit 3D representations in the latent space. Although recent work explores 3D latent representations, their large scale hinders high-resolution encoding and efficient training. Given these challenges, we introduce Hyper3D, which enhances VAE reconstruction through efficient 3D representation that integrates hybrid triplane and octree features. First, we adopt an octree-based feature representation to embed mesh information into the network, mitigating the limitations of uniform point sampling in capturing geometric distributions along the mesh surface. Furthermore, we propose a hybrid latent space representation that integrates a high-resolution triplane with a low-resolution 3D grid. This design not only compensates for the lack of explicit 3D representations but also leverages a triplane to preserve high-resolution details. Experimental results demonstrate that Hyper3D outperforms traditional representations by reconstructing 3D shapes with higher fidelity and finer details, making it well-suited for 3D generation pipelines.",
        "arxiv_id": "2503.10403",
        "ARXIVID": "2503.10403",
        "COMMENT": "Does not match any specific criterion but is relevant to 3D representation and generative modeling, which aligns with your friend's general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.10259": {
        "authors": [
            "Yunpeng Qu",
            "Kun Yuan",
            "Qizhi Xie",
            "Ming Sun",
            "Chao Zhou",
            "Jian Wang"
        ],
        "title": "KVQ: Boosting Video Quality Assessment via Saliency-guided Local Perception",
        "abstract": "arXiv:2503.10259v1 Announce Type: new  Abstract: Video Quality Assessment (VQA), which intends to predict the perceptual quality of videos, has attracted increasing attention. Due to factors like motion blur or specific distortions, the quality of different regions in a video varies. Recognizing the region-wise local quality within a video is beneficial for assessing global quality and can guide us in adopting fine-grained enhancement or transcoding strategies. Due to the heavy cost of annotating region-wise quality, the lack of ground truth constraints from relevant datasets further complicates the utilization of local perception. Inspired by the Human Visual System (HVS) that links global quality to the local texture of different regions and their visual saliency, we propose a Kaleidoscope Video Quality Assessment (KVQ) framework, which aims to effectively assess both saliency and local texture, thereby facilitating the assessment of global quality. Our framework extracts visual saliency and allocates attention using Fusion-Window Attention (FWA) while incorporating a Local Perception Constraint (LPC) to mitigate the reliance of regional texture perception on neighboring areas. KVQ obtains significant improvements across multiple scenarios on five VQA benchmarks compared to SOTA methods. Furthermore, to assess local perception, we establish a new Local Perception Visual Quality (LPVQ) dataset with region-wise annotations. Experimental results demonstrate the capability of KVQ in perceiving local distortions. KVQ models and the LPVQ dataset will be available at https://github.com/qyp2000/KVQ.",
        "arxiv_id": "2503.10259",
        "ARXIVID": "2503.10259",
        "COMMENT": "Does not match any specific criteria but proposes a novel framework for video quality assessment, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.10638": {
        "authors": [
            "Xiaoming Zhao",
            "Alexander G. Schwing"
        ],
        "title": "Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective",
        "abstract": "arXiv:2503.10638v1 Announce Type: new  Abstract: Classifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance. Concretely, instead of solely focusing on classifier-free guidance, we trace back to the root, i.e., classifier guidance, pinpoint the key assumption for the derivation, and conduct a systematic study to understand the role of the classifier. We find that both classifier guidance and classifier-free guidance achieve conditional generation by pushing the denoising diffusion trajectories away from decision boundaries, i.e., areas where conditional information is usually entangled and is hard to learn. Based on this classifier-centric understanding, we propose a generic postprocessing step built upon flow-matching to shrink the gap between the learned distribution for a pre-trained denoising diffusion model and the real data distribution, majorly around the decision boundaries. Experiments on various datasets verify the effectiveness of the proposed approach.",
        "arxiv_id": "2503.10638",
        "ARXIVID": "2503.10638",
        "COMMENT": "Does not match any specific criteria but provides insights into classifier-free guidance in diffusion models, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09763": {
        "authors": [
            "Pushkar Shukla",
            "Aditya Chinchure",
            "Emily Diana",
            "Alexander Tolbert",
            "Kartik Hosanagar",
            "Vineeth N. Balasubramanian",
            "Leonid Sigal",
            "Matthew A. Turk"
        ],
        "title": "BiasConnect: Investigating Bias Interactions in Text-to-Image Models",
        "abstract": "arXiv:2503.09763v1 Announce Type: new  Abstract: The biases exhibited by Text-to-Image (TTI) models are often treated as if they are independent, but in reality, they may be deeply interrelated. Addressing bias along one dimension, such as ethnicity or age, can inadvertently influence another dimension, like gender, either mitigating or exacerbating existing disparities. Understanding these interdependencies is crucial for designing fairer generative models, yet measuring such effects quantitatively remains a challenge. In this paper, we aim to address these questions by introducing BiasConnect, a novel tool designed to analyze and quantify bias interactions in TTI models. Our approach leverages a counterfactual-based framework to generate pairwise causal graphs that reveals the underlying structure of bias interactions for the given text prompt. Additionally, our method provides empirical estimates that indicate how other bias dimensions shift toward or away from an ideal distribution when a given bias is modified. Our estimates have a strong correlation (+0.69) with the interdependency observations post bias mitigation. We demonstrate the utility of BiasConnect for selecting optimal bias mitigation axes, comparing different TTI models on the dependencies they learn, and understanding the amplification of intersectional societal biases in TTI models.",
        "arxiv_id": "2503.09763",
        "ARXIVID": "2503.09763",
        "COMMENT": "Does not match any specific criteria but discusses bias in generative models, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.10112": {
        "authors": [
            "Yanfeng Li",
            "Kahou Chan",
            "Yue Sun",
            "Chantong Lam",
            "Tong Tong",
            "Zitong Yu",
            "Keren Fu",
            "Xiaohong Liu",
            "Tao Tan"
        ],
        "title": "MoEdit: On Learning Quantity Perception for Multi-object Image Editing",
        "abstract": "arXiv:2503.10112v1 Announce Type: new  Abstract: Multi-object images are prevalent in various real-world scenarios, including augmented reality, advertisement design, and medical imaging. Efficient and precise editing of these images is critical for these applications. With the advent of Stable Diffusion (SD), high-quality image generation and editing have entered a new era. However, existing methods often struggle to consider each object both individually and part of the whole image editing, both of which are crucial for ensuring consistent quantity perception, resulting in suboptimal perceptual performance. To address these challenges, we propose MoEdit, an auxiliary-free multi-object image editing framework. MoEdit facilitates high-quality multi-object image editing in terms of style transfer, object reinvention, and background regeneration, while ensuring consistent quantity perception between inputs and outputs, even with a large number of objects. To achieve this, we introduce the Feature Compensation (FeCom) module, which ensures the distinction and separability of each object attribute by minimizing the in-between interlacing. Additionally, we present the Quantity Attention (QTTN) module, which perceives and preserves quantity consistency by effective control in editing, without relying on auxiliary tools. By leveraging the SD model, MoEdit enables customized preservation and modification of specific concepts in inputs with high quality. Experimental results demonstrate that our MoEdit achieves State-Of-The-Art (SOTA) performance in multi-object image editing. Data and codes will be available at https://github.com/Tear-kitty/MoEdit.",
        "arxiv_id": "2503.10112",
        "ARXIVID": "2503.10112",
        "COMMENT": "Does not match any specific criterion but is related to multi-object image editing and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.10508": {
        "authors": [
            "Yuhan Wang",
            "Cheng Liu",
            "Daou Zhang",
            "Weichao Wu"
        ],
        "title": "Hoi2Anomaly: An Explainable Anomaly Detection Approach Guided by Human-Object Interaction",
        "abstract": "arXiv:2503.10508v1 Announce Type: new  Abstract: In the domain of Image Anomaly Detection (IAD), Existing methods frequently exhibit a paucity of fine-grained, interpretable semantic information, resulting in the detection of anomalous entities or activities that are susceptible to machine illusions. This deficiency often leads to the detection of anomalous entities or actions that are susceptible to machine illusions and lack sufficient explanation. In this thesis, we propose a novel approach to anomaly detection, termed Hoi2Anomaly, which aims to achieve precise discrimination and localization of anomalies. The proposed methodology involves the construction of a multi-modal instruction tuning dataset comprising human-object interaction (HOI) pairs in anomalous scenarios. Second, we have trained an HOI extractor in threat scenarios to localize and match anomalous actions and entities. Finally, explanatory content is generated for the detected anomalous HOI by fine-tuning the visual language pretraining (VLP) framework. The experimental results demonstrate that Hoi2Anomaly surpasses existing generative approaches in terms of precision and explainability. We will release Hoi2Anomaly for the advancement of the field of anomaly detection.",
        "arxiv_id": "2503.10508",
        "ARXIVID": "2503.10508",
        "COMMENT": "Does not match any specific criterion but is related to anomaly detection with human-object interaction.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.10027": {
        "authors": [
            "Xiao Pan",
            "Sina Tavasoli",
            "T. Y. Yang",
            "Sina Poorghasem"
        ],
        "title": "Post-disaster building indoor damage and survivor detection using autonomous path planning and deep learning with unmanned aerial vehicles",
        "abstract": "arXiv:2503.10027v1 Announce Type: new  Abstract: Rapid response to natural disasters such as earthquakes is a crucial element in ensuring the safety of civil infrastructures and minimizing casualties. Traditional manual inspection is labour-intensive, time-consuming, and can be dangerous for inspectors and rescue workers. This paper proposed an autonomous inspection approach for structural damage inspection and survivor detection in the post-disaster building indoor scenario, which incorporates an autonomous navigation method, deep learning-based damage and survivor detection method, and a customized low-cost micro aerial vehicle (MAV) with onboard sensors. Experimental studies in a pseudo-post-disaster office building have shown the proposed methodology can achieve high accuracy in structural damage inspection and survivor detection. Overall, the proposed inspection approach shows great potential to improve the efficiency of existing manual post-disaster building inspection.",
        "arxiv_id": "2503.10027",
        "ARXIVID": "2503.10027",
        "COMMENT": "Does not match any specific criterion but is related to embodied AI applications in disaster scenarios.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.10078": {
        "authors": [
            "Chunyi Li",
            "Yuan Tian",
            "Xiaoyue Ling",
            "Zicheng Zhang",
            "Haodong Duan",
            "Haoning Wu",
            "Ziheng Jia",
            "Xiaohong Liu",
            "Xiongkuo Min",
            "Guo Lu",
            "Weisi Lin",
            "Guangtao Zhai"
        ],
        "title": "Image Quality Assessment: From Human to Machine Preference",
        "abstract": "arXiv:2503.10078v1 Announce Type: new  Abstract: Image Quality Assessment (IQA) based on human subjective preferences has undergone extensive research in the past decades. However, with the development of communication protocols, the visual data consumption volume of machines has gradually surpassed that of humans. For machines, the preference depends on downstream tasks such as segmentation and detection, rather than visual appeal. Considering the huge gap between human and machine visual systems, this paper proposes the topic: Image Quality Assessment for Machine Vision for the first time. Specifically, we (1) defined the subjective preferences of machines, including downstream tasks, test models, and evaluation metrics; (2) established the Machine Preference Database (MPD), which contains 2.25M fine-grained annotations and 30k reference/distorted image pair instances; (3) verified the performance of mainstream IQA algorithms on MPD. Experiments show that current IQA metrics are human-centric and cannot accurately characterize machine preferences. We sincerely hope that MPD can promote the evolution of IQA from human to machine preferences. Project page is on: https://github.com/lcysyzxdxc/MPD.",
        "arxiv_id": "2503.10078",
        "ARXIVID": "2503.10078",
        "COMMENT": "Does not match any specific criterion but is tangentially related to image quality assessment, which is outside the scope of the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09780": {
        "authors": [
            "Arman Zharmagambetov",
            "Chuan Guo",
            "Ivan Evtimov",
            "Maya Pavlova",
            "Ruslan Salakhutdinov",
            "Kamalika Chaudhuri"
        ],
        "title": "AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents",
        "abstract": "arXiv:2503.09780v1 Announce Type: new  Abstract: LLM-powered AI agents are an emerging frontier with tremendous potential to increase human productivity. However, empowering AI agents to take action on their user's behalf in day-to-day tasks involves giving them access to potentially sensitive and private information, which leads to a possible risk of inadvertent privacy leakage when the agent malfunctions. In this work, we propose one way to address that potential risk, by training AI agents to better satisfy the privacy principle of data minimization. For the purposes of this benchmark, by \"data minimization\" we mean instances where private information is shared only when it is necessary to fulfill a specific task-relevant purpose. We develop a benchmark called AgentDAM to evaluate how well existing and future AI agents can limit processing of potentially private information that we designate \"necessary\" to fulfill the task. Our benchmark simulates realistic web interaction scenarios and is adaptable to all existing web navigation agents. We use AgentDAM to evaluate how well AI agents built on top of GPT-4, Llama-3 and Claude can limit processing of potentially private information when unnecessary, and show that these agents are often prone to inadvertent use of unnecessary sensitive information. We finally propose a prompting-based approach that reduces this.",
        "arxiv_id": "2503.09780",
        "ARXIVID": "2503.09780",
        "COMMENT": "Does not match any specific criterion but is tangentially related to AI agents and privacy, which is outside the scope of the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09993": {
        "authors": [
            "JunYong Choi",
            "Min-Cheol Sagong",
            "SeokYeong Lee",
            "Seung-Won Jung",
            "Ig-Jae Kim",
            "Junghyun Cho"
        ],
        "title": "Channel-wise Noise Scheduled Diffusion for Inverse Rendering in Indoor Scenes",
        "abstract": "arXiv:2503.09993v1 Announce Type: new  Abstract: We propose a diffusion-based inverse rendering framework that decomposes a single RGB image into geometry, material, and lighting. Inverse rendering is inherently ill-posed, making it difficult to predict a single accurate solution. To address this challenge, recent generative model-based methods aim to present a range of possible solutions. However, finding a single accurate solution and generating diverse solutions can be conflicting. In this paper, we propose a channel-wise noise scheduling approach that allows a single diffusion model architecture to achieve two conflicting objectives. The resulting two diffusion models, trained with different channel-wise noise schedules, can predict a single highly accurate solution and present multiple possible solutions. The experimental results demonstrate the superiority of our two models in terms of both diversity and accuracy, which translates to enhanced performance in downstream applications such as object insertion and material editing.",
        "arxiv_id": "2503.09993",
        "ARXIVID": "2503.09993",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and inverse rendering in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.10603": {
        "authors": [
            "Jun Yu",
            "Lingsi Zhu",
            "Yanjun Chi",
            "Yunxiang Zhang",
            "Yang Zheng",
            "Yongqi Wang",
            "Xilong Lu"
        ],
        "title": "Dual-Stage Cross-Modal Network with Dynamic Feature Fusion for Emotional Mimicry Intensity Estimation",
        "abstract": "arXiv:2503.10603v1 Announce Type: new  Abstract: Emotional Mimicry Intensity (EMI) estimation serves as a critical technology for understanding human social behavior and enhancing human-computer interaction experiences, where the core challenge lies in dynamic correlation modeling and robust fusion of multimodal temporal signals. To address the limitations of existing methods in insufficient exploitation of modal synergistic effects, noise sensitivity, and limited fine-grained alignment capabilities, this paper proposes a dual-stage cross-modal alignment framework. First, we construct vision-text and audio-text contrastive learning networks based on an improved CLIP architecture, achieving preliminary alignment in the feature space through modality-decoupled pre-training. Subsequently, we design a temporal-aware dynamic fusion module that combines Temporal Convolutional Networks (TCN) and gated bidirectional LSTM to respectively capture the macro-evolution patterns of facial expressions and local dynamics of acoustic features. Innovatively, we introduce a quality-guided modality fusion strategy that enables modality compensation under occlusion and noisy scenarios through differentiable weight allocation. Experimental results on the Hume-Vidmimic2 dataset demonstrate that our method achieves an average Pearson correlation coefficient of 0.35 across six emotion dimensions, outperforming the best baseline by 40\\%. Ablation studies further validate the effectiveness of the dual-stage training strategy and dynamic fusion mechanism, providing a novel technical pathway for fine-grained emotion analysis in open environments.",
        "arxiv_id": "2503.10603",
        "ARXIVID": "2503.10603",
        "COMMENT": "Does not match any specific criteria but is related to multi-modal learning and dynamic fusion, which is tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09830": {
        "authors": [
            "Feng Zhou",
            "Pu Cao",
            "Yiyang Ma",
            "Lu Yang",
            "Jianqin Yin"
        ],
        "title": "Exploring Position Encoding in Diffusion U-Net for Training-free High-resolution Image Generation",
        "abstract": "arXiv:2503.09830v1 Announce Type: new  Abstract: Denoising higher-resolution latents via a pre-trained U-Net leads to repetitive and disordered image patterns. Although recent studies make efforts to improve generative quality by aligning denoising process across original and higher resolutions, the root cause of suboptimal generation is still lacking exploration. Through comprehensive analysis of position encoding in U-Net, we attribute it to inconsistent position encoding, sourced by the inadequate propagation of position information from zero-padding to latent features in convolution layers as resolution increases. To address this issue, we propose a novel training-free approach, introducing a Progressive Boundary Complement (PBC) method. This method creates dynamic virtual image boundaries inside the feature map to enhance position information propagation, enabling high-quality and rich-content high-resolution image synthesis. Extensive experiments demonstrate the superiority of our method.",
        "arxiv_id": "2503.09830",
        "ARXIVID": "2503.09830",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.09662": {
        "authors": [
            "Shitong Shao",
            "Zikai Zhou",
            "Dian Xie",
            "Yuetong Fang",
            "Tian Ye",
            "Lichen Bai",
            "Zeke Xie"
        ],
        "title": "CoRe^2: Collect, Reflect and Refine to Generate Better and Faster",
        "abstract": "arXiv:2503.09662v1 Announce Type: new  Abstract: Making text-to-image (T2I) generative model sample both fast and well represents a promising research direction. Previous studies have typically focused on either enhancing the visual quality of synthesized images at the expense of sampling efficiency or dramatically accelerating sampling without improving the base model's generative capacity. Moreover, nearly all inference methods have not been able to ensure stable performance simultaneously on both diffusion models (DMs) and visual autoregressive models (ARMs). In this paper, we introduce a novel plug-and-play inference paradigm, CoRe^2, which comprises three subprocesses: Collect, Reflect, and Refine. CoRe^2 first collects classifier-free guidance (CFG) trajectories, and then use collected data to train a weak model that reflects the easy-to-learn contents while reducing number of function evaluations during inference by half. Subsequently, CoRe^2 employs weak-to-strong guidance to refine the conditional output, thereby improving the model's capacity to generate high-frequency and realistic content, which is difficult for the base model to capture. To the best of our knowledge, CoRe^2 is the first to demonstrate both efficiency and effectiveness across a wide range of DMs, including SDXL, SD3.5, and FLUX, as well as ARMs like LlamaGen. It has exhibited significant performance improvements on HPD v2, Pick-of-Pic, Drawbench, GenEval, and T2I-Compbench. Furthermore, CoRe^2 can be seamlessly integrated with the state-of-the-art Z-Sampling, outperforming it by 0.3 and 0.16 on PickScore and AES, while achieving 5.64s time saving using SD3.5.Code is released at https://github.com/xie-lab-ml/CoRe/tree/main.",
        "arxiv_id": "2503.09662",
        "ARXIVID": "2503.09662",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of improving text-to-image generative models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.10589": {
        "authors": [
            "Yuwei Guo",
            "Ceyuan Yang",
            "Ziyan Yang",
            "Zhibei Ma",
            "Zhijie Lin",
            "Zhenheng Yang",
            "Dahua Lin",
            "Lu Jiang"
        ],
        "title": "Long Context Tuning for Video Generation",
        "abstract": "arXiv:2503.10589v1 Announce Type: new  Abstract: Recent advances in video generation can produce realistic, minute-long single-shot videos with scalable diffusion transformers. However, real-world narrative videos require multi-shot scenes with visual and dynamic consistency across shots. In this work, we introduce Long Context Tuning (LCT), a training paradigm that expands the context window of pre-trained single-shot video diffusion models to learn scene-level consistency directly from data. Our method expands full attention mechanisms from individual shots to encompass all shots within a scene, incorporating interleaved 3D position embedding and an asynchronous noise strategy, enabling both joint and auto-regressive shot generation without additional parameters. Models with bidirectional attention after LCT can further be fine-tuned with context-causal attention, facilitating auto-regressive generation with efficient KV-cache. Experiments demonstrate single-shot models after LCT can produce coherent multi-shot scenes and exhibit emerging capabilities, including compositional generation and interactive shot extension, paving the way for more practical visual content creation. See https://guoyww.github.io/projects/long-context-video/ for more details.",
        "arxiv_id": "2503.10589",
        "ARXIVID": "2503.10589",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling and video generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.10020": {
        "authors": [
            "Ali Abedi",
            "Q. M. Jonathan Wu",
            "Ning Zhang",
            "Farhad Pourpanah"
        ],
        "title": "One-Shot Federated Unsupervised Domain Adaptation with Scaled Entropy Attention and Multi-Source Smoothed Pseudo Labeling",
        "abstract": "arXiv:2503.10020v1 Announce Type: new  Abstract: Federated Learning (FL) is a promising approach for privacy-preserving collaborative learning. However, it faces significant challenges when dealing with domain shifts, especially when each client has access only to its source data and cannot share it during target domain adaptation. Moreover, FL methods often require high communication overhead due to multiple rounds of model updates between clients and the server. We propose a one-shot Federated Unsupervised Domain Adaptation (FUDA) method to address these limitations. Specifically, we introduce Scaled Entropy Attention (SEA) for model aggregation and Multi-Source Pseudo Labeling (MSPL) for target domain adaptation. SEA uses scaled prediction entropy on target domain to assign higher attention to reliable models. This improves the global model quality and ensures balanced weighting of contributions. MSPL distills knowledge from multiple source models to generate pseudo labels and manage noisy labels using smoothed soft-label cross-entropy (SSCE). Our approach outperforms state-of-the-art methods across four standard benchmarks while reducing communication and computation costs, making it highly suitable for real-world applications. The implementation code will be made publicly available upon publication.",
        "arxiv_id": "2503.10020",
        "ARXIVID": "2503.10020",
        "COMMENT": "Does not match any specific criteria but is relevant to federated learning and domain adaptation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.10635": {
        "authors": [
            "Zhaoyi Li",
            "Xiaohan Zhao",
            "Dong-Dong Wu",
            "Jiacheng Cui",
            "Zhiqiang Shen"
        ],
        "title": "A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1",
        "abstract": "arXiv:2503.10635v1 Announce Type: new  Abstract: Despite promising performance on open-source large vision-language models (LVLMs), transfer-based targeted attacks often fail against black-box commercial LVLMs. Analyzing failed adversarial perturbations reveals that the learned perturbations typically originate from a uniform distribution and lack clear semantic details, resulting in unintended responses. This critical absence of semantic information leads commercial LVLMs to either ignore the perturbation entirely or misinterpret its embedded semantics, thereby causing the attack to fail. To overcome these issues, we notice that identifying core semantic objects is a key objective for models trained with various datasets and methodologies. This insight motivates our approach that refines semantic clarity by encoding explicit semantic details within local regions, thus ensuring interoperability and capturing finer-grained features, and by concentrating modifications on semantically rich areas rather than applying them uniformly. To achieve this, we propose a simple yet highly effective solution: at each optimization step, the adversarial image is cropped randomly by a controlled aspect ratio and scale, resized, and then aligned with the target image in the embedding space. Experimental results confirm our hypothesis. Our adversarial examples crafted with local-aggregated perturbations focused on crucial regions exhibit surprisingly good transferability to commercial LVLMs, including GPT-4.5, GPT-4o, Gemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning models like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach achieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly outperforming all prior state-of-the-art attack methods. Our optimized adversarial examples under different configurations and training code are available at https://github.com/VILA-Lab/M-Attack.",
        "arxiv_id": "2503.10635",
        "ARXIVID": "2503.10635",
        "COMMENT": "Does not match any specific criterion but discusses adversarial attacks on vision-language models, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.10247": {
        "authors": [
            "Zhijie Zhu",
            "Lei Fan",
            "Maurice Pagnucco",
            "Yang Song"
        ],
        "title": "Interpretable Image Classification via Non-parametric Part Prototype Learning",
        "abstract": "arXiv:2503.10247v1 Announce Type: new  Abstract: Classifying images with an interpretable decision-making process is a long-standing problem in computer vision. In recent years, Prototypical Part Networks has gained traction as an approach for self-explainable neural networks, due to their ability to mimic human visual reasoning by providing explanations based on prototypical object parts. However, the quality of the explanations generated by these methods leaves room for improvement, as the prototypes usually focus on repetitive and redundant concepts. Leveraging recent advances in prototype learning, we present a framework for part-based interpretable image classification that learns a set of semantically distinctive object parts for each class, and provides diverse and comprehensive explanations. The core of our method is to learn the part-prototypes in a non-parametric fashion, through clustering deep features extracted from foundation vision models that encode robust semantic information. To quantitatively evaluate the quality of explanations provided by ProtoPNets, we introduce Distinctiveness Score and Comprehensiveness Score. Through evaluation on CUB-200-2011, Stanford Cars and Stanford Dogs datasets, we show that our framework compares favourably against existing ProtoPNets while achieving better interpretability. Code is available at: https://github.com/zijizhu/proto-non-param.",
        "arxiv_id": "2503.10247",
        "ARXIVID": "2503.10247",
        "COMMENT": "Does not match any specific criteria but is related to interpretable image classification, which is tangentially relevant to vision models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.10143": {
        "authors": [
            "Jinfeng Liu",
            "Lingtong Kong",
            "Bo Li",
            "Dan Xu"
        ],
        "title": "GaussHDR: High Dynamic Range Gaussian Splatting via Learning Unified 3D and 2D Local Tone Mapping",
        "abstract": "arXiv:2503.10143v1 Announce Type: new  Abstract: High dynamic range (HDR) novel view synthesis (NVS) aims to reconstruct HDR scenes by leveraging multi-view low dynamic range (LDR) images captured at different exposure levels. Current training paradigms with 3D tone mapping often result in unstable HDR reconstruction, while training with 2D tone mapping reduces the model's capacity to fit LDR images. Additionally, the global tone mapper used in existing methods can impede the learning of both HDR and LDR representations. To address these challenges, we present GaussHDR, which unifies 3D and 2D local tone mapping through 3D Gaussian splatting. Specifically, we design a residual local tone mapper for both 3D and 2D tone mapping that accepts an additional context feature as input. We then propose combining the dual LDR rendering results from both 3D and 2D local tone mapping at the loss level. Finally, recognizing that different scenes may exhibit varying balances between the dual results, we introduce uncertainty learning and use the uncertainties for adaptive modulation. Extensive experiments demonstrate that GaussHDR significantly outperforms state-of-the-art methods in both synthetic and real-world scenarios.",
        "arxiv_id": "2503.10143",
        "ARXIVID": "2503.10143",
        "COMMENT": "Does not match any specific criteria but is related to novel view synthesis and tone mapping in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.09873": {
        "authors": [
            "Shoaib Meraj Sami",
            "Md Mahedi Hasan",
            "Nasser M. Nasrabadi",
            "Raghuveer Rao"
        ],
        "title": "FDCT: Frequency-Aware Decomposition and Cross-Modal Token-Alignment for Multi-Sensor Target Classification",
        "abstract": "arXiv:2503.09873v1 Announce Type: new  Abstract: In automatic target recognition (ATR) systems, sensors may fail to capture discriminative, fine-grained detail features due to environmental conditions, noise created by CMOS chips, occlusion, parallaxes, and sensor misalignment. Therefore, multi-sensor image fusion is an effective choice to overcome these constraints. However, multi-modal image sensors are heterogeneous and have domain and granularity gaps. In addition, the multi-sensor images can be misaligned due to intricate background clutters, fluctuating illumination conditions, and uncontrolled sensor settings. In this paper, to overcome these issues, we decompose, align, and fuse multiple image sensor data for target classification. We extract the domain-specific and domain-invariant features from each sensor data. We propose to develop a shared unified discrete token (UDT) space between sensors to reduce the domain and granularity gaps. Additionally, we develop an alignment module to overcome the misalignment between multi-sensors and emphasize the discriminative representation of the UDT space. In the alignment module, we introduce sparsity constraints to provide a better cross-modal representation of the UDT space and robustness against various sensor settings. We achieve superior classification performance compared to single-modality classifiers and several state-of-the-art multi-modal fusion algorithms on four multi-sensor ATR datasets.",
        "arxiv_id": "2503.09873",
        "ARXIVID": "2503.09873",
        "COMMENT": "Does not match any specific criteria but is related to multi-modal fusion and target classification, which is tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.10350": {
        "authors": [
            "Ali Salar",
            "Qing Liu",
            "Yingli Tian",
            "Guoying Zhao"
        ],
        "title": "Enhancing Facial Privacy Protection via Weakening Diffusion Purification",
        "abstract": "arXiv:2503.10350v1 Announce Type: new  Abstract: The rapid growth of social media has led to the widespread sharing of individual portrait images, which pose serious privacy risks due to the capabilities of automatic face recognition (AFR) systems for mass surveillance. Hence, protecting facial privacy against unauthorized AFR systems is essential. Inspired by the generation capability of the emerging diffusion models, recent methods employ diffusion models to generate adversarial face images for privacy protection. However, they suffer from the diffusion purification effect, leading to a low protection success rate (PSR). In this paper, we first propose learning unconditional embeddings to increase the learning capacity for adversarial modifications and then use them to guide the modification of the adversarial latent code to weaken the diffusion purification effect. Moreover, we integrate an identity-preserving structure to maintain structural consistency between the original and generated images, allowing human observers to recognize the generated image as having the same identity as the original. Extensive experiments conducted on two public datasets, i.e., CelebA-HQ and LADN, demonstrate the superiority of our approach. The protected faces generated by our method outperform those produced by existing facial privacy protection approaches in terms of transferability and natural appearance.",
        "arxiv_id": "2503.10350",
        "ARXIVID": "2503.10350",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and privacy protection in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.10109": {
        "authors": [
            "Xingxin Xu",
            "Bing Cao",
            "Yinan Xia",
            "Pengfei Zhu",
            "Qinghua Hu"
        ],
        "title": "Dream-IF: Dynamic Relative EnhAnceMent for Image Fusion",
        "abstract": "arXiv:2503.10109v1 Announce Type: new  Abstract: Image fusion aims to integrate comprehensive information from images acquired through multiple sources. However, images captured by diverse sensors often encounter various degradations that can negatively affect fusion quality. Traditional fusion methods generally treat image enhancement and fusion as separate processes, overlooking the inherent correlation between them; notably, the dominant regions in one modality of a fused image often indicate areas where the other modality might benefit from enhancement. Inspired by this observation, we introduce the concept of dominant regions for image enhancement and present a Dynamic Relative EnhAnceMent framework for Image Fusion (Dream-IF). This framework quantifies the relative dominance of each modality across different layers and leverages this information to facilitate reciprocal cross-modal enhancement. By integrating the relative dominance derived from image fusion, our approach supports not only image restoration but also a broader range of image enhancement applications. Furthermore, we employ prompt-based encoding to capture degradation-specific details, which dynamically steer the restoration process and promote coordinated enhancement in both multi-modal image fusion and image enhancement scenarios. Extensive experimental results demonstrate that Dream-IF consistently outperforms its counterparts.",
        "arxiv_id": "2503.10109",
        "ARXIVID": "2503.10109",
        "COMMENT": "Does not match any specific criteria but is related to multi-modal image fusion, which is tangentially relevant to vision and multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.10270": {
        "authors": [
            "Zexuan Yan",
            "Yue Ma",
            "Chang Zou",
            "Wenteng Chen",
            "Qifeng Chen",
            "Linfeng Zhang"
        ],
        "title": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient Image Editing",
        "abstract": "arXiv:2503.10270v1 Announce Type: new  Abstract: Inversion-based image editing is rapidly gaining momentum while suffering from significant computation overhead, hindering its application in real-time interactive scenarios. In this paper, we rethink that the redundancy in inversion-based image editing exists in both the spatial and temporal dimensions, such as the unnecessary computation in unedited regions and the redundancy in the inversion progress. To tackle these challenges, we propose a practical framework, named EEdit, to achieve efficient image editing. Specifically, we introduce three techniques to solve them one by one. For spatial redundancy, spatial locality caching is introduced to compute the edited region and its neighboring regions while skipping the unedited regions, and token indexing preprocessing is designed to further accelerate the caching. For temporal redundancy, inversion step skipping is proposed to reuse the latent for efficient editing. Our experiments demonstrate an average of 2.46 $\\times$ acceleration without performance drop in a wide range of editing tasks including prompt-guided image editing, dragging and image composition. Our codes are available at https://github.com/yuriYanZeXuan/EEdit",
        "arxiv_id": "2503.10270",
        "ARXIVID": "2503.10270",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of efficient image editing and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.10104": {
        "authors": [
            "Yuheng Liang",
            "Zheyu Wang",
            "Feng Liu",
            "Mingzhou Liu",
            "Yu Yao"
        ],
        "title": "Mamba-VA: A Mamba-based Approach for Continuous Emotion Recognition in Valence-Arousal Space",
        "abstract": "arXiv:2503.10104v1 Announce Type: new  Abstract: Continuous Emotion Recognition (CER) plays a crucial role in intelligent human-computer interaction, mental health monitoring, and autonomous driving. Emotion modeling based on the Valence-Arousal (VA) space enables a more nuanced representation of emotional states. However, existing methods still face challenges in handling long-term dependencies and capturing complex temporal dynamics. To address these issues, this paper proposes a novel emotion recognition model, Mamba-VA, which leverages the Mamba architecture to efficiently model sequential emotional variations in video frames. First, the model employs a Masked Autoencoder (MAE) to extract deep visual features from video frames, enhancing the robustness of temporal information. Then, a Temporal Convolutional Network (TCN) is utilized for temporal modeling to capture local temporal dependencies. Subsequently, Mamba is applied for long-sequence modeling, enabling the learning of global emotional trends. Finally, a fully connected (FC) layer performs regression analysis to predict continuous valence and arousal values. Experimental results on the Valence-Arousal (VA) Estimation task of the 8th competition on Affective Behavior Analysis in-the-wild (ABAW) demonstrate that the proposed model achieves valence and arousal scores of 0.5362 (0.5036) and 0.4310 (0.4119) on the validation (test) set, respectively, outperforming the baseline. The source code is available on GitHub:https://github.com/FreedomPuppy77/Charon.",
        "arxiv_id": "2503.10104",
        "ARXIVID": "2503.10104",
        "COMMENT": "Does not match any specific criteria but is relevant to temporal modeling and emotion recognition.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.10523": {
        "authors": [
            "Jun Yu",
            "Yongqi Wang",
            "Lei Wang",
            "Yang Zheng",
            "Shengfan Xu"
        ],
        "title": "Interactive Multimodal Fusion with Temporal Modeling",
        "abstract": "arXiv:2503.10523v1 Announce Type: new  Abstract: This paper presents our method for the estimation of valence-arousal (VA) in the 8th Affective Behavior Analysis in-the-Wild (ABAW) competition. Our approach integrates visual and audio information through a multimodal framework. The visual branch uses a pre-trained ResNet model to extract spatial features from facial images. The audio branches employ pre-trained VGG models to extract VGGish and LogMel features from speech signals. These features undergo temporal modeling using Temporal Convolutional Networks (TCNs). We then apply cross-modal attention mechanisms, where visual features interact with audio features through query-key-value attention structures. Finally, the features are concatenated and passed through a regression layer to predict valence and arousal. Our method achieves competitive performance on the Aff-Wild2 dataset, demonstrating effective multimodal fusion for VA estimation in-the-wild.",
        "arxiv_id": "2503.10523",
        "ARXIVID": "2503.10523",
        "COMMENT": "Does not match any specific criteria but is relevant to multimodal learning and temporal modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}