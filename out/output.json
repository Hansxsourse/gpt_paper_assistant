{
    "2512.14099": {
        "authors": [
            "Ruishu Zhu",
            "Zhihao Huang",
            "Jiacheng Sun",
            "Ping Luo",
            "Hongyuan Zhang",
            "Xuelong Li"
        ],
        "title": "ViewMask-1-to-3: Multi-View Consistent Image Generation via Multimodal Diffusion Models",
        "abstract": "arXiv:2512.14099v1 Announce Type: new  Abstract: Multi-view image generation from a single image and text description remains challenging due to the difficulty of maintaining geometric consistency across different viewpoints. Existing approaches typically rely on 3D-aware architectures or specialized diffusion models that require extensive multi-view training data and complex geometric priors. In this work, we introduce ViewMask-1-to-3, a pioneering approach to apply discrete diffusion models to multi-view image generation. Unlike continuous diffusion methods that operate in latent spaces, ViewMask-1-to-3 formulates multi-view synthesis as a discrete sequence modeling problem, where each viewpoint is represented as visual tokens obtained through MAGVIT-v2 tokenization. By unifying language and vision through masked token prediction, our approach enables progressive generation of multiple viewpoints through iterative token unmasking with text input. ViewMask-1-to-3 achieves cross-view consistency through simple random masking combined with self-attention, eliminating the requirement for complex 3D geometric constraints or specialized attention architectures. Our approach demonstrates that discrete diffusion provides a viable and simple alternative to existing multi-view generation methods, ranking first on average across GSO and 3D-FUTURE datasets in terms of PSNR, SSIM, and LPIPS, while maintaining architectural simplicity.",
        "arxiv_id": "2512.14099",
        "ARXIVID": "2512.14099",
        "COMMENT": "The paper does not match any of the specific criteria closely. It focuses on multi-view image generation using discrete diffusion models, which does not involve joint generation and segmentation, unified diffusion models for multiple tasks, or image/video matting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    }
}