{
    "2501.18880": {
        "authors": [
            "Joshua R. Waite",
            "Md. Zahid Hasan",
            "Qisai Liu",
            "Zhanhong Jiang",
            "Chinmay Hegde",
            "Soumik Sarkar"
        ],
        "title": "RLS3: RL-Based Synthetic Sample Selection to Enhance Spatial Reasoning in Vision-Language Models for Indoor Autonomous Perception",
        "abstract": "arXiv:2501.18880v1 Announce Type: new  Abstract: Vision-language model (VLM) fine-tuning for application-specific visual grounding based on natural language instructions has become one of the most popular approaches for learning-enabled autonomous systems. However, such fine-tuning relies heavily on high-quality datasets to achieve successful performance in various downstream tasks. Additionally, VLMs often encounter limitations due to insufficient and imbalanced fine-tuning data. To address these issues, we propose a new generalizable framework to improve VLM fine-tuning by integrating it with a reinforcement learning (RL) agent. Our method utilizes the RL agent to manipulate objects within an indoor setting to create synthetic data for fine-tuning to address certain vulnerabilities of the VLM. Specifically, we use the performance of the VLM to provide feedback to the RL agent to generate informative data that efficiently fine-tune the VLM over the targeted task (e.g. spatial reasoning). The key contribution of this work is developing a framework where the RL agent serves as an informative data sampling tool and assists the VLM in order to enhance performance and address task-specific vulnerabilities. By targeting the data sampling process to address the weaknesses of the VLM, we can effectively train a more context-aware model. In addition, generating synthetic data allows us to have precise control over each scene and generate granular ground truth captions. Our results show that the proposed data generation approach improves the spatial reasoning performance of VLMs, which demonstrates the benefits of using RL-guided data generation in vision-language tasks.",
        "arxiv_id": "2501.18880",
        "ARXIVID": "2501.18880",
        "COMMENT": "This paper matches criterion 1 and 3 as it focuses on improving spatial reasoning in vision-language models using reinforcement learning and synthetic data generation, which is a novel method for embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2501.18867": {
        "authors": [
            "Jianke Zhang",
            "Yanjiang Guo",
            "Yucheng Hu",
            "Xiaoyu Chen",
            "Xiang Zhu",
            "Jianyu Chen"
        ],
        "title": "UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent",
        "abstract": "arXiv:2501.18867v1 Announce Type: new  Abstract: Recent advancements in Vision-Language-Action (VLA) models have leveraged pre-trained Vision-Language Models (VLMs) to improve the generalization capabilities. VLMs, typically pre-trained on vision-language understanding tasks, provide rich semantic knowledge and reasoning abilities. However, prior research has shown that VLMs often focus on high-level semantic content and neglect low-level features, limiting their ability to capture detailed spatial information and understand physical dynamics. These aspects, which are crucial for embodied control tasks, remain underexplored in existing pre-training paradigms. In this paper, we investigate the training paradigm for VLAs, and introduce \\textbf{UP-VLA}, a \\textbf{U}nified VLA model training with both multi-modal \\textbf{U}nderstanding and future \\textbf{P}rediction objectives, enhancing both high-level semantic comprehension and low-level spatial understanding. Experimental results show that UP-VLA achieves a 33% improvement on the Calvin ABC-D benchmark compared to the previous state-of-the-art method. Additionally, UP-VLA demonstrates improved success rates in real-world manipulation tasks, particularly those requiring precise spatial information.",
        "arxiv_id": "2501.18867",
        "ARXIVID": "2501.18867",
        "COMMENT": "Matches criterion 1 and 3 as it focuses on improving spatial understanding and introduces a new training paradigm for Vision-Language-Action models in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2501.19318": {
        "authors": [
            "Anirudh Chari",
            "Suraj Reddy",
            "Aditya Tiwari",
            "Richard Lian",
            "Brian Zhou"
        ],
        "title": "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented Reinforcement in Embodied Systems",
        "abstract": "arXiv:2501.19318v1 Announce Type: new  Abstract: While large language models (LLMs) have shown promising capabilities as zero-shot planners for embodied agents, their inability to learn from experience and build persistent mental models limits their robustness in complex open-world environments like Minecraft. We introduce MINDSTORES, an experience-augmented planning framework that enables embodied agents to build and leverage mental models through natural interaction with their environment. Drawing inspiration from how humans construct and refine cognitive mental models, our approach extends existing zero-shot LLM planning by maintaining a database of past experiences that informs future planning iterations. The key innovation is representing accumulated experiences as natural language embeddings of (state, task, plan, outcome) tuples, which can then be efficiently retrieved and reasoned over by an LLM planner to generate insights and guide plan refinement for novel states and tasks. Through extensive experiments in the MineDojo environment, a simulation environment for agents in Minecraft that provides low-level controls for Minecraft, we find that MINDSTORES learns and applies its knowledge significantly better than existing memory-based LLM planners while maintaining the flexibility and generalization benefits of zero-shot approaches, representing an important step toward more capable embodied AI systems that can learn continuously through natural experience.",
        "arxiv_id": "2501.19318",
        "ARXIVID": "2501.19318",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for embodied AI with memory-augmented planning in a simulator environment.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2501.19060": {
        "authors": [
            "Song-Lin Lv",
            "Yu-Yang Chen",
            "Zhi Zhou",
            "Yu-Feng Li",
            "Lan-Zhe Guo"
        ],
        "title": "Contrast-Aware Calibration for Fine-Tuned CLIP: Leveraging Image-Text Alignment",
        "abstract": "arXiv:2501.19060v1 Announce Type: new  Abstract: Vision-language models (VLMs), such as CLIP, have demonstrated exceptional generalization capabilities and can quickly adapt to downstream tasks through prompt fine-tuning. Unfortunately, in classification tasks involving non-training classes, known as open-vocabulary setting, fine-tuned VLMs often overfit to train classes, resulting in a misalignment between confidence scores and actual accuracy on unseen classes, which significantly undermines their reliability in real-world deployments. Existing confidence calibration methods typically require training parameters or analyzing features from the training dataset, restricting their ability to generalize unseen classes without corresponding train data. Moreover, VLM-specific calibration methods rely solely on text features from train classes as calibration indicators, which inherently limits their ability to calibrate train classes. To address these challenges, we propose an effective multimodal calibration method Contrast-Aware Calibration (CAC). Building on the original CLIP's zero-shot adaptability and the conclusion from empirical analysis that poor intra-class and inter-class discriminative ability on unseen classes is the root cause, we calculate calibration weights based on the contrastive difference between the original and fine-tuned CLIP. This method not only adapts to calibrating unseen classes but also overcomes the limitations of previous VLM calibration methods that could not calibrate train classes. In experiments involving 11 datasets with 5 fine-tuning methods, CAC consistently achieved the best calibration effect on both train and unseen classes without sacrificing accuracy and inference speed.",
        "arxiv_id": "2501.19060",
        "ARXIVID": "2501.19060",
        "COMMENT": "This paper matches criterion 2 as it focuses on fine-tuning and calibration of vision-language models like CLIP, which is directly relevant to VLLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2501.18940": {
        "authors": [
            "Sai Wang",
            "Fan Ma",
            "Xinyi Li",
            "Hehe Fan",
            "Yu Wu"
        ],
        "title": "TV-Dialogue: Crafting Theme-Aware Video Dialogues with Immersive Interaction",
        "abstract": "arXiv:2501.18940v1 Announce Type: new  Abstract: Recent advancements in LLMs have accelerated the development of dialogue generation across text and images, yet video-based dialogue generation remains underexplored and presents unique challenges. In this paper, we introduce Theme-aware Video Dialogue Crafting (TVDC), a novel task aimed at generating new dialogues that align with video content and adhere to user-specified themes. We propose TV-Dialogue, a novel multi-modal agent framework that ensures both theme alignment (i.e., the dialogue revolves around the theme) and visual consistency (i.e., the dialogue matches the emotions and behaviors of characters in the video) by enabling real-time immersive interactions among video characters, thereby accurately understanding the video content and generating new dialogue that aligns with the given themes. To assess the generated dialogues, we present a multi-granularity evaluation benchmark with high accuracy, interpretability and reliability, demonstrating the effectiveness of TV-Dialogue on self-collected dataset over directly using existing LLMs. Extensive experiments reveal that TV-Dialogue can generate dialogues for videos of any length and any theme in a zero-shot manner without training. Our findings underscore the potential of TV-Dialogue for various applications, such as video re-creation, film dubbing and its use in downstream multimodal tasks.",
        "arxiv_id": "2501.18940",
        "ARXIVID": "2501.18940",
        "COMMENT": "Matches criterion 2 as it introduces a novel multi-modal agent framework for video-based dialogue generation.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2501.18804": {
        "authors": [
            "Vitor Guizilini",
            "Muhammad Zubair Irshad",
            "Dian Chen",
            "Greg Shakhnarovich",
            "Rares Ambrus"
        ],
        "title": "Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion",
        "abstract": "arXiv:2501.18804v1 Announce Type: new  Abstract: Current methods for 3D scene reconstruction from sparse posed images employ intermediate 3D representations such as neural fields, voxel grids, or 3D Gaussians, to achieve multi-view consistent scene appearance and geometry. In this paper we introduce MVGD, a diffusion-based architecture capable of direct pixel-level generation of images and depth maps from novel viewpoints, given an arbitrary number of input views. Our method uses raymap conditioning to both augment visual features with spatial information from different viewpoints, as well as to guide the generation of images and depth maps from novel views. A key aspect of our approach is the multi-task generation of images and depth maps, using learnable task embeddings to guide the diffusion process towards specific modalities. We train this model on a collection of more than 60 million multi-view samples from publicly available datasets, and propose techniques to enable efficient and consistent learning in such diverse conditions. We also propose a novel strategy that enables the efficient training of larger models by incrementally fine-tuning smaller ones, with promising scaling behavior. Through extensive experiments, we report state-of-the-art results in multiple novel view synthesis benchmarks, as well as multi-view stereo and video depth estimation.",
        "arxiv_id": "2501.18804",
        "ARXIVID": "2501.18804",
        "COMMENT": "This paper matches criterion 3 as it introduces a novel diffusion-based architecture for multi-view geometric tasks, which is relevant to embodied AI and novel methods.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2501.19084": {
        "authors": [
            "Xingyu Miao",
            "Haoran Duan",
            "Yang Bai",
            "Tejal Shah",
            "Jun Song",
            "Yang Long",
            "Rajiv Ranjan",
            "Ling Shao"
        ],
        "title": "Laser: Efficient Language-Guided Segmentation in Neural Radiance Fields",
        "abstract": "arXiv:2501.19084v1 Announce Type: new  Abstract: In this work, we propose a method that leverages CLIP feature distillation, achieving efficient 3D segmentation through language guidance. Unlike previous methods that rely on multi-scale CLIP features and are limited by processing speed and storage requirements, our approach aims to streamline the workflow by directly and effectively distilling dense CLIP features, thereby achieving precise segmentation of 3D scenes using text. To achieve this, we introduce an adapter module and mitigate the noise issue in the dense CLIP feature distillation process through a self-cross-training strategy. Moreover, to enhance the accuracy of segmentation edges, this work presents a low-rank transient query attention mechanism. To ensure the consistency of segmentation for similar colors under different viewpoints, we convert the segmentation task into a classification task through label volume, which significantly improves the consistency of segmentation in color-similar areas. We also propose a simplified text augmentation strategy to alleviate the issue of ambiguity in the correspondence between CLIP features and text. Extensive experimental results show that our method surpasses current state-of-the-art technologies in both training speed and performance. Our code is available on: https://github.com/xingy038/Laser.git.",
        "arxiv_id": "2501.19084",
        "ARXIVID": "2501.19084",
        "COMMENT": "Matches criterion 4 as it focuses on efficient 3D segmentation using CLIP features, a vision foundation model.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2501.19319": {
        "authors": [
            "Yiming Huang",
            "Beilei Cui",
            "Long Bai",
            "Zhen Chen",
            "Jinlin Wu",
            "Zhen Li",
            "Hongbin Liu",
            "Hongliang Ren"
        ],
        "title": "Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven Surface Normal-aware Tracking and Mapping",
        "abstract": "arXiv:2501.19319v1 Announce Type: new  Abstract: Simultaneous Localization and Mapping (SLAM) is essential for precise surgical interventions and robotic tasks in minimally invasive procedures. While recent advancements in 3D Gaussian Splatting (3DGS) have improved SLAM with high-quality novel view synthesis and fast rendering, these systems struggle with accurate depth and surface reconstruction due to multi-view inconsistencies. Simply incorporating SLAM and 3DGS leads to mismatches between the reconstructed frames. In this work, we present Endo-2DTAM, a real-time endoscopic SLAM system with 2D Gaussian Splatting (2DGS) to address these challenges. Endo-2DTAM incorporates a surface normal-aware pipeline, which consists of tracking, mapping, and bundle adjustment modules for geometrically accurate reconstruction. Our robust tracking module combines point-to-point and point-to-plane distance metrics, while the mapping module utilizes normal consistency and depth distortion to enhance surface reconstruction quality. We also introduce a pose-consistent strategy for efficient and geometrically coherent keyframe sampling. Extensive experiments on public endoscopic datasets demonstrate that Endo-2DTAM achieves an RMSE of $1.87\\pm 0.63$ mm for depth reconstruction of surgical scenes while maintaining computationally efficient tracking, high-quality visual appearance, and real-time rendering. Our code will be released at github.com/lastbasket/Endo-2DTAM.",
        "arxiv_id": "2501.19319",
        "ARXIVID": "2501.19319",
        "COMMENT": "Matches criterion 1 as it focuses on improving spatial understanding in endoscopic SLAM systems.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2501.19143": {
        "authors": [
            "Ching-Chun Chang",
            "Fan-Yun Chen",
            "Shih-Hong Gu",
            "Kai Gao",
            "Hanrui Wang",
            "Isao Echizen"
        ],
        "title": "Imitation Game for Adversarial Disillusion with Multimodal Generative Chain-of-Thought Role-Play",
        "abstract": "arXiv:2501.19143v1 Announce Type: new  Abstract: As the cornerstone of artificial intelligence, machine perception confronts a fundamental threat posed by adversarial illusions. These adversarial attacks manifest in two primary forms: deductive illusion, where specific stimuli are crafted based on the victim model's general decision logic, and inductive illusion, where the victim model's general decision logic is shaped by specific stimuli. The former exploits the model's decision boundaries to create a stimulus that, when applied, interferes with its decision-making process. The latter reinforces a conditioned reflex in the model, embedding a backdoor during its learning phase that, when triggered by a stimulus, causes aberrant behaviours. The multifaceted nature of adversarial illusions calls for a unified defence framework, addressing vulnerabilities across various forms of attack. In this study, we propose a disillusion paradigm based on the concept of an imitation game. At the heart of the imitation game lies a multimodal generative agent, steered by chain-of-thought reasoning, which observes, internalises and reconstructs the semantic essence of a sample, liberated from the classic pursuit of reversing the sample to its original state. As a proof of concept, we conduct experimental simulations using a multimodal generative dialogue agent and evaluates the methodology under a variety of attack scenarios.",
        "arxiv_id": "2501.19143",
        "ARXIVID": "2501.19143",
        "COMMENT": "This paper introduces a multimodal generative agent with chain-of-thought reasoning, which aligns with criterion 2 on VLLMs/MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2501.19069": {
        "authors": [
            "Siyu Zhang",
            "Heming Zheng",
            "Yiming Wu",
            "Yeming Chen"
        ],
        "title": "Improving vision-language alignment with graph spiking hybrid Networks",
        "abstract": "arXiv:2501.19069v1 Announce Type: new  Abstract: To bridge the semantic gap between vision and language (VL), it is necessary to develop a good alignment strategy, which includes handling semantic diversity, abstract representation of visual information, and generalization ability of models. Recent works use detector-based bounding boxes or patches with regular partitions to represent visual semantics. While current paradigms have made strides, they are still insufficient for fully capturing the nuanced contextual relations among various objects. This paper proposes a comprehensive visual semantic representation module, necessitating the utilization of panoptic segmentation to generate coherent fine-grained semantic features. Furthermore, we propose a novel Graph Spiking Hybrid Network (GSHN) that integrates the complementary advantages of Spiking Neural Networks (SNNs) and Graph Attention Networks (GATs) to encode visual semantic information. Intriguingly, the model not only encodes the discrete and continuous latent variables of instances but also adeptly captures both local and global contextual features, thereby significantly enhancing the richness and diversity of semantic representations. Leveraging the spatiotemporal properties inherent in SNNs, we employ contrastive learning (CL) to enhance the similarity-based representation of embeddings. This strategy alleviates the computational overhead of the model and enriches meaningful visual representations by constructing positive and negative sample pairs. We design an innovative pre-training method, Spiked Text Learning (STL), which uses text features to improve the encoding ability of discrete semantics. Experiments show that the proposed GSHN exhibits promising results on multiple VL downstream tasks.",
        "arxiv_id": "2501.19069",
        "ARXIVID": "2501.19069",
        "COMMENT": "This paper matches criterion 2 as it proposes a novel vision-language alignment strategy using graph spiking hybrid networks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2501.19184": {
        "authors": [
            "Luca Ciampi",
            "Ali Azmoudeh",
            "Elif Ecem Akbaba",
            "Erdi Sar{\\i}ta\\c{s}",
            "Ziya Ata Yaz{\\i}c{\\i}",
            "Haz{\\i}m Kemal Ekenel",
            "Giuseppe Amato",
            "Fabrizio Falchi"
        ],
        "title": "A Survey on Class-Agnostic Counting: Advancements from Reference-Based to Open-World Text-Guided Approaches",
        "abstract": "arXiv:2501.19184v1 Announce Type: new  Abstract: Object counting has recently shifted towards class-agnostic counting (CAC), which addresses the challenge of counting objects across arbitrary categories, tackling a critical need in versatile counting systems. While humans effortlessly identify and count objects from diverse categories without prior knowledge, most counting methods remain restricted to enumerating instances of known classes, requiring extensive labeled datasets for training, and struggling under open-vocabulary settings. Conversely, CAC aims to count objects belonging to classes never seen during training, typically operating in a few-shot setting. In this paper, for the first time, we review advancements in CAC methodologies, categorizing them into three paradigms based on how target object classes can be specified: reference-based, reference-less, and open-world text-guided. Reference-based approaches have set performance benchmarks using exemplar-guided mechanisms. Reference-less methods eliminate exemplar dependency by leveraging inherent image patterns. Finally, open-world text-guided methods utilize vision-language models, enabling object class descriptions through textual prompts, representing a flexible and appealing solution. We analyze state-of-the-art techniques and we report their results on existing gold standard benchmarks, comparing their performance and identifying and discussing their strengths and limitations. Persistent challenges -- such as annotation dependency, scalability, and generalization -- are discussed, alongside future directions. We believe this survey serves as a valuable resource for researchers to understand the progressive developments and contributions over time and the current state-of-the-art of CAC, suggesting insights for future directions and challenges to be addressed.",
        "arxiv_id": "2501.19184",
        "ARXIVID": "2501.19184",
        "COMMENT": "This paper matches criterion 2 as it discusses vision-language models in the context of class-agnostic counting and open-world text-guided approaches.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2501.19382": {
        "authors": [
            "Liudi Yang",
            "Ruben Mascaro",
            "Ignacio Alzugaray",
            "Sai Manoj Prakhya",
            "Marco Karrer",
            "Ziyuan Liu",
            "Margarita Chli"
        ],
        "title": "LiDAR Loop Closure Detection using Semantic Graphs with Graph Attention Networks",
        "abstract": "arXiv:2501.19382v1 Announce Type: new  Abstract: In this paper, we propose a novel loop closure detection algorithm that uses graph attention neural networks to encode semantic graphs to perform place recognition and then use semantic registration to estimate the 6 DoF relative pose constraint. Our place recognition algorithm has two key modules, namely, a semantic graph encoder module and a graph comparison module. The semantic graph encoder employs graph attention networks to efficiently encode spatial, semantic and geometric information from the semantic graph of the input point cloud. We then use self-attention mechanism in both node-embedding and graph-embedding steps to create distinctive graph vectors. The graph vectors of the current scan and a keyframe scan are then compared in the graph comparison module to identify a possible loop closure. Specifically, employing the difference of the two graph vectors showed a significant improvement in performance, as shown in ablation studies. Lastly, we implemented a semantic registration algorithm that takes in loop closure candidate scans and estimates the relative 6 DoF pose constraint for the LiDAR SLAM system. Extensive evaluation on public datasets shows that our model is more accurate and robust, achieving 13% improvement in maximum F1 score on the SemanticKITTI dataset, when compared to the baseline semantic graph algorithm. For the benefit of the community, we open-source the complete implementation of our proposed algorithm and custom implementation of semantic registration at https://github.com/crepuscularlight/SemanticLoopClosure",
        "arxiv_id": "2501.19382",
        "ARXIVID": "2501.19382",
        "COMMENT": "Matches criterion 1 as it proposes a novel method for LiDAR loop closure detection using semantic graphs, enhancing spatial understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2501.18994": {
        "authors": [
            "Jian-Yu Chen",
            "Yi-Ru Chen",
            "Yin-Qiao Chang",
            "Che-Ming Li",
            "Jann-Long Chern",
            "Chih-Wei Huang"
        ],
        "title": "VKFPos: A Learning-Based Monocular Positioning with Variational Bayesian Extended Kalman Filter Integration",
        "abstract": "arXiv:2501.18994v1 Announce Type: new  Abstract: This paper addresses the challenges in learning-based monocular positioning by proposing VKFPos, a novel approach that integrates Absolute Pose Regression (APR) and Relative Pose Regression (RPR) via an Extended Kalman Filter (EKF) within a variational Bayesian inference framework. Our method shows that the essential posterior probability of the monocular positioning problem can be decomposed into APR and RPR components. This decomposition is embedded in the deep learning model by predicting covariances in both APR and RPR branches, allowing them to account for associated uncertainties. These covariances enhance the loss functions and facilitate EKF integration. Experimental evaluations on both indoor and outdoor datasets show that the single-shot APR branch achieves accuracy on par with state-of-the-art methods. Furthermore, for temporal positioning, where consecutive images allow for RPR and EKF integration, VKFPos outperforms temporal APR and model-based integration methods, achieving superior accuracy.",
        "arxiv_id": "2501.18994",
        "ARXIVID": "2501.18994",
        "COMMENT": "Matches criterion 1 as it proposes a novel approach for monocular positioning, enhancing spatial intelligence.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2501.18851": {
        "authors": [
            "Xiaoyan Jiang",
            "Bohan Wang",
            "Xinlong Wan",
            "Zhi Zhou",
            "Hamido Fujita"
        ],
        "title": "Project-and-Fuse: Improving RGB-D Semantic Segmentation via Graph Convolution Networks",
        "abstract": "arXiv:2501.18851v1 Announce Type: new  Abstract: Most existing RGB-D semantic segmentation methods focus on the feature level fusion, including complex cross-modality and cross-scale fusion modules. However, these methods may cause misalignment problem in the feature fusion process and counter-intuitive patches in the segmentation results. Inspired by the popular pixel-node-pixel pipeline, we propose to 1) fuse features from two modalities in a late fusion style, during which the geometric feature injection is guided by texture feature prior; 2) employ Graph Neural Networks (GNNs) on the fused feature to alleviate the emergence of irregular patches by inferring patch relationship. At the 3D feature extraction stage, we argue that traditional CNNs are not efficient enough for depth maps. So, we encode depth map into normal map, after which CNNs can easily extract object surface tendencies.At projection matrix generation stage, we find the existence of Biased-Assignment and Ambiguous-Locality issues in the original pipeline. Therefore, we propose to 1) adopt the Kullback-Leibler Loss to ensure no missing important pixel features, which can be viewed as hard pixel mining process; 2) connect regions that are close to each other in the Euclidean space as well as in the semantic space with larger edge weights so that location informations can been considered. Extensive experiments on two public datasets, NYU-DepthV2 and SUN RGB-D, have shown that our approach can consistently boost the performance of RGB-D semantic segmentation task.",
        "arxiv_id": "2501.18851",
        "ARXIVID": "2501.18851",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for RGB-D semantic segmentation, improving spatial understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2501.18758": {
        "authors": [
            "Haozhou Hu",
            "Harpreet S. Dhillon",
            "R. Michael Buehrer"
        ],
        "title": "A New Statistical Approach to the Performance Analysis of Vision-based Localization",
        "abstract": "arXiv:2501.18758v1 Announce Type: new  Abstract: Many modern wireless devices with accurate positioning needs also have access to vision sensors, such as a camera, radar, and Light Detection and Ranging (LiDAR). In scenarios where wireless-based positioning is either inaccurate or unavailable, using information from vision sensors becomes highly desirable for determining the precise location of the wireless device. Specifically, vision data can be used to estimate distances between the target (where the sensors are mounted) and nearby landmarks. However, a significant challenge in positioning using these measurements is the inability to uniquely identify which specific landmark is visible in the data. For instance, when the target is located close to a lamppost, it becomes challenging to precisely identify the specific lamppost (among several in the region) that is near the target. This work proposes a new framework for target localization using range measurements to multiple proximate landmarks. The geometric constraints introduced by these measurements are utilized to narrow down candidate landmark combinations corresponding to the range measurements and, consequently, the target's location on a map. By modeling landmarks as a marked Poisson point process (PPP), we show that three noise-free range measurements are sufficient to uniquely determine the correct combination of landmarks in a two-dimensional plane. For noisy measurements, we provide a mathematical characterization of the probability of correctly identifying the observed landmark combination based on a novel joint distribution of key random variables. Our results demonstrate that the landmark combination can be identified using ranges, even when individual landmarks are visually indistinguishable.",
        "arxiv_id": "2501.18758",
        "ARXIVID": "2501.18758",
        "COMMENT": "Matches criterion 1 as it proposes a new statistical framework for vision-based localization, which is a spatial understanding task.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2501.19034": {
        "authors": [
            "Bo Lan",
            "Pei Li",
            "Jiaxi Yin",
            "Yunpeng Song",
            "Ge Wang",
            "Han Ding",
            "Jinsong Han",
            "Fei Wang"
        ],
        "title": "XRF V2: A Dataset for Action Summarization with Wi-Fi Signals, and IMUs in Phones, Watches, Earbuds, and Glasses",
        "abstract": "arXiv:2501.19034v1 Announce Type: new  Abstract: Human Action Recognition (HAR) plays a crucial role in applications such as health monitoring, smart home automation, and human-computer interaction. While HAR has been extensively studied, action summarization, which involves identifying and summarizing continuous actions, remains an emerging task. This paper introduces the novel XRF V2 dataset, designed for indoor daily activity Temporal Action Localization (TAL) and action summarization. XRF V2 integrates multimodal data from Wi-Fi signals, IMU sensors (smartphones, smartwatches, headphones, and smart glasses), and synchronized video recordings, offering a diverse collection of indoor activities from 16 volunteers across three distinct environments. To tackle TAL and action summarization, we propose the XRFMamba neural network, which excels at capturing long-term dependencies in untrimmed sensory sequences and outperforms state-of-the-art methods, such as ActionFormer and WiFiTAD. We envision XRF V2 as a valuable resource for advancing research in human action localization, action forecasting, pose estimation, multimodal foundation models pre-training, synthetic data generation, and more.",
        "arxiv_id": "2501.19034",
        "ARXIVID": "2501.19034",
        "COMMENT": "Matches criterion 3 as it introduces a new multimodal dataset for human action recognition and summarization.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2501.19196": {
        "authors": [
            "Krzysztof Byrski",
            "Marcin Mazur",
            "Jacek Tabor",
            "Tadeusz Dziarmaga",
            "Marcin K\\k{a}dzio{\\l}ka",
            "Dawid Baran",
            "Przemys{\\l}aw Spurek"
        ],
        "title": "RaySplats: Ray Tracing based Gaussian Splatting",
        "abstract": "arXiv:2501.19196v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) is a process that enables the direct creation of 3D objects from 2D images. This representation offers numerous advantages, including rapid training and rendering. However, a significant limitation of 3DGS is the challenge of incorporating light and shadow reflections, primarily due to the utilization of rasterization rather than ray tracing for rendering. This paper introduces RaySplats, a model that employs ray-tracing based Gaussian Splatting. Rather than utilizing the projection of Gaussians, our method employs a ray-tracing mechanism, operating directly on Gaussian primitives represented by confidence ellipses with RGB colors. In practice, we compute the intersection between ellipses and rays to construct ray-tracing algorithms, facilitating the incorporation of meshes with Gaussian Splatting models and the addition of lights, shadows, and other related effects.",
        "arxiv_id": "2501.19196",
        "ARXIVID": "2501.19196",
        "COMMENT": "Matches criterion 4 as it introduces a novel ray-tracing-based Gaussian Splatting method for 3D object creation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2501.18616": {
        "authors": [
            "Xiangbo Gao",
            "Runsheng Xu",
            "Jiachen Li",
            "Ziran Wang",
            "Zhiwen Fan",
            "Zhengzhong Tu"
        ],
        "title": "STAMP: Scalable Task And Model-agnostic Collaborative Perception",
        "abstract": "arXiv:2501.18616v1 Announce Type: new  Abstract: Perception is crucial for autonomous driving, but single-agent perception is often constrained by sensors' physical limitations, leading to degraded performance under severe occlusion, adverse weather conditions, and when detecting distant objects. Multi-agent collaborative perception offers a solution, yet challenges arise when integrating heterogeneous agents with varying model architectures. To address these challenges, we propose STAMP, a scalable task- and model-agnostic, collaborative perception pipeline for heterogeneous agents. STAMP utilizes lightweight adapter-reverter pairs to transform Bird's Eye View (BEV) features between agent-specific and shared protocol domains, enabling efficient feature sharing and fusion. This approach minimizes computational overhead, enhances scalability, and preserves model security. Experiments on simulated and real-world datasets demonstrate STAMP's comparable or superior accuracy to state-of-the-art models with significantly reduced computational costs. As a first-of-its-kind task- and model-agnostic framework, STAMP aims to advance research in scalable and secure mobility systems towards Level 5 autonomy. Our project page is at https://xiangbogaobarry.github.io/STAMP and the code is available at https://github.com/taco-group/STAMP.",
        "arxiv_id": "2501.18616",
        "ARXIVID": "2501.18616",
        "COMMENT": "Matches criterion 3 as it proposes a novel collaborative perception framework for autonomous driving.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2501.18954": {
        "authors": [
            "Shenghao Fu",
            "Qize Yang",
            "Qijie Mo",
            "Junkai Yan",
            "Xihan Wei",
            "Jingke Meng",
            "Xiaohua Xie",
            "Wei-Shi Zheng"
        ],
        "title": "LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models",
        "abstract": "arXiv:2501.18954v1 Announce Type: new  Abstract: Recent open-vocabulary detectors achieve promising performance with abundant region-level annotated data. In this work, we show that an open-vocabulary detector co-training with a large language model by generating image-level detailed captions for each image can further improve performance. To achieve the goal, we first collect a dataset, GroundingCap-1M, wherein each image is accompanied by associated grounding labels and an image-level detailed caption. With this dataset, we finetune an open-vocabulary detector with training objectives including a standard grounding loss and a caption generation loss. We take advantage of a large language model to generate both region-level short captions for each region of interest and image-level long captions for the whole image. Under the supervision of the large language model, the resulting detector, LLMDet, outperforms the baseline by a clear margin, enjoying superior open-vocabulary ability. Further, we show that the improved LLMDet can in turn build a stronger large multi-modal model, achieving mutual benefits. The code, model, and dataset is available at https://github.com/iSEE-Laboratory/LLMDet.",
        "arxiv_id": "2501.18954",
        "ARXIVID": "2501.18954",
        "COMMENT": "Matches criterion 2 as it introduces a new method for open-vocabulary object detection using large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2501.18864": {
        "authors": [
            "Aodi Li",
            "Liansheng Zhuang",
            "Xiao Long",
            "Minghong Yao",
            "Shafei Wang"
        ],
        "title": "Test-time Loss Landscape Adaptation for Zero-Shot Generalization in Vision-Language Models",
        "abstract": "arXiv:2501.18864v1 Announce Type: new  Abstract: Test-time adaptation of pre-trained vision-language models has emerged as a technique for tackling distribution shifts during the test time. Although existing methods, especially those based on Test-time Prompt Tuning (TPT), have shown promising results, their high computational cost associated with parameter optimization presents challenges for scalability and practical application. This paper unveils the unnecessary nature of backpropagation in existing methods from a loss landscape perspective. Building on this insight, this paper proposes a simple yet effective framework called Test-time Loss Landscape Adaptation (TLLA). TLLA leverages the relative position between the training minimum and test loss landscapes to guide the adaptation process, avoiding the update of model parameters at test time. Specifically, it mainly consists of two main stages: In the prompt tuning stage, a Sharpness-Aware Prompt Tuning (SAPT) method is introduced to identify the training flat minimum, setting the foundation for the subsequent test-time adaptation; In the test stage, a Sharpness-based Test Sample Selection (STSS) approach is utilized to ensure the alignment of flat minima within the training loss landscape and each augmented test sample's loss landscape. Extensive experiments on both domain generalization and cross-dataset benchmarks demonstrate that TLLA achieves state-of-the-art performances while significantly reducing computational overhead. Notably, TLLA surpasses TPT by an average of 5.32\\% and 6.98\\% on four ImageNet variant datasets when employing ResNet50 and ViT-B/16 image encoders, respectively. The code will be available soon.",
        "arxiv_id": "2501.18864",
        "ARXIVID": "2501.18864",
        "COMMENT": "Matches criterion 2 as it focuses on vision-language models and introduces a novel test-time adaptation method.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2501.18716": {
        "authors": [
            "Andrew M Birnbaum",
            "Adam Buchwald",
            "Peter Turkeltaub",
            "Adam Jacks",
            "Yu Huang",
            "Abhisheck Datta",
            "Lucas C Parra",
            "Lukas A Hirsch"
        ],
        "title": "Full-Head Segmentation of MRI with Abnormal Brain Anatomy: Model and Data Release",
        "abstract": "arXiv:2501.18716v1 Announce Type: new  Abstract: The goal of this work was to develop a deep network for whole-head segmentation, including clinical MRIs with abnormal anatomy, and compile the first public benchmark dataset for this purpose. We collected 91 MRIs with volumetric segmentation labels for a diverse set of human subjects (4 normal, 32 traumatic brain injuries, and 57 strokes). These clinical cases are characterized by extended cerebrospinal fluid (CSF) in regions normally containing the brain. Training labels were generated by manually correcting initial automated segmentations for skin/scalp, skull, CSF, gray matter, white matter, air cavity, and extracephalic air. We developed a MultiAxial network consisting of three 2D U-Net models that operate independently in sagittal, axial, and coronal planes and are then combined to produce a single 3D segmentation. The MultiAxial network achieved test-set Dice scores of 0.88 (median plus-minus 0.04). For brain tissue, it significantly outperforms existing brain segmentation methods (MultiAxial: 0.898 plus-minus 0.041, SynthSeg: 0.758 plus-minus 0.054, BrainChop: 0.757 plus-minus 0.125). The MultiAxial network gains in robustness by avoiding the need for coregistration with an atlas. It performed well in regions with abnormal anatomy and on images that have been de-identified. It enables more robust current flow modeling when incorporated into ROAST, a widely-used modeling toolbox for transcranial electric stimulation. We are releasing a state-of-the-art model for whole-head MRI segmentation, along with a dataset of 61 clinical MRIs and training labels, including non-brain structures. Together, the model and data may serve as a benchmark for future efforts.",
        "arxiv_id": "2501.18716",
        "ARXIVID": "2501.18716",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for MRI segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2501.18753": {
        "authors": [
            "Jian Hu",
            "Zixu Cheng",
            "Shaogang Gong"
        ],
        "title": "INT: Instance-Specific Negative Mining for Task-Generic Promptable Segmentation",
        "abstract": "arXiv:2501.18753v1 Announce Type: new  Abstract: Task-generic promptable image segmentation aims to achieve segmentation of diverse samples under a single task description by utilizing only one task-generic prompt. Current methods leverage the generalization capabilities of Vision-Language Models (VLMs) to infer instance-specific prompts from these task-generic prompts in order to guide the segmentation process. However, when VLMs struggle to generalise to some image instances, predicting instance-specific prompts becomes poor. To solve this problem, we introduce \\textbf{I}nstance-specific \\textbf{N}egative Mining for \\textbf{T}ask-Generic Promptable Segmentation (\\textbf{INT}). The key idea of INT is to adaptively reduce the influence of irrelevant (negative) prior knowledge whilst to increase the use the most plausible prior knowledge, selected by negative mining with higher contrast, in order to optimise instance-specific prompts generation. Specifically, INT consists of two components: (1) instance-specific prompt generation, which progressively fliters out incorrect information in prompt generation; (2) semantic mask generation, which ensures each image instance segmentation matches correctly the semantics of the instance-specific prompts. INT is validated on six datasets, including camouflaged objects and medical images, demonstrating its effectiveness, robustness and scalability.",
        "arxiv_id": "2501.18753",
        "ARXIVID": "2501.18753",
        "COMMENT": "Matches criterion 4 as it discusses task-generic segmentation leveraging vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2501.19035": {
        "authors": [
            "Javier Montalvo",
            "Pablo Carballeira",
            "\\'Alvaro Garc\\'ia-Mart\\'in"
        ],
        "title": "SynthmanticLiDAR: A Synthetic Dataset for Semantic Segmentation on LiDAR Imaging",
        "abstract": "arXiv:2501.19035v1 Announce Type: new  Abstract: Semantic segmentation on LiDAR imaging is increasingly gaining attention, as it can provide useful knowledge for perception systems and potential for autonomous driving. However, collecting and labeling real LiDAR data is an expensive and time-consuming task. While datasets such as SemanticKITTI have been manually collected and labeled, the introduction of simulation tools such as CARLA, has enabled the creation of synthetic datasets on demand.   In this work, we present a modified CARLA simulator designed with LiDAR semantic segmentation in mind, with new classes, more consistent object labeling with their counterparts from real datasets such as SemanticKITTI, and the possibility to adjust the object class distribution. Using this tool, we have generated SynthmanticLiDAR, a synthetic dataset for semantic segmentation on LiDAR imaging, designed to be similar to SemanticKITTI, and we evaluate its contribution to the training process of different semantic segmentation algorithms by using a naive transfer learning approach. Our results show that incorporating SynthmanticLiDAR into the training process improves the overall performance of tested algorithms, proving the usefulness of our dataset, and therefore, our adapted CARLA simulator.   The dataset and simulator are available in https://github.com/vpulab/SynthmanticLiDAR.",
        "arxiv_id": "2501.19035",
        "ARXIVID": "2501.19035",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark and simulator for LiDAR semantic segmentation.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2501.18623": {
        "authors": [
            "Beichen Li",
            "Rundi Wu",
            "Armando Solar-Lezama",
            "Changxi Zheng",
            "Liang Shi",
            "Bernd Bickel",
            "Wojciech Matusik"
        ],
        "title": "VLMaterial: Procedural Material Generation with Large Vision-Language Models",
        "abstract": "arXiv:2501.18623v1 Announce Type: new  Abstract: Procedural materials, represented as functional node graphs, are ubiquitous in computer graphics for photorealistic material appearance design. They allow users to perform intuitive and precise editing to achieve desired visual appearances. However, creating a procedural material given an input image requires professional knowledge and significant effort. In this work, we leverage the ability to convert procedural materials into standard Python programs and fine-tune a large pre-trained vision-language model (VLM) to generate such programs from input images. To enable effective fine-tuning, we also contribute an open-source procedural material dataset and propose to perform program-level augmentation by prompting another pre-trained large language model (LLM). Through extensive evaluation, we show that our method outperforms previous methods on both synthetic and real-world examples.",
        "arxiv_id": "2501.18623",
        "ARXIVID": "2501.18623",
        "COMMENT": "Matches criterion 2 as it discusses fine-tuning a large vision-language model (VLM) for procedural material generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2501.19086": {
        "authors": [
            "Xiangyu Sun",
            "Xiaoguang Zou",
            "Yuanquan Wu",
            "Guotai Wang",
            "Shaoting Zhang"
        ],
        "title": "Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image Classification",
        "abstract": "arXiv:2501.19086v1 Announce Type: new  Abstract: X-ray imaging is pivotal in medical diagnostics, offering non-invasive insights into a range of health conditions. Recently, vision-language models, such as the Contrastive Language-Image Pretraining (CLIP) model, have demonstrated potential in improving diagnostic accuracy by leveraging large-scale image-text datasets. However, since CLIP was not initially designed for medical images, several CLIP-like models trained specifically on medical images have been developed. Despite their enhanced performance, issues of fairness - particularly regarding demographic attributes - remain largely unaddressed. In this study, we perform a comprehensive fairness analysis of CLIP-like models applied to X-ray image classification. We assess their performance and fairness across diverse patient demographics and disease categories using zero-shot inference and various fine-tuning techniques, including Linear Probing, Multilayer Perceptron (MLP), Low-Rank Adaptation (LoRA), and full fine-tuning. Our results indicate that while fine-tuning improves model accuracy, fairness concerns persist, highlighting the need for further fairness interventions in these foundational models.",
        "arxiv_id": "2501.19086",
        "ARXIVID": "2501.19086",
        "COMMENT": "Matches criterion 4 as it analyzes fairness in vision foundation models applied to X-ray image classification.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2501.19111": {
        "authors": [
            "Zhengqin Lai",
            "Xiaopeng Hong",
            "Yabin Wang",
            "Xiaobai Li"
        ],
        "title": "A Benchmark for Incremental Micro-expression Recognition",
        "abstract": "arXiv:2501.19111v1 Announce Type: new  Abstract: Micro-expression recognition plays a pivotal role in understanding hidden emotions and has applications across various fields. Traditional recognition methods assume access to all training data at once, but real-world scenarios involve continuously evolving data streams. To respond to the requirement of adapting to new data while retaining previously learned knowledge, we introduce the first benchmark specifically designed for incremental micro-expression recognition. Our contributions include: Firstly, we formulate the incremental learning setting tailored for micro-expression recognition. Secondly, we organize sequential datasets with carefully curated learning orders to reflect real-world scenarios. Thirdly, we define two cross-evaluation-based testing protocols, each targeting distinct evaluation objectives. Finally, we provide six baseline methods and their corresponding evaluation results. This benchmark lays the groundwork for advancing incremental micro-expression recognition research. All code used in this study will be made publicly available.",
        "arxiv_id": "2501.19111",
        "ARXIVID": "2501.19111",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for incremental micro-expression recognition.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2501.18620": {
        "authors": [
            "Ping-Rui Tsai",
            "Chi-hsiang Wang",
            "Yu-Cheng Liao",
            "Tzay-Ming Hong"
        ],
        "title": "Three Laws of Statistical Linguistics Emerging in images",
        "abstract": "arXiv:2501.18620v1 Announce Type: new  Abstract: Images, as a product evolving alongside civilization, develop similarly to natural languages with the advancement of civilization. Not only are images abundant in daily life, but are also influenced by technology in shaping their forms, embodying various characteristics as they evolve in time. Language is a sequence of symbols that represents thoughts. While a written language is typically associated with the close integration of text and sound, as a combination of visual symbols and perception, the communicative power of image is no less significant. This is especially notable since 60% of the sensory input received by our central nervous system comes from vision. Given the symbolic system inherent in images, we are curious whether images can also exhibit the laws of statistical linguistics. To explore this, we begin with the relationship between human thought and visual perception to decode how images are formed by the latter mechanism. Building upon previous studies that established the high correlation between pre-trained deep convolutional neural networks and the human visual system, we use the VGG-19 to define words via each kernel and calculate the number of pixels with grayscale values greater than 90%. By (a) ranking words frequency, (b) randomizing the order of kernel appearances and performing the same word count accumulation, and (c) summing the word counts layer by layer, we are surprised to find that Zipf's, Heaps', and Benford's laws of statistical linguistics also exist in the words that comprises the text representing different images.",
        "arxiv_id": "2501.18620",
        "ARXIVID": "2501.18620",
        "COMMENT": "This paper does not match any specific criteria but explores statistical linguistics in images, which is an interesting conceptual study.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2501.18635": {
        "authors": [
            "Sophie Kerga{\\ss}ner",
            "Taimoor Tariq",
            "Piotr Didyk"
        ],
        "title": "Towards Understanding Depth Perception in Foveated Rendering",
        "abstract": "arXiv:2501.18635v1 Announce Type: new  Abstract: The true vision for real-time virtual and augmented reality is reproducing our visual reality in its entirety on immersive displays. To this end, foveated rendering leverages the limitations of spatial acuity in human peripheral vision to allocate computational resources to the fovea while reducing quality in the periphery. Such methods are often derived from studies on the spatial resolution of the human visual system and its ability to perceive blur in the periphery, enabling the potential for high spatial quality in real-time. However, the effects of blur on other visual cues that depend on luminance contrast, such as depth, remain largely unexplored. It is critical to understand this interplay, as accurate depth representation is a fundamental aspect of visual realism. In this paper, we present the first evaluation exploring the effects of foveated rendering on stereoscopic depth perception. We design a psychovisual experiment to quantitatively study the effects of peripheral blur on depth perception. Our analysis demonstrates that stereoscopic acuity remains unaffected (or even improves) by high levels of peripheral blur. Based on our studies, we derive a simple perceptual model that determines the amount of foveation that does not affect stereoacuity. Furthermore, we analyze the model in the context of common foveation practices reported in literature. The findings indicate that foveated rendering does not impact stereoscopic depth perception, and stereoacuity remains unaffected up to 2x stronger foveation than commonly used. Finally, we conduct a validation experiment and show that our findings hold for complex natural stimuli.",
        "arxiv_id": "2501.18635",
        "ARXIVID": "2501.18635",
        "COMMENT": "This paper does not match any specific criteria but explores depth perception in foveated rendering, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2501.18817": {
        "authors": [
            "Andrey Borro",
            "Patricia J Riddle",
            "Michael W Barley",
            "Michael J Witbrock"
        ],
        "title": "Bridging the Reasoning Gap: Small LLMs Can Plan with Generalised Strategies",
        "abstract": "arXiv:2501.18817v1 Announce Type: new  Abstract: Recent advancements in the reasoning skills of Large Language Models (LLMs) demonstrate an increase in the ability of LLMs to solve simple planning tasks. However, as long as the driving force behind improved reasoning capability is the size and complexity of the model, the financial and computational costs associated with running them will also increase. This trend raises questions about continued accessibility and whether these improvements will increase at the same pace as models continue to grow in size and expense. We propose two approaches to enhance the reasoning ability of less resource-intensive LLMs. (1) Provide them with a generalised strategy for solving tasks within a given domain, generated by a more resource-intensive LLM. (2) Exploit their cost-effectiveness by iteratively prompting these models to correct errors in their proposed solutions. Our empirical results from planning and mathematical reasoning tasks demonstrate that these methods improve the performance of less resource-intensive LLMs to levels comparable with their more resource-intensive counterparts, at a fraction of the cost. Additionally, we show that the utilisation of generalised strategies in our experiments reduced the cost of the less resource-intensive model by nearly 30 percent on average.",
        "arxiv_id": "2501.18817",
        "ARXIVID": "2501.18817",
        "COMMENT": "This paper does not match any specific criteria but discusses reasoning and planning in LLMs, which is tangentially relevant to your friend's interest in generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2501.18913": {
        "authors": [
            "Tongda Xu",
            "Xiyan Cai",
            "Xinjie Zhang",
            "Xingtong Ge",
            "Dailan He",
            "Ming Sun",
            "Jingjing Liu",
            "Ya-Qin Zhang",
            "Jian Li",
            "Yan Wang"
        ],
        "title": "Rethinking Diffusion Posterior Sampling: From Conditional Score Estimator to Maximizing a Posterior",
        "abstract": "arXiv:2501.18913v1 Announce Type: new  Abstract: Recent advancements in diffusion models have been leveraged to address inverse problems without additional training, and Diffusion Posterior Sampling (DPS) (Chung et al., 2022a) is among the most popular approaches. Previous analyses suggest that DPS accomplishes posterior sampling by approximating the conditional score. While in this paper, we demonstrate that the conditional score approximation employed by DPS is not as effective as previously assumed, but rather aligns more closely with the principle of maximizing a posterior (MAP). This assertion is substantiated through an examination of DPS on 512x512 ImageNet images, revealing that: 1) DPS's conditional score estimation significantly diverges from the score of a well-trained conditional diffusion model and is even inferior to the unconditional score; 2) The mean of DPS's conditional score estimation deviates significantly from zero, rendering it an invalid score estimation; 3) DPS generates high-quality samples with significantly lower diversity. In light of the above findings, we posit that DPS more closely resembles MAP than a conditional score estimator, and accordingly propose the following enhancements to DPS: 1) we explicitly maximize the posterior through multi-step gradient ascent and projection; 2) we utilize a light-weighted conditional score estimator trained with only 100 images and 8 GPU hours. Extensive experimental results indicate that these proposed improvements significantly enhance DPS's performance. The source code for these improvements is provided in https://github.com/tongdaxu/Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posterior.",
        "arxiv_id": "2501.18913",
        "ARXIVID": "2501.18913",
        "COMMENT": "This paper does not match any specific criteria but discusses improvements to diffusion posterior sampling, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2501.18993": {
        "authors": [
            "Yunpeng Qu",
            "Kun Yuan",
            "Jinhua Hao",
            "Kai Zhao",
            "Qizhi Xie",
            "Ming Sun",
            "Chao Zhou"
        ],
        "title": "Visual Autoregressive Modeling for Image Super-Resolution",
        "abstract": "arXiv:2501.18993v1 Announce Type: new  Abstract: Image Super-Resolution (ISR) has seen significant progress with the introduction of remarkable generative models. However, challenges such as the trade-off issues between fidelity and realism, as well as computational complexity, have also posed limitations on their application. Building upon the tremendous success of autoregressive models in the language domain, we propose \\textbf{VARSR}, a novel visual autoregressive modeling for ISR framework with the form of next-scale prediction. To effectively integrate and preserve semantic information in low-resolution images, we propose using prefix tokens to incorporate the condition. Scale-aligned Rotary Positional Encodings are introduced to capture spatial structures and the diffusion refiner is utilized for modeling quantization residual loss to achieve pixel-level fidelity. Image-based Classifier-free Guidance is proposed to guide the generation of more realistic images. Furthermore, we collect large-scale data and design a training process to obtain robust generative priors. Quantitative and qualitative results show that VARSR is capable of generating high-fidelity and high-realism images with more efficiency than diffusion-based methods. Our codes will be released at https://github.com/qyp2000/VARSR.",
        "arxiv_id": "2501.18993",
        "ARXIVID": "2501.18993",
        "COMMENT": "This paper does not match any specific criteria but discusses a novel autoregressive modeling approach for image super-resolution, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2501.19331": {
        "authors": [
            "Han Wang",
            "Yuang Zhang",
            "Yuhong Zhang",
            "Lingxiao Lu",
            "Li Song"
        ],
        "title": "Consistent Video Colorization via Palette Guidance",
        "abstract": "arXiv:2501.19331v1 Announce Type: new  Abstract: Colorization is a traditional computer vision task and it plays an important role in many time-consuming tasks, such as old film restoration. Existing methods suffer from unsaturated color and temporally inconsistency. In this paper, we propose a novel pipeline to overcome the challenges. We regard the colorization task as a generative task and introduce Stable Video Diffusion (SVD) as our base model. We design a palette-based color guider to assist the model in generating vivid and consistent colors. The color context introduced by the palette not only provides guidance for color generation, but also enhances the stability of the generated colors through a unified color context across multiple sequences. Experiments demonstrate that the proposed method can provide vivid and stable colors for videos, surpassing previous methods.",
        "arxiv_id": "2501.19331",
        "ARXIVID": "2501.19331",
        "COMMENT": "This paper does not match any specific criteria but discusses a novel method for video colorization, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2501.19088": {
        "authors": [
            "Zhoutao Sun",
            "Xukun Shen",
            "Yong Hu",
            "Yuyou Zhong",
            "Xueyang Zhou"
        ],
        "title": "JGHand: Joint-Driven Animatable Hand Avater via 3D Gaussian Splatting",
        "abstract": "arXiv:2501.19088v1 Announce Type: new  Abstract: Since hands are the primary interface in daily interactions, modeling high-quality digital human hands and rendering realistic images is a critical research problem. Furthermore, considering the requirements of interactive and rendering applications, it is essential to achieve real-time rendering and driveability of the digital model without compromising rendering quality. Thus, we propose Jointly 3D Gaussian Hand (JGHand), a novel joint-driven 3D Gaussian Splatting (3DGS)-based hand representation that renders high-fidelity hand images in real-time for various poses and characters. Distinct from existing articulated neural rendering techniques, we introduce a differentiable process for spatial transformations based on 3D key points. This process supports deformations from the canonical template to a mesh with arbitrary bone lengths and poses. Additionally, we propose a real-time shadow simulation method based on per-pixel depth to simulate self-occlusion shadows caused by finger movements. Finally, we embed the hand prior and propose an animatable 3DGS representation of the hand driven solely by 3D key points. We validate the effectiveness of each component of our approach through comprehensive ablation studies. Experimental results on public datasets demonstrate that JGHand achieves real-time rendering speeds with enhanced quality, surpassing state-of-the-art methods.",
        "arxiv_id": "2501.19088",
        "ARXIVID": "2501.19088",
        "COMMENT": "This paper does not match any of the specific criteria but is related to computer vision and rendering techniques.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2501.19298": {
        "authors": [
            "Zhiyao Xu",
            "Dan Zhao",
            "Qingsong Zou",
            "Jingyu Xiao",
            "Yong Jiang",
            "Zhenhui Yuan",
            "Qing Li"
        ],
        "title": "Synthetic User Behavior Sequence Generation with Large Language Models for Smart Homes",
        "abstract": "arXiv:2501.19298v1 Announce Type: new  Abstract: In recent years, as smart home systems have become more widespread, security concerns within these environments have become a growing threat. Currently, most smart home security solutions, such as anomaly detection and behavior prediction models, are trained using fixed datasets that are precollected. However, the process of dataset collection is time-consuming and lacks the flexibility needed to adapt to the constantly evolving smart home environment. Additionally, the collection of personal data raises significant privacy concerns for users. Lately, large language models (LLMs) have emerged as a powerful tool for a wide range of tasks across diverse application domains, thanks to their strong capabilities in natural language processing, reasoning, and problem-solving. In this paper, we propose an LLM-based synthetic dataset generation IoTGen framework to enhance the generalization of downstream smart home intelligent models. By generating new synthetic datasets that reflect changes in the environment, smart home intelligent models can be retrained to overcome the limitations of fixed and outdated data, allowing them to better align with the dynamic nature of real-world home environments. Specifically, we first propose a Structure Pattern Perception Compression (SPPC) method tailored for IoT behavior data, which preserves the most informative content in the data while significantly reducing token consumption. Then, we propose a systematic approach to create prompts and implement data generation to automatically generate IoT synthetic data with normative and reasonable properties, assisting task models in adaptive training to improve generalization and real-world performance.",
        "arxiv_id": "2501.19298",
        "ARXIVID": "2501.19298",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and synthetic data generation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2501.18642": {
        "authors": [
            "Sarah Bonna",
            "Yu-Cheng Huang",
            "Ekaterina Novozhilova",
            "Sejin Paik",
            "Zhengyang Shan",
            "Michelle Yilin Feng",
            "Ge Gao",
            "Yonish Tayal",
            "Rushil Kulkarni",
            "Jialin Yu",
            "Nupur Divekar",
            "Deepti Ghadiyaram",
            "Derry Wijaya",
            "Margrit Betke"
        ],
        "title": "DebiasPI: Inference-time Debiasing by Prompt Iteration of a Text-to-Image Generative Model",
        "abstract": "arXiv:2501.18642v1 Announce Type: new  Abstract: Ethical intervention prompting has emerged as a tool to counter demographic biases of text-to-image generative AI models. Existing solutions either require to retrain the model or struggle to generate images that reflect desired distributions on gender and race. We propose an inference-time process called DebiasPI for Debiasing-by-Prompt-Iteration that provides prompt intervention by enabling the user to control the distributions of individuals' demographic attributes in image generation. DebiasPI keeps track of which attributes have been generated either by probing the internal state of the model or by using external attribute classifiers. Its control loop guides the text-to-image model to select not yet sufficiently represented attributes, With DebiasPI, we were able to create images with equal representations of race and gender that visualize challenging concepts of news headlines. We also experimented with the attributes age, body type, profession, and skin tone, and measured how attributes change when our intervention prompt targets the distribution of an unrelated attribute type. We found, for example, if the text-to-image model is asked to balance racial representation, gender representation improves but the skin tone becomes less diverse. Attempts to cover a wide range of skin colors with various intervention prompts showed that the model struggles to generate the palest skin tones. We conducted various ablation studies, in which we removed DebiasPI's attribute control, that reveal the model's propensity to generate young, male characters. It sometimes visualized career success by generating two-panel images with a pre-success dark-skinned person becoming light-skinned with success, or switching gender from pre-success female to post-success male, thus further motivating ethical intervention prompting with DebiasPI.",
        "arxiv_id": "2501.18642",
        "ARXIVID": "2501.18642",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and ethical considerations in text-to-image models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2501.19083": {
        "authors": [
            "Lei Jiang",
            "Ye Wei",
            "Hao Ni"
        ],
        "title": "MotionPCM: Real-Time Motion Synthesis with Phased Consistency Model",
        "abstract": "arXiv:2501.19083v1 Announce Type: new  Abstract: Diffusion models have become a popular choice for human motion synthesis due to their powerful generative capabilities. However, their high computational complexity and large sampling steps pose challenges for real-time applications. Fortunately, the Consistency Model (CM) provides a solution to greatly reduce the number of sampling steps from hundreds to a few, typically fewer than four, significantly accelerating the synthesis of diffusion models. However, its application to text-conditioned human motion synthesis in latent space remains challenging. In this paper, we introduce \\textbf{MotionPCM}, a phased consistency model-based approach designed to improve the quality and efficiency of real-time motion synthesis in latent space.",
        "arxiv_id": "2501.19083",
        "ARXIVID": "2501.19083",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in motion synthesis.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2501.19265": {
        "authors": [
            "David Li",
            "Anvar Kurmukov",
            "Mikhail Goncharov",
            "Roman Sokolov",
            "Mikhail Belyaev"
        ],
        "title": "Medical Semantic Segmentation with Diffusion Pretrain",
        "abstract": "arXiv:2501.19265v1 Announce Type: new  Abstract: Recent advances in deep learning have shown that learning robust feature representations is critical for the success of many computer vision tasks, including medical image segmentation. In particular, both transformer and convolutional-based architectures have benefit from leveraging pretext tasks for pretraining. However, the adoption of pretext tasks in 3D medical imaging has been less explored and remains a challenge, especially in the context of learning generalizable feature representations.   We propose a novel pretraining strategy using diffusion models with anatomical guidance, tailored to the intricacies of 3D medical image data. We introduce an auxiliary diffusion process to pretrain a model that produce generalizable feature representations, useful for a variety of downstream segmentation tasks. We employ an additional model that predicts 3D universal body-part coordinates, providing guidance during the diffusion process and improving spatial awareness in generated representations. This approach not only aids in resolving localization inaccuracies but also enriches the model's ability to understand complex anatomical structures.   Empirical validation on a 13-class organ segmentation task demonstrate the effectiveness of our pretraining technique. It surpasses existing restorative pretraining methods in 3D medical image segmentation by $7.5\\%$, and is competitive with the state-of-the-art contrastive pretraining approach, achieving an average Dice coefficient of 67.8 in a non-linear evaluation scenario.",
        "arxiv_id": "2501.19265",
        "ARXIVID": "2501.19265",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and machine learning in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2501.18674": {
        "authors": [
            "Mingyang Li",
            "Michelle Kuchera",
            "Raghuram Ramanujan",
            "Adam Anthony",
            "Curtis Hunt",
            "Yassid Ayyad"
        ],
        "title": "Unpaired Translation of Point Clouds for Modeling Detector Response",
        "abstract": "arXiv:2501.18674v1 Announce Type: new  Abstract: Modeling detector response is a key challenge in time projection chambers. We cast this problem as an unpaired point cloud translation task, between data collected from simulations and from experimental runs. Effective translation can assist with both noise rejection and the construction of high-fidelity simulators. Building on recent work in diffusion probabilistic models, we present a novel framework for performing this mapping. We demonstrate the success of our approach in both synthetic domains and in data sourced from the Active-Target Time Projection Chamber.",
        "arxiv_id": "2501.18674",
        "ARXIVID": "2501.18674",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling in a specific domain.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2501.18801": {
        "authors": [
            "Zhikang Dong",
            "Weituo Hao",
            "Ju-Chiang Wang",
            "Peng Zhang",
            "Pawel Polak"
        ],
        "title": "Every Image Listens, Every Image Dances: Music-Driven Image Animation",
        "abstract": "arXiv:2501.18801v1 Announce Type: new  Abstract: Image animation has become a promising area in multimodal research, with a focus on generating videos from reference images. While prior work has largely emphasized generic video generation guided by text, music-driven dance video generation remains underexplored. In this paper, we introduce MuseDance, an innovative end-to-end model that animates reference images using both music and text inputs. This dual input enables MuseDance to generate personalized videos that follow text descriptions and synchronize character movements with the music. Unlike existing approaches, MuseDance eliminates the need for complex motion guidance inputs, such as pose or depth sequences, making flexible and creative video generation accessible to users of all expertise levels. To advance research in this field, we present a new multimodal dataset comprising 2,904 dance videos with corresponding background music and text descriptions. Our approach leverages diffusion-based methods to achieve robust generalization, precise control, and temporal consistency, setting a new baseline for the music-driven image animation task.",
        "arxiv_id": "2501.18801",
        "ARXIVID": "2501.18801",
        "COMMENT": "Does not match any specific criteria but is related to multimodal generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2501.19255": {
        "authors": [
            "Mian Muhammad Naeem Abid",
            "Nancy Mehta",
            "Zongwei Wu",
            "Fayaz Ali Dharejo",
            "Radu Timofte"
        ],
        "title": "ContextFormer: Redefining Efficiency in Semantic Segmentation",
        "abstract": "arXiv:2501.19255v1 Announce Type: new  Abstract: Semantic segmentation assigns labels to pixels in images, a critical yet challenging task in computer vision. Convolutional methods, although capturing local dependencies well, struggle with long-range relationships. Vision Transformers (ViTs) excel in global context capture but are hindered by high computational demands, especially for high-resolution inputs. Most research optimizes the encoder architecture, leaving the bottleneck underexplored - a key area for enhancing performance and efficiency. We propose ContextFormer, a hybrid framework leveraging the strengths of CNNs and ViTs in the bottleneck to balance efficiency, accuracy, and robustness for real-time semantic segmentation. The framework's efficiency is driven by three synergistic modules: the Token Pyramid Extraction Module (TPEM) for hierarchical multi-scale representation, the Transformer and Modulating DepthwiseConv (Trans-MDC) block for dynamic scale-aware feature modeling, and the Feature Merging Module (FMM) for robust integration with enhanced spatial and contextual consistency. Extensive experiments on ADE20K, Pascal Context, CityScapes, and COCO-Stuff datasets show ContextFormer significantly outperforms existing models, achieving state-of-the-art mIoU scores, setting a new benchmark for efficiency and performance. The codes will be made publicly available.",
        "arxiv_id": "2501.19255",
        "ARXIVID": "2501.19255",
        "COMMENT": "Does not match any specific criteria but is related to computer vision advancements.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2501.18729": {
        "authors": [
            "Anthony Mendil",
            "Felix Putze"
        ],
        "title": "Motion Diffusion Autoencoders: Enabling Attribute Manipulation in Human Motion Demonstrated on Karate Techniques",
        "abstract": "arXiv:2501.18729v1 Announce Type: new  Abstract: Attribute manipulation deals with the problem of changing individual attributes of a data point or a time series, while leaving all other aspects unaffected. This work focuses on the domain of human motion, more precisely karate movement patterns. To the best of our knowledge, it presents the first success at manipulating attributes of human motion data. One of the key requirements for achieving attribute manipulation on human motion is a suitable pose representation. Therefore, we design a novel rotation-based pose representation that enables the disentanglement of the human skeleton and the motion trajectory, while still allowing an accurate reconstruction of the original anatomy. The core idea of the manipulation approach is to use a transformer encoder for discovering high-level semantics, and a diffusion probabilistic model for modeling the remaining stochastic variations. We show that the embedding space obtained from the transformer encoder is semantically meaningful and linear. This enables the manipulation of high-level attributes, by discovering their linear direction of change in the semantic embedding space and moving the embedding along said direction. The code and data are available at https://github.com/anthony-mendil/MoDiffAE.",
        "arxiv_id": "2501.18729",
        "ARXIVID": "2501.18729",
        "COMMENT": "Does not match any specific criterion but is generally relevant to generative modeling in human motion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2501.19306": {
        "authors": [
            "Jiefeng Chen",
            "Jie Ren",
            "Xinyun Chen",
            "Chengrun Yang",
            "Ruoxi Sun",
            "Sercan \\\"O Ar{\\i}k"
        ],
        "title": "SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling",
        "abstract": "arXiv:2501.19306v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have created new opportunities to enhance performance on complex reasoning tasks by leveraging test-time computation. However, conventional approaches such as repeated sampling with majority voting or reward model scoring, often face diminishing returns as test-time compute scales, in addition to requiring costly task-specific reward model training. In this paper, we present Self-Enhanced Test-Time Scaling (SETS), a novel method that leverages the self-verification and self-correction capabilities of recent advanced LLMs to overcome these limitations. SETS integrates sampling, self-verification, and self-correction into a unified framework, enabling efficient and scalable test-time computation for improved capabilities at complex tasks. Through extensive experiments on challenging planning and reasoning benchmarks, compared to the alternatives, we demonstrate that SETS achieves significant performance improvements and more favorable test-time scaling laws.",
        "arxiv_id": "2501.19306",
        "ARXIVID": "2501.19306",
        "COMMENT": "Does not match any specific criterion but is generally relevant to machine learning advancements.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2501.19155": {
        "authors": [
            "Zixi Wang",
            "Yubo Huang",
            "Wenwei Luo",
            "Tonglan Xie",
            "Mengmeng Jing",
            "Lin Zuo"
        ],
        "title": "SWAT: Sliding Window Adversarial Training for Gradual Domain Adaptation",
        "abstract": "arXiv:2501.19155v1 Announce Type: new  Abstract: Domain shifts are critical issues that harm the performance of machine learning. Unsupervised Domain Adaptation (UDA) mitigates this issue but suffers when the domain shifts are steep and drastic. Gradual Domain Adaptation (GDA) alleviates this problem in a mild way by gradually adapting from the source to the target domain using multiple intermediate domains. In this paper, we propose Sliding Window Adversarial Training (SWAT) for Gradual Domain Adaptation. SWAT uses the construction of adversarial streams to connect the feature spaces of the source and target domains. In order to gradually narrow the small gap between adjacent intermediate domains, a sliding window paradigm is designed that moves along the adversarial stream. When the window moves to the end of the stream, i.e., the target domain, the domain shift is drastically reduced. Extensive experiments are conducted on public GDA benchmarks, and the results demonstrate that the proposed SWAT significantly outperforms the state-of-the-art approaches. The implementation is available at: https://anonymous.4open.science/r/SWAT-8677.",
        "arxiv_id": "2501.19155",
        "ARXIVID": "2501.19155",
        "COMMENT": "This paper does not match any of the specific criteria but is related to domain adaptation, which is tangentially relevant to your friend's interest in machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2501.19325": {
        "authors": [
            "Daniel Rika",
            "Dror Sholomon",
            "Eli David",
            "Alexandre Pais",
            "Nathan S. Netanyahu"
        ],
        "title": "A Generic Hybrid Framework for 2D Visual Reconstruction",
        "abstract": "arXiv:2501.19325v1 Announce Type: new  Abstract: This paper presents a versatile hybrid framework for addressing 2D real-world reconstruction tasks formulated as jigsaw puzzle problems (JPPs) with square, non-overlapping pieces. Our approach integrates a deep learning (DL)-based compatibility measure (CM) model that evaluates pairs of puzzle pieces holistically, rather than focusing solely on their adjacent edges as traditionally done. This DL-based CM is paired with an optimized genetic algorithm (GA)-based solver, which iteratively searches for a global optimal arrangement using the pairwise CM scores of the puzzle pieces. Extensive experimental results highlight the framework's adaptability and robustness across multiple real-world domains. Notably, our unique hybrid methodology achieves state-of-the-art (SOTA) results in reconstructing Portuguese tile panels and large degraded puzzles with eroded boundaries.",
        "arxiv_id": "2501.19325",
        "ARXIVID": "2501.19325",
        "COMMENT": "This paper does not match any specific criteria but discusses a hybrid framework for 2D visual reconstruction, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2501.18618": {
        "authors": [
            "Xuejian Zhang",
            "Ruisi He",
            "Mi Yang",
            "Zhengyu Zhang",
            "Ziyi Qi",
            "Bo Ai"
        ],
        "title": "Vision Aided Channel Prediction for Vehicular Communications: A Case Study of Received Power Prediction Using RGB Images",
        "abstract": "arXiv:2501.18618v1 Announce Type: new  Abstract: The communication scenarios and channel characteristics of 6G will be more complex and difficult to characterize. Conventional methods for channel prediction face challenges in achieving an optimal balance between accuracy, practicality, and generalizability. Additionally, they often fail to effectively leverage environmental features. Within the framework of integration communication and artificial intelligence as a pivotal development vision for 6G, it is imperative to achieve intelligent prediction of channel characteristics. Vision-aided methods have been employed in various wireless communication tasks, excluding channel prediction, and have demonstrated enhanced efficiency and performance. In this paper, we propose a vision-aided two-stage model for channel prediction in millimeter wave vehicular communication scenarios, realizing accurate received power prediction utilizing solely RGB images. Firstly, we obtain original images of propagation environment through an RGB camera. Secondly, three typical computer vision methods including object detection, instance segmentation and binary mask are employed for environmental information extraction from original images in stage 1, and prediction of received power based on processed images is implemented in stage 2. Pre-trained YOLOv8 and ResNets are used in stages 1 and 2, respectively, and fine-tuned on datasets. Finally, we conduct five experiments to evaluate the performance of proposed model, demonstrating its feasibility, accuracy and generalization capabilities. The model proposed in this paper offers novel solutions for achieving intelligent channel prediction in vehicular communications.",
        "arxiv_id": "2501.18618",
        "ARXIVID": "2501.18618",
        "COMMENT": "This paper does not match any specific criteria but applies computer vision techniques to vehicular communication, which is tangentially related to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2501.19329": {
        "authors": [
            "Ying Zang",
            "Runlong Cao",
            "Jianqi Zhang",
            "Yidong Han",
            "Ziyue Cao",
            "Wenjun Hu",
            "Didi Zhu",
            "Lanyun Zhu",
            "Zejian Li",
            "Deyi Ji",
            "Tianrun Chen"
        ],
        "title": "Let Human Sketches Help: Empowering Challenging Image Segmentation Task with Freehand Sketches",
        "abstract": "arXiv:2501.19329v1 Announce Type: new  Abstract: Sketches, with their expressive potential, allow humans to convey the essence of an object through even a rough contour. For the first time, we harness this expressive potential to improve segmentation performance in challenging tasks like camouflaged object detection (COD). Our approach introduces an innovative sketch-guided interactive segmentation framework, allowing users to intuitively annotate objects with freehand sketches (drawing a rough contour of the object) instead of the traditional bounding boxes or points used in classic interactive segmentation models like SAM. We demonstrate that sketch input can significantly improve performance in existing iterative segmentation methods, outperforming text or bounding box annotations. Additionally, we introduce key modifications to network architectures and a novel sketch augmentation technique to fully harness the power of sketch input and further boost segmentation accuracy. Remarkably, our model' s output can be directly used to train other neural networks, achieving results comparable to pixel-by-pixel annotations--while reducing annotation time by up to 120 times, which shows great potential in democratizing the annotation process and enabling model training with less reliance on resource-intensive, laborious pixel-level annotations. We also present KOSCamo+, the first freehand sketch dataset for camouflaged object detection. The dataset, code, and the labeling tool will be open sourced.",
        "arxiv_id": "2501.19329",
        "ARXIVID": "2501.19329",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and segmentation tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}