{
    "2507.08801": {
        "authors": [
            "Hangjie Yuan",
            "Weihua Chen",
            "Jun Cen",
            "Hu Yu",
            "Jingyun Liang",
            "Shuning Chang",
            "Zhihui Lin",
            "Tao Feng",
            "Pengwei Liu",
            "Jiazheng Xing",
            "Hao Luo",
            "Jiasheng Tang",
            "Fan Wang",
            "Yi Yang"
        ],
        "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective",
        "abstract": "arXiv:2507.08801v1 Announce Type: new  Abstract: Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with a compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at https://github.com/alibaba-damo-academy/Lumos.",
        "arxiv_id": "2507.08801",
        "ARXIVID": "2507.08801",
        "COMMENT": "None of the criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.08648": {
        "authors": [
            "Haoran Sun",
            "Haoyu Bian",
            "Shaoning Zeng",
            "Yunbo Rao",
            "Xu Xu",
            "Lin Mei",
            "Jianping Gou"
        ],
        "title": "DatasetAgent: A Novel Multi-Agent System for Auto-Constructing Datasets from Real-World Images",
        "abstract": "arXiv:2507.08648v1 Announce Type: new  Abstract: Common knowledge indicates that the process of constructing image datasets usually depends on the time-intensive and inefficient method of manual collection and annotation. Large models offer a solution via data generation. Nonetheless, real-world data are obviously more valuable comparing to artificially intelligence generated data, particularly in constructing image datasets. For this reason, we propose a novel method for auto-constructing datasets from real-world images by a multiagent collaborative system, named as DatasetAgent. By coordinating four different agents equipped with Multi-modal Large Language Models (MLLMs), as well as a tool package for image optimization, DatasetAgent is able to construct high-quality image datasets according to user-specified requirements. In particular, two types of experiments are conducted, including expanding existing datasets and creating new ones from scratch, on a variety of open-source datasets. In both cases, multiple image datasets constructed by DatasetAgent are used to train various vision models for image classification, object detection, and image segmentation.",
        "arxiv_id": "2507.08648",
        "ARXIVID": "2507.08648",
        "COMMENT": "None of the criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2507.08554": {
        "authors": [
            "Cristina Mata",
            "Michael S. Ryoo",
            "Henrik Turbell"
        ],
        "title": "Image Translation with Kernel Prediction Networks for Semantic Segmentation",
        "abstract": "arXiv:2507.08554v1 Announce Type: new  Abstract: Semantic segmentation relies on many dense pixel-wise annotations to achieve the best performance, but owing to the difficulty of obtaining accurate annotations for real world data, practitioners train on large-scale synthetic datasets. Unpaired image translation is one method used to address the ensuing domain gap by generating more realistic training data in low-data regimes. Current methods for unpaired image translation train generative adversarial networks (GANs) to perform the translation and enforce pixel-level semantic matching through cycle consistency. These methods do not guarantee that the semantic matching holds, posing a problem for semantic segmentation where performance is sensitive to noisy pixel labels. We propose a novel image translation method, Domain Adversarial Kernel Prediction Network (DA-KPN), that guarantees semantic matching between the synthetic label and translation. DA-KPN estimates pixel-wise input transformation parameters of a lightweight and simple translation function. To ensure the pixel-wise transformation is realistic, DA-KPN uses multi-scale discriminators to distinguish between translated and target samples. We show DA-KPN outperforms previous GAN-based methods on syn2real benchmarks for semantic segmentation with limited access to real image labels and achieves comparable performance on face parsing.",
        "arxiv_id": "2507.08554",
        "ARXIVID": "2507.08554",
        "COMMENT": "None of the criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}