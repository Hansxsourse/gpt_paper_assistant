{
    "2509.03324": {
        "authors": [
            "Yixiong Jing",
            "Cheng Zhang",
            "Haibing Wu",
            "Guangming Wang",
            "Olaf Wysocki",
            "Brian Sheil"
        ],
        "title": "InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds",
        "abstract": "arXiv:2509.03324v1 Announce Type: new  Abstract: Point clouds are widely used for infrastructure monitoring by providing geometric information, where segmentation is required for downstream tasks such as defect detection. Existing research has automated semantic segmentation of structural components, while brick-level segmentation (identifying defects such as spalling and mortar loss) has been primarily conducted from RGB images. However, acquiring high-resolution images is impractical in low-light environments like masonry tunnels. Point clouds, though robust to dim lighting, are typically unstructured, sparse, and noisy, limiting fine-grained segmentation. We present InfraDiffusion, a zero-shot framework that projects masonry point clouds into depth maps using virtual cameras and restores them by adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific training, InfraDiffusion enhances visual clarity and geometric consistency of depth maps. Experiments on masonry bridge and tunnel point cloud datasets show significant improvements in brick-level segmentation using the Segment Anything Model (SAM), underscoring its potential for automated inspection of masonry assets. Our code and data is available at https://github.com/Jingyixiong/InfraDiffusion-official-implement.",
        "arxiv_id": "2509.03324",
        "ARXIVID": "2509.03324",
        "COMMENT": "Matches criterion 2 as it uses diffusion models for depth map restoration and segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.03044": {
        "authors": [
            "Chengjie Huang",
            "Jiafeng Yan",
            "Jing Li",
            "Lu Bai"
        ],
        "title": "DCDB: Dynamic Conditional Dual Diffusion Bridge for Ill-posed Multi-Tasks",
        "abstract": "arXiv:2509.03044v1 Announce Type: new  Abstract: Conditional diffusion models have made impressive progress in the field of image processing, but the characteristics of constructing data distribution pathways make it difficult to exploit the intrinsic correlation between tasks in multi-task scenarios, which is even worse in ill-posed tasks with a lack of training data. In addition, traditional static condition control makes it difficult for networks to learn in multi-task scenarios with its dynamically evolving characteristics. To address these challenges, we propose a dynamic conditional double diffusion bridge training paradigm to build a general framework for ill-posed multi-tasks. Firstly, this paradigm decouples the diffusion and condition generation processes, avoiding the dependence of the diffusion model on supervised data in ill-posed tasks. Secondly, generated by the same noise schedule, dynamic conditions are used to gradually adjust their statistical characteristics, naturally embed time-related information, and reduce the difficulty of network learning. We analyze the learning objectives of the network under different conditional forms in the single-step denoising process and compare the changes in its attention weights in the network, demonstrating the superiority of our dynamic conditions. Taking dehazing and visible-infrared fusion as typical ill-posed multi-task scenarios, we achieve the best performance in multiple indicators on public datasets. The code has been publicly released at: https://anonymous.4open.science/r/DCDB-D3C2.",
        "arxiv_id": "2509.03044",
        "ARXIVID": "2509.03044",
        "COMMENT": "Matches criterion 2 closely as it introduces a diffusion model for multi-task vision tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2509.02973": {
        "authors": [
            "Xianbao Hou",
            "Yonghao He",
            "Zeyd Boukhers",
            "John See",
            "Hu Su",
            "Wei Sui",
            "Cong Yang"
        ],
        "title": "InstaDA: Augmenting Instance Segmentation Data with Dual-Agent System",
        "abstract": "arXiv:2509.02973v1 Announce Type: new  Abstract: Acquiring high-quality instance segmentation data is challenging due to the labor-intensive nature of the annotation process and significant class imbalances within datasets. Recent studies have utilized the integration of Copy-Paste and diffusion models to create more diverse datasets. However, these studies often lack deep collaboration between large language models (LLMs) and diffusion models, and underutilize the rich information within the existing training data. To address these limitations, we propose InstaDA, a novel, training-free Dual-Agent system designed to augment instance segmentation datasets. First, we introduce a Text-Agent (T-Agent) that enhances data diversity through collaboration between LLMs and diffusion models. This agent features a novel Prompt Rethink mechanism, which iteratively refines prompts based on the generated images. This process not only fosters collaboration but also increases image utilization and optimizes the prompts themselves. Additionally, we present an Image-Agent (I-Agent) aimed at enriching the overall data distribution. This agent augments the training set by generating new instances conditioned on the training images. To ensure practicality and efficiency, both agents operate as independent and automated workflows, enhancing usability. Experiments conducted on the LVIS 1.0 validation set indicate that InstaDA achieves significant improvements, with an increase of +4.0 in box average precision (AP) and +3.3 in mask AP compared to the baseline. Furthermore, it outperforms the leading model, DiverGen, by +0.3 in box AP and +0.1 in mask AP, with a notable +0.7 gain in box AP on common categories and mask AP gains of +0.2 on common categories and +0.5 on frequent categories.",
        "arxiv_id": "2509.02973",
        "ARXIVID": "2509.02973",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.03498": {
        "authors": [
            "Han Li",
            "Xinyu Peng",
            "Yaoming Wang",
            "Zelin Peng",
            "Xin Chen",
            "Rongxiang Weng",
            "Jingang Wang",
            "Xunliang Cai",
            "Wenrui Dai",
            "Hongkai Xiong"
        ],
        "title": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation",
        "abstract": "arXiv:2509.03498v1 Announce Type: new  Abstract: We introduce OneCAT, a unified multimodal model that seamlessly integrates understanding, generation, and editing within a novel, pure decoder-only transformer architecture. Our framework uniquely eliminates the need for external components such as Vision Transformers (ViT) or vision tokenizer during inference, leading to significant efficiency gains, especially for high-resolution inputs. This is achieved through a modality-specific Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR) objective, which also natively supports dynamic resolutions. Furthermore, we pioneer a multi-scale visual autoregressive mechanism within the Large Language Model (LLM) that drastically reduces decoding steps compared to diffusion-based methods while maintaining state-of-the-art performance. Our findings demonstrate the powerful potential of pure autoregressive modeling as a sufficient and elegant foundation for unified multimodal intelligence. As a result, OneCAT sets a new performance standard, outperforming existing open-source unified multimodal models across benchmarks for multimodal generation, editing, and understanding.",
        "arxiv_id": "2509.03498",
        "ARXIVID": "2509.03498",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.03002": {
        "authors": [
            "Chenhao Wang",
            "Yingrui Ji",
            "Yu Meng",
            "Yunjian Zhang",
            "Yao Zhu"
        ],
        "title": "SOPSeg: Prompt-based Small Object Instance Segmentation in Remote Sensing Imagery",
        "abstract": "arXiv:2509.03002v1 Announce Type: new  Abstract: Extracting small objects from remote sensing imagery plays a vital role in various applications, including urban planning, environmental monitoring, and disaster management. While current research primarily focuses on small object detection, instance segmentation for small objects remains underexplored, with no dedicated datasets available. This gap stems from the technical challenges and high costs of pixel-level annotation for small objects. While the Segment Anything Model (SAM) demonstrates impressive zero-shot generalization, its performance on small-object segmentation deteriorates significantly, largely due to the coarse 1/16 feature resolution that causes severe loss of fine spatial details. To this end, we propose SOPSeg, a prompt-based framework specifically designed for small object segmentation in remote sensing imagery. It incorporates a region-adaptive magnification strategy to preserve fine-grained details, and employs a customized decoder that integrates edge prediction and progressive refinement for accurate boundary delineation. Moreover, we introduce a novel prompting mechanism tailored to the oriented bounding boxes widely adopted in remote sensing applications. SOPSeg outperforms existing methods in small object segmentation and facilitates efficient dataset construction for remote sensing tasks. We further construct a comprehensive small object instance segmentation dataset based on SODA-A, and will release both the model and dataset to support future research.",
        "arxiv_id": "2509.03002",
        "ARXIVID": "2509.03002",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.03154": {
        "authors": [
            "Karol Szustakowski",
            "Luk Frank",
            "Julia Esser",
            "Jan Gr\\\"undemann",
            "Marie Piraud"
        ],
        "title": "Preserving instance continuity and length in segmentation through connectivity-aware loss computation",
        "abstract": "arXiv:2509.03154v1 Announce Type: new  Abstract: In many biomedical segmentation tasks, the preservation of elongated structure continuity and length is more important than voxel-wise accuracy. We propose two novel loss functions, Negative Centerline Loss and Simplified Topology Loss, that, applied to Convolutional Neural Networks (CNNs), help preserve connectivity of output instances. Moreover, we discuss characteristics of experiment design, such as downscaling and spacing correction, that help obtain continuous segmentation masks. We evaluate our approach on a 3D light-sheet fluorescence microscopy dataset of axon initial segments (AIS), a task prone to discontinuity due to signal dropout. Compared to standard CNNs and existing topology-aware losses, our methods reduce the number of segmentation discontinuities per instance, particularly in regions with missing input signal, resulting in improved instance length calculation in downstream applications. Our findings demonstrate that structural priors embedded in the loss design can significantly enhance the reliability of segmentation for biological applications.",
        "arxiv_id": "2509.03154",
        "ARXIVID": "2509.03154",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.03041": {
        "authors": [
            "Pengyang Yu",
            "Haoquan Wang",
            "Gerard Marks",
            "Tahar Kechadi",
            "Laurence T. Yang",
            "Sahraoui Dhelim",
            "Nyothiri Aung"
        ],
        "title": "MedLiteNet: Lightweight Hybrid Medical Image Segmentation Model",
        "abstract": "arXiv:2509.03041v1 Announce Type: new  Abstract: Accurate skin-lesion segmentation remains a key technical challenge for computer-aided diagnosis of skin cancer. Convolutional neural networks, while effective, are constrained by limited receptive fields and thus struggle to model long-range dependencies. Vision Transformers capture global context, yet their quadratic complexity and large parameter budgets hinder use on the small-sample medical datasets common in dermatology. We introduce the MedLiteNet, a lightweight CNN Transformer hybrid tailored for dermoscopic segmentation that achieves high precision through hierarchical feature extraction and multi-scale context aggregation. The encoder stacks depth-wise Mobile Inverted Bottleneck blocks to curb computation, inserts a bottleneck-level cross-scale token-mixing unit to exchange information between resolutions, and embeds a boundary-aware self-attention module to sharpen lesion contours.",
        "arxiv_id": "2509.03041",
        "ARXIVID": "2509.03041",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.03465": {
        "authors": [
            "Kuan-Chuan Peng"
        ],
        "title": "Joint Training of Image Generator and Detector for Road Defect Detection",
        "abstract": "arXiv:2509.03465v1 Announce Type: new  Abstract: Road defect detection is important for road authorities to reduce the vehicle damage caused by road defects. Considering the practical scenarios where the defect detectors are typically deployed on edge devices with limited memory and computational resource, we aim at performing road defect detection without using ensemble-based methods or test-time augmentation (TTA). To this end, we propose to Jointly Train the image Generator and Detector for road defect detection (dubbed as JTGD). We design the dual discriminators for the generative model to enforce both the synthesized defect patches and overall images to look plausible. The synthesized image quality is improved by our proposed CLIP-based Fr\\'echet Inception Distance loss. The generative model in JTGD is trained jointly with the detector to encourage the generative model to synthesize harder examples for the detector. Since harder synthesized images of better quality caused by the aforesaid design are used in the data augmentation, JTGD outperforms the state-of-the-art method in the RDD2022 road defect detection benchmark across various countries under the condition of no ensemble and TTA. JTGD only uses less than 20% of the number of parameters compared with the competing baseline, which makes it more suitable for deployment on edge devices in practice.",
        "arxiv_id": "2509.03465",
        "ARXIVID": "2509.03465",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}