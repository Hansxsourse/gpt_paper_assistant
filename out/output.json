{
    "2508.21019": {
        "authors": [
            "Jiaxiang Cheng",
            "Bing Ma",
            "Xuhua Ren",
            "Hongyi Jin",
            "Kai Yu",
            "Peng Zhang",
            "Wenyue Li",
            "Yuan Zhou",
            "Tianxiang Zheng",
            "Qinglin Lu"
        ],
        "title": "POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models",
        "abstract": "arXiv:2508.21019v1 Announce Type: new  Abstract: The field of video diffusion generation faces critical bottlenecks in sampling efficiency, especially for large-scale models and long sequences. Existing video acceleration methods adopt image-based techniques but suffer from fundamental limitations: they neither model the temporal coherence of video frames nor provide single-step distillation for large-scale video models. To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a distillation framework that reduces the sampling steps of large-scale video diffusion models, enabling the generation of high-quality videos in a single step. POSE employs a carefully designed two-phase process to distill video models:(i) stability priming: a warm-up mechanism to stabilize adversarial distillation that adapts the high-quality trajectory of the one-step generator from high to low signal-to-noise ratio regimes, optimizing the video quality of single-step mappings near the endpoints of flow trajectories. (ii) unified adversarial equilibrium: a flexible self-adversarial distillation mechanism that promotes stable single-step adversarial training towards a Nash equilibrium within the Gaussian noise space, generating realistic single-step videos close to real videos. For conditional video generation, we propose (iii) conditional adversarial consistency, a method to improve both semantic consistency and frame consistency between conditional frames and generated frames. Comprehensive experiments demonstrate that POSE outperforms other acceleration methods on VBench-I2V by average 7.15% in semantic alignment, temporal conference and frame quality, reducing the latency of the pre-trained model by 100$\\times$, from 1000 seconds to 10 seconds, while maintaining competitive performance.",
        "arxiv_id": "2508.21019",
        "ARXIVID": "2508.21019",
        "COMMENT": "Matches criteria 2 closely with a unified framework for video diffusion models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2508.21066": {
        "authors": [
            "Yuan Gong",
            "Xionghui Wang",
            "Jie Wu",
            "Shiyin Wang",
            "Yitong Wang",
            "Xinglong Wu"
        ],
        "title": "OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning",
        "abstract": "arXiv:2508.21066v1 Announce Type: new  Abstract: In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only \\textit{One Reward} model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io",
        "arxiv_id": "2508.21066",
        "ARXIVID": "2508.21066",
        "COMMENT": "Matches criteria 1 closely with a unified framework for mask-guided image generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.20376": {
        "authors": [
            "Mang Cao",
            "Sanping Zhou",
            "Yizhe Li",
            "Ye Deng",
            "Wenli Huang",
            "Le Wang"
        ],
        "title": "Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction",
        "abstract": "arXiv:2508.20376v1 Announce Type: new  Abstract: Sufficient cross-task interaction is crucial for success in multi-task dense prediction. However, sufficient interaction often results in high computational complexity, forcing existing methods to face the trade-off between interaction completeness and computational efficiency. To address this limitation, this work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel scanning mechanisms to adapt the Mamba modeling approach for multi-task dense prediction. On the one hand, we introduce a novel Bidirectional Interaction Scan (BI-Scan) mechanism, which constructs task-specific representations as bidirectional sequences during interaction. By integrating task-first and position-first scanning modes within a unified linear complexity architecture, BI-Scan efficiently preserves critical cross-task information. On the other hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve multi-granularity scene modeling. This design not only meets the diverse granularity requirements of various tasks but also enhances nuanced cross-task feature interactions. Extensive experiments on two challenging benchmarks, \\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its state-of-the-art competitors.",
        "arxiv_id": "2508.20376",
        "ARXIVID": "2508.20376",
        "COMMENT": "Matches criterion 2",
        "RELEVANCE": 5,
        "NOVELTY": 5
    }
}