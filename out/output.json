{
    "2503.16980": {
        "authors": [
            "Haichao Zhang",
            "Zhuowei Li",
            "Dimitris Metaxas",
            "Yun Fu"
        ],
        "title": "Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models",
        "abstract": "arXiv:2503.16980v1 Announce Type: new  Abstract: Token-based video representation has emerged as a promising approach for enabling large language models to interpret video content. However, existing token reduction techniques, such as token pruning and token merging, often disrupt essential spatial-temporal positional embeddings, failing to adequately balance computational efficiency with fewer tokens. Consequently, these methods result in relatively lengthy token sequences, limiting their applicability in scenarios requiring extreme token compression, such as video large language models. In this paper, we introduce the novel task of extreme short token reduction, aiming to represent extensive video sequences with a minimal number of tokens. To address this challenge, we propose Token Dynamics, a new video representation framework that dynamically reduces token count while preserving spatial-temporal coherence. Specifically, we disentangle video representations by separating visual embeddings from grid-level motion information, structuring them into: 1. a concise token base, created by clustering tokens that describe object-level content; 2. a token dynamics map, capturing detailed spatial-temporal motion patterns across grids. Furthermore, we introduce a cross-dynamics attention mechanism that integrates motion features into the token base without increasing token length, thereby maintaining compactness and spatial-temporal integrity. The experiments demonstrate a reduction of token count to merely 0.07% of the original tokens, with only a minor performance drop of 1.13%. Additionally, we propose two novel subtasks within extreme token reduction (fixed-length and adaptive-length compression), both effectively representing long token sequences for video-language tasks. Our method offers significantly lower theoretical complexity, fewer tokens, and enhanced throughput, thus providing an efficient solution for video LLMs.",
        "arxiv_id": "2503.16980",
        "ARXIVID": "2503.16980",
        "COMMENT": "Matches criterion 2 as it proposes a novel token representation framework for Video LLMs, which is directly related to multi-modal large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.16707": {
        "authors": [
            "Jinlong Li",
            "Cristiano Saltori",
            "Fabio Poiesi",
            "Nicu Sebe"
        ],
        "title": "Cross-Modal and Uncertainty-Aware Agglomeration for Open-Vocabulary 3D Scene Understanding",
        "abstract": "arXiv:2503.16707v1 Announce Type: new  Abstract: The lack of a large-scale 3D-text corpus has led recent works to distill open-vocabulary knowledge from vision-language models (VLMs). owever, these methods typically rely on a single VLM to align the feature spaces of 3D models within a common language space, which limits the potential of 3D models to leverage the diverse spatial and semantic capabilities encapsulated in various foundation models. In this paper, we propose Cross-modal and Uncertainty-aware Agglomeration for Open-vocabulary 3D Scene Understanding dubbed CUA-O3D, the first model to integrate multiple foundation models-such as CLIP, DINOv2, and Stable Diffusion-into 3D scene understanding. We further introduce a deterministic uncertainty estimation to adaptively distill and harmonize the heterogeneous 2D feature embeddings from these models. Our method addresses two key challenges: (1) incorporating semantic priors from VLMs alongside the geometric knowledge of spatially-aware vision foundation models, and (2) using a novel deterministic uncertainty estimation to capture model-specific uncertainties across diverse semantic and geometric sensitivities, helping to reconcile heterogeneous representations during training. Extensive experiments on ScanNetV2 and Matterport3D demonstrate that our method not only advances open-vocabulary segmentation but also achieves robust cross-domain alignment and competitive spatial perception capabilities. The code will be available at \\href{https://github.com/TyroneLi/CUA_O3D}{CUA_O3D}.",
        "arxiv_id": "2503.16707",
        "ARXIVID": "2503.16707",
        "COMMENT": "Matches criterion 4 as it integrates multiple foundation models for open-vocabulary 3D scene understanding, addressing novel challenges in cross-modal alignment.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.16710": {
        "authors": [
            "Yanyan Li",
            "Youxu Fang",
            "Zunjie Zhu",
            "Kunyi Li",
            "Yong Ding",
            "Federico Tombari"
        ],
        "title": "4D Gaussian Splatting SLAM",
        "abstract": "arXiv:2503.16710v1 Announce Type: new  Abstract: Simultaneously localizing camera poses and constructing Gaussian radiance fields in dynamic scenes establish a crucial bridge between 2D images and the 4D real world. Instead of removing dynamic objects as distractors and reconstructing only static environments, this paper proposes an efficient architecture that incrementally tracks camera poses and establishes the 4D Gaussian radiance fields in unknown scenarios by using a sequence of RGB-D images. First, by generating motion masks, we obtain static and dynamic priors for each pixel. To eliminate the influence of static scenes and improve the efficiency on learning the motion of dynamic objects, we classify the Gaussian primitives into static and dynamic Gaussian sets, while the sparse control points along with an MLP is utilized to model the transformation fields of the dynamic Gaussians. To more accurately learn the motion of dynamic Gaussians, a novel 2D optical flow map reconstruction algorithm is designed to render optical flows of dynamic objects between neighbor images, which are further used to supervise the 4D Gaussian radiance fields along with traditional photometric and geometric constraints. In experiments, qualitative and quantitative evaluation results show that the proposed method achieves robust tracking and high-quality view synthesis performance in real-world environments.",
        "arxiv_id": "2503.16710",
        "ARXIVID": "2503.16710",
        "COMMENT": "Matches criterion 3 as it introduces a novel SLAM method for dynamic scenes using 4D Gaussian radiance fields, which is a new benchmark-related approach.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.17142": {
        "authors": [
            "Davide Berasi",
            "Matteo Farina",
            "Massimiliano Mancini",
            "Elisa Ricci",
            "Nicola Strisciuglio"
        ],
        "title": "Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models",
        "abstract": "arXiv:2503.17142v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) learn a shared feature space for text and images, enabling the comparison of inputs of different modalities. While prior works demonstrated that VLMs organize natural language representations into regular structures encoding composite meanings, it remains unclear if compositional patterns also emerge in the visual embedding space. In this work, we investigate compositionality in the image domain, where the analysis of compositional properties is challenged by noise and sparsity of visual data. We address these problems and propose a framework, called Geodesically Decomposable Embeddings (GDE), that approximates image representations with geometry-aware compositional structures in the latent space. We demonstrate that visual embeddings of pre-trained VLMs exhibit a compositional arrangement, and evaluate the effectiveness of this property in the tasks of compositional classification and group robustness. GDE achieves stronger performance in compositional classification compared to its counterpart method that assumes linear geometry of the latent space. Notably, it is particularly effective for group robustness, where we achieve higher results than task-specific solutions. Our results indicate that VLMs can automatically develop a human-like form of compositional reasoning in the visual domain, making their underlying processes more interpretable. Code is available at https://github.com/BerasiDavide/vlm_image_compositionality.",
        "arxiv_id": "2503.17142",
        "ARXIVID": "2503.17142",
        "COMMENT": "Matches criterion 2 as it explores compositionality in visual representations of vision-language models, providing insights into their latent space.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2503.17352": {
        "authors": [
            "Yihe Deng",
            "Hritik Bansal",
            "Fan Yin",
            "Nanyun Peng",
            "Wei Wang",
            "Kai-Wei Chang"
        ],
        "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement",
        "abstract": "arXiv:2503.17352v1 Announce Type: new  Abstract: Recent advancements demonstrated by DeepSeek-R1 have shown that complex reasoning abilities in large language models (LLMs), including sophisticated behaviors such as self-verification and self-correction, can be achieved by RL with verifiable rewards and significantly improves model performance on challenging tasks such as AIME. Motivated by these findings, our study investigates whether similar reasoning capabilities can be successfully integrated into large vision-language models (LVLMs) and assesses their impact on challenging multimodal reasoning tasks. We consider an approach that iteratively leverages supervised fine-tuning (SFT) on lightweight training data and Reinforcement Learning (RL) to further improve model generalization. Initially, reasoning capabilities were distilled from pure-text R1 models by generating reasoning steps using high-quality captions of the images sourced from diverse visual datasets. Subsequently, iterative RL training further enhance reasoning skills, with each iteration's RL-improved model generating refined SFT datasets for the next round. This iterative process yielded OpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on challenging benchmarks such as MathVista, MathVerse, and MathVision, demonstrating the potential of our strategy for robust vision-language reasoning. The code, model and data are held at https://github.com/yihedeng9/OpenVLThinker.",
        "arxiv_id": "2503.17352",
        "ARXIVID": "2503.17352",
        "COMMENT": "Matches criterion 2 as it introduces a new vision-language model (OpenVLThinker) with iterative self-improvement for complex reasoning tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2503.17071": {
        "authors": [
            "Pablo Garcia-Fernandez",
            "Lorenzo Vaquero",
            "Mingxuan Liu",
            "Feng Xue",
            "Daniel Cores",
            "Nicu Sebe",
            "Manuel Mucientes",
            "Elisa Ricci"
        ],
        "title": "Superpowering Open-Vocabulary Object Detectors for X-ray Vision",
        "abstract": "arXiv:2503.17071v1 Announce Type: new  Abstract: Open-vocabulary object detection (OvOD) is set to revolutionize security screening by enabling systems to recognize any item in X-ray scans. However, developing effective OvOD models for X-ray imaging presents unique challenges due to data scarcity and the modality gap that prevents direct adoption of RGB-based solutions. To overcome these limitations, we propose RAXO, a training-free framework that repurposes off-the-shelf RGB OvOD detectors for robust X-ray detection. RAXO builds high-quality X-ray class descriptors using a dual-source retrieval strategy. It gathers relevant RGB images from the web and enriches them via a novel X-ray material transfer mechanism, eliminating the need for labeled databases. These visual descriptors replace text-based classification in OvOD, leveraging intra-modal feature distances for robust detection. Extensive experiments demonstrate that RAXO consistently improves OvOD performance, providing an average mAP increase of up to 17.0 points over base detectors. To further support research in this emerging field, we also introduce DET-COMPASS, a new benchmark featuring bounding box annotations for over 300 object categories, enabling large-scale evaluation of OvOD in X-ray. Code and dataset available at: https://github.com/PAGF188/RAXO.",
        "arxiv_id": "2503.17071",
        "ARXIVID": "2503.17071",
        "COMMENT": "Matches criterion 4 as it discusses open-vocabulary object detection in X-ray vision and introduces a new benchmark, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.16929": {
        "authors": [
            "Shicheng Li",
            "Lei Li",
            "Kun Ouyang",
            "Shuhuai Ren",
            "Yuanxin Liu",
            "Yuanxing Zhang",
            "Fuzheng Zhang",
            "Lingpeng Kong",
            "Qi Liu",
            "Xu Sun"
        ],
        "title": "TEMPO: Temporal Preference Optimization of Video LLMs via Difficulty Scheduling and Pre-SFT Alignment",
        "abstract": "arXiv:2503.16929v1 Announce Type: new  Abstract: Video Large Language Models (Video LLMs) have achieved significant success by leveraging a two-stage paradigm: pretraining on large-scale video-text data for vision-language alignment, followed by supervised fine-tuning (SFT) for task-specific capabilities. However, existing approaches struggle with temporal reasoning due to weak temporal correspondence in the data and reliance on the next-token prediction paradigm during training. To address these limitations, we propose TEMPO (TEMporal Preference Optimization), a systematic framework that enhances Video LLMs' temporal reasoning capabilities through Direct Preference Optimization (DPO). To facilitate this, we introduce an automated preference data generation pipeline that systematically constructs preference pairs by selecting videos that are rich in temporal information, designing video-specific perturbation strategies, and finally evaluating model responses on clean and perturbed video inputs. Our temporal alignment features two key innovations: curriculum learning which that progressively increases perturbation difficulty to improve model robustness and adaptability; and ``Pre-SFT Alignment'', applying preference optimization before instruction tuning to prioritize fine-grained temporal comprehension. Extensive experiments demonstrate that our approach consistently improves Video LLM performance across multiple benchmarks with a relatively small set of self-generated DPO data. We further analyze the transferability of DPO data across architectures and the role of difficulty scheduling in optimization. Our findings highlight our TEMPO as a scalable and efficient complement to SFT-based methods, paving the way for developing reliable Video LLMs.",
        "arxiv_id": "2503.16929",
        "ARXIVID": "2503.16929",
        "COMMENT": "Matches criterion 2 as it introduces a new framework for improving Video LLMs, focusing on temporal reasoning and optimization strategies.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.16843": {
        "authors": [
            "Jian Liang",
            "Wenke Huang",
            "Guancheng Wan",
            "Qu Yang",
            "Mang Ye"
        ],
        "title": "LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models",
        "abstract": "arXiv:2503.16843v1 Announce Type: new  Abstract: While Multimodal Large Language Models (MLLMs) excel at generalizing across modalities and tasks, effectively adapting them to specific downstream tasks while simultaneously retaining both general and specialized knowledge remains challenging. Although Low-Rank Adaptation (LoRA) is widely used to efficiently acquire specialized knowledge in MLLMs, it introduces substantial harmful redundancy during visual instruction tuning, which exacerbates the forgetting of general knowledge and degrades downstream task performance. To address this issue, we propose LoRASculpt to eliminate harmful redundant parameters, thereby harmonizing general and specialized knowledge. Specifically, under theoretical guarantees, we introduce sparse updates into LoRA to discard redundant parameters effectively. Furthermore, we propose a Conflict Mitigation Regularizer to refine the update trajectory of LoRA, mitigating knowledge conflicts with the pretrained weights. Extensive experimental results demonstrate that even at very high degree of sparsity ($\\le$ 5%), our method simultaneously enhances generalization and downstream task performance. This confirms that our approach effectively mitigates the catastrophic forgetting issue and further promotes knowledge harmonization in MLLMs.",
        "arxiv_id": "2503.16843",
        "ARXIVID": "2503.16843",
        "COMMENT": "Matches criterion 2 as it proposes LoRASculpt for improving MLLMs by harmonizing general and specialized knowledge.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.16549": {
        "authors": [
            "Felix Chen",
            "Hangjie Yuan",
            "Yunqiu Xu",
            "Tao Feng",
            "Jun Cen",
            "Pengwei Liu",
            "Zeying Huang",
            "Yi Yang"
        ],
        "title": "MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical Problems",
        "abstract": "arXiv:2503.16549v1 Announce Type: new  Abstract: Despite impressive performance across diverse tasks, Multimodal Large Language Models (MLLMs) have yet to fully demonstrate their potential in visual mathematical problem-solving, particularly in accurately perceiving and interpreting diagrams. Inspired by typical processes of humans, we hypothesize that the perception capabilities to extract meaningful information from diagrams is crucial, as it directly impacts subsequent inference processes. To validate this hypothesis, we developed FlowVerse, a comprehensive benchmark that categorizes all information used during problem-solving into four components, which are then combined into six problem versions for evaluation. Our preliminary results on FlowVerse reveal that existing MLLMs exhibit substantial limitations when extracting essential information and reasoned property from diagrams and performing complex reasoning based on these visual inputs. In response, we introduce MathFlow, a modular problem-solving pipeline that decouples perception and inference into distinct stages, thereby optimizing each independently. Given the perceptual limitations observed in current MLLMs, we trained MathFlow-P-7B as a dedicated perception model. Experimental results indicate that MathFlow-P-7B yields substantial performance gains when integrated with various closed-source and open-source inference models. This demonstrates the effectiveness of the MathFlow pipeline and its compatibility to diverse inference frameworks. The FlowVerse benchmark and code are available at https://github.com/MathFlow-zju/MathFlow.",
        "arxiv_id": "2503.16549",
        "ARXIVID": "2503.16549",
        "COMMENT": "Matches criterion 2 as it introduces MathFlow, a modular pipeline for MLLMs, and evaluates their performance on visual mathematical problems.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.16611": {
        "authors": [
            "Katja Schwarz",
            "Denys Rozumnyi",
            "Samuel Rota Bul\\`o",
            "Lorenzo Porzi",
            "Peter Kontschieder"
        ],
        "title": "A Recipe for Generating 3D Worlds From a Single Image",
        "abstract": "arXiv:2503.16611v1 Announce Type: new  Abstract: We introduce a recipe for generating immersive 3D worlds from a single image by framing the task as an in-context learning problem for 2D inpainting models. This approach requires minimal training and uses existing generative models. Our process involves two steps: generating coherent panoramas using a pre-trained diffusion model and lifting these into 3D with a metric depth estimator. We then fill unobserved regions by conditioning the inpainting model on rendered point clouds, requiring minimal fine-tuning. Tested on both synthetic and real images, our method produces high-quality 3D environments suitable for VR display. By explicitly modeling the 3D structure of the generated environment from the start, our approach consistently outperforms state-of-the-art, video synthesis-based methods along multiple quantitative image quality metrics. Project Page: https://katjaschwarz.github.io/worlds/",
        "arxiv_id": "2503.16611",
        "ARXIVID": "2503.16611",
        "COMMENT": "Matches criterion 3 as it introduces a new method for generating 3D worlds from a single image, focusing on novel angles in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2503.17153": {
        "authors": [
            "Fouad Makiyeh",
            "Huy-Dung Nguyen",
            "Patrick Chareyre",
            "Ramin Hasani",
            "Marc Blanchon",
            "Daniela Rus"
        ],
        "title": "Enhancing Steering Estimation with Semantic-Aware GNNs",
        "abstract": "arXiv:2503.17153v1 Announce Type: new  Abstract: Steering estimation is a critical task in autonomous driving, traditionally relying on 2D image-based models. In this work, we explore the advantages of incorporating 3D spatial information through hybrid architectures that combine 3D neural network models with recurrent neural networks (RNNs) for temporal modeling, using LiDAR-based point clouds as input. We systematically evaluate four hybrid 3D models, all of which outperform the 2D-only baseline, with the Graph Neural Network (GNN) - RNN model yielding the best results.   To reduce reliance on LiDAR, we leverage a pretrained unified model to estimate depth from monocular images, reconstructing pseudo-3D point clouds. We then adapt the GNN-RNN model, originally designed for LiDAR-based point clouds, to work with these pseudo-3D representations, achieving comparable or even superior performance compared to the LiDAR-based model. Additionally, the unified model provides semantic labels for each point, enabling a more structured scene representation. To further optimize graph construction, we introduce an efficient connectivity strategy where connections are predominantly formed between points of the same semantic class, with only 20\\% of inter-class connections retained. This targeted approach reduces graph complexity and computational cost while preserving critical spatial relationships.   Finally, we validate our approach on the KITTI dataset, achieving a 71% improvement over 2D-only models. Our findings highlight the advantages of 3D spatial information and efficient graph construction for steering estimation, while maintaining the cost-effectiveness of monocular images and avoiding the expense of LiDAR-based systems.",
        "arxiv_id": "2503.17153",
        "ARXIVID": "2503.17153",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for spatial understanding using semantic-aware GNNs for steering estimation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.17106": {
        "authors": [
            "Yizhe Liu",
            "Tong Jia",
            "Da Cai",
            "Hao Wang",
            "Dongyue Chen"
        ],
        "title": "GAA-TSO: Geometry-Aware Assisted Depth Completion for Transparent and Specular Objects",
        "abstract": "arXiv:2503.17106v1 Announce Type: new  Abstract: Transparent and specular objects are frequently encountered in daily life, factories, and laboratories. However, due to the unique optical properties, the depth information on these objects is usually incomplete and inaccurate, which poses significant challenges for downstream robotics tasks. Therefore, it is crucial to accurately restore the depth information of transparent and specular objects. Previous depth completion methods for these objects usually use RGB information as an additional channel of the depth image to perform depth prediction. Due to the poor-texture characteristics of transparent and specular objects, these methods that rely heavily on color information tend to generate structure-less depth predictions. Moreover, these 2D methods cannot effectively explore the 3D structure hidden in the depth channel, resulting in depth ambiguity. To this end, we propose a geometry-aware assisted depth completion method for transparent and specular objects, which focuses on exploring the 3D structural cues of the scene. Specifically, besides extracting 2D features from RGB-D input, we back-project the input depth to a point cloud and build the 3D branch to extract hierarchical scene-level 3D structural features. To exploit 3D geometric information, we design several gated cross-modal fusion modules to effectively propagate multi-level 3D geometric features to the image branch. In addition, we propose an adaptive correlation aggregation strategy to appropriately assign 3D features to the corresponding 2D features. Extensive experiments on ClearGrasp, OOD, TransCG, and STD datasets show that our method outperforms other state-of-the-art methods. We further demonstrate that our method significantly enhances the performance of downstream robotic grasping tasks.",
        "arxiv_id": "2503.17106",
        "ARXIVID": "2503.17106",
        "COMMENT": "Matches criterion 1 as it proposes a geometry-aware depth completion method for transparent and specular objects, focusing on spatial understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.16579": {
        "authors": [
            "Jonas Krumme",
            "Christoph Zetzsche"
        ],
        "title": "World Knowledge from AI Image Generation for Robot Control",
        "abstract": "arXiv:2503.16579v1 Announce Type: new  Abstract: When interacting with the world robots face a number of difficult questions, having to make decisions when given under-specified tasks where they need to make choices, often without clearly defined right and wrong answers. Humans, on the other hand, can often rely on their knowledge and experience to fill in the gaps. For example, the simple task of organizing newly bought produce into the fridge involves deciding where to put each thing individually, how to arrange them together meaningfully, e.g. putting related things together, all while there is no clear right and wrong way to accomplish this task. We could encode all this information on how to do such things explicitly into the robots' knowledge base, but this can quickly become overwhelming, considering the number of potential tasks and circumstances the robot could encounter. However, images of the real world often implicitly encode answers to such questions and can show which configurations of objects are meaningful or are usually used by humans. An image of a full fridge can give a lot of information about how things are usually arranged in relation to each other and the full fridge at large. Modern generative systems are capable of generating plausible images of the real world and can be conditioned on the environment in which the robot operates. Here we investigate the idea of using the implicit knowledge about the world of modern generative AI systems given by their ability to generate convincing images of the real world to solve under-specified tasks.",
        "arxiv_id": "2503.16579",
        "ARXIVID": "2503.16579",
        "COMMENT": "Matches criterion 1 as it explores using generative AI systems for spatial understanding in robots to solve under-specified tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.16683": {
        "authors": [
            "Zeping Liu",
            "Fan Zhang",
            "Junfeng Jiao",
            "Ni Lao",
            "Gengchen Mai"
        ],
        "title": "GAIR: Improving Multimodal Geo-Foundation Model with Geo-Aligned Implicit Representations",
        "abstract": "arXiv:2503.16683v1 Announce Type: new  Abstract: Advancements in vision and language foundation models have inspired the development of geo-foundation models (GeoFMs), enhancing performance across diverse geospatial tasks. However, many existing GeoFMs primarily focus on overhead remote sensing (RS) data while neglecting other data modalities such as ground-level imagery. A key challenge in multimodal GeoFM development is to explicitly model geospatial relationships across modalities, which enables generalizability across tasks, spatial scales, and temporal contexts. To address these limitations, we propose GAIR, a novel multimodal GeoFM architecture integrating overhead RS data, street view (SV) imagery, and their geolocation metadata. We utilize three factorized neural encoders to project an SV image, its geolocation, and an RS image into the embedding space. The SV image needs to be located within the RS image's spatial footprint but does not need to be at its geographic center. In order to geographically align the SV image and RS image, we propose a novel implicit neural representations (INR) module that learns a continuous RS image representation and looks up the RS embedding at the SV image's geolocation. Next, these geographically aligned SV embedding, RS embedding, and location embedding are trained with contrastive learning objectives from unlabeled data. We evaluate GAIR across 10 geospatial tasks spanning RS image-based, SV image-based, and location embedding-based benchmarks. Experimental results demonstrate that GAIR outperforms state-of-the-art GeoFMs and other strong baselines, highlighting its effectiveness in learning generalizable and transferable geospatial representations.",
        "arxiv_id": "2503.16683",
        "ARXIVID": "2503.16683",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models and their applications in geospatial tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.17358": {
        "authors": [
            "Jerred Chen",
            "Ronald Clark"
        ],
        "title": "Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image",
        "abstract": "arXiv:2503.17358v1 Announce Type: new  Abstract: In many robotics and VR/AR applications, fast camera motions cause a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP.",
        "arxiv_id": "2503.17358",
        "ARXIVID": "2503.17358",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for spatial understanding by estimating camera motion from motion-blurred images.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.17316": {
        "authors": [
            "Wonbong Jang",
            "Philippe Weinzaepfel",
            "Vincent Leroy",
            "Lourdes Agapito",
            "Jerome Revaud"
        ],
        "title": "Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors",
        "abstract": "arXiv:2503.17316v1 Announce Type: new  Abstract: We present Pow3r, a novel large 3D vision regression model that is highly versatile in the input modalities it accepts. Unlike previous feed-forward models that lack any mechanism to exploit known camera or scene priors at test time, Pow3r incorporates any combination of auxiliary information such as intrinsics, relative pose, dense or sparse depth, alongside input images, within a single network. Building upon the recent DUSt3R paradigm, a transformer-based architecture that leverages powerful pre-training, our lightweight and versatile conditioning acts as additional guidance for the network to predict more accurate estimates when auxiliary information is available. During training we feed the model with random subsets of modalities at each iteration, which enables the model to operate under different levels of known priors at test time. This in turn opens up new capabilities, such as performing inference in native image resolution, or point-cloud completion. Our experiments on 3D reconstruction, depth completion, multi-view depth prediction, multi-view stereo, and multi-view pose estimation tasks yield state-of-the-art results and confirm the effectiveness of Pow3r at exploiting all available information. The project webpage is https://europe.naverlabs.com/pow3r.",
        "arxiv_id": "2503.17316",
        "ARXIVID": "2503.17316",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for spatial understanding in 3D reconstruction using camera and scene priors.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.17359": {
        "authors": [
            "Jiwen Yu",
            "Yiran Qin",
            "Haoxuan Che",
            "Quande Liu",
            "Xintao Wang",
            "Pengfei Wan",
            "Di Zhang",
            "Xihui Liu"
        ],
        "title": "Position: Interactive Generative Video as Next-Generation Game Engine",
        "abstract": "arXiv:2503.17359v1 Announce Type: new  Abstract: Modern game development faces significant challenges in creativity and cost due to predetermined content in traditional game engines. Recent breakthroughs in video generation models, capable of synthesizing realistic and interactive virtual environments, present an opportunity to revolutionize game creation. In this position paper, we propose Interactive Generative Video (IGV) as the foundation for Generative Game Engines (GGE), enabling unlimited novel content generation in next-generation gaming. GGE leverages IGV's unique strengths in unlimited high-quality content synthesis, physics-aware world modeling, user-controlled interactivity, long-term memory capabilities, and causal reasoning. We present a comprehensive framework detailing GGE's core modules and a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work charts a new course for game development in the AI era, envisioning a future where AI-powered generative systems fundamentally reshape how games are created and experienced.",
        "arxiv_id": "2503.17359",
        "ARXIVID": "2503.17359",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework for generative game engines, which could be considered a new benchmark or simulator-related work in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.16832": {
        "authors": [
            "Ali Shah Ali",
            "Syed Ahmed Mahmood",
            "Mubin Saeed",
            "Andrey Konin",
            "M. Zeeshan Zia",
            "Quoc-Huy Tran"
        ],
        "title": "Joint Self-Supervised Video Alignment and Action Segmentation",
        "abstract": "arXiv:2503.16832v1 Announce Type: new  Abstract: We introduce a novel approach for simultaneous self-supervised video alignment and action segmentation based on a unified optimal transport framework. In particular, we first tackle self-supervised video alignment by developing a fused Gromov-Wasserstein optimal transport formulation with a structural prior, which trains efficiently on GPUs and needs only a few iterations for solving the optimal transport problem. Our single-task method achieves the state-of-the-art performance on multiple video alignment benchmarks and outperforms VAVA, which relies on a traditional Kantorovich optimal transport formulation with an optimality prior. Furthermore, we extend our approach by proposing a unified optimal transport framework for joint self-supervised video alignment and action segmentation, which requires training and storing a single model and saves both time and memory consumption as compared to two different single-task models. Extensive evaluations on several video alignment and action segmentation datasets demonstrate that our multi-task method achieves comparable video alignment yet superior action segmentation results over previous methods in video alignment and action segmentation respectively. Finally, to the best of our knowledge, this is the first work to unify video alignment and action segmentation into a single model.",
        "arxiv_id": "2503.16832",
        "ARXIVID": "2503.16832",
        "COMMENT": "Matches criterion 3 as it proposes a unified framework for video alignment and action segmentation, which is a novel method in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.16742": {
        "authors": [
            "Esther Y. H. Lin",
            "Yimin Ding",
            "Jogendra Kundu",
            "Yatong An",
            "Mohamed T. El-Haddad",
            "Alexander Fix"
        ],
        "title": "Digitally Prototype Your Eye Tracker: Simulating Hardware Performance using 3D Synthetic Data",
        "abstract": "arXiv:2503.16742v1 Announce Type: new  Abstract: Eye tracking (ET) is a key enabler for Augmented and Virtual Reality (AR/VR). Prototyping new ET hardware requires assessing the impact of hardware choices on eye tracking performance. This task is compounded by the high cost of obtaining data from sufficiently many variations of real hardware, especially for machine learning, which requires large training datasets. We propose a method for end-to-end evaluation of how hardware changes impact machine learning-based ET performance using only synthetic data. We utilize a dataset of real 3D eyes, reconstructed from light dome data using neural radiance fields (NeRF), to synthesize captured eyes from novel viewpoints and camera parameters. Using this framework, we demonstrate that we can predict the relative performance across various hardware configurations, accounting for variations in sensor noise, illumination brightness, and optical blur. We also compare our simulator with the publicly available eye tracking dataset from the Project Aria glasses, demonstrating a strong correlation with real-world performance. Finally, we present a first-of-its-kind analysis in which we vary ET camera positions, evaluating ET performance ranging from on-axis direct views of the eye to peripheral views on the frame. Such an analysis would have previously required manufacturing physical devices to capture evaluation data. In short, our method enables faster prototyping of ET hardware.",
        "arxiv_id": "2503.16742",
        "ARXIVID": "2503.16742",
        "COMMENT": "Matches criterion 3 as it introduces a novel simulator-based approach for prototyping eye-tracking hardware, which is a new method in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.17221": {
        "authors": [
            "Fanghua Yu",
            "Jinjin Gu",
            "Jinfan Hu",
            "Zheyuan Li",
            "Chao Dong"
        ],
        "title": "UniCon: Unidirectional Information Flow for Effective Control of Large-Scale Diffusion Models",
        "abstract": "arXiv:2503.17221v1 Announce Type: new  Abstract: We introduce UniCon, a novel architecture designed to enhance control and efficiency in training adapters for large-scale diffusion models. Unlike existing methods that rely on bidirectional interaction between the diffusion model and control adapter, UniCon implements a unidirectional flow from the diffusion network to the adapter, allowing the adapter alone to generate the final output. UniCon reduces computational demands by eliminating the need for the diffusion model to compute and store gradients during adapter training. Our results indicate that UniCon reduces GPU memory usage by one-third and increases training speed by 2.3 times, while maintaining the same adapter parameter size. Additionally, without requiring extra computational resources, UniCon enables the training of adapters with double the parameter volume of existing ControlNets. In a series of image conditional generation tasks, UniCon has demonstrated precise responsiveness to control inputs and exceptional generation capabilities.",
        "arxiv_id": "2503.17221",
        "ARXIVID": "2503.17221",
        "COMMENT": "Matches criterion 4 as it introduces UniCon, a novel architecture for controlling large-scale diffusion models, which is related to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.16776": {
        "authors": [
            "Valentin Bieri",
            "Marco Zamboni",
            "Nicolas S. Blumer",
            "Qingxuan Chen",
            "Francis Engelmann"
        ],
        "title": "OpenCity3D: What do Vision-Language Models know about Urban Environments?",
        "abstract": "arXiv:2503.16776v1 Announce Type: new  Abstract: Vision-language models (VLMs) show great promise for 3D scene understanding but are mainly applied to indoor spaces or autonomous driving, focusing on low-level tasks like segmentation. This work expands their use to urban-scale environments by leveraging 3D reconstructions from multi-view aerial imagery. We propose OpenCity3D, an approach that addresses high-level tasks, such as population density estimation, building age classification, property price prediction, crime rate assessment, and noise pollution evaluation. Our findings highlight OpenCity3D's impressive zero-shot and few-shot capabilities, showcasing adaptability to new contexts. This research establishes a new paradigm for language-driven urban analytics, enabling applications in planning, policy, and environmental monitoring. See our project page: opencity3d.github.io",
        "arxiv_id": "2503.16776",
        "ARXIVID": "2503.16776",
        "COMMENT": "Matches criterion 4 as it explores vision-language models for urban-scale 3D scene understanding, which is a novel application.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2503.17080": {
        "authors": [
            "Gensheng Pei",
            "Tao Chen",
            "Yujia Wang",
            "Xinhao Cai",
            "Xiangbo Shu",
            "Tianfei Zhou",
            "Yazhou Yao"
        ],
        "title": "Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection",
        "abstract": "arXiv:2503.17080v1 Announce Type: new  Abstract: The CLIP model has demonstrated significant advancements in aligning visual and language modalities through large-scale pre-training on image-text pairs, enabling strong zero-shot classification and retrieval capabilities on various domains. However, CLIP's training remains computationally intensive, with high demands on both data processing and memory. To address these challenges, recent masking strategies have emerged, focusing on the selective removal of image patches to improve training efficiency. Although effective, these methods often compromise key semantic information, resulting in suboptimal alignment between visual features and text descriptions. In this work, we present a concise yet effective approach called Patch Generation-to-Selection to enhance CLIP's training efficiency while preserving critical semantic content. Our method introduces a gradual masking process in which a small set of candidate patches is first pre-selected as potential mask regions. Then, we apply Sobel edge detection across the entire image to generate an edge mask that prioritizes the retention of the primary object areas. Finally, similarity scores between the candidate mask patches and their neighboring patches are computed, with optimal transport normalization refining the selection process to ensure a balanced similarity matrix. Our approach, CLIP-PGS, sets new state-of-the-art results in zero-shot classification and retrieval tasks, achieving superior performance in robustness evaluation and language compositionality benchmarks.",
        "arxiv_id": "2503.17080",
        "ARXIVID": "2503.17080",
        "COMMENT": "Matches criterion 4 as it proposes a novel method to enhance CLIP, a vision foundation model, with patch generation-to-selection.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.16873": {
        "authors": [
            "Dongseob Kim",
            "Hyunjung Shim"
        ],
        "title": "Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification",
        "abstract": "arXiv:2503.16873v1 Announce Type: new  Abstract: Multi-label classification is crucial for comprehensive image understanding, yet acquiring accurate annotations is challenging and costly. To address this, a recent study suggests exploiting unsupervised multi-label classification leveraging CLIP, a powerful vision-language model. Despite CLIP's proficiency, it suffers from view-dependent predictions and inherent bias, limiting its effectiveness. We propose a novel method that addresses these issues by leveraging multiple views near target objects, guided by Class Activation Mapping (CAM) of the classifier, and debiasing pseudo-labels derived from CLIP predictions. Our Classifier-guided CLIP Distillation (CCD) enables selecting multiple local views without extra labels and debiasing predictions to enhance classification performance. Experimental results validate our method's superiority over existing techniques across diverse datasets. The code is available at https://github.com/k0u-id/CCD.",
        "arxiv_id": "2503.16873",
        "ARXIVID": "2503.16873",
        "COMMENT": "Matches criterion 4 as it focuses on unsupervised multi-label classification using CLIP, a vision-language model.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.17238": {
        "authors": [
            "Devavrat Tomar",
            "Guillaume Vray",
            "Dwarikanath Mahapatra",
            "Sudipta Roy",
            "Jean-Philippe Thiran",
            "Behzad Bozorgtabar"
        ],
        "title": "Slide-Level Prompt Learning with Vision Language Models for Few-Shot Multiple Instance Learning in Histopathology",
        "abstract": "arXiv:2503.17238v1 Announce Type: new  Abstract: In this paper, we address the challenge of few-shot classification in histopathology whole slide images (WSIs) by utilizing foundational vision-language models (VLMs) and slide-level prompt learning. Given the gigapixel scale of WSIs, conventional multiple instance learning (MIL) methods rely on aggregation functions to derive slide-level (bag-level) predictions from patch representations, which require extensive bag-level labels for training. In contrast, VLM-based approaches excel at aligning visual embeddings of patches with candidate class text prompts but lack essential pathological prior knowledge. Our method distinguishes itself by utilizing pathological prior knowledge from language models to identify crucial local tissue types (patches) for WSI classification, integrating this within a VLM-based MIL framework. Our approach effectively aligns patch images with tissue types, and we fine-tune our model via prompt learning using only a few labeled WSIs per category. Experimentation on real-world pathological WSI datasets and ablation studies highlight our method's superior performance over existing MIL- and VLM-based methods in few-shot WSI classification tasks. Our code is publicly available at https://github.com/LTS5/SLIP.",
        "arxiv_id": "2503.17238",
        "ARXIVID": "2503.17238",
        "COMMENT": "Matches criterion 4 as it applies vision-language models to histopathology with a novel slide-level prompt learning approach.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.16854": {
        "authors": [
            "Zhibo Yang",
            "Wei Hua",
            "Sibo Song",
            "Cong Yao",
            "Yingying Zhu",
            "Wenqing Cheng",
            "Xiang Bai"
        ],
        "title": "Generative Compositor for Few-Shot Visual Information Extraction",
        "abstract": "arXiv:2503.16854v1 Announce Type: new  Abstract: Visual Information Extraction (VIE), aiming at extracting structured information from visually rich document images, plays a pivotal role in document processing. Considering various layouts, semantic scopes, and languages, VIE encompasses an extensive range of types, potentially numbering in the thousands. However, many of these types suffer from a lack of training data, which poses significant challenges. In this paper, we propose a novel generative model, named Generative Compositor, to address the challenge of few-shot VIE. The Generative Compositor is a hybrid pointer-generator network that emulates the operations of a compositor by retrieving words from the source text and assembling them based on the provided prompts. Furthermore, three pre-training strategies are employed to enhance the model's perception of spatial context information. Besides, a prompt-aware resampler is specially designed to enable efficient matching by leveraging the entity-semantic prior contained in prompts. The introduction of the prompt-based retrieval mechanism and the pre-training strategies enable the model to acquire more effective spatial and semantic clues with limited training samples. Experiments demonstrate that the proposed method achieves highly competitive results in the full-sample training, while notably outperforms the baseline in the 1-shot, 5-shot, and 10-shot settings.",
        "arxiv_id": "2503.16854",
        "ARXIVID": "2503.16854",
        "COMMENT": "Matches criterion 4 as it proposes a novel generative model for visual information extraction, which is relevant to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.16734": {
        "authors": [
            "Chengkai Huang",
            "Junda Wu",
            "Yu Xia",
            "Zixu Yu",
            "Ruhan Wang",
            "Tong Yu",
            "Ruiyi Zhang",
            "Ryan A. Rossi",
            "Branislav Kveton",
            "Dongruo Zhou",
            "Julian McAuley",
            "Lina Yao"
        ],
        "title": "Towards Agentic Recommender Systems in the Era of Multimodal Large Language Models",
        "abstract": "arXiv:2503.16734v1 Announce Type: new  Abstract: Recent breakthroughs in Large Language Models (LLMs) have led to the emergence of agentic AI systems that extend beyond the capabilities of standalone models. By empowering LLMs to perceive external environments, integrate multimodal information, and interact with various tools, these agentic systems exhibit greater autonomy and adaptability across complex tasks. This evolution brings new opportunities to recommender systems (RS): LLM-based Agentic RS (LLM-ARS) can offer more interactive, context-aware, and proactive recommendations, potentially reshaping the user experience and broadening the application scope of RS. Despite promising early results, fundamental challenges remain, including how to effectively incorporate external knowledge, balance autonomy with controllability, and evaluate performance in dynamic, multimodal settings. In this perspective paper, we first present a systematic analysis of LLM-ARS: (1) clarifying core concepts and architectures; (2) highlighting how agentic capabilities -- such as planning, memory, and multimodal reasoning -- can enhance recommendation quality; and (3) outlining key research questions in areas such as safety, efficiency, and lifelong personalization. We also discuss open problems and future directions, arguing that LLM-ARS will drive the next wave of RS innovation. Ultimately, we foresee a paradigm shift toward intelligent, autonomous, and collaborative recommendation experiences that more closely align with users' evolving needs and complex decision-making processes.",
        "arxiv_id": "2503.16734",
        "ARXIVID": "2503.16734",
        "COMMENT": "Matches criterion 2 as it discusses multimodal large language models (MLLMs) and their application in recommender systems.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.16825": {
        "authors": [
            "Xiyue Guo",
            "Jiarui Hu",
            "Junjie Hu",
            "Hujun Bao",
            "Guofeng Zhang"
        ],
        "title": "SGFormer: Satellite-Ground Fusion for 3D Semantic Scene Completion",
        "abstract": "arXiv:2503.16825v1 Announce Type: new  Abstract: Recently, camera-based solutions have been extensively explored for scene semantic completion (SSC). Despite their success in visible areas, existing methods struggle to capture complete scene semantics due to frequent visual occlusions. To address this limitation, this paper presents the first satellite-ground cooperative SSC framework, i.e., SGFormer, exploring the potential of satellite-ground image pairs in the SSC task. Specifically, we propose a dual-branch architecture that encodes orthogonal satellite and ground views in parallel, unifying them into a common domain. Additionally, we design a ground-view guidance strategy that corrects satellite image biases during feature encoding, addressing misalignment between satellite and ground views. Moreover, we develop an adaptive weighting strategy that balances contributions from satellite and ground views. Experiments demonstrate that SGFormer outperforms the state of the art on SemanticKITTI and SSCBench-KITTI-360 datasets. Our code is available on https://github.com/gxytcrc/SGFormer.",
        "arxiv_id": "2503.16825",
        "ARXIVID": "2503.16825",
        "COMMENT": "Matches criterion 3 as it proposes a new benchmark and method for satellite-ground fusion in semantic scene completion, addressing occlusion issues.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.16867": {
        "authors": [
            "Kaisi Guan",
            "Zhengfeng Lai",
            "Yuchong Sun",
            "Peng Zhang",
            "Wei Liu",
            "Kieran Liu",
            "Meng Cao",
            "Ruihua Song"
        ],
        "title": "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering",
        "abstract": "arXiv:2503.16867v1 Announce Type: new  Abstract: Precisely evaluating semantic alignment between text prompts and generated videos remains a challenge in Text-to-Video (T2V) Generation. Existing text-to-video alignment metrics like CLIPScore only generate coarse-grained scores without fine-grained alignment details, failing to align with human preference. To address this limitation, we propose ETVA, a novel Evaluation method of Text-to-Video Alignment via fine-grained question generation and answering. First, a multi-agent system parses prompts into semantic scene graphs to generate atomic questions. Then we design a knowledge-augmented multi-stage reasoning framework for question answering, where an auxiliary LLM first retrieves relevant common-sense knowledge (e.g., physical laws), and then video LLM answers the generated questions through a multi-stage reasoning mechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's correlation coefficient of 58.47, showing a much higher correlation with human judgment than existing metrics which attain only 31.0. We also construct a comprehensive benchmark specifically designed for text-to-video alignment evaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10 categories. Through a systematic evaluation of 15 existing text-to-video models, we identify their key capabilities and limitations, paving the way for next-generation T2V generation.",
        "arxiv_id": "2503.16867",
        "ARXIVID": "2503.16867",
        "COMMENT": "This paper proposes ETVA, a novel evaluation method for text-to-video alignment, which aligns with criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.17096": {
        "authors": [
            "Ruiyang Ha",
            "Songyi Jiang",
            "Bin Li",
            "Bikang Pan",
            "Yihang Zhu",
            "Junjie Zhang",
            "Xiatian Zhu",
            "Shaogang Gong",
            "Jingya Wang"
        ],
        "title": "Multi-modal Multi-platform Person Re-Identification: Benchmark and Method",
        "abstract": "arXiv:2503.17096v1 Announce Type: new  Abstract: Conventional person re-identification (ReID) research is often limited to single-modality sensor data from static cameras, which fails to address the complexities of real-world scenarios where multi-modal signals are increasingly prevalent. For instance, consider an urban ReID system integrating stationary RGB cameras, nighttime infrared sensors, and UAVs equipped with dynamic tracking capabilities. Such systems face significant challenges due to variations in camera perspectives, lighting conditions, and sensor modalities, hindering effective person ReID. To address these challenges, we introduce the MP-ReID benchmark, a novel dataset designed specifically for multi-modality and multi-platform ReID. This benchmark uniquely compiles data from 1,930 identities across diverse modalities, including RGB, infrared, and thermal imaging, captured by both UAVs and ground-based cameras in indoor and outdoor environments. Building on this benchmark, we introduce Uni-Prompt ReID, a framework with specific-designed prompts, tailored for cross-modality and cross-platform scenarios. Our method consistently outperforms state-of-the-art approaches, establishing a robust foundation for future research in complex and dynamic ReID environments. Our dataset are available at:https://mp-reid.github.io/.",
        "arxiv_id": "2503.17096",
        "ARXIVID": "2503.17096",
        "COMMENT": "This paper introduces a multi-modal and multi-platform person re-identification benchmark, which aligns with criterion 3 (new benchmarks for embodied AI).",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.17285": {
        "authors": [
            "Louis Y. Kim",
            "Michelle Karker",
            "Victoria Valledor",
            "Seiyoung C. Lee",
            "Karl F. Brzoska",
            "Margaret Duff",
            "Anthony Palladino"
        ],
        "title": "An Iterative Feedback Mechanism for Improving Natural Language Class Descriptions in Open-Vocabulary Object Detection",
        "abstract": "arXiv:2503.17285v1 Announce Type: new  Abstract: Recent advances in open-vocabulary object detection models will enable Automatic Target Recognition systems to be sustainable and repurposed by non-technical end-users for a variety of applications or missions. New, and potentially nuanced, classes can be defined with natural language text descriptions in the field, immediately before runtime, without needing to retrain the model. We present an approach for improving non-technical users' natural language text descriptions of their desired targets of interest, using a combination of analysis techniques on the text embeddings, and proper combinations of embeddings for contrastive examples. We quantify the improvement that our feedback mechanism provides by demonstrating performance with multiple publicly-available open-vocabulary object detection models.",
        "arxiv_id": "2503.17285",
        "ARXIVID": "2503.17285",
        "COMMENT": "Matches criterion 1 as it discusses methodological improvements in open-vocabulary object detection with natural language descriptions, which could be relevant to spatial understanding in embodied agents.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2503.16566": {
        "authors": [
            "Jie Zhang",
            "Zheng Yuan",
            "Zhongqi Wang",
            "Bei Yan",
            "Sibo Wang",
            "Xiangkui Cao",
            "Zonghui Guo",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "title": "REVAL: A Comprehension Evaluation on Reliability and Values of Large Vision-Language Models",
        "abstract": "arXiv:2503.16566v1 Announce Type: new  Abstract: The rapid evolution of Large Vision-Language Models (LVLMs) has highlighted the necessity for comprehensive evaluation frameworks that assess these models across diverse dimensions. While existing benchmarks focus on specific aspects such as perceptual abilities, cognitive capabilities, and safety against adversarial attacks, they often lack the breadth and depth required to provide a holistic understanding of LVLMs' strengths and limitations. To address this gap, we introduce REVAL, a comprehensive benchmark designed to evaluate the \\textbf{RE}liability and \\textbf{VAL}ue of LVLMs. REVAL encompasses over 144K image-text Visual Question Answering (VQA) samples, structured into two primary sections: Reliability, which assesses truthfulness (\\eg, perceptual accuracy and hallucination tendencies) and robustness (\\eg, resilience to adversarial attacks, typographic attacks, and image corruption), and Values, which evaluates ethical concerns (\\eg, bias and moral understanding), safety issues (\\eg, toxicity and jailbreak vulnerabilities), and privacy problems (\\eg, privacy awareness and privacy leakage). We evaluate 26 models, including mainstream open-source LVLMs and prominent closed-source models like GPT-4o and Gemini-1.5-Pro. Our findings reveal that while current LVLMs excel in perceptual tasks and toxicity avoidance, they exhibit significant vulnerabilities in adversarial scenarios, privacy preservation, and ethical reasoning. These insights underscore critical areas for future improvements, guiding the development of more secure, reliable, and ethically aligned LVLMs. REVAL provides a robust framework for researchers to systematically assess and compare LVLMs, fostering advancements in the field.",
        "arxiv_id": "2503.16566",
        "ARXIVID": "2503.16566",
        "COMMENT": "Matches criterion 2 as it evaluates large vision-language models comprehensively, focusing on their reliability and values.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.16547": {
        "authors": [
            "Sihan Wang",
            "Suiyang Jiang",
            "Yibo Gao",
            "Boming Wang",
            "Shangqi Gao",
            "Xiahai Zhuang"
        ],
        "title": "Empowering Medical Multi-Agents with Clinical Consultation Flow for Dynamic Diagnosis",
        "abstract": "arXiv:2503.16547v1 Announce Type: new  Abstract: Traditional AI-based healthcare systems often rely on single-modal data, limiting diagnostic accuracy due to incomplete information. However, recent advancements in foundation models show promising potential for enhancing diagnosis combining multi-modal information. While these models excel in static tasks, they struggle with dynamic diagnosis, failing to manage multi-turn interactions and often making premature diagnostic decisions due to insufficient persistence in information collection.To address this, we propose a multi-agent framework inspired by consultation flow and reinforcement learning (RL) to simulate the entire consultation process, integrating multiple clinical information for effective diagnosis. Our approach incorporates a hierarchical action set, structured from clinic consultation flow and medical textbook, to effectively guide the decision-making process. This strategy improves agent interactions, enabling them to adapt and optimize actions based on the dynamic state. We evaluated our framework on a public dynamic diagnosis benchmark. The proposed framework evidentially improves the baseline methods and achieves state-of-the-art performance compared to existing foundation model-based methods.",
        "arxiv_id": "2503.16547",
        "ARXIVID": "2503.16547",
        "COMMENT": "Matches criterion 3 as it introduces a multi-agent framework for dynamic diagnosis, focusing on multi-modal integration and reinforcement learning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.17267": {
        "authors": [
            "Hiromu Taketsugu",
            "Takeru Oba",
            "Takahiro Maeda",
            "Shohei Nobuhara",
            "Norimichi Ukita"
        ],
        "title": "Physical Plausibility-aware Trajectory Prediction via Locomotion Embodiment",
        "abstract": "arXiv:2503.17267v1 Announce Type: new  Abstract: Humans can predict future human trajectories even from momentary observations by using human pose-related cues. However, previous Human Trajectory Prediction (HTP) methods leverage the pose cues implicitly, resulting in implausible predictions. To address this, we propose Locomotion Embodiment, a framework that explicitly evaluates the physical plausibility of the predicted trajectory by locomotion generation under the laws of physics. While the plausibility of locomotion is learned with an indifferentiable physics simulator, it is replaced by our differentiable Locomotion Value function to train an HTP network in a data-driven manner. In particular, our proposed Embodied Locomotion loss is beneficial for efficiently training a stochastic HTP network using multiple heads. Furthermore, the Locomotion Value filter is proposed to filter out implausible trajectories at inference. Experiments demonstrate that our method enhances even the state-of-the-art HTP methods across diverse datasets and problem settings. Our code is available at: https://github.com/ImIntheMiddle/EmLoco.",
        "arxiv_id": "2503.17267",
        "ARXIVID": "2503.17267",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for human trajectory prediction with a focus on physical plausibility, which is a new angle.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.16905": {
        "authors": [
            "Jian Zhang",
            "Zhiyuan Wang",
            "Zhangqi Wang",
            "Xinyu Zhang",
            "Fangzhi Xu",
            "Qika Lin",
            "Rui Mao",
            "Erik Cambria",
            "Jun Liu"
        ],
        "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and Socratic Guidance for Multimodal Scientific Problem Solving",
        "abstract": "arXiv:2503.16905v1 Announce Type: new  Abstract: Multimodal scientific problems (MSPs) involve complex issues that require the integration of multiple modalities, such as text and diagrams, presenting a significant challenge in artificial intelligence. While progress has been made in addressing traditional scientific problems, MSPs still face two primary issues: the challenge of multi-modal comprehensive reasoning in scientific problem-solving and the lack of reflective and rethinking capabilities. To address these issues, we introduce a Multi-Agent framework based on the Big Seven Personality and Socratic guidance (MAPS). This framework employs seven distinct agents that leverage feedback mechanisms and the Socratic method to guide the resolution of MSPs. To tackle the first issue, we propose a progressive four-agent solving strategy, where each agent focuses on a specific stage of the problem-solving process. For the second issue, we introduce a Critic agent, inspired by Socratic questioning, which prompts critical thinking and stimulates autonomous learning. We conduct extensive experiments on the EMMA, Olympiad, and MathVista datasets, achieving promising results that outperform the current SOTA model by 15.84% across all tasks. Meanwhile, the additional analytical experiments also verify the model's progress as well as generalization ability.",
        "arxiv_id": "2503.16905",
        "ARXIVID": "2503.16905",
        "COMMENT": "Matches criterion 2 as it discusses a multi-modal framework for scientific problem-solving, which involves reasoning across modalities.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.16979": {
        "authors": [
            "Jinbo Yan",
            "Rui Peng",
            "Zhiyan Wang",
            "Luyang Tang",
            "Jiayu Yang",
            "Jie Liang",
            "Jiahao Wu",
            "Ronggang Wang"
        ],
        "title": "Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic Scene Reconstruction via Gaussian Splatting",
        "abstract": "arXiv:2503.16979v1 Announce Type: new  Abstract: Building Free-Viewpoint Videos in a streaming manner offers the advantage of rapid responsiveness compared to offline training methods, greatly enhancing user experience. However, current streaming approaches face challenges of high per-frame reconstruction time (10s+) and error accumulation, limiting their broader application. In this paper, we propose Instant Gaussian Stream (IGS), a fast and generalizable streaming framework, to address these issues. First, we introduce a generalized Anchor-driven Gaussian Motion Network, which projects multi-view 2D motion features into 3D space, using anchor points to drive the motion of all Gaussians. This generalized Network generates the motion of Gaussians for each target frame in the time required for a single inference. Second, we propose a Key-frame-guided Streaming Strategy that refines each key frame, enabling accurate reconstruction of temporally complex scenes while mitigating error accumulation. We conducted extensive in-domain and cross-domain evaluations, demonstrating that our approach can achieve streaming with a average per-frame reconstruction time of 2s+, alongside a enhancement in view synthesis quality.",
        "arxiv_id": "2503.16979",
        "ARXIVID": "2503.16979",
        "COMMENT": "This paper introduces Instant Gaussian Stream for dynamic scene reconstruction, which partially aligns with criterion 3 (new methods for embodied AI).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.17122": {
        "authors": [
            "Jonas Mirlach",
            "Lei Wan",
            "Andreas Wiedholz",
            "Hannan Ejaz Keen",
            "Andreas Eich"
        ],
        "title": "R-LiViT: A LiDAR-Visual-Thermal Dataset Enabling Vulnerable Road User Focused Roadside Perception",
        "abstract": "arXiv:2503.17122v1 Announce Type: new  Abstract: In autonomous driving, the integration of roadside perception systems is essential for overcoming occlusion challenges and enhancing the safety of Vulnerable Road Users (VRUs). While LiDAR and visual (RGB) sensors are commonly used, thermal imaging remains underrepresented in datasets, despite its acknowledged advantages for VRU detection in extreme lighting conditions. In this paper, we present R-LiViT, the first dataset to combine LiDAR, RGB, and thermal imaging from a roadside perspective, with a strong focus on VRUs. R-LiViT captures three intersections during both day and night, ensuring a diverse dataset. It includes 10,000 LiDAR frames and 2,400 temporally and spatially aligned RGB and thermal images across over 150 traffic scenarios, with 6 and 8 annotated classes respectively, providing a comprehensive resource for tasks such as object detection and tracking. The dataset1 and the code for reproducing our evaluation results2 are made publicly available.",
        "arxiv_id": "2503.17122",
        "ARXIVID": "2503.17122",
        "COMMENT": "This paper presents a new dataset combining LiDAR, RGB, and thermal imaging for roadside perception, which partially aligns with criterion 3 (new benchmarks for embodied AI).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.16983": {
        "authors": [
            "Xu Zhang",
            "Hao Zhou",
            "Haoming Qin",
            "Xiaobin Lu",
            "Jiaxing Yan",
            "Guanzhong Wang",
            "Zeyu Chen",
            "Yi Liu"
        ],
        "title": "Enabling Versatile Controls for Video Diffusion Models",
        "abstract": "arXiv:2503.16983v1 Announce Type: new  Abstract: Despite substantial progress in text-to-video generation, achieving precise and flexible control over fine-grained spatiotemporal attributes remains a significant unresolved challenge in video generation research. To address these limitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework designed to enable fine-grained control over pre-trained video diffusion models in a unified manner. VCtrl integrates diverse user-specified control signals-such as Canny edges, segmentation masks, and human keypoints-into pretrained video diffusion models via a generalizable conditional module capable of uniformly encoding multiple types of auxiliary signals without modifying the underlying generator. Additionally, we design a unified control signal encoding pipeline and a sparse residual connection mechanism to efficiently incorporate control representations. Comprehensive experiments and human evaluations demonstrate that VCtrl effectively enhances controllability and generation quality. The source code and pre-trained models are publicly available and implemented using the PaddlePaddle framework at http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.",
        "arxiv_id": "2503.16983",
        "ARXIVID": "2503.16983",
        "COMMENT": "This paper introduces VCtrl, a framework for fine-grained control in video diffusion models, which aligns with criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.16997": {
        "authors": [
            "Qinghe Ma",
            "Jian Zhang",
            "Zekun Li",
            "Lei Qi",
            "Qian Yu",
            "Yinghuan Shi"
        ],
        "title": "Steady Progress Beats Stagnation: Mutual Aid of Foundation and Conventional Models in Mixed Domain Semi-Supervised Medical Image Segmentation",
        "abstract": "arXiv:2503.16997v1 Announce Type: new  Abstract: Large pretrained visual foundation models exhibit impressive general capabilities. However, the extensive prior knowledge inherent in these models can sometimes be a double-edged sword when adapting them to downstream tasks in specific domains. In the context of semi-supervised medical image segmentation with domain shift, foundation models like MedSAM tend to make overconfident predictions, some of which are incorrect. The error accumulation hinders the effective utilization of unlabeled data and limits further improvements. In this paper, we introduce a Synergistic training framework for Foundation and Conventional models (SynFoC) to address the issue. We observe that a conventional model trained from scratch has the ability to correct the high-confidence mispredictions of the foundation model, while the foundation model can supervise it with high-quality pseudo-labels in the early training stages. Furthermore, to enhance the collaborative training effectiveness of both models and promote reliable convergence towards optimization, the consensus-divergence consistency regularization is proposed. We demonstrate the superiority of our method across four public multi-domain datasets. In particular, our method improves the Dice score by 10.31\\% on the Prostate dataset. Our code is available at https://github.com/MQinghe/SynFoC .",
        "arxiv_id": "2503.16997",
        "ARXIVID": "2503.16997",
        "COMMENT": "Matches criterion 4 as it explores the synergy between foundation models and conventional models for medical image segmentation.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2503.17097": {
        "authors": [
            "Boyuan Zheng",
            "Shouyi Lu",
            "Renbo Huang",
            "Minqing Huang",
            "Fan Lu",
            "Wei Tian",
            "Guirong Zhuo",
            "Lu Xiong"
        ],
        "title": "R2LDM: An Efficient 4D Radar Super-Resolution Framework Leveraging Diffusion Model",
        "abstract": "arXiv:2503.17097v1 Announce Type: new  Abstract: We introduce R2LDM, an innovative approach for generating dense and accurate 4D radar point clouds, guided by corresponding LiDAR point clouds. Instead of utilizing range images or bird's eye view (BEV) images, we represent both LiDAR and 4D radar point clouds using voxel features, which more effectively capture 3D shape information. Subsequently, we propose the Latent Voxel Diffusion Model (LVDM), which performs the diffusion process in the latent space. Additionally, a novel Latent Point Cloud Reconstruction (LPCR) module is utilized to reconstruct point clouds from high-dimensional latent voxel features. As a result, R2LDM effectively generates LiDAR-like point clouds from paired raw radar data. We evaluate our approach on two different datasets, and the experimental results demonstrate that our model achieves 6- to 10-fold densification of radar point clouds, outperforming state-of-the-art baselines in 4D radar point cloud super-resolution. Furthermore, the enhanced radar point clouds generated by our method significantly improve downstream tasks, achieving up to 31.7% improvement in point cloud registration recall rate and 24.9% improvement in object detection accuracy.",
        "arxiv_id": "2503.17097",
        "ARXIVID": "2503.17097",
        "COMMENT": "Does not match any specific criteria but focuses on radar point cloud super-resolution, which is tangentially related to spatial understanding.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2503.16835": {
        "authors": [
            "Huiqiang Chen",
            "Tianqing Zhu",
            "Linlin Wang",
            "Xin Yu",
            "Longxiang Gao",
            "Wanlei Zhou"
        ],
        "title": "Safe and Reliable Diffusion Models via Subspace Projection",
        "abstract": "arXiv:2503.16835v1 Announce Type: new  Abstract: Large-scale text-to-image (T2I) diffusion models have revolutionized image generation, enabling the synthesis of highly detailed visuals from textual descriptions. However, these models may inadvertently generate inappropriate content, such as copyrighted works or offensive images. While existing methods attempt to eliminate specific unwanted concepts, they often fail to ensure complete removal, allowing the concept to reappear in subtle forms. For instance, a model may successfully avoid generating images in Van Gogh's style when explicitly prompted with 'Van Gogh', yet still reproduce his signature artwork when given the prompt 'Starry Night'. In this paper, we propose SAFER, a novel and efficient approach for thoroughly removing target concepts from diffusion models. At a high level, SAFER is inspired by the observed low-dimensional structure of the text embedding space. The method first identifies a concept-specific subspace $S_c$ associated with the target concept c. It then projects the prompt embeddings onto the complementary subspace of $S_c$, effectively erasing the concept from the generated images. Since concepts can be abstract and difficult to fully capture using natural language alone, we employ textual inversion to learn an optimized embedding of the target concept from a reference image. This enables more precise subspace estimation and enhances removal performance. Furthermore, we introduce a subspace expansion strategy to ensure comprehensive and robust concept erasure. Extensive experiments demonstrate that SAFER consistently and effectively erases unwanted concepts from diffusion models while preserving generation quality.",
        "arxiv_id": "2503.16835",
        "ARXIVID": "2503.16835",
        "COMMENT": "Does not match any specific criteria but discusses diffusion models, which are relevant to generative modeling in general.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.17032": {
        "authors": [
            "Jianchuan Chen",
            "Jingchuan Hu",
            "Gaige Wang",
            "Zhonghua Jiang",
            "Tiansong Zhou",
            "Zhiwen Chen",
            "Chengfei Lv"
        ],
        "title": "TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting",
        "abstract": "arXiv:2503.17032v1 Announce Type: new  Abstract: Realistic 3D full-body talking avatars hold great potential in AR, with applications ranging from e-commerce live streaming to holographic communication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike avatar creation, existing methods struggle with fine-grained control of facial expressions and body movements in full-body talking tasks. Additionally, they often lack sufficient details and cannot run in real-time on mobile devices. We present TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking avatar driven by various signals. Our approach starts by creating a personalized clothed human parametric template that binds Gaussians to represent appearances. We then pre-train a StyleUnet-based network to handle complex pose-dependent non-rigid deformation, which can capture high-frequency appearance details but is too resource-intensive for mobile devices. To overcome this, we \"bake\" the non-rigid deformations into a lightweight MLP-based network using a distillation technique and develop blend shapes to compensate for details. Extensive experiments show that TaoAvatar achieves state-of-the-art rendering quality while running in real-time across various devices, maintaining 90 FPS on high-definition stereo devices such as the Apple Vision Pro.",
        "arxiv_id": "2503.17032",
        "ARXIVID": "2503.17032",
        "COMMENT": "Does not match any specific criteria but focuses on creating lifelike 3D avatars, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.17354": {
        "authors": [
            "David Rein",
            "Joel Becker",
            "Amy Deng",
            "Seraphina Nix",
            "Chris Canal",
            "Daniel O'Connel",
            "Pip Arnott",
            "Ryan Bloom",
            "Thomas Broadley",
            "Katharyn Garcia",
            "Brian Goodrich",
            "Max Hasin",
            "Sami Jawhar",
            "Megan Kinniment",
            "Thomas Kwa",
            "Aron Lajko",
            "Nate Rush",
            "Lucas Jun Koba Sato",
            "Sydney Von Arx",
            "Ben West",
            "Lawrence Chan",
            "Elizabeth Barnes"
        ],
        "title": "HCAST: Human-Calibrated Autonomy Software Tasks",
        "abstract": "arXiv:2503.17354v1 Announce Type: new  Abstract: To understand and predict the societal impacts of highly autonomous AI systems, we need benchmarks with grounding, i.e., metrics that directly connect AI performance to real-world effects we care about. We present HCAST (Human-Calibrated Autonomy Software Tasks), a benchmark of 189 machine learning engineering, cybersecurity, software engineering, and general reasoning tasks. We collect 563 human baselines (totaling over 1500 hours) from people skilled in these domains, working under identical conditions as AI agents, which lets us estimate that HCAST tasks take humans between one minute and 8+ hours. Measuring the time tasks take for humans provides an intuitive metric for evaluating AI capabilities, helping answer the question \"can an agent be trusted to complete a task that would take a human X hours?\" We evaluate the success rates of AI agents built on frontier foundation models, and we find that current agents succeed 70-80% of the time on tasks that take humans less than one hour, and less than 20% of the time on tasks that take humans more than 4 hours.",
        "arxiv_id": "2503.17354",
        "ARXIVID": "2503.17354",
        "COMMENT": "Does not match any specific criteria but introduces a benchmark for evaluating AI performance, which is tangentially related to embodied AI.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2503.16795": {
        "authors": [
            "Yihan Hu",
            "Jianing Peng",
            "Yiheng Lin",
            "Ting Liu",
            "Xiaochao Qu",
            "Luoqi Liu",
            "Yao Zhao",
            "Yunchao Wei"
        ],
        "title": "DCEdit: Dual-Level Controlled Image Editing via Precisely Localized Semantics",
        "abstract": "arXiv:2503.16795v1 Announce Type: new  Abstract: This paper presents a novel approach to improving text-guided image editing using diffusion-based models. Text-guided image editing task poses key challenge of precisly locate and edit the target semantic, and previous methods fall shorts in this aspect. Our method introduces a Precise Semantic Localization strategy that leverages visual and textual self-attention to enhance the cross-attention map, which can serve as a regional cues to improve editing performance. Then we propose a Dual-Level Control mechanism for incorporating regional cues at both feature and latent levels, offering fine-grained control for more precise edits. To fully compare our methods with other DiT-based approaches, we construct the RW-800 benchmark, featuring high resolution images, long descriptive texts, real-world images, and a new text editing task. Experimental results on the popular PIE-Bench and RW-800 benchmarks demonstrate the superior performance of our approach in preserving background and providing accurate edits.",
        "arxiv_id": "2503.16795",
        "ARXIVID": "2503.16795",
        "COMMENT": "Does not match any specific criteria but focuses on text-guided image editing, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.17109": {
        "authors": [
            "Yuanmin Tang",
            "Jing Yu",
            "Keke Gai",
            "Jiamin Zhuang",
            "Gang Xiong",
            "Gaopeng Gou",
            "Qi Wu"
        ],
        "title": "Missing Target-Relevant Information Prediction with World Model for Accurate Zero-Shot Composed Image Retrieval",
        "abstract": "arXiv:2503.17109v1 Announce Type: new  Abstract: Zero-Shot Composed Image Retrieval (ZS-CIR) involves diverse tasks with a broad range of visual content manipulation intent across domain, scene, object, and attribute. The key challenge for ZS-CIR tasks is to modify a reference image according to manipulation text to accurately retrieve a target image, especially when the reference image is missing essential target content. In this paper, we propose a novel prediction-based mapping network, named PrediCIR, to adaptively predict the missing target visual content in reference images in the latent space before mapping for accurate ZS-CIR. Specifically, a world view generation module first constructs a source view by omitting certain visual content of a target view, coupled with an action that includes the manipulation intent derived from existing image-caption pairs. Then, a target content prediction module trains a world model as a predictor to adaptively predict the missing visual information guided by user intention in manipulating text at the latent space. The two modules map an image with the predicted relevant information to a pseudo-word token without extra supervision. Our model shows strong generalization ability on six ZS-CIR tasks. It obtains consistent and significant performance boosts ranging from 1.73% to 4.45% over the best methods and achieves new state-of-the-art results on ZS-CIR. Our code is available at https://github.com/Pter61/predicir.",
        "arxiv_id": "2503.17109",
        "ARXIVID": "2503.17109",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of vision-language models and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.17029": {
        "authors": [
            "Junjie Hu",
            "Shuyong Gao",
            "Qianyu Guo",
            "Yan Wang",
            "Qishan Wang",
            "Yuang Feng",
            "Wenqiang Zhang"
        ],
        "title": "AnimatePainter: A Self-Supervised Rendering Framework for Reconstructing Painting Process",
        "abstract": "arXiv:2503.17029v1 Announce Type: new  Abstract: Humans can intuitively decompose an image into a sequence of strokes to create a painting, yet existing methods for generating drawing processes are limited to specific data types and often rely on expensive human-annotated datasets. We propose a novel self-supervised framework for generating drawing processes from any type of image, treating the task as a video generation problem. Our approach reverses the drawing process by progressively removing strokes from a reference image, simulating a human-like creation sequence. Crucially, our method does not require costly datasets of real human drawing processes; instead, we leverage depth estimation and stroke rendering to construct a self-supervised dataset. We model human drawings as \"refinement\" and \"layering\" processes and introduce depth fusion layers to enable video generation models to learn and replicate human drawing behavior. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to generate realistic drawings without the need for real drawing process data.",
        "arxiv_id": "2503.17029",
        "ARXIVID": "2503.17029",
        "COMMENT": "Does not match any specific criterion but discusses a novel self-supervised framework for reconstructing painting processes, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.16782": {
        "authors": [
            "Enguang Wang",
            "Zhimao Peng",
            "Zhengyuan Xie",
            "Haori Lu",
            "Fei Yang",
            "Xialei Liu"
        ],
        "title": "Learning Part Knowledge to Facilitate Category Understanding for Fine-Grained Generalized Category Discovery",
        "abstract": "arXiv:2503.16782v1 Announce Type: new  Abstract: Generalized Category Discovery (GCD) aims to classify unlabeled data containing both seen and novel categories. Although existing methods perform well on generic datasets, they struggle in fine-grained scenarios. We attribute this difficulty to their reliance on contrastive learning over global image features to automatically capture discriminative cues, which fails to capture the subtle local differences essential for distinguishing fine-grained categories. Therefore, in this paper, we propose incorporating part knowledge to address fine-grained GCD, which introduces two key challenges: the absence of annotations for novel classes complicates the extraction of the part features, and global contrastive learning prioritizes holistic feature invariance, inadvertently suppressing discriminative local part patterns. To address these challenges, we propose PartGCD, including 1) Adaptive Part Decomposition, which automatically extracts class-specific semantic parts via Gaussian Mixture Models, and 2) Part Discrepancy Regularization, enforcing explicit separation between part features to amplify fine-grained local part distinctions.   Experiments demonstrate state-of-the-art performance across multiple fine-grained benchmarks while maintaining competitiveness on generic datasets, validating the effectiveness and robustness of our approach.",
        "arxiv_id": "2503.16782",
        "ARXIVID": "2503.16782",
        "COMMENT": "Does not match any specific criteria but focuses on fine-grained category discovery, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.16963": {
        "authors": [
            "Wei Zhang",
            "Mengting Ma",
            "Yizhen Jiang",
            "Rongrong Lian",
            "Zhenkai Wu",
            "Kangning Cui",
            "Xiaowen Ma"
        ],
        "title": "Center-guided Classifier for Semantic Segmentation of Remote Sensing Images",
        "abstract": "arXiv:2503.16963v1 Announce Type: new  Abstract: Compared with natural images, remote sensing images (RSIs) have the unique characteristic. i.e., larger intraclass variance, which makes semantic segmentation for remote sensing images more challenging. Moreover, existing semantic segmentation models for remote sensing images usually employ a vanilla softmax classifier, which has three drawbacks: (1) non-direct supervision for the pixel representations during training; (2) inadequate modeling ability of parametric softmax classifiers under large intraclass variance; and (3) opaque process of classification decision. In this paper, we propose a novel classifier (called CenterSeg) customized for RSI semantic segmentation, which solves the abovementioned problems with multiple prototypes, direct supervision under Grassmann manifold, and interpretability strategy. Specifically, for each class, our CenterSeg obtains local class centers by aggregating corresponding pixel features based on ground-truth masks, and generates multiple prototypes through hard attention assignment and momentum updating. In addition, we introduce the Grassmann manifold and constrain the joint embedding space of pixel features and prototypes based on two additional regularization terms. Especially, during the inference, CenterSeg can further provide interpretability to the model by restricting the prototype as a sample of the training set. Experimental results on three remote sensing segmentation datasets validate the effectiveness of the model. Besides the superior performance, CenterSeg has the advantages of simplicity, lightweight, compatibility, and interpretability. Code is available at https://github.com/xwmaxwma/rssegmentation.",
        "arxiv_id": "2503.16963",
        "ARXIVID": "2503.16963",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of computer vision and semantic segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.16855": {
        "authors": [
            "Koki Hirooka",
            "Abu Saleh Musa Miah",
            "Tatsuya Murakami",
            "Yuto Akiba",
            "Yong Seok Hwang",
            "Jungpil Shin"
        ],
        "title": "Stack Transformer Based Spatial-Temporal Attention Model for Dynamic Multi-Culture Sign Language Recognition",
        "abstract": "arXiv:2503.16855v1 Announce Type: new  Abstract: Hand gesture-based Sign Language Recognition (SLR) serves as a crucial communication bridge between deaf and non-deaf individuals. Existing SLR systems perform well for their cultural SL but may struggle with multi-cultural sign languages (McSL). To address these challenges, this paper proposes a Stack Spatial-Temporal Transformer Network that leverages multi-head attention mechanisms to capture both spatial and temporal dependencies with hierarchical features using the Stack Transfer concept. In the proceed, firstly, we applied a fully connected layer to make a embedding vector which has high expressive power from the original dataset, then fed them a stack newly proposed transformer to achieve hierarchical features with short-range and long-range dependency. The network architecture is composed of several stages that process spatial and temporal relationships sequentially, ensuring effective feature extraction. After making the fully connected layer, the embedding vector is processed by the Spatial Multi-Head Attention Transformer, which captures spatial dependencies between joints. In the next stage, the Temporal Multi-Head Attention Transformer captures long-range temporal dependencies, and again, the features are concatenated with the output using another skip connection. The processed features are then passed to the Feed-Forward Network (FFN), which refines the feature representations further. After the FFN, additional skip connections are applied to combine the output with earlier layers, followed by a final normalization layer to produce the final output feature tensor. This process is repeated for 10 transformer blocks. The extensive experiment shows that the JSL, KSL and ASL datasets achieved good performance accuracy. Our approach demonstrates improved performance in McSL, and it will be consider as a novel work in this domain.",
        "arxiv_id": "2503.16855",
        "ARXIVID": "2503.16855",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of spatial-temporal modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.17155": {
        "authors": [
            "Panpan Wang",
            "Liqiang Niu",
            "Fandong Meng",
            "Jinan Xu",
            "Yufeng Chen",
            "Jie Zhou"
        ],
        "title": "D2C: Unlocking the Potential of Continuous Autoregressive Image Generation with Discrete Tokens",
        "abstract": "arXiv:2503.17155v1 Announce Type: new  Abstract: In the domain of image generation, latent-based generative models occupy a dominant status; however, these models rely heavily on image tokenizer. To meet modeling requirements, autoregressive models possessing the characteristics of scalability and flexibility embrace a discrete-valued tokenizer, but face the challenge of poor image generation quality. In contrast, diffusion models take advantage of the continuous-valued tokenizer to achieve better generation quality but are subject to low efficiency and complexity. The existing hybrid models are mainly to compensate for information loss and simplify the diffusion learning process. The potential of merging discrete-valued and continuous-valued tokens in the field of image generation has not yet been explored. In this paper, we propose D2C, a novel two-stage method to enhance model generation capacity. In the first stage, the discrete-valued tokens representing coarse-grained image features are sampled by employing a small discrete-valued generator. Then in the second stage, the continuous-valued tokens representing fine-grained image features are learned conditioned on the discrete token sequence. In addition, we design two kinds of fusion modules for seamless interaction. On the ImageNet-256 benchmark, extensive experiment results validate that our model achieves superior performance compared with several continuous-valued and discrete-valued generative models on the class-conditional image generation tasks.",
        "arxiv_id": "2503.17155",
        "ARXIVID": "2503.17155",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.17276": {
        "authors": [
            "Maria Pilligua",
            "Danna Xue",
            "Javier Vazquez-Corral"
        ],
        "title": "HyperNVD: Accelerating Neural Video Decomposition via Hypernetworks",
        "abstract": "arXiv:2503.17276v1 Announce Type: new  Abstract: Decomposing a video into a layer-based representation is crucial for easy video editing for the creative industries, as it enables independent editing of specific layers. Existing video-layer decomposition models rely on implicit neural representations (INRs) trained independently for each video, making the process time-consuming when applied to new videos. Noticing this limitation, we propose a meta-learning strategy to learn a generic video decomposition model to speed up the training on new videos. Our model is based on a hypernetwork architecture which, given a video-encoder embedding, generates the parameters for a compact INR-based neural video decomposition model. Our strategy mitigates the problem of single-video overfitting and, importantly, shortens the convergence of video decomposition on new, unseen videos. Our code is available at: https://hypernvd.github.io/",
        "arxiv_id": "2503.17276",
        "ARXIVID": "2503.17276",
        "COMMENT": "Does not match any specific criterion but is relevant to video decomposition and meta-learning strategies.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.17226": {
        "authors": [
            "Aryan Yazdan Parast",
            "Basim Azam",
            "Naveed Akhtar"
        ],
        "title": "Leveraging Text-to-Image Generation for Handling Spurious Correlation",
        "abstract": "arXiv:2503.17226v1 Announce Type: new  Abstract: Deep neural networks trained with Empirical Risk Minimization (ERM) perform well when both training and test data come from the same domain, but they often fail to generalize to out-of-distribution samples. In image classification, these models may rely on spurious correlations that often exist between labels and irrelevant features of images, making predictions unreliable when those features do not exist. We propose a technique to generate training samples with text-to-image (T2I) diffusion models for addressing the spurious correlation problem. First, we compute the best describing token for the visual features pertaining to the causal components of samples by a textual inversion mechanism. Then, leveraging a language segmentation method and a diffusion model, we generate new samples by combining the causal component with the elements from other classes. We also meticulously prune the generated samples based on the prediction probabilities and attribution scores of the ERM model to ensure their correct composition for our objective. Finally, we retrain the ERM model on our augmented dataset. This process reduces the model's reliance on spurious correlations by learning from carefully crafted samples for in which this correlation does not exist. Our experiments show that across different benchmarks, our technique achieves better worst-group accuracy than the existing state-of-the-art methods.",
        "arxiv_id": "2503.17226",
        "ARXIVID": "2503.17226",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and addressing spurious correlations in image classification.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.17262": {
        "authors": [
            "Shuang Guo",
            "Friedhelm Hamann",
            "Guillermo Gallego"
        ],
        "title": "Unsupervised Joint Learning of Optical Flow and Intensity with Event Cameras",
        "abstract": "arXiv:2503.17262v1 Announce Type: new  Abstract: Event cameras rely on motion to obtain information about scene appearance. In other words, for event cameras, motion and appearance are seen both or neither, which are encoded in the output event stream. Previous works consider recovering these two visual quantities as separate tasks, which does not fit with the nature of event cameras and neglects the inherent relations between both tasks. In this paper, we propose an unsupervised learning framework that jointly estimates optical flow (motion) and image intensity (appearance), with a single network. Starting from the event generation model, we newly derive the event-based photometric error as a function of optical flow and image intensity, which is further combined with the contrast maximization framework, yielding a comprehensive loss function that provides proper constraints for both flow and intensity estimation. Exhaustive experiments show that our model achieves state-of-the-art performance for both optical flow (achieves 20% and 25% improvement in EPE and AE respectively in the unsupervised learning category) and intensity estimation (produces competitive results with other baselines, particularly in high dynamic range scenarios). Last but not least, our model achieves shorter inference time than all the other optical flow models and many of the image reconstruction models, while they output only one quantity. Project page: https://github.com/tub-rip/e2fai",
        "arxiv_id": "2503.17262",
        "ARXIVID": "2503.17262",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and unsupervised learning for event cameras.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.16921": {
        "authors": [
            "Lingfan Zhang",
            "Chen Liu",
            "Chengming Xu",
            "Kai Hu",
            "Donghao Luo",
            "Chengjie Wang",
            "Yanwei Fu",
            "Yuan Yao"
        ],
        "title": "When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive DPO",
        "abstract": "arXiv:2503.16921v1 Announce Type: new  Abstract: In recent years, the field of image generation has witnessed significant advancements, particularly in fine-tuning methods that align models with universal human preferences. This paper explores the critical role of preference data in the training process of diffusion models, particularly in the context of Diffusion-DPO and its subsequent adaptations. We investigate the complexities surrounding universal human preferences in image generation, highlighting the subjective nature of these preferences and the challenges posed by minority samples in preference datasets. Through pilot experiments, we demonstrate the existence of minority samples and their detrimental effects on model performance. We propose Adaptive-DPO -- a novel approach that incorporates a minority-instance-aware metric into the DPO objective. This metric, which includes intra-annotator confidence and inter-annotator stability, distinguishes between majority and minority samples. We introduce an Adaptive-DPO loss function which improves the DPO loss in two ways: enhancing the model's learning of majority labels while mitigating the negative impact of minority samples. Our experiments demonstrate that this method effectively handles both synthetic minority data and real-world preference data, paving the way for more effective training methodologies in image generation tasks.",
        "arxiv_id": "2503.16921",
        "ARXIVID": "2503.16921",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and improving diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.16948": {
        "authors": [
            "Yinhan Zhang",
            "Yue Ma",
            "Bingyuan Wang",
            "Qifeng Chen",
            "Zeyu Wang"
        ],
        "title": "MagicColor: Multi-Instance Sketch Colorization",
        "abstract": "arXiv:2503.16948v1 Announce Type: new  Abstract: We present \\textit{MagicColor}, a diffusion-based framework for multi-instance sketch colorization. The production of multi-instance 2D line art colorization adheres to an industry-standard workflow, which consists of three crucial stages: the design of line art characters, the coloring of individual objects, and the refinement process. The artists are required to repeat the process of coloring each instance one by one, which is inaccurate and inefficient. Meanwhile, current generative methods fail to solve this task due to the challenge of multi-instance pair data collection. To tackle these challenges, we incorporate three technical designs to ensure precise character detail transcription and achieve multi-instance sketch colorization in a single forward. Specifically, we first propose the self-play training strategy to solve the lack of training data. Then we introduce an instance guider to feed the color of the instance. To achieve accurate color matching, we present fine-grained color matching with edge loss to enhance visual quality. Equipped with the proposed modules, MagicColor enables automatically transforming sketches into vividly-colored images with accurate consistency and multi-instance control. Experiments on our collected datasets show that our model outperforms existing methods regarding chromatic precision. Specifically, our model critically automates the colorization process with zero manual adjustments, so novice users can produce stylistically consistent artwork by providing reference instances and the original line art. Our code and additional details are available at https://yinhan-zhang.github.io/color",
        "arxiv_id": "2503.16948",
        "ARXIVID": "2503.16948",
        "COMMENT": "This paper introduces MagicColor, a framework for multi-instance sketch colorization, which does not match any of the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.16944": {
        "authors": [
            "Mengtian Li",
            "Jinshu Chen",
            "Wanquan Feng",
            "Bingchuan Li",
            "Fei Dai",
            "Songtao Zhao",
            "Qian He"
        ],
        "title": "HyperLoRA: Parameter-Efficient Adaptive Generation for Portrait Synthesis",
        "abstract": "arXiv:2503.16944v1 Announce Type: new  Abstract: Personalized portrait synthesis, essential in domains like social entertainment, has recently made significant progress. Person-wise fine-tuning based methods, such as LoRA and DreamBooth, can produce photorealistic outputs but need training on individual samples, consuming time and resources and posing an unstable risk. Adapter based techniques such as IP-Adapter freeze the foundational model parameters and employ a plug-in architecture to enable zero-shot inference, but they often exhibit a lack of naturalness and authenticity, which are not to be overlooked in portrait synthesis tasks. In this paper, we introduce a parameter-efficient adaptive generation method, namely HyperLoRA, that uses an adaptive plug-in network to generate LoRA weights, merging the superior performance of LoRA with the zero-shot capability of adapter scheme. Through our carefully designed network structure and training strategy, we achieve zero-shot personalized portrait generation (supporting both single and multiple image inputs) with high photorealism, fidelity, and editability.",
        "arxiv_id": "2503.16944",
        "ARXIVID": "2503.16944",
        "COMMENT": "This paper introduces HyperLoRA for personalized portrait synthesis, which does not match any of the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.16861": {
        "authors": [
            "Shayne Longpre",
            "Kevin Klyman",
            "Ruth E. Appel",
            "Sayash Kapoor",
            "Rishi Bommasani",
            "Michelle Sahar",
            "Sean McGregor",
            "Avijit Ghosh",
            "Borhane Blili-Hamelin",
            "Nathan Butters",
            "Alondra Nelson",
            "Amit Elazari",
            "Andrew Sellars",
            "Casey John Ellis",
            "Dane Sherrets",
            "Dawn Song",
            "Harley Geiger",
            "Ilona Cohen",
            "Lauren McIlvenny",
            "Madhulika Srikumar",
            "Mark M. Jaycox",
            "Markus Anderljung",
            "Nadine Farid Johnson",
            "Nicholas Carlini",
            "Nicolas Miailhe",
            "Nik Marda",
            "Peter Henderson",
            "Rebecca S. Portnoff",
            "Rebecca Weiss",
            "Victoria Westerhoff",
            "Yacine Jernite",
            "Rumman Chowdhury",
            "Percy Liang",
            "Arvind Narayanan"
        ],
        "title": "In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI",
        "abstract": "arXiv:2503.16861v1 Announce Type: new  Abstract: The widespread deployment of general-purpose AI (GPAI) systems introduces significant new risks. Yet the infrastructure, practices, and norms for reporting flaws in GPAI systems remain seriously underdeveloped, lagging far behind more established fields like software security. Based on a collaboration between experts from the fields of software security, machine learning, law, social science, and policy, we identify key gaps in the evaluation and reporting of flaws in GPAI systems. We call for three interventions to advance system safety. First, we propose using standardized AI flaw reports and rules of engagement for researchers in order to ease the process of submitting, reproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system providers adopt broadly-scoped flaw disclosure programs, borrowing from bug bounties, with legal safe harbors to protect researchers. Third, we advocate for the development of improved infrastructure to coordinate distribution of flaw reports across the many stakeholders who may be impacted. These interventions are increasingly urgent, as evidenced by the prevalence of jailbreaks and other flaws that can transfer across different providers' GPAI systems. By promoting robust reporting and coordination in the AI ecosystem, these proposals could significantly improve the safety, security, and accountability of GPAI systems.",
        "arxiv_id": "2503.16861",
        "ARXIVID": "2503.16861",
        "COMMENT": "Does not match any specific criterion but discusses general-purpose AI safety and flaw reporting, which is tangentially related to your friend's interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.16616": {
        "authors": [
            "Xiaoran Zhang",
            "Byung-Woo Hong",
            "Hyoungseob Park",
            "Daniel H. Pak",
            "Anne-Marie Rickmann",
            "Lawrence H. Staib",
            "James S. Duncan",
            "Alex Wong"
        ],
        "title": "Progressive Test Time Energy Adaptation for Medical Image Segmentation",
        "abstract": "arXiv:2503.16616v1 Announce Type: new  Abstract: We propose a model-agnostic, progressive test-time energy adaptation approach for medical image segmentation. Maintaining model performance across diverse medical datasets is challenging, as distribution shifts arise from inconsistent imaging protocols and patient variations. Unlike domain adaptation methods that require multiple passes through target data - impractical in clinical settings - our approach adapts pretrained models progressively as they process test data. Our method leverages a shape energy model trained on source data, which assigns an energy score at the patch level to segmentation maps: low energy represents in-distribution (accurate) shapes, while high energy signals out-of-distribution (erroneous) predictions. By minimizing this energy score at test time, we refine the segmentation model to align with the target distribution. To validate the effectiveness and adaptability, we evaluated our framework on eight public MRI (bSSFP, T1- and T2-weighted) and X-ray datasets spanning cardiac, spinal cord, and lung segmentation. We consistently outperform baselines both quantitatively and qualitatively.",
        "arxiv_id": "2503.16616",
        "ARXIVID": "2503.16616",
        "COMMENT": "Does not match any specific criterion but is relevant to medical image segmentation and domain adaptation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.17182": {
        "authors": [
            "Patrick Rim",
            "Hyoungseob Park",
            "Vadim Ezhov",
            "Jeffrey Moon",
            "Alex Wong"
        ],
        "title": "Radar-Guided Polynomial Fitting for Metric Depth Estimation",
        "abstract": "arXiv:2503.17182v1 Announce Type: new  Abstract: We propose PolyRad, a novel radar-guided depth estimation method that introduces polynomial fitting to transform scaleless depth predictions from pretrained monocular depth estimation (MDE) models into metric depth maps. Unlike existing approaches that rely on complex architectures or expensive sensors, our method is grounded in a simple yet fundamental insight: using polynomial coefficients predicted from cheap, ubiquitous radar data to adaptively adjust depth predictions non-uniformly across depth ranges. Although MDE models often infer reasonably accurate local depth structure within each object or local region, they may misalign these regions relative to one another, making a linear scale-and-shift transformation insufficient given three or more of these regions. In contrast, PolyRad generalizes beyond linear transformations and is able to correct such misalignments by introducing inflection points. Importantly, our polynomial fitting framework preserves structural consistency through a novel training objective that enforces monotonicity via first-derivative regularization. PolyRad achieves state-of-the-art performance on the nuScenes, ZJU-4DRadarCam, and View-of-Delft datasets, outperforming existing methods by 30.3% in MAE and 37.2% in RMSE.",
        "arxiv_id": "2503.17182",
        "ARXIVID": "2503.17182",
        "COMMENT": "Does not match any specific criterion but is relevant to depth estimation and computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.16793": {
        "authors": [
            "Haori Lu",
            "Xusheng Cao",
            "Linlan Huang",
            "Enguang Wang",
            "Fei Yang",
            "Xialei Liu"
        ],
        "title": "Restoring Forgotten Knowledge in Non-Exemplar Class Incremental Learning through Test-Time Semantic Evolution",
        "abstract": "arXiv:2503.16793v1 Announce Type: new  Abstract: Continual learning aims to accumulate knowledge over a data stream while mitigating catastrophic forgetting. In Non-exemplar Class Incremental Learning (NECIL), forgetting arises during incremental optimization because old classes are inaccessible, hindering the retention of prior knowledge. To solve this, previous methods struggle in achieving the stability-plasticity balance in the training stages. However, we note that the testing stage is rarely considered among them, but is promising to be a solution to forgetting. Therefore, we propose RoSE, which is a simple yet effective method that \\textbf{R}est\\textbf{o}res forgotten knowledge through test-time \\textbf{S}emantic \\textbf{E}volution. Specifically designed for minimizing forgetting, RoSE is a test-time semantic drift compensation framework that enables more accurate drift estimation in a self-supervised manner. Moreover, to avoid incomplete optimization during online testing, we derive an analytical solution as an alternative to gradient descent. We evaluate RoSE on CIFAR-100, TinyImageNet, and ImageNet100 datasets, under both cold-start and warm-start settings. Our method consistently outperforms most state-of-the-art (SOTA) methods across various scenarios, validating the potential and feasibility of test-time evolution in NECIL.",
        "arxiv_id": "2503.16793",
        "ARXIVID": "2503.16793",
        "COMMENT": "Does not match any specific criterion but is generally relevant to continual learning and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.17212": {
        "authors": [
            "Matthew Kenely",
            "Dylan Seychell",
            "Carl James Debono",
            "Chris Porter"
        ],
        "title": "A Deep Learning Framework for Visual Attention Prediction and Analysis of News Interfaces",
        "abstract": "arXiv:2503.17212v1 Announce Type: new  Abstract: News outlets' competition for attention in news interfaces has highlighted the need for demographically-aware saliency prediction models. Despite recent advancements in saliency detection applied to user interfaces (UI), existing datasets are limited in size and demographic representation. We present a deep learning framework that enhances the SaRa (Saliency Ranking) model with DeepGaze IIE, improving Salient Object Ranking (SOR) performance by 10.7%. Our framework optimizes three key components: saliency map generation, grid segment scoring, and map normalization. Through a two-fold experiment using eye-tracking (30 participants) and mouse-tracking (375 participants aged 13--70), we analyze attention patterns across demographic groups. Statistical analysis reveals significant age-based variations (p < 0.05, {\\epsilon^2} = 0.042), with older users (36--70) engaging more with textual content and younger users (13--35) interacting more with images. Mouse-tracking data closely approximates eye-tracking behavior (sAUC = 0.86) and identifies UI elements that immediately stand out, validating its use in large-scale studies. We conclude that saliency studies should prioritize gathering data from a larger, demographically representative sample and report exact demographic distributions.",
        "arxiv_id": "2503.17212",
        "ARXIVID": "2503.17212",
        "COMMENT": "Does not match any specific criteria but is tangentially related to computer vision and saliency prediction, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.16780": {
        "authors": [
            "Uihyun Cho",
            "Namhun Kim"
        ],
        "title": "A-IDE : Agent-Integrated Denoising Experts",
        "abstract": "arXiv:2503.16780v1 Announce Type: new  Abstract: Recent advances in deep-learning based denoising methods have improved Low-Dose CT image quality. However, due to distinct HU distributions and diverse anatomical characteristics, a single model often struggles to generalize across multiple anatomies. To address this limitation, we introduce \\textbf{Agent-Integrated Denoising Experts (A-IDE)} framework, which integrates three anatomical region-specialized RED-CNN models under the management of decision-making LLM agent. The agent analyzes semantic cues from BiomedCLIP to dynamically route incoming LDCT scans to the most appropriate expert model. We highlight three major advantages of our approach. A-IDE excels in heterogeneous, data-scarce environments. The framework automatically prevents overfitting by distributing tasks among multiple experts. Finally, our LLM-driven agentic pipeline eliminates the need for manual interventions. Experimental evaluations on the Mayo-2016 dataset confirm that A-IDE achieves superior performance in RMSE, PSNR, and SSIM compared to a single unified denoiser.",
        "arxiv_id": "2503.16780",
        "ARXIVID": "2503.16780",
        "COMMENT": "Does not match any specific criterion but is relevant to multi-modal learning and denoising methods.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.16709": {
        "authors": [
            "Xuan Shen",
            "Weize Ma",
            "Jing Liu",
            "Changdi Yang",
            "Rui Ding",
            "Quanyi Wang",
            "Henghui Ding",
            "Wei Niu",
            "Yanzhi Wang",
            "Pu Zhao",
            "Jun Lin",
            "Jiuxiang Gu"
        ],
        "title": "QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on the Edge",
        "abstract": "arXiv:2503.16709v1 Announce Type: new  Abstract: Monocular Depth Estimation (MDE) has emerged as a pivotal task in computer vision, supporting numerous real-world applications. However, deploying accurate depth estimation models on resource-limited edge devices, especially Application-Specific Integrated Circuits (ASICs), is challenging due to the high computational and memory demands. Recent advancements in foundational depth estimation deliver impressive results but further amplify the difficulty of deployment on ASICs. To address this, we propose QuartDepth which adopts post-training quantization to quantize MDE models with hardware accelerations for ASICs. Our approach involves quantizing both weights and activations to 4-bit precision, reducing the model size and computation cost. To mitigate the performance degradation, we introduce activation polishing and compensation algorithm applied before and after activation quantization, as well as a weight reconstruction method for minimizing errors in weight quantization. Furthermore, we design a flexible and programmable hardware accelerator by supporting kernel fusion and customized instruction programmability, enhancing throughput and efficiency. Experimental results demonstrate that our framework achieves competitive accuracy while enabling fast inference and higher energy efficiency on ASICs, bridging the gap between high-performance depth estimation and practical edge-device applicability. Code: https://github.com/shawnricecake/quart-depth",
        "arxiv_id": "2503.16709",
        "ARXIVID": "2503.16709",
        "COMMENT": "Does not match any specific criterion but is relevant to vision foundation models and their deployment on edge devices.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.17351": {
        "authors": [
            "Vineet R. Shenoy",
            "Shaoju Wu",
            "Armand Comas",
            "Tim K. Marks",
            "Suhas Lohit",
            "Hassan Mansour"
        ],
        "title": "Time-Series U-Net with Recurrence for Noise-Robust Imaging Photoplethysmography",
        "abstract": "arXiv:2503.17351v1 Announce Type: new  Abstract: Remote estimation of vital signs enables health monitoring for situations in which contact-based devices are either not available, too intrusive, or too expensive. In this paper, we present a modular, interpretable pipeline for pulse signal estimation from video of the face that achieves state-of-the-art results on publicly available datasets.Our imaging photoplethysmography (iPPG) system consists of three modules: face and landmark detection, time-series extraction, and pulse signal/pulse rate estimation. Unlike many deep learning methods that make use of a single black-box model that maps directly from input video to output signal or heart rate, our modular approach enables each of the three parts of the pipeline to be interpreted individually. The pulse signal estimation module, which we call TURNIP (Time-Series U-Net with Recurrence for Noise-Robust Imaging Photoplethysmography), allows the system to faithfully reconstruct the underlying pulse signal waveform and uses it to measure heart rate and pulse rate variability metrics, even in the presence of motion. When parts of the face are occluded due to extreme head poses, our system explicitly detects such \"self-occluded\" regions and maintains estimation robustness despite the missing information. Our algorithm provides reliable heart rate estimates without the need for specialized sensors or contact with the skin, outperforming previous iPPG methods on both color (RGB) and near-infrared (NIR) datasets.",
        "arxiv_id": "2503.17351",
        "ARXIVID": "2503.17351",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and signal processing.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.16975": {
        "authors": [
            "Xiaofeng Mao",
            "Yuefeng Chen",
            "Rong Zhang",
            "Hui Xue",
            "Zhao Li",
            "Hang Su"
        ],
        "title": "EasyRobust: A Comprehensive and Easy-to-use Toolkit for Robust and Generalized Vision",
        "abstract": "arXiv:2503.16975v1 Announce Type: new  Abstract: Deep neural networks (DNNs) has shown great promise in computer vision tasks. However, machine vision achieved by DNNs cannot be as robust as human perception. Adversarial attacks and data distribution shifts have been known as two major scenarios which degrade machine performance and obstacle the wide deployment of machines \"in the wild\". In order to break these obstructions and facilitate the research of model robustness, we develop EasyRobust, a comprehensive and easy-to-use toolkit for training, evaluation and analysis of robust vision models. EasyRobust targets at two types of robustness: 1) Adversarial robustness enables the model to defense against malicious inputs crafted by worst-case perturbations, also known as adversarial examples; 2) Non-adversarial robustness enhances the model performance on natural test images with corruptions or distribution shifts. Thorough benchmarks on image classification enable EasyRobust to provide an accurate robustness evaluation on vision models. We wish our EasyRobust can help for training practically-robust models and promote academic and industrial progress in closing the gap between human and machine vision. Codes and models of EasyRobust have been open-sourced in https://github.com/alibaba/easyrobust.",
        "arxiv_id": "2503.16975",
        "ARXIVID": "2503.16975",
        "COMMENT": "Does not match any specific criterion but is relevant to robust vision models and computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}