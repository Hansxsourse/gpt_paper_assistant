{
    "2512.23705": {
        "authors": [
            "Shaocong Xu",
            "Songlin Wei",
            "Qizhe Wei",
            "Zheng Geng",
            "Hong Li",
            "Licheng Shen",
            "Qianpu Sun",
            "Shu Han",
            "Bin Ma",
            "Bohan Li",
            "Chongjie Ye",
            "Yuhang Zheng",
            "Nan Wang",
            "Saining Zhang",
            "Hao Zhao"
        ],
        "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation",
        "abstract": "arXiv:2512.23705v1 Announce Type: new  Abstract: Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.",
        "arxiv_id": "2512.23705",
        "ARXIVID": "2512.23705",
        "COMMENT": "The paper matches criterion 2 closely as it discusses a diffusion model handling multiple vision tasks including depth and normal estimation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.22310": {
        "authors": [
            "Run Ling",
            "Ke Cao",
            "Jian Lu",
            "Ao Ma",
            "Haowei Liu",
            "Runze He",
            "Changwei Wang",
            "Rongtao Xu",
            "Yihua Shao",
            "Zhanjie Zhang",
            "Peng Wu",
            "Guibing Guo",
            "Wei Feng",
            "Zheng Zhang",
            "Jingjing Lv",
            "Junjie Shen",
            "Ching Law",
            "Xingwei Wang"
        ],
        "title": "MoFu: Scale-Aware Modulation and Fourier Fusion for Multi-Subject Video Generation",
        "abstract": "arXiv:2512.22310v1 Announce Type: new  Abstract: Multi-subject video generation aims to synthesize videos from textual prompts and multiple reference images, ensuring that each subject preserves natural scale and visual fidelity. However, current methods face two challenges: scale inconsistency, where variations in subject size lead to unnatural generation, and permutation sensitivity, where the order of reference inputs causes subject distortion. In this paper, we propose MoFu, a unified framework that tackles both challenges. For scale inconsistency, we introduce Scale-Aware Modulation (SMO), an LLM-guided module that extracts implicit scale cues from the prompt and modulates features to ensure consistent subject sizes. To address permutation sensitivity, we present a simple yet effective Fourier Fusion strategy that processes the frequency information of reference features via the Fast Fourier Transform to produce a unified representation. Besides, we design a Scale-Permutation Stability Loss to jointly encourage scale-consistent and permutation-invariant generation. To further evaluate these challenges, we establish a dedicated benchmark with controlled variations in subject scale and reference permutation. Extensive experiments demonstrate that MoFu significantly outperforms existing methods in preserving natural scale, subject fidelity, and overall visual quality.",
        "arxiv_id": "2512.22310",
        "ARXIVID": "2512.22310",
        "COMMENT": "The paper does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.22615": {
        "authors": [
            "Jiacheng Ye",
            "Shansan Gong",
            "Jiahui Gao",
            "Junming Fan",
            "Shuang Wu",
            "Wei Bi",
            "Haoli Bai",
            "Lifeng Shang",
            "Lingpeng Kong"
        ],
        "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone",
        "abstract": "arXiv:2512.22615v1 Announce Type: new  Abstract: While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as $\\pi_0$ and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.",
        "arxiv_id": "2512.22615",
        "ARXIVID": "2512.22615",
        "COMMENT": "The paper does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.22664": {
        "authors": [
            "Qiankun Li",
            "Feng He",
            "Huabao Chen",
            "Xin Ning",
            "Kun Wang",
            "Zengfu Wang"
        ],
        "title": "Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains",
        "abstract": "arXiv:2512.22664v1 Announce Type: new  Abstract: In the big data era, the computer vision field benefits from large-scale datasets such as LAION-2B, LAION-400M, and ImageNet-21K, Kinetics, on which popular models like the ViT and ConvNeXt series have been pre-trained, acquiring substantial knowledge. However, numerous downstream tasks in specialized and data-limited scientific domains continue to pose significant challenges. In this paper, we propose a novel Cluster Attention Adapter (CLAdapter), which refines and adapts the rich representations learned from large-scale data to various data-limited downstream tasks. Specifically, CLAdapter introduces attention mechanisms and cluster centers to personalize the enhancement of transformed features through distribution correlation and transformation matrices. This enables models fine-tuned with CLAdapter to learn distinct representations tailored to different feature sets, facilitating the models' adaptation from rich pre-trained features to various downstream scenarios effectively. In addition, CLAdapter's unified interface design allows for seamless integration with multiple model architectures, including CNNs and Transformers, in both 2D and 3D contexts. Through extensive experiments on 10 datasets spanning domains such as generic, multimedia, biological, medical, industrial, agricultural, environmental, geographical, materials science, out-of-distribution (OOD), and 3D analysis, CLAdapter achieves state-of-the-art performance across diverse data-limited scientific domains, demonstrating its effectiveness in unleashing the potential of foundation vision models via adaptive transfer. Code is available at https://github.com/qklee-lz/CLAdapter.",
        "arxiv_id": "2512.22664",
        "ARXIVID": "2512.22664",
        "COMMENT": "The paper does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.22905": {
        "authors": [
            "Kai Liu",
            "Jungang Li",
            "Yuchong Sun",
            "Shengqiong Wu",
            "Jianzhang Gao",
            "Daoan Zhang",
            "Wei Zhang",
            "Sheng Jin",
            "Sicheng Yu",
            "Geng Zhan",
            "Jiayi Ji",
            "Fan Zhou",
            "Liang Zheng",
            "Shuicheng Yan",
            "Hao Fei",
            "Tat-Seng Chua"
        ],
        "title": "JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation",
        "abstract": "arXiv:2512.22905v1 Announce Type: new  Abstract: This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.",
        "arxiv_id": "2512.22905",
        "ARXIVID": "2512.22905",
        "COMMENT": "The paper does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.23222": {
        "authors": [
            "Jiaxu Zhang",
            "Tianshu Hu",
            "Yuan Zhang",
            "Zenan Li",
            "Linjie Luo",
            "Guosheng Lin",
            "Xin Chen"
        ],
        "title": "Bridging Your Imagination with Audio-Video Generation via a Unified Director",
        "abstract": "arXiv:2512.23222v1 Announce Type: new  Abstract: Existing AI-driven video creation systems typically treat script drafting and key-shot design as two disjoint tasks: the former relies on large language models, while the latter depends on image generation models. We argue that these two tasks should be unified within a single framework, as logical reasoning and imaginative thinking are both fundamental qualities of a film director. In this work, we propose UniMAGE, a unified director model that bridges user prompts with well-structured scripts, thereby empowering non-experts to produce long-context, multi-shot films by leveraging existing audio-video generation models. To achieve this, we employ the Mixture-of-Transformers architecture that unifies text and image generation. To further enhance narrative logic and keyframe consistency, we introduce a ``first interleaving, then disentangling'' training paradigm. Specifically, we first perform Interleaved Concept Learning, which utilizes interleaved text-image data to foster the model's deeper understanding and imaginative interpretation of scripts. We then conduct Disentangled Expert Learning, which decouples script writing from keyframe generation, enabling greater flexibility and creativity in storytelling. Extensive experiments demonstrate that UniMAGE achieves state-of-the-art performance among open-source models, generating logically coherent video scripts and visually consistent keyframe images.",
        "arxiv_id": "2512.23222",
        "ARXIVID": "2512.23222",
        "COMMENT": "The paper does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.22525": {
        "authors": [
            "Bin Xia",
            "Bohao Peng",
            "Jiyang Liu",
            "Sitong Wu",
            "Jingyao Li",
            "Junjia Huang",
            "Xu Zhao",
            "Yitong Wang",
            "Ruihang Chu",
            "Bei Yu",
            "Jiaya Jia"
        ],
        "title": "DreamOmni3: Scribble-based Editing and Generation",
        "abstract": "arXiv:2512.22525v1 Announce Type: new  Abstract: Recently unified generation and editing models have achieved remarkable success with their impressive performance. These models rely mainly on text prompts for instruction-based editing and generation, but language often fails to capture users intended edit locations and fine-grained visual details. To this end, we propose two tasks: scribble-based editing and generation, that enables more flexible creation on graphical user interface (GUI) combining user textual, images, and freehand sketches. We introduce DreamOmni3, tackling two challenges: data creation and framework design. Our data synthesis pipeline includes two parts: scribble-based editing and generation. For scribble-based editing, we define four tasks: scribble and instruction-based editing, scribble and multimodal instruction-based editing, image fusion, and doodle editing. Based on DreamOmni2 dataset, we extract editable regions and overlay hand-drawn boxes, circles, doodles or cropped image to construct training data. For scribble-based generation, we define three tasks: scribble and instruction-based generation, scribble and multimodal instruction-based generation, and doodle generation, following similar data creation pipelines. For the framework, instead of using binary masks, which struggle with complex edits involving multiple scribbles, images, and instructions, we propose a joint input scheme that feeds both the original and scribbled source images into the model, using different colors to distinguish regions and simplify processing. By applying the same index and position encodings to both images, the model can precisely localize scribbled regions while maintaining accurate editing. Finally, we establish comprehensive benchmarks for these tasks to promote further research. Experimental results demonstrate that DreamOmni3 achieves outstanding performance, and models and code will be publicly released.",
        "arxiv_id": "2512.22525",
        "ARXIVID": "2512.22525",
        "COMMENT": "The paper does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}