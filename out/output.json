{
    "2511.00468": {
        "authors": [
            "Panwang Pan",
            "Tingting Shen",
            "Chenxin Li",
            "Yunlong Lin",
            "Kairun Wen",
            "Jingjing Zhao",
            "Yixuan Yuan"
        ],
        "title": "HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation",
        "abstract": "arXiv:2511.00468v1 Announce Type: new  Abstract: Recent advances in generative models have achieved high-fidelity in 3D human reconstruction, yet their utility for specific tasks (e.g., human 3D segmentation) remains constrained. We propose HumanCrafter, a unified framework that enables the joint modeling of appearance and human-part semantics from a single image in a feed-forward manner. Specifically, we integrate human geometric priors in the reconstruction stage and self-supervised semantic priors in the segmentation stage. To address labeled 3D human datasets scarcity, we further develop an interactive annotation procedure for generating high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task synergy, while the multi-task objective simultaneously optimizes texture modeling fidelity and semantic consistency. Extensive experiments demonstrate that HumanCrafter surpasses existing state-of-the-art methods in both 3D human-part segmentation and 3D human reconstruction from a single image.",
        "arxiv_id": "2511.00468",
        "ARXIVID": "2511.00468",
        "COMMENT": "3",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2511.01593": {
        "authors": [
            "Yizhu Chen",
            "Chen Ju",
            "Zhicheng Wang",
            "Shuai Xiao",
            "Xu Chen",
            "Jinsong Lan",
            "Xiaoyong Zhu",
            "Ying Chen"
        ],
        "title": "Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation",
        "abstract": "arXiv:2511.01593v1 Announce Type: new  Abstract: The unification of understanding and generation within a single multi-modal large model (MLLM) remains one significant challenge, largely due to the dichotomy between continuous and discrete visual tokenizations. Continuous tokenizer (CT) achieves strong performance by bridging multiple independently-trained understanding modules and generation modules, but suffers from complex multi-stage pipelines and substantial engineering overhead. Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by quantizing each image into a primitive, but inevitably leading to information loss and performance degradation. To resolve this tension, we question the binary choice between CT and DT, inspired by the wave-particle duality of light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT). We treat visual data as a flexible composition of image primitives derived from quantized codebooks, with the crucial insight that the primitive number assigned to each visual sample is adaptively determined according to its complexity: simple instances use a few primitives, emulating discrete tokenization, while complex instances use many, approximating continuous tokenization. Two core components are designed: Diverse Quantitative Primitives, which encourage primitives orthogonality to better populate information space, and Dynamic Primitive Allocator, which assesses sample complexity to determine the optimal set of primitives. Extensive experiments on reconstruction, retrieval and classification show that CDD-VT achieves superior performance over to specialized CT and DT, effectively getting strong result within a concise and scalable MLLM.",
        "arxiv_id": "2511.01593",
        "ARXIVID": "2511.01593",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}