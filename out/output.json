{
    "2509.10441": {
        "authors": [
            "Tao Han",
            "Wanghan Xu",
            "Junchao Gong",
            "Xiaoyu Yue",
            "Song Guo",
            "Luping Zhou",
            "Lei Bai"
        ],
        "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis",
        "abstract": "arXiv:2509.10441v1 Announce Type: new  Abstract: Arbitrary resolution image generation provides a consistent visual experience across devices, having extensive applications for producers and consumers. Current diffusion models increase computational demand quadratically with resolution, causing 4K image generation delays over 100 seconds. To solve this, we explore the second generation upon the latent diffusion models, where the fixed latent generated by diffusion models is regarded as the content representation and we propose to decode arbitrary resolution images with a compact generated latent using a one-step generator. Thus, we present the \\textbf{InfGen}, replacing the VAE decoder with the new generator, for generating images at any resolution from a fixed-size latent without retraining the diffusion models, which simplifies the process, reducing computational complexity and can be applied to any model using the same latent space. Experiments show InfGen is capable of improving many models into the arbitrary high-resolution era while cutting 4K image generation time to under 10 seconds.",
        "arxiv_id": "2509.10441",
        "ARXIVID": "2509.10441",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.10058": {
        "authors": [
            "Sung-Lin Tsai",
            "Bo-Lun Huang",
            "Yu Ting Shen",
            "Cheng Yu Yeo",
            "Chiang Tseng",
            "Bo-Kai Ruan",
            "Wen-Sheng Lien",
            "Hong-Han Shuai"
        ],
        "title": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation",
        "abstract": "arXiv:2509.10058v1 Announce Type: new  Abstract: Accurate color alignment in text-to-image (T2I) generation is critical for applications such as fashion, product visualization, and interior design, yet current diffusion models struggle with nuanced and compound color terms (e.g., Tiffany blue, lime green, hot pink), often producing images that are misaligned with human intent. Existing approaches rely on cross-attention manipulation, reference images, or fine-tuning but fail to systematically resolve ambiguous color descriptions. To precisely render colors under prompt ambiguity, we propose a training-free framework that enhances color fidelity by leveraging a large language model (LLM) to disambiguate color-related prompts and guiding color blending operations directly in the text embedding space. Our method first employs a large language model (LLM) to resolve ambiguous color terms in the text prompt, and then refines the text embeddings based on the spatial relationships of the resulting color terms in the CIELAB color space. Unlike prior methods, our approach improves color accuracy without requiring additional training or external reference images. Experimental results demonstrate that our framework improves color alignment without compromising image quality, bridging the gap between text semantics and visual generation.",
        "arxiv_id": "2509.10058",
        "ARXIVID": "2509.10058",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2509.10312": {
        "authors": [
            "Zhixin Zheng",
            "Xinyu Wang",
            "Chang Zou",
            "Shaobo Wang",
            "Linfeng Zhang"
        ],
        "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching",
        "abstract": "arXiv:2509.10312v1 Announce Type: new  Abstract: Diffusion transformers have gained significant attention in recent years for their ability to generate high-quality images and videos, yet still suffer from a huge computational cost due to their iterative denoising process. Recently, feature caching has been introduced to accelerate diffusion transformers by caching the feature computation in previous timesteps and reusing it in the following timesteps, which leverage the temporal similarity of diffusion models while ignoring the similarity in the spatial dimension. In this paper, we introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and complementary perspective for previous feature caching. Specifically, ClusCa performs spatial clustering on tokens in each timestep, computes only one token in each cluster and propagates their information to all the other tokens, which is able to reduce the number of tokens by over 90%. Extensive experiments on DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image and text-to-video generation. Besides, it can be directly applied to any diffusion transformer without requirements for training. For instance, ClusCa achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing the original model by 0.51%. The code is available at https://github.com/Shenyi-Z/Cache4Diffusion.",
        "arxiv_id": "2509.10312",
        "ARXIVID": "2509.10312",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}