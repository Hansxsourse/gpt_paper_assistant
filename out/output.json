{
    "2504.07962": {
        "authors": [
            "Lang Lin",
            "Xueyang Yu",
            "Ziqi Pang",
            "Yu-Xiong Wang"
        ],
        "title": "GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation",
        "abstract": "arXiv:2504.07962v1 Announce Type: new  Abstract: This paper proposes a novel framework utilizing multi-modal large language models (MLLMs) for referring video object segmentation (RefVOS). Previous MLLM-based methods commonly struggle with the dilemma between \"Ref\" and \"VOS\": they either specialize in understanding a few key frames (global reasoning) or tracking objects on continuous frames (local reasoning), and rely on external VOS or frame selectors to mitigate the other end of the challenge. However, our framework GLUS shows that global and local consistency can be unified into a single video segmentation MLLM: a set of sparse \"context frames\" provides global information, while a stream of continuous \"query frames\" conducts local object tracking. This is further supported by jointly training the MLLM with a pre-trained VOS memory bank to simultaneously digest short-range and long-range temporal information. To improve the information efficiency within the limited context window of MLLMs, we introduce object contrastive learning to distinguish hard false-positive objects and a self-refined framework to identify crucial frames and perform propagation. By collectively integrating these insights, our GLUS delivers a simple yet effective baseline, achieving new state-of-the-art for MLLMs on the MeViS and Ref-Youtube-VOS benchmark. Our project page is at https://glus-video.github.io/.",
        "arxiv_id": "2504.07962",
        "ARXIVID": "2504.07962",
        "COMMENT": "Matches criterion 2 and 4 as it proposes a novel MLLM framework (GLUS) for video segmentation, integrating global-local reasoning and achieving state-of-the-art results.",
        "RELEVANCE": 10,
        "NOVELTY": 9
    },
    "2504.07491": {
        "authors": [
            "Kimi Team",
            "Angang Du",
            "Bohong Yin",
            "Bowei Xing",
            "Bowen Qu",
            "Bowen Wang",
            "Cheng Chen",
            "Chenlin Zhang",
            "Chenzhuang Du",
            "Chu Wei",
            "Congcong Wang",
            "Dehao Zhang",
            "Dikang Du",
            "Dongliang Wang",
            "Enming Yuan",
            "Enzhe Lu",
            "Fang Li",
            "Flood Sung",
            "Guangda Wei",
            "Guokun Lai",
            "Han Zhu",
            "Hao Ding",
            "Hao Hu",
            "Hao Yang",
            "Hao Zhang",
            "Haoning Wu",
            "Haotian Yao",
            "Haoyu Lu",
            "Heng Wang",
            "Hongcheng Gao",
            "Huabin Zheng",
            "Jiaming Li",
            "Jianlin Su",
            "Jianzhou Wang",
            "Jiaqi Deng",
            "Jiezhong Qiu",
            "Jin Xie",
            "Jinhong Wang",
            "Jingyuan Liu",
            "Junjie Yan",
            "Kun Ouyang",
            "Liang Chen",
            "Lin Sui",
            "Longhui Yu",
            "Mengfan Dong",
            "Mengnan Dong",
            "Nuo Xu",
            "Pengyu Cheng",
            "Qizheng Gu",
            "Runjie Zhou",
            "Shaowei Liu",
            "Sihan Cao",
            "Tao Yu",
            "Tianhui Song",
            "Tongtong Bai",
            "Wei Song",
            "Weiran He",
            "Weixiao Huang",
            "Weixin Xu",
            "Xiaokun Yuan",
            "Xingcheng Yao",
            "Xingzhe Wu",
            "Xinxing Zu",
            "Xinyu Zhou",
            "Xinyuan Wang",
            "Y. Charles",
            "Yan Zhong",
            "Yang Li",
            "Yangyang Hu",
            "Yanru Chen",
            "Yejie Wang",
            "Yibo Liu",
            "Yibo Miao",
            "Yidao Qin",
            "Yimin Chen",
            "Yiping Bao",
            "Yiqin Wang",
            "Yongsheng Kang",
            "Yuanxin Liu",
            "Yulun Du",
            "Yuxin Wu",
            "Yuzhi Wang",
            "Yuzi Yan",
            "Zaida Zhou",
            "Zhaowei Li",
            "Zhejun Jiang",
            "Zheng Zhang",
            "Zhilin Yang",
            "Zhiqi Huang",
            "Zihao Huang",
            "Zijia Zhao",
            "Ziwei Chen"
        ],
        "title": "Kimi-VL Technical Report",
        "abstract": "arXiv:2504.07491v1 Announce Type: new  Abstract: We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrates strong performance across challenging domains: as a general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld), matching flagship models. Furthermore, it exhibits remarkable capabilities across diverse challenging vision language tasks, including college-level image and video comprehension, OCR, mathematical reasoning, and multi-image understanding. In comparative evaluations, it effectively competes with cutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also advances in processing long contexts and perceiving clearly. With a 128K extended context window, Kimi-VL can process diverse long inputs, achieving impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its native-resolution vision encoder, MoonViT, further allows it to see and understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and 34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant: Kimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong long-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8 on MathVision, and 71.3 on MathVista while maintaining the compact 2.8B activated LLM parameters, setting a new standard for efficient multimodal thinking models. Code and models are publicly accessible at https://github.com/MoonshotAI/Kimi-VL.",
        "arxiv_id": "2504.07491",
        "ARXIVID": "2504.07491",
        "COMMENT": "Matches criterion 2 as it introduces a new visual large language model (Kimi-VL) with advanced multimodal reasoning and long-context understanding.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2504.07745": {
        "authors": [
            "Yangliu Hu",
            "Zikai Song",
            "Na Feng",
            "Yawei Luo",
            "Junqing Yu",
            "Yi-Ping Phoebe Chen",
            "Wei Yang"
        ],
        "title": "SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained Understanding",
        "abstract": "arXiv:2504.07745v1 Announce Type: new  Abstract: Video-based Large Language Models (Video-LLMs) have witnessed substantial advancements in recent years, propelled by the advancement in multi-modal LLMs. Although these models have demonstrated proficiency in providing the overall description of videos, they struggle with fine-grained understanding, particularly in aspects such as visual dynamics and video details inquiries. To tackle these shortcomings, we find that fine-tuning Video-LLMs on self-supervised fragment tasks, greatly improve their fine-grained video understanding abilities. Hence we propose two key contributions:(1) Self-Supervised Fragment Fine-Tuning (SF$^2$T), a novel effortless fine-tuning method, employs the rich inherent characteristics of videos for training, while unlocking more fine-grained understanding ability of Video-LLMs. Moreover, it relieves researchers from labor-intensive annotations and smartly circumvents the limitations of natural language, which often fails to capture the complex spatiotemporal variations in videos; (2) A novel benchmark dataset, namely FineVidBench, for rigorously assessing Video-LLMs' performance at both the scene and fragment levels, offering a comprehensive evaluation of their capabilities. We assessed multiple models and validated the effectiveness of SF$^2$T on them. Experimental results reveal that our approach improves their ability to capture and interpret spatiotemporal details.",
        "arxiv_id": "2504.07745",
        "ARXIVID": "2504.07745",
        "COMMENT": "Matches criterion 2 as it proposes a fine-tuning method for Video-LLMs and introduces a new benchmark (FineVidBench) for fine-grained video understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2504.07960": {
        "authors": [
            "Zhong-Yu Li",
            "Ruoyi Du",
            "Juncheng Yan",
            "Le Zhuo",
            "Zhen Li",
            "Peng Gao",
            "Zhanyu Ma",
            "Ming-Ming Cheng"
        ],
        "title": "VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning",
        "abstract": "arXiv:2504.07960v1 Announce Type: new  Abstract: Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose VisualCloze, a universal image generation framework, which supports a wide range of in-domain tasks, generalization to unseen ones, unseen unification of multiple tasks, and reverse generation. Unlike existing methods that rely on language-based task instruction, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from visual demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and transferable knowledge. Furthermore, we uncover that our unified image generation formulation shared a consistent objective with image infilling, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying the architectures.",
        "arxiv_id": "2504.07960",
        "ARXIVID": "2504.07960",
        "COMMENT": "Matches criterion 4 as it introduces a universal image generation framework (VisualCloze) leveraging visual in-context learning, which is related to vision foundation models and their applications.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2504.07943": {
        "authors": [
            "Yunhan Yang",
            "Yuan-Chen Guo",
            "Yukun Huang",
            "Zi-Xin Zou",
            "Zhipeng Yu",
            "Yangguang Li",
            "Yan-Pei Cao",
            "Xihui Liu"
        ],
        "title": "HoloPart: Generative 3D Part Amodal Segmentation",
        "abstract": "arXiv:2504.07943v1 Announce Type: new  Abstract: 3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding. Existing 3D part segmentation methods only identify visible surface patches, limiting their utility. Inspired by 2D amodal segmentation, we introduce this novel task to the 3D domain and propose a practical, two-stage approach, addressing the key challenges of inferring occluded 3D geometry, maintaining global shape consistency, and handling diverse shapes with limited training data. First, we leverage existing 3D part segmentation to obtain initial, incomplete part segments. Second, we introduce HoloPart, a novel diffusion-based model, to complete these segments into full 3D parts. HoloPart utilizes a specialized architecture with local attention to capture fine-grained part geometry and global shape context attention to ensure overall shape consistency. We introduce new benchmarks based on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart significantly outperforms state-of-the-art shape completion methods. By incorporating HoloPart with existing segmentation techniques, we achieve promising results on 3D part amodal segmentation, opening new avenues for applications in geometry editing, animation, and material assignment.",
        "arxiv_id": "2504.07943",
        "ARXIVID": "2504.07943",
        "COMMENT": "Matches criterion 3 as it introduces a novel task (3D part amodal segmentation) and a new method (HoloPart) for completing 3D shapes, focusing on previously ignored challenges like occlusion.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2504.07958": {
        "authors": [
            "Hanxue Zhang",
            "Haoran Jiang",
            "Qingsong Yao",
            "Yanan Sun",
            "Renrui Zhang",
            "Hao Zhao",
            "Hongyang Li",
            "Hongzi Zhu",
            "Zetong Yang"
        ],
        "title": "Detect Anything 3D in the Wild",
        "abstract": "arXiv:2504.07958v1 Announce Type: new  Abstract: Despite the success of deep learning in close-set 3D object detection, existing approaches struggle with zero-shot generalization to novel objects and camera configurations. We introduce DetAny3D, a promptable 3D detection foundation model capable of detecting any novel object under arbitrary camera configurations using only monocular inputs. Training a foundation model for 3D detection is fundamentally constrained by the limited availability of annotated 3D data, which motivates DetAny3D to leverage the rich prior knowledge embedded in extensively pre-trained 2D foundation models to compensate for this scarcity. To effectively transfer 2D knowledge to 3D, DetAny3D incorporates two core modules: the 2D Aggregator, which aligns features from different 2D foundation models, and the 3D Interpreter with Zero-Embedding Mapping, which mitigates catastrophic forgetting in 2D-to-3D knowledge transfer. Experimental results validate the strong generalization of our DetAny3D, which not only achieves state-of-the-art performance on unseen categories and novel camera configurations, but also surpasses most competitors on in-domain data.DetAny3D sheds light on the potential of the 3D foundation model for diverse applications in real-world scenarios, e.g., rare object detection in autonomous driving, and demonstrates promise for further exploration of 3D-centric tasks in open-world settings. More visualization results can be found at DetAny3D project page.",
        "arxiv_id": "2504.07958",
        "ARXIVID": "2504.07958",
        "COMMENT": "Matches criterion 4 as it introduces a 3D detection foundation model (DetAny3D) leveraging 2D foundation models for 3D tasks, which is related to vision foundation models and their applications.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2504.07961": {
        "authors": [
            "Zeren Jiang",
            "Chuanxia Zheng",
            "Iro Laina",
            "Diane Larlus",
            "Andrea Vedaldi"
        ],
        "title": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction",
        "abstract": "arXiv:2504.07961v1 Announce Type: new  Abstract: We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic prior captured by such video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner. Geo4D predicts several complementary geometric modalities, namely point, depth, and ray maps. It uses a new multi-modal alignment algorithm to align and fuse these modalities, as well as multiple sliding windows, at inference time, thus obtaining robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods, including recent methods such as MonST3R, which are also designed to handle dynamic scenes.",
        "arxiv_id": "2504.07961",
        "ARXIVID": "2504.07961",
        "COMMENT": "Matches criterion 1 as it introduces a novel method (Geo4D) for 4D scene reconstruction using video diffusion models, which is a methodological improvement in spatial understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2504.07956": {
        "authors": [
            "Yukun Qi",
            "Yiming Zhao",
            "Yu Zeng",
            "Xikun Bao",
            "Wenxuan Huang",
            "Lin Chen",
            "Zehui Chen",
            "Jie Zhao",
            "Zhongang Qi",
            "Feng Zhao"
        ],
        "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning",
        "abstract": "arXiv:2504.07956v1 Announce Type: new  Abstract: The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task.",
        "arxiv_id": "2504.07956",
        "ARXIVID": "2504.07956",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (VCR-Bench) for video chain-of-thought reasoning, focusing on LVLMs and their reasoning capabilities.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2504.07954": {
        "authors": [
            "En Yu",
            "Kangheng Lin",
            "Liang Zhao",
            "Jisheng Yin",
            "Yana Wei",
            "Yuang Peng",
            "Haoran Wei",
            "Jianjian Sun",
            "Chunrui Han",
            "Zheng Ge",
            "Xiangyu Zhang",
            "Daxin Jiang",
            "Jingyu Wang",
            "Wenbing Tao"
        ],
        "title": "Perception-R1: Pioneering Perception Policy with Reinforcement Learning",
        "abstract": "arXiv:2504.07954v1 Announce Type: new  Abstract: Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in MLLM post-training for perception policy learning. While promising, our initial experiments reveal that incorporating a thinking process through RL does not consistently lead to performance gains across all visual perception tasks. This leads us to delve into the essential role of RL in the context of visual perception. In this work, we return to the fundamentals and explore the effects of RL on different perception tasks. We observe that the perceptual complexity is a major factor in determining the effectiveness of RL. We also observe that reward design plays a crucial role in further approching the upper limit of model perception. To leverage these findings, we propose Perception-R1, a scalable RL framework using GRPO during MLLM post-training. With a standard Qwen2.5-VL-3B-Instruct, Perception-R1 achieves +4.2% on RefCOCO+, +17.9% on PixMo-Count, +4.2% on PageOCR, and notably, 31.9% AP on COCO2017 val for the first time, establishing a strong baseline for perception policy learning.",
        "arxiv_id": "2504.07954",
        "ARXIVID": "2504.07954",
        "COMMENT": "Matches criterion 3 as it explores reinforcement learning for perception policy learning in MLLMs, focusing on novel insights like perceptual complexity and reward design.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.07521": {
        "authors": [
            "Yuxiang Lin",
            "Jingdong Sun",
            "Zhi-Qi Cheng",
            "Jue Wang",
            "Haomin Liang",
            "Zebang Cheng",
            "Yifei Dong",
            "Jun-Yan He",
            "Xiaojiang Peng",
            "Xian-Sheng Hua"
        ],
        "title": "Why We Feel: Breaking Boundaries in Emotional Reasoning with Multimodal Large Language Models",
        "abstract": "arXiv:2504.07521v1 Announce Type: new  Abstract: Most existing emotion analysis emphasizes which emotion arises (e.g., happy, sad, angry) but neglects the deeper why. We propose Emotion Interpretation (EI), focusing on causal factors-whether explicit (e.g., observable objects, interpersonal interactions) or implicit (e.g., cultural context, off-screen events)-that drive emotional responses. Unlike traditional emotion recognition, EI tasks require reasoning about triggers instead of mere labeling. To facilitate EI research, we present EIBench, a large-scale benchmark encompassing 1,615 basic EI samples and 50 complex EI samples featuring multifaceted emotions. Each instance demands rationale-based explanations rather than straightforward categorization. We further propose a Coarse-to-Fine Self-Ask (CFSA) annotation pipeline, which guides Vision-Language Models (VLLMs) through iterative question-answer rounds to yield high-quality labels at scale. Extensive evaluations on open-source and proprietary large language models under four experimental settings reveal consistent performance gaps-especially for more intricate scenarios-underscoring EI's potential to enrich empathetic, context-aware AI applications. Our benchmark and methods are publicly available at: https://github.com/Lum1104/EIBench, offering a foundation for advanced multimodal causal analysis and next-generation affective computing.",
        "arxiv_id": "2504.07521",
        "ARXIVID": "2504.07521",
        "COMMENT": "Matches criterion 2 as it introduces a new benchmark (EIBench) and methods for Vision-Language Models (VLLMs) focusing on emotional reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.07165": {
        "authors": [
            "Yana Wei",
            "Liang Zhao",
            "Kangheng Lin",
            "En Yu",
            "Yuang Peng",
            "Runpei Dong",
            "Jianjian Sun",
            "Haoran Wei",
            "Zheng Ge",
            "Xiangyu Zhang",
            "Vishal M. Patel"
        ],
        "title": "Perception in Reflection",
        "abstract": "arXiv:2504.07165v1 Announce Type: new  Abstract: We present a perception in reflection paradigm designed to transcend the limitations of current large vision-language models (LVLMs), which are expected yet often fail to achieve perfect perception initially. Specifically, we propose Reflective Perception (RePer), a dual-model reflection mechanism that systematically alternates between policy and critic models, enables iterative refinement of visual perception. This framework is powered by Reflective Perceptual Learning (RPL), which reinforces intrinsic reflective capabilities through a methodically constructed visual reflection dataset and reflective unlikelihood training. Comprehensive experimental evaluation demonstrates RePer's quantifiable improvements in image understanding, captioning precision, and hallucination reduction. Notably, RePer achieves strong alignment between model attention patterns and human visual focus, while RPL optimizes fine-grained and free-form preference alignment. These advancements establish perception in reflection as a robust paradigm for future multimodal agents, particularly in tasks requiring complex reasoning and multi-step manipulation.",
        "arxiv_id": "2504.07165",
        "ARXIVID": "2504.07165",
        "COMMENT": "Matches criterion 2 as it proposes a novel reflective perception paradigm for improving large vision-language models (LVLMs).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.07524": {
        "authors": [
            "Xu Zhao",
            "Pengju Zhang",
            "Bo Liu",
            "Yihong Wu"
        ],
        "title": "DGOcc: Depth-aware Global Query-based Network for Monocular 3D Occupancy Prediction",
        "abstract": "arXiv:2504.07524v1 Announce Type: new  Abstract: Monocular 3D occupancy prediction, aiming to predict the occupancy and semantics within interesting regions of 3D scenes from only 2D images, has garnered increasing attention recently for its vital role in 3D scene understanding. Predicting the 3D occupancy of large-scale outdoor scenes from 2D images is ill-posed and resource-intensive. In this paper, we present \\textbf{DGOcc}, a \\textbf{D}epth-aware \\textbf{G}lobal query-based network for monocular 3D \\textbf{Occ}upancy prediction. We first explore prior depth maps to extract depth context features that provide explicit geometric information for the occupancy network. Then, in order to fully exploit the depth context features, we propose a Global Query-based (GQ) Module. The cooperation of attention mechanisms and scale-aware operations facilitates the feature interaction between images and 3D voxels. Moreover, a Hierarchical Supervision Strategy (HSS) is designed to avoid upsampling the high-dimension 3D voxel features to full resolution, which mitigates GPU memory utilization and time cost. Extensive experiments on SemanticKITTI and SSCBench-KITTI-360 datasets demonstrate that the proposed method achieves the best performance on monocular semantic occupancy prediction while reducing GPU and time overhead.",
        "arxiv_id": "2504.07524",
        "ARXIVID": "2504.07524",
        "COMMENT": "Matches criterion 1 as it proposes a novel method for spatial understanding in monocular 3D occupancy prediction, which is relevant to spatial intelligence in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.07951": {
        "authors": [
            "Mustafa Shukor",
            "Enrico Fini",
            "Victor Guilherme Turrisi da Costa",
            "Matthieu Cord",
            "Joshua Susskind",
            "Alaaeldin El-Nouby"
        ],
        "title": "Scaling Laws for Native Multimodal Models Scaling Laws for Native Multimodal Models",
        "abstract": "arXiv:2504.07951v1 Announce Type: new  Abstract: Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approaches exhibit remarkable sample efficiency, it remains an open question whether such late-fusion architectures are inherently superior. In this work, we revisit the architectural design of native multimodal models (NMMs)--those trained from the ground up on all modalities--and conduct an extensive scaling laws study, spanning 457 trained models with different architectures and training mixtures. Our investigation reveals no inherent advantage to late-fusion architectures over early-fusion ones, which do not rely on image encoders. On the contrary, early-fusion exhibits stronger performance at lower parameter counts, is more efficient to train, and is easier to deploy. Motivated by the strong performance of the early-fusion architectures, we show that incorporating Mixture of Experts (MoEs) allows for models that learn modality-specific weights, significantly enhancing performance.",
        "arxiv_id": "2504.07951",
        "ARXIVID": "2504.07951",
        "COMMENT": "Matches criterion 2 as it discusses scaling laws for native multimodal models and compares early-fusion and late-fusion architectures, which is relevant to VLLMs/MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.07836": {
        "authors": [
            "Junli Liu",
            "Qizhi Chen",
            "Zhigang Wang",
            "Yiwen Tang",
            "Yiting Zhang",
            "Chi Yan",
            "Dong Wang",
            "Xuelong Li",
            "Bin Zhao"
        ],
        "title": "AerialVG: A Challenging Benchmark for Aerial Visual Grounding by Exploring Positional Relations",
        "abstract": "arXiv:2504.07836v1 Announce Type: new  Abstract: Visual grounding (VG) aims to localize target objects in an image based on natural language descriptions. In this paper, we propose AerialVG, a new task focusing on visual grounding from aerial views. Compared to traditional VG, AerialVG poses new challenges, \\emph{e.g.}, appearance-based grounding is insufficient to distinguish among multiple visually similar objects, and positional relations should be emphasized. Besides, existing VG models struggle when applied to aerial imagery, where high-resolution images cause significant difficulties. To address these challenges, we introduce the first AerialVG dataset, consisting of 5K real-world aerial images, 50K manually annotated descriptions, and 103K objects. Particularly, each annotation in AerialVG dataset contains multiple target objects annotated with relative spatial relations, requiring models to perform comprehensive spatial reasoning. Furthermore, we propose an innovative model especially for the AerialVG task, where a Hierarchical Cross-Attention is devised to focus on target regions, and a Relation-Aware Grounding module is designed to infer positional relations. Experimental results validate the effectiveness of our dataset and method, highlighting the importance of spatial reasoning in aerial visual grounding. The code and dataset will be released.",
        "arxiv_id": "2504.07836",
        "ARXIVID": "2504.07836",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (AerialVG) for aerial visual grounding, focusing on spatial reasoning and positional relations.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.07334": {
        "authors": [
            "Chendi Lin",
            "Heshan Liu",
            "Qunshu Lin",
            "Zachary Bright",
            "Shitao Tang",
            "Yihui He",
            "Minghao Liu",
            "Ling Zhu",
            "Cindy Le"
        ],
        "title": "Objaverse++: Curated 3D Object Dataset with Quality Annotations",
        "abstract": "arXiv:2504.07334v1 Announce Type: new  Abstract: This paper presents Objaverse++, a curated subset of Objaverse enhanced with detailed attribute annotations by human experts. Recent advances in 3D content generation have been driven by large-scale datasets such as Objaverse, which contains over 800,000 3D objects collected from the Internet. Although Objaverse represents the largest available 3D asset collection, its utility is limited by the predominance of low-quality models. To address this limitation, we manually annotate 10,000 3D objects with detailed attributes, including aesthetic quality scores, texture color classifications, multi-object composition flags, transparency characteristics, etc. Then, we trained a neural network capable of annotating the tags for the rest of the Objaverse dataset. Through experiments and a user study on generation results, we demonstrate that models pre-trained on our quality-focused subset achieve better performance than those trained on the larger dataset of Objaverse in image-to-3D generation tasks. In addition, by comparing multiple subsets of training data filtered by our tags, our results show that the higher the data quality, the faster the training loss converges. These findings suggest that careful curation and rich annotation can compensate for the raw dataset size, potentially offering a more efficient path to develop 3D generative models. We release our enhanced dataset of approximately 500,000 curated 3D models to facilitate further research on various downstream tasks in 3D computer vision. In the near future, we aim to extend our annotations to cover the entire Objaverse dataset.",
        "arxiv_id": "2504.07334",
        "ARXIVID": "2504.07334",
        "COMMENT": "Matches criterion 3 as it introduces a curated 3D object dataset (Objaverse++) with quality annotations, which can be used for benchmarking and improving 3D generative models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.07424": {
        "authors": [
            "Chenxi Sun",
            "Hongzhi Zhang",
            "Qi Wang",
            "Fuzheng Zhang"
        ],
        "title": "Routing to the Right Expertise: A Trustworthy Judge for Instruction-based Image Editing",
        "abstract": "arXiv:2504.07424v1 Announce Type: new  Abstract: Instruction-based Image Editing (IIE) models have made significantly improvement due to the progress of multimodal large language models (MLLMs) and diffusion models, which can understand and reason about complex editing instructions. In addition to advancing current IIE models, accurately evaluating their output has become increasingly critical and challenging. Current IIE evaluation methods and their evaluation procedures often fall short of aligning with human judgment and often lack explainability. To address these limitations, we propose JUdgement through Routing of Expertise (JURE). Each expert in JURE is a pre-selected model assumed to be equipped with an atomic expertise that can provide useful feedback to judge output, and the router dynamically routes the evaluation task of a given instruction and its output to appropriate experts, aggregating their feedback into a final judge. JURE is trustworthy in two aspects. First, it can effortlessly provide explanations about its judge by examining the routed experts and their feedback. Second, experimental results demonstrate that JURE is reliable by achieving superior alignment with human judgments, setting a new standard for automated IIE evaluation. Moreover, JURE's flexible design is future-proof - modular experts can be seamlessly replaced or expanded to accommodate advancements in IIE, maintaining consistently high evaluation quality. Our evaluation data and results are available at https://github.com/Cyyyyyrus/JURE.git.",
        "arxiv_id": "2504.07424",
        "ARXIVID": "2504.07424",
        "COMMENT": "Matches criterion 2 as it focuses on evaluation methods for instruction-based image editing using MLLMs and diffusion models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.07336": {
        "authors": [
            "Siyuan Dai",
            "Kai Ye",
            "Guodong Liu",
            "Haoteng Tang",
            "Liang Zhan"
        ],
        "title": "Zeus: Zero-shot LLM Instruction for Union Segmentation in Multimodal Medical Imaging",
        "abstract": "arXiv:2504.07336v1 Announce Type: new  Abstract: Medical image segmentation has achieved remarkable success through the continuous advancement of UNet-based and Transformer-based foundation backbones. However, clinical diagnosis in the real world often requires integrating domain knowledge, especially textual information. Conducting multimodal learning involves visual and text modalities shown as a solution, but collecting paired vision-language datasets is expensive and time-consuming, posing significant challenges. Inspired by the superior ability in numerous cross-modal tasks for Large Language Models (LLMs), we proposed a novel Vision-LLM union framework to address the issues. Specifically, we introduce frozen LLMs for zero-shot instruction generation based on corresponding medical images, imitating the radiology scanning and report generation process. {To better approximate real-world diagnostic processes}, we generate more precise text instruction from multimodal radiology images (e.g., T1-w or T2-w MRI and CT). Based on the impressive ability of semantic understanding and rich knowledge of LLMs. This process emphasizes extracting special features from different modalities and reunion the information for the ultimate clinical diagnostic. With generated text instruction, our proposed union segmentation framework can handle multimodal segmentation without prior collected vision-language datasets. To evaluate our proposed method, we conduct comprehensive experiments with influential baselines, the statistical results and the visualized case study demonstrate the superiority of our novel method.}",
        "arxiv_id": "2504.07336",
        "ARXIVID": "2504.07336",
        "COMMENT": "Matches criterion 2 as it introduces a Vision-LLM union framework for multimodal medical imaging, leveraging LLMs for zero-shot instruction generation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.07942": {
        "authors": [
            "Nico Catalano",
            "Stefano Samele",
            "Paolo Pertino",
            "Matteo Matteucci"
        ],
        "title": "MARS: a Multimodal Alignment and Ranking System for Few-Shot Segmentation",
        "abstract": "arXiv:2504.07942v1 Announce Type: new  Abstract: Current Few Shot Segmentation literature lacks a mask selection method that goes beyond visual similarity between the query and example images, leading to suboptimal predictions. We present MARS, a plug-and-play ranking system that leverages multimodal cues to filter and merge mask proposals robustly. Starting from a set of mask predictions for a single query image, we score, filter, and merge them to improve results. Proposals are evaluated using multimodal scores computed at local and global levels. Extensive experiments on COCO-20i, Pascal-5i, LVIS-92i, and FSS-1000 demonstrate that integrating all four scoring components is crucial for robust ranking, validating our contribution. As MARS can be effortlessly integrated with various mask proposal systems, we deploy it across a wide range of top-performer methods and achieve new state-of-the-art results on multiple existing benchmarks. Code will be available upon acceptance.",
        "arxiv_id": "2504.07942",
        "ARXIVID": "2504.07942",
        "COMMENT": "Matches criterion 4 as it discusses a multimodal alignment and ranking system for few-shot segmentation, which is related to vision foundation models and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2504.07667": {
        "authors": [
            "Yujin Wang",
            "Jiarui Wu",
            "Yichen Bian",
            "Fan Zhang",
            "Tianfan Xue"
        ],
        "title": "S2R-HDR: A Large-Scale Rendered Dataset for HDR Fusion",
        "abstract": "arXiv:2504.07667v1 Announce Type: new  Abstract: The generalization of learning-based high dynamic range (HDR) fusion is often limited by the availability of training data, as collecting large-scale HDR images from dynamic scenes is both costly and technically challenging. To address these challenges, we propose S2R-HDR, the first large-scale high-quality synthetic dataset for HDR fusion, with 24,000 HDR samples. Using Unreal Engine 5, we design a diverse set of realistic HDR scenes that encompass various dynamic elements, motion types, high dynamic range scenes, and lighting. Additionally, we develop an efficient rendering pipeline to generate realistic HDR images. To further mitigate the domain gap between synthetic and real-world data, we introduce S2R-Adapter, a domain adaptation designed to bridge this gap and enhance the generalization ability of models. Experimental results on real-world datasets demonstrate that our approach achieves state-of-the-art HDR reconstruction performance. Dataset and code will be available at https://openimaginglab.github.io/S2R-HDR.",
        "arxiv_id": "2504.07667",
        "ARXIVID": "2504.07667",
        "COMMENT": "Matches criterion 3 as it introduces a new large-scale synthetic dataset (S2R-HDR) for HDR fusion, which could be relevant to embodied AI benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.07603": {
        "authors": [
            "Youngwan Jin",
            "Michal Kovac",
            "Yagiz Nalcakan",
            "Hyeongjin Ju",
            "Hanbin Song",
            "Sanghyeop Yeo",
            "Shiho Kim"
        ],
        "title": "RASMD: RGB And SWIR Multispectral Driving Dataset for Robust Perception in Adverse Conditions",
        "abstract": "arXiv:2504.07603v1 Announce Type: new  Abstract: Current autonomous driving algorithms heavily rely on the visible spectrum, which is prone to performance degradation in adverse conditions like fog, rain, snow, glare, and high contrast. Although other spectral bands like near-infrared (NIR) and long-wave infrared (LWIR) can enhance vision perception in such situations, they have limitations and lack large-scale datasets and benchmarks. Short-wave infrared (SWIR) imaging offers several advantages over NIR and LWIR. However, no publicly available large-scale datasets currently incorporate SWIR data for autonomous driving. To address this gap, we introduce the RGB and SWIR Multispectral Driving (RASMD) dataset, which comprises 100,000 synchronized and spatially aligned RGB-SWIR image pairs collected across diverse locations, lighting, and weather conditions. In addition, we provide a subset for RGB-SWIR translation and object detection annotations for a subset of challenging traffic scenarios to demonstrate the utility of SWIR imaging through experiments on both object detection and RGB-to-SWIR image translation. Our experiments show that combining RGB and SWIR data in an ensemble framework significantly improves detection accuracy compared to RGB-only approaches, particularly in conditions where visible-spectrum sensors struggle. We anticipate that the RASMD dataset will advance research in multispectral imaging for autonomous driving and robust perception systems.",
        "arxiv_id": "2504.07603",
        "ARXIVID": "2504.07603",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset (RASMD) for robust perception in adverse conditions, which could be relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.07395": {
        "authors": [
            "Arya Fayyazi",
            "Mehdi Kamal",
            "Massoud Pedram"
        ],
        "title": "FAIR-SIGHT: Fairness Assurance in Image Recognition via Simultaneous Conformal Thresholding and Dynamic Output Repair",
        "abstract": "arXiv:2504.07395v1 Announce Type: new  Abstract: We introduce FAIR-SIGHT, an innovative post-hoc framework designed to ensure fairness in computer vision systems by combining conformal prediction with a dynamic output repair mechanism. Our approach calculates a fairness-aware non-conformity score that simultaneously assesses prediction errors and fairness violations. Using conformal prediction, we establish an adaptive threshold that provides rigorous finite-sample, distribution-free guarantees. When the non-conformity score for a new image exceeds the calibrated threshold, FAIR-SIGHT implements targeted corrective adjustments, such as logit shifts for classification and confidence recalibration for detection, to reduce both group and individual fairness disparities, all without the need for retraining or having access to internal model parameters. Comprehensive theoretical analysis validates our method's error control and convergence properties. At the same time, extensive empirical evaluations on benchmark datasets show that FAIR-SIGHT significantly reduces fairness disparities while preserving high predictive performance.",
        "arxiv_id": "2504.07395",
        "ARXIVID": "2504.07395",
        "COMMENT": "Does not match any specific criteria but is related to fairness in computer vision systems.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.07963": {
        "authors": [
            "Shoufa Chen",
            "Chongjian Ge",
            "Shilong Zhang",
            "Peize Sun",
            "Ping Luo"
        ],
        "title": "PixelFlow: Pixel-Space Generative Models with Flow",
        "abstract": "arXiv:2504.07963v1 Announce Type: new  Abstract: We present PixelFlow, a family of image generation models that operate directly in the raw pixel space, in contrast to the predominant latent-space models. This approach simplifies the image generation process by eliminating the need for a pre-trained Variational Autoencoder (VAE) and enabling the whole model end-to-end trainable. Through efficient cascade flow modeling, PixelFlow achieves affordable computation cost in pixel space. It achieves an FID of 1.98 on 256$\\times$256 ImageNet class-conditional image generation benchmark. The qualitative text-to-image results demonstrate that PixelFlow excels in image quality, artistry, and semantic control. We hope this new paradigm will inspire and open up new opportunities for next-generation visual generation models. Code and models are available at https://github.com/ShoufaChen/PixelFlow.",
        "arxiv_id": "2504.07963",
        "ARXIVID": "2504.07963",
        "COMMENT": "Does not match any specific criteria. Focuses on generative modeling in pixel space, which is not directly related to vision-language models or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.07335": {
        "authors": [
            "Akash Jadhav",
            "Michael Greenspan"
        ],
        "title": "DLTPose: 6DoF Pose Estimation From Accurate Dense Surface Point Estimates",
        "abstract": "arXiv:2504.07335v1 Announce Type: new  Abstract: We propose DLTPose, a novel method for 6DoF object pose estimation from RGB-D images that combines the accuracy of sparse keypoint methods with the robustness of dense pixel-wise predictions. DLTPose predicts per-pixel radial distances to a set of minimally four keypoints, which are then fed into our novel Direct Linear Transform (DLT) formulation to produce accurate 3D object frame surface estimates, leading to better 6DoF pose estimation. Additionally, we introduce a novel symmetry-aware keypoint ordering approach, designed to handle object symmetries that otherwise cause inconsistencies in keypoint assignments. Previous keypoint-based methods relied on fixed keypoint orderings, which failed to account for the multiple valid configurations exhibited by symmetric objects, which our ordering approach exploits to enhance the model's ability to learn stable keypoint representations. Extensive experiments on the benchmark LINEMOD, Occlusion LINEMOD and YCB-Video datasets show that DLTPose outperforms existing methods, especially for symmetric and occluded objects, demonstrating superior Mean Average Recall values of 86.5% (LM), 79.7% (LM-O) and 89.5% (YCB-V). The code is available at https://anonymous.4open.science/r/DLTPose_/ .",
        "arxiv_id": "2504.07335",
        "ARXIVID": "2504.07335",
        "COMMENT": "Does not match any specific criteria. Focuses on 6DoF pose estimation, which is tangentially related to spatial intelligence but not directly aligned with the criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2504.07503": {
        "authors": [
            "Jinze Chen",
            "Wei Zhai",
            "Yang Cao",
            "Bin Li",
            "Zheng-Jun Zha"
        ],
        "title": "Event Signal Filtering via Probability Flux Estimation",
        "abstract": "arXiv:2504.07503v1 Announce Type: new  Abstract: Events offer a novel paradigm for capturing scene dynamics via asynchronous sensing, but their inherent randomness often leads to degraded signal quality. Event signal filtering is thus essential for enhancing fidelity by reducing this internal randomness and ensuring consistent outputs across diverse acquisition conditions. Unlike traditional time series that rely on fixed temporal sampling to capture steady-state behaviors, events encode transient dynamics through polarity and event intervals, making signal modeling significantly more complex. To address this, the theoretical foundation of event generation is revisited through the lens of diffusion processes. The state and process information within events is modeled as continuous probability flux at threshold boundaries of the underlying irradiance diffusion. Building on this insight, a generative, online filtering framework called Event Density Flow Filter (EDFilter) is introduced. EDFilter estimates event correlation by reconstructing the continuous probability flux from discrete events using nonparametric kernel smoothing, and then resamples filtered events from this flux. To optimize fidelity over time, spatial and temporal kernels are employed in a time-varying optimization framework. A fast recursive solver with O(1) complexity is proposed, leveraging state-space models and lookup tables for efficient likelihood computation. Furthermore, a new real-world benchmark Rotary Event Dataset (RED) is released, offering microsecond-level ground truth irradiance for full-reference event filtering evaluation. Extensive experiments validate EDFilter's performance across tasks like event filtering, super-resolution, and direct event-based blob tracking. Significant gains in downstream applications such as SLAM and video reconstruction underscore its robustness and effectiveness.",
        "arxiv_id": "2504.07503",
        "ARXIVID": "2504.07503",
        "COMMENT": "Does not match any specific criterion but is relevant to event-based vision and signal filtering, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.07813": {
        "authors": [
            "Pengfei Chen",
            "Xuehui Yu",
            "Xumeng Han",
            "Kuiran Wang",
            "Guorong Li",
            "Lingxi Xie",
            "Zhenjun Han",
            "Jianbin Jiao"
        ],
        "title": "P2Object: Single Point Supervised Object Detection and Instance Segmentation",
        "abstract": "arXiv:2504.07813v1 Announce Type: new  Abstract: Object recognition using single-point supervision has attracted increasing attention recently. However, the performance gap compared with fully-supervised algorithms remains large. Previous works generated class-agnostic \\textbf{\\textit{proposals in an image}} offline and then treated mixed candidates as a single bag, putting a huge burden on multiple instance learning (MIL). In this paper, we introduce Point-to-Box Network (P2BNet), which constructs balanced \\textbf{\\textit{instance-level proposal bags}} by generating proposals in an anchor-like way and refining the proposals in a coarse-to-fine paradigm. Through further research, we find that the bag of proposals, either at the image level or the instance level, is established on discrete box sampling. This leads the pseudo box estimation into a sub-optimal solution, resulting in the truncation of object boundaries or the excessive inclusion of background. Hence, we conduct a series exploration of discrete-to-continuous optimization, yielding P2BNet++ and Point-to-Mask Network (P2MNet). P2BNet++ conducts an approximately continuous proposal sampling strategy by better utilizing spatial clues. P2MNet further introduces low-level image information to assist in pixel prediction, and a boundary self-prediction is designed to relieve the limitation of the estimated boxes. Benefiting from the continuous object-aware \\textbf{\\textit{pixel-level perception}}, P2MNet can generate more precise bounding boxes and generalize to segmentation tasks. Our method largely surpasses the previous methods in terms of the mean average precision on COCO, VOC, SBD, and Cityscapes, demonstrating great potential to bridge the performance gap compared with fully supervised tasks.",
        "arxiv_id": "2504.07813",
        "ARXIVID": "2504.07813",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and object detection, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.07744": {
        "authors": [
            "Jenna Kline",
            "Samuel Stevens",
            "Guy Maalouf",
            "Camille Rondeau Saint-Jean",
            "Dat Nguyen Ngoc",
            "Majid Mirmehdi",
            "David Guerin",
            "Tilo Burghardt",
            "Elzbieta Pastucha",
            "Blair Costelloe",
            "Matthew Watson",
            "Thomas Richardson",
            "Ulrik Pagh Schultz Lundquist"
        ],
        "title": "MMLA: Multi-Environment, Multi-Species, Low-Altitude Aerial Footage Dataset",
        "abstract": "arXiv:2504.07744v1 Announce Type: new  Abstract: Real-time wildlife detection in drone imagery is critical for numerous applications, including animal ecology, conservation, and biodiversity monitoring. Low-altitude drone missions are effective for collecting fine-grained animal movement and behavior data, particularly if missions are automated for increased speed and consistency. However, little work exists on evaluating computer vision models on low-altitude aerial imagery and generalizability across different species and settings. To fill this gap, we present a novel multi-environment, multi-species, low-altitude aerial footage (MMLA) dataset. MMLA consists of drone footage collected across three diverse environments: Ol Pejeta Conservancy and Mpala Research Centre in Kenya, and The Wilds Conservation Center in Ohio, which includes five species: Plains zebras, Grevy's zebras, giraffes, onagers, and African Painted Dogs. We comprehensively evaluate three YOLO models (YOLOv5m, YOLOv8m, and YOLOv11m) for detecting animals. Results demonstrate significant performance disparities across locations and species-specific detection variations. Our work highlights the importance of evaluating detection algorithms across different environments for robust wildlife monitoring applications using drones.",
        "arxiv_id": "2504.07744",
        "ARXIVID": "2504.07744",
        "COMMENT": "Does not match any specific criterion but introduces a dataset (MMLA) for wildlife detection in drone imagery, which is tangentially related to computer vision but not directly relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.07463": {
        "authors": [
            "Rahul K. Dass",
            "Rochan H. Madhusudhana",
            "Erin C. Deye",
            "Shashank Verma",
            "Timothy A. Bydlon",
            "Grace Brazil",
            "Ashok K. Goel"
        ],
        "title": "Enhanced Question-Answering for Skill-based learning using Knowledge-based AI and Generative AI",
        "abstract": "arXiv:2504.07463v1 Announce Type: new  Abstract: Supporting learners' understanding of taught skills in online settings is a longstanding challenge. While exercises and chat-based agents can evaluate understanding in limited contexts, this challenge is magnified when learners seek explanations that delve into procedural knowledge (how things are done) and reasoning (why things happen). We hypothesize that an intelligent agent's ability to understand and explain learners' questions about skills can be significantly enhanced using the TMK (Task-Method-Knowledge) model, a Knowledge-based AI framework. We introduce Ivy, an intelligent agent that leverages an LLM and iterative refinement techniques to generate explanations that embody teleological, causal, and compositional principles. Our initial evaluation demonstrates that this approach goes beyond the typical shallow responses produced by an agent with access to unstructured text, thereby substantially improving the depth and relevance of feedback. This can potentially ensure learners develop a comprehensive understanding of skills crucial for effective problem-solving in online environments.",
        "arxiv_id": "2504.07463",
        "ARXIVID": "2504.07463",
        "COMMENT": "Does not match any specific criteria but is related to generative AI and knowledge-based AI for skill-based learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.07301": {
        "authors": [
            "Krzysztof Byrski",
            "Jacek Tabor",
            "Przemys{\\l}aw Spurek",
            "Marcin Mazur"
        ],
        "title": "CEC-MMR: Cross-Entropy Clustering Approach to Multi-Modal Regression",
        "abstract": "arXiv:2504.07301v1 Announce Type: new  Abstract: In practical applications of regression analysis, it is not uncommon to encounter a multitude of values for each attribute. In such a situation, the univariate distribution, which is typically Gaussian, is suboptimal because the mean may be situated between modes, resulting in a predicted value that differs significantly from the actual data. Consequently, to address this issue, a mixture distribution with parameters learned by a neural network, known as a Mixture Density Network (MDN), is typically employed. However, this approach has an important inherent limitation, in that it is not feasible to ascertain the precise number of components with a reasonable degree of accuracy. In this paper, we introduce CEC-MMR, a novel approach based on Cross-Entropy Clustering (CEC), which allows for the automatic detection of the number of components in a regression problem. Furthermore, given an attribute and its value, our method is capable of uniquely identifying it with the underlying component. The experimental results demonstrate that CEC-MMR yields superior outcomes compared to classical MDNs.",
        "arxiv_id": "2504.07301",
        "ARXIVID": "2504.07301",
        "COMMENT": "Does not match any specific criteria but is related to general machine learning methods for regression analysis.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.07418": {
        "authors": [
            "Anning Hu",
            "Ang Li",
            "Xirui Jin",
            "Danping Zou"
        ],
        "title": "ThermoStereoRT: Thermal Stereo Matching in Real Time via Knowledge Distillation and Attention-based Refinement",
        "abstract": "arXiv:2504.07418v1 Announce Type: new  Abstract: We introduce ThermoStereoRT, a real-time thermal stereo matching method designed for all-weather conditions that recovers disparity from two rectified thermal stereo images, envisioning applications such as night-time drone surveillance or under-bed cleaning robots. Leveraging a lightweight yet powerful backbone, ThermoStereoRT constructs a 3D cost volume from thermal images and employs multi-scale attention mechanisms to produce an initial disparity map. To refine this map, we design a novel channel and spatial attention module. Addressing the challenge of sparse ground truth data in thermal imagery, we utilize knowledge distillation to boost performance without increasing computational demands. Comprehensive evaluations on multiple datasets demonstrate that ThermoStereoRT delivers both real-time capacity and robust accuracy, making it a promising solution for real-world deployment in various challenging environments. Our code will be released on https://github.com/SJTU-ViSYS-team/ThermoStereoRT",
        "arxiv_id": "2504.07418",
        "ARXIVID": "2504.07418",
        "COMMENT": "Does not match any specific criteria. Focuses on thermal stereo matching, which is not directly related to the specified interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.07640": {
        "authors": [
            "Ruslan Idelfonso Magana Vsevolodovna",
            "Marco Monti"
        ],
        "title": "Enhancing Large Language Models through Neuro-Symbolic Integration and Ontological Reasoning",
        "abstract": "arXiv:2504.07640v1 Announce Type: new  Abstract: Large Language Models (LLMs) demonstrate impressive capabilities in natural language processing but suffer from inaccuracies and logical inconsistencies known as hallucinations. This compromises their reliability, especially in domains requiring factual accuracy. We propose a neuro-symbolic approach integrating symbolic ontological reasoning and machine learning methods to enhance the consistency and reliability of LLM outputs. Our workflow utilizes OWL ontologies, a symbolic reasoner (e.g., HermiT) for consistency checking, and a lightweight machine learning model (logistic regression) for mapping natural language statements into logical forms compatible with the ontology. When inconsistencies between LLM outputs and the ontology are detected, the system generates explanatory feedback to guide the LLM towards a corrected, logically coherent response in an iterative refinement loop. We present a working Python prototype demonstrating this pipeline. Experimental results in a defined domain suggest significant improvements in semantic coherence and factual accuracy of LLM outputs, showcasing the potential of combining LLM fluency with the rigor of formal semantics.",
        "arxiv_id": "2504.07640",
        "ARXIVID": "2504.07640",
        "COMMENT": "Does not match any specific criteria. Focuses on neuro-symbolic integration for enhancing LLMs, which is not directly related to vision-language models or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.07382": {
        "authors": [
            "Qingchao Jiang",
            "Zhishuo Xu",
            "Zhiying Zhu",
            "Ning Chen",
            "Haoyue Wang",
            "Zhongjie Ba"
        ],
        "title": "Model Discrepancy Learning: Synthetic Faces Detection Based on Multi-Reconstruction",
        "abstract": "arXiv:2504.07382v1 Announce Type: new  Abstract: Advances in image generation enable hyper-realistic synthetic faces but also pose risks, thus making synthetic face detection crucial. Previous research focuses on the general differences between generated images and real images, often overlooking the discrepancies among various generative techniques. In this paper, we explore the intrinsic relationship between synthetic images and their corresponding generation technologies. We find that specific images exhibit significant reconstruction discrepancies across different generative methods and that matching generation techniques provide more accurate reconstructions. Based on this insight, we propose a Multi-Reconstruction-based detector. By reversing and reconstructing images using multiple generative models, we analyze the reconstruction differences among real, GAN-generated, and DM-generated images to facilitate effective differentiation. Additionally, we introduce the Asian Synthetic Face Dataset (ASFD), containing synthetic Asian faces generated with various GANs and DMs. This dataset complements existing synthetic face datasets. Experimental results demonstrate that our detector achieves exceptional performance, with strong generalization and robustness.",
        "arxiv_id": "2504.07382",
        "ARXIVID": "2504.07382",
        "COMMENT": "Does not match any specific criteria. Focuses on synthetic face detection, which is not directly related to the specified interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.07779": {
        "authors": [
            "Xian Chen",
            "Rong Qu",
            "Jing Dong",
            "Ruibin Bai",
            "Yaochu Jin"
        ],
        "title": "Genetic Programming with Reinforcement Learning Trained Transformer for Real-World Dynamic Scheduling Problems",
        "abstract": "arXiv:2504.07779v1 Announce Type: new  Abstract: Dynamic scheduling in real-world environments often struggles to adapt to unforeseen disruptions, making traditional static scheduling methods and human-designed heuristics inadequate. This paper introduces an innovative approach that combines Genetic Programming (GP) with a Transformer trained through Reinforcement Learning (GPRT), specifically designed to tackle the complexities of dynamic scheduling scenarios. GPRT leverages the Transformer to refine heuristics generated by GP while also seeding and guiding the evolution of GP. This dual functionality enhances the adaptability and effectiveness of the scheduling heuristics, enabling them to better respond to the dynamic nature of real-world tasks. The efficacy of this integrated approach is demonstrated through a practical application in container terminal truck scheduling, where the GPRT method outperforms traditional GP, standalone Transformer methods, and other state-of-the-art competitors. The key contribution of this research is the development of the GPRT method, which showcases a novel combination of GP and Reinforcement Learning (RL) to produce robust and efficient scheduling solutions. Importantly, GPRT is not limited to container port truck scheduling; it offers a versatile framework applicable to various dynamic scheduling challenges. Its practicality, coupled with its interpretability and ease of modification, makes it a valuable tool for diverse real-world scenarios.",
        "arxiv_id": "2504.07779",
        "ARXIVID": "2504.07779",
        "COMMENT": "Does not match any specific criterion but is relevant to reinforcement learning and scheduling, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.07758": {
        "authors": [
            "Shuangfan Zhou",
            "Chu Zhou",
            "Youwei Lyu",
            "Heng Guo",
            "Zhanyu Ma",
            "Boxin Shi",
            "Imari Sato"
        ],
        "title": "PIDSR:ComplementaryPolarizedImageDemosaicingandSuper-Resolution",
        "abstract": "arXiv:2504.07758v1 Announce Type: new  Abstract: Polarization cameras can capture multiple polarized images with different polarizer angles in a single shot, bringing convenience to polarization-based downstream tasks. However, their direct outputs are color-polarization filter array (CPFA) raw images, requiring demosaicing to reconstruct full-resolution, full-color polarized images; unfortunately, this necessary step introduces artifacts that make polarization-related parameters such as the degree of polarization (DoP) and angle of polarization (AoP) prone to error. Besides, limited by the hardware design, the resolution of a polarization camera is often much lower than that of a conventional RGB camera. Existing polarized image demosaicing (PID) methods are limited in that they cannot enhance resolution, while polarized image super-resolution (PISR) methods, though designed to obtain high-resolution (HR) polarized images from the demosaicing results, tend to retain or even amplify errors in the DoP and AoP introduced by demosaicing artifacts. In this paper, we propose PIDSR, a joint framework that performs complementary Polarized Image Demosaicing and Super-Resolution, showing the ability to robustly obtain high-quality HR polarized images with more accurate DoP and AoP from a CPFA raw image in a direct manner. Experiments show our PIDSR not only achieves state-of-the-art performance on both synthetic and real data, but also facilitates downstream tasks.",
        "arxiv_id": "2504.07758",
        "ARXIVID": "2504.07758",
        "COMMENT": "Does not match any specific criterion but is relevant to polarization imaging and super-resolution, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.07761": {
        "authors": [
            "Javier Mu\\~noz-Haro",
            "Ruben Tolosana",
            "Ruben Vera-Rodriguez",
            "Aythami Morales",
            "Julian Fierrez"
        ],
        "title": "Exploring a Patch-Wise Approach for Privacy-Preserving Fake ID Detection",
        "abstract": "arXiv:2504.07761v1 Announce Type: new  Abstract: In an increasingly digitalized world, verifying the authenticity of ID documents has become a critical challenge for real-life applications such as digital banking, crypto-exchanges, renting, etc. This study focuses on the topic of fake ID detection, covering several limitations in the field. In particular, no publicly available data from real ID documents exists, and most studies rely on proprietary in-house databases that are not available due to privacy reasons. In order to shed some light on this critical challenge that makes difficult to advance in the field, we explore a trade-off between privacy (i.e., amount of sensitive data available) and performance, proposing a novel patch-wise approach for privacy-preserving fake ID detection. Our proposed approach explores how privacy can be enhanced through: i) two levels of anonymization for an ID document (i.e., fully- and pseudo-anonymized), and ii) different patch size configurations, varying the amount of sensitive data visible in the patch image. Also, state-of-the-art methods such as Vision Transformers and Foundation Models are considered in the analysis. The experimental framework shows that, on an unseen database (DLC-2021), our proposal achieves 13.91% and 0% EERs at patch and ID document level, showing a good generalization to other databases. In addition to this exploration, another key contribution of our study is the release of the first publicly available database that contains 48,400 patches from both real and fake ID documents, along with the experimental framework and models, which will be available in our GitHub.",
        "arxiv_id": "2504.07761",
        "ARXIVID": "2504.07761",
        "COMMENT": "Does not match any specific criterion but is relevant to privacy-preserving methods and vision transformers, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.07405": {
        "authors": [
            "Linyan Huang",
            "Haonan Lin",
            "Yanning Zhou",
            "Kaiwen Xiao"
        ],
        "title": "FlexIP: Dynamic Control of Preservation and Personality for Customized Image Generation",
        "abstract": "arXiv:2504.07405v1 Announce Type: new  Abstract: With the rapid advancement of 2D generative models, preserving subject identity while enabling diverse editing has emerged as a critical research focus. Existing methods typically face inherent trade-offs between identity preservation and personalized manipulation. We introduce FlexIP, a novel framework that decouples these objectives through two dedicated components: a Personalization Adapter for stylistic manipulation and a Preservation Adapter for identity maintenance. By explicitly injecting both control mechanisms into the generative model, our framework enables flexible parameterized control during inference through dynamic tuning of the weight adapter. Experimental results demonstrate that our approach breaks through the performance limitations of conventional methods, achieving superior identity preservation while supporting more diverse personalized generation capabilities (Project Page: https://flexip-tech.github.io/flexip/).",
        "arxiv_id": "2504.07405",
        "ARXIVID": "2504.07405",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and identity preservation in image generation, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}