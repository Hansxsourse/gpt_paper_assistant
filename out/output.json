{
    "2505.01809": {
        "authors": [
            "Xiaoqi Li",
            "Jiaming Liu",
            "Nuowei Han",
            "Liang Heng",
            "Yandong Guo",
            "Hao Dong",
            "Yang Liu"
        ],
        "title": "3DWG: 3D Weakly Supervised Visual Grounding via Category and Instance-Level Alignment",
        "abstract": "arXiv:2505.01809v1 Announce Type: new  Abstract: The 3D weakly-supervised visual grounding task aims to localize oriented 3D boxes in point clouds based on natural language descriptions without requiring annotations to guide model learning. This setting presents two primary challenges: category-level ambiguity and instance-level complexity. Category-level ambiguity arises from representing objects of fine-grained categories in a highly sparse point cloud format, making category distinction challenging. Instance-level complexity stems from multiple instances of the same category coexisting in a scene, leading to distractions during grounding. To address these challenges, we propose a novel weakly-supervised grounding approach that explicitly differentiates between categories and instances. In the category-level branch, we utilize extensive category knowledge from a pre-trained external detector to align object proposal features with sentence-level category features, thereby enhancing category awareness. In the instance-level branch, we utilize spatial relationship descriptions from language queries to refine object proposal features, ensuring clear differentiation among objects. These designs enable our model to accurately identify target-category objects while distinguishing instances within the same category. Compared to previous methods, our approach achieves state-of-the-art performance on three widely used benchmarks: Nr3D, Sr3D, and ScanRef.",
        "arxiv_id": "2505.01809",
        "ARXIVID": "2505.01809",
        "COMMENT": "Matches criterion 3 as it focuses on embodied AI with a novel weakly-supervised method for 3D visual grounding, addressing previously ignored challenges.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.02471": {
        "authors": [
            "Biao Gong",
            "Cheng Zou",
            "Dandan Zheng",
            "Hu Yu",
            "Jingdong Chen",
            "Jianxin Sun",
            "Junbo Zhao",
            "Jun Zhou",
            "Kaixiang Ji",
            "Lixiang Ru",
            "Libin Wang",
            "Qingpei Guo",
            "Rui Liu",
            "Weilong Chai",
            "Xinyu Xiao",
            "Ziyuan Huang"
        ],
        "title": "Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction",
        "abstract": "arXiv:2505.02471v1 Announce Type: new  Abstract: We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries and M2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. By leveraging a fixed MLLM and a learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. Our experimental results demonstrate the strong performance of Ming-Lite-Uni and illustrate the impressive fluid nature of its interactive process. All code and model weights are open-sourced to foster further exploration within the community. Notably, this work aligns with concurrent multimodal AI milestones - such as ChatGPT-4o with native image generation updated in March 25, 2025 - underscoring the broader significance of unified models like Ming-Lite-Uni on the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further refined.",
        "arxiv_id": "2505.02471",
        "ARXIVID": "2505.02471",
        "COMMENT": "Matches criterion 2 as it introduces a new multimodal large language model (MLLM) framework, Ming-Lite-Uni, with novel architectural advancements.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.02753": {
        "authors": [
            "Yankai Jiang",
            "Peng Zhang",
            "Donglin Yang",
            "Yuan Tian",
            "Hai Lin",
            "Xiaosong Wang"
        ],
        "title": "Advancing Generalizable Tumor Segmentation with Anomaly-Aware Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models",
        "abstract": "arXiv:2505.02753v1 Announce Type: new  Abstract: We explore Generalizable Tumor Segmentation, aiming to train a single model for zero-shot tumor segmentation across diverse anatomical regions. Existing methods face limitations related to segmentation quality, scalability, and the range of applicable imaging modalities. In this paper, we uncover the potential of the internal representations within frozen medical foundation diffusion models as highly efficient zero-shot learners for tumor segmentation by introducing a novel framework named DiffuGTS. DiffuGTS creates anomaly-aware open-vocabulary attention maps based on text prompts to enable generalizable anomaly segmentation without being restricted by a predefined training category list. To further improve and refine anomaly segmentation masks, DiffuGTS leverages the diffusion model, transforming pathological regions into high-quality pseudo-healthy counterparts through latent space inpainting, and applies a novel pixel-level and feature-level residual learning approach, resulting in segmentation masks with significantly enhanced quality and generalization. Comprehensive experiments on four datasets and seven tumor categories demonstrate the superior performance of our method, surpassing current state-of-the-art models across multiple zero-shot settings. Codes are available at https://github.com/Yankai96/DiffuGTS.",
        "arxiv_id": "2505.02753",
        "ARXIVID": "2505.02753",
        "COMMENT": "Matches criterion 4 as it explores the use of frozen foundation diffusion models for generalizable tumor segmentation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.02626": {
        "authors": [
            "Sassan Mokhtar",
            "Arian Mousakhan",
            "Silvio Galesso",
            "Jawad Tayyub",
            "Thomas Brox"
        ],
        "title": "Detect, Classify, Act: Categorizing Industrial Anomalies with Multi-Modal Large Language Models",
        "abstract": "arXiv:2505.02626v1 Announce Type: new  Abstract: Recent advances in visual industrial anomaly detection have demonstrated exceptional performance in identifying and segmenting anomalous regions while maintaining fast inference speeds. However, anomaly classification-distinguishing different types of anomalies-remains largely unexplored despite its critical importance in real-world inspection tasks. To address this gap, we propose VELM, a novel LLM-based pipeline for anomaly classification. Given the critical importance of inference speed, we first apply an unsupervised anomaly detection method as a vision expert to assess the normality of an observation. If an anomaly is detected, the LLM then classifies its type. A key challenge in developing and evaluating anomaly classification models is the lack of precise annotations of anomaly classes in existing datasets. To address this limitation, we introduce MVTec-AC and VisA-AC, refined versions of the widely used MVTec-AD and VisA datasets, which include accurate anomaly class labels for rigorous evaluation. Our approach achieves a state-of-the-art anomaly classification accuracy of 80.4% on MVTec-AD, exceeding the prior baselines by 5%, and 84% on MVTec-AC, demonstrating the effectiveness of VELM in understanding and categorizing anomalies. We hope our methodology and benchmark inspire further research in anomaly classification, helping bridge the gap between detection and comprehensive anomaly characterization.",
        "arxiv_id": "2505.02626",
        "ARXIVID": "2505.02626",
        "COMMENT": "Matches criterion 2 as it proposes a multi-modal large language model (VELM) for anomaly classification.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2505.02766": {
        "authors": [
            "Nam H. Le",
            "Patrick Erikson",
            "Yanbo Zhang",
            "Michael Levin",
            "Josh Bongard"
        ],
        "title": "Giving Simulated Cells a Voice: Evolving Prompt-to-Intervention Models for Cellular Control",
        "abstract": "arXiv:2505.02766v1 Announce Type: new  Abstract: Guiding biological systems toward desired states, such as morphogenetic outcomes, remains a fundamental challenge with far-reaching implications for medicine and synthetic biology. While large language models (LLMs) have enabled natural language as an interface for interpretable control in AI systems, their use as mediators for steering biological or cellular dynamics remains largely unexplored.   In this work, we present a functional pipeline that translates natural language prompts into spatial vector fields capable of directing simulated cellular collectives. Our approach combines a large language model with an evolvable neural controller (Prompt-to-Intervention, or P2I), optimized via evolutionary strategies to generate behaviors such as clustering or scattering in a simulated 2D environment.   We demonstrate that even with constrained vocabulary and simplified cell models, evolved P2I networks can successfully align cellular dynamics with user-defined goals expressed in plain language. This work offers a complete loop from language input to simulated bioelectric-like intervention to behavioral output, providing a foundation for future systems capable of natural language-driven cellular control.",
        "arxiv_id": "2505.02766",
        "ARXIVID": "2505.02766",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for spatial understanding and control in simulated cellular systems using natural language prompts.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.01950": {
        "authors": [
            "Dong Xing",
            "Xianxun Zhu",
            "Wei Zhou",
            "Qika Lin",
            "Hang Yang",
            "Yuqing Wang"
        ],
        "title": "Segment Any RGB-Thermal Model with Language-aided Distillation",
        "abstract": "arXiv:2505.01950v1 Announce Type: new  Abstract: The recent Segment Anything Model (SAM) demonstrates strong instance segmentation performance across various downstream tasks. However, SAM is trained solely on RGB data, limiting its direct applicability to RGB-thermal (RGB-T) semantic segmentation. Given that RGB-T provides a robust solution for scene understanding in adverse weather and lighting conditions, such as low light and overexposure, we propose a novel framework, SARTM, which customizes the powerful SAM for RGB-T semantic segmentation. Our key idea is to unleash the potential of SAM while introduce semantic understanding modules for RGB-T data pairs. Specifically, our framework first involves fine tuning the original SAM by adding extra LoRA layers, aiming at preserving SAM's strong generalization and segmentation capabilities for downstream tasks. Secondly, we introduce language information as guidance for training our SARTM. To address cross-modal inconsistencies, we introduce a Cross-Modal Knowledge Distillation(CMKD) module that effectively achieves modality adaptation while maintaining its generalization capabilities. This semantic module enables the minimization of modality gaps and alleviates semantic ambiguity, facilitating the combination of any modality under any visual conditions. Furthermore, we enhance the segmentation performance by adjusting the segmentation head of SAM and incorporating an auxiliary semantic segmentation head, which integrates multi-scale features for effective fusion. Extensive experiments are conducted across three multi-modal RGBT semantic segmentation benchmarks: MFNET, PST900, and FMB. Both quantitative and qualitative results consistently demonstrate that the proposed SARTM significantly outperforms state-of-the-art approaches across a variety of conditions.",
        "arxiv_id": "2505.01950",
        "ARXIVID": "2505.01950",
        "COMMENT": "Matches criterion 4. Proposes a framework (SARTM) for RGB-T semantic segmentation using SAM and cross-modal knowledge distillation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.01578": {
        "authors": [
            "Gabriel Sarch",
            "Balasaravanan Thoravi Kumaravel",
            "Sahithya Ravi",
            "Vibhav Vineet",
            "Andrew D. Wilson"
        ],
        "title": "Grounding Task Assistance with Multimodal Cues from a Single Demonstration",
        "abstract": "arXiv:2505.01578v1 Announce Type: new  Abstract: A person's demonstration often serves as a key reference for others learning the same task. However, RGB video, the dominant medium for representing these demonstrations, often fails to capture fine-grained contextual cues such as intent, safety-critical environmental factors, and subtle preferences embedded in human behavior. This sensory gap fundamentally limits the ability of Vision Language Models (VLMs) to reason about why actions occur and how they should adapt to individual users. To address this, we introduce MICA (Multimodal Interactive Contextualized Assistance), a framework that improves conversational agents for task assistance by integrating eye gaze and speech cues. MICA segments demonstrations into meaningful sub-tasks and extracts keyframes and captions that capture fine-grained intent and user-specific cues, enabling richer contextual grounding for visual question answering. Evaluations on questions derived from real-time chat-assisted task replication show that multimodal cues significantly improve response quality over frame-based retrieval. Notably, gaze cues alone achieves 93% of speech performance, and their combination yields the highest accuracy. Task type determines the effectiveness of implicit (gaze) vs. explicit (speech) cues, underscoring the need for adaptable multimodal models. These results highlight the limitations of frame-based context and demonstrate the value of multimodal signals for real-world AI task assistance.",
        "arxiv_id": "2505.01578",
        "ARXIVID": "2505.01578",
        "COMMENT": "Matches criterion 3. Introduces a novel multimodal framework (MICA) for task assistance using gaze and speech cues, focusing on embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.02831": {
        "authors": [
            "Dengyang Jiang",
            "Mengmeng Wang",
            "Liuzhuozheng Li",
            "Lei Zhang",
            "Haoyu Wang",
            "Wei Wei",
            "Guang Dai",
            "Yanning Zhang",
            "Jingdong Wang"
        ],
        "title": "No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves",
        "abstract": "arXiv:2505.02831v1 Announce Type: new  Abstract: Recent studies have demonstrated that learning a meaningful internal representation can both accelerate generative training and enhance generation quality of the diffusion transformers. However, existing approaches necessitate to either introduce an additional and complex representation training framework or rely on a large-scale, pre-trained representation foundation model to provide representation guidance during the original generative training process. In this study, we posit that the unique discriminative process inherent to diffusion transformers enables them to offer such guidance without requiring external representation components. We therefore propose Self-Representation A}lignment (SRA), a simple yet straightforward method that obtain representation guidance through a self-distillation manner. Specifically, SRA aligns the output latent representation of the diffusion transformer in earlier layer with higher noise to that in later layer with lower noise to progressively enhance the overall representation learning during only generative training process. Experimental results indicate that applying SRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA not only significantly outperforms approaches relying on auxiliary, complex representation training frameworks but also achieves performance comparable to methods that heavily dependent on powerful external representation priors.",
        "arxiv_id": "2505.02831",
        "ARXIVID": "2505.02831",
        "COMMENT": "Matches criterion 4 as it explores diffusion transformers and their self-representation alignment for improved generative modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2505.01729": {
        "authors": [
            "Bu Jin",
            "Weize Li",
            "Baihan Yang",
            "Zhenxin Zhu",
            "Junpeng Jiang",
            "Huan-ang Gao",
            "Haiyang Sun",
            "Kun Zhan",
            "Hengtong Hu",
            "Xueyang Zhang",
            "Peng Jia",
            "Hao Zhao"
        ],
        "title": "PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth",
        "abstract": "arXiv:2505.01729v1 Announce Type: new  Abstract: Recent advancements in autonomous driving (AD) systems have highlighted the potential of world models in achieving robust and generalizable performance across both ordinary and challenging driving conditions. However, a key challenge remains: precise and flexible camera pose control, which is crucial for accurate viewpoint transformation and realistic simulation of scene dynamics. In this paper, we introduce PosePilot, a lightweight yet powerful framework that significantly enhances camera pose controllability in generative world models. Drawing inspiration from self-supervised depth estimation, PosePilot leverages structure-from-motion principles to establish a tight coupling between camera pose and video generation. Specifically, we incorporate self-supervised depth and pose readouts, allowing the model to infer depth and relative camera motion directly from video sequences. These outputs drive pose-aware frame warping, guided by a photometric warping loss that enforces geometric consistency across synthesized frames. To further refine camera pose estimation, we introduce a reverse warping step and a pose regression loss, improving viewpoint precision and adaptability. Extensive experiments on autonomous driving and general-domain video datasets demonstrate that PosePilot significantly enhances structural understanding and motion reasoning in both diffusion-based and auto-regressive world models. By steering camera pose with self-supervised depth, PosePilot sets a new benchmark for pose controllability, enabling physically consistent, reliable viewpoint synthesis in generative world models.",
        "arxiv_id": "2505.01729",
        "ARXIVID": "2505.01729",
        "COMMENT": "This paper introduces PosePilot, a framework for camera pose control in generative world models, which aligns with criterion 1 on spatial understanding and criterion 3 on embodied AI with new methods.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2505.02703": {
        "authors": [
            "Zibo Xu",
            "Qiang Li",
            "Weizhi Nie",
            "Weijie Wang",
            "Anan Liu"
        ],
        "title": "Structure Causal Models and LLMs Integration in Medical Visual Question Answering",
        "abstract": "arXiv:2505.02703v1 Announce Type: new  Abstract: Medical Visual Question Answering (MedVQA) aims to answer medical questions according to medical images. However, the complexity of medical data leads to confounders that are difficult to observe, so bias between images and questions is inevitable. Such cross-modal bias makes it challenging to infer medically meaningful answers. In this work, we propose a causal inference framework for the MedVQA task, which effectively eliminates the relative confounding effect between the image and the question to ensure the precision of the question-answering (QA) session. We are the first to introduce a novel causal graph structure that represents the interaction between visual and textual elements, explicitly capturing how different questions influence visual features. During optimization, we apply the mutual information to discover spurious correlations and propose a multi-variable resampling front-door adjustment method to eliminate the relative confounding effect, which aims to align features based on their true causal relevance to the question-answering task. In addition, we also introduce a prompt strategy that combines multiple prompt forms to improve the model's ability to understand complex medical data and answer accurately. Extensive experiments on three MedVQA datasets demonstrate that 1) our method significantly improves the accuracy of MedVQA, and 2) our method achieves true causal correlations in the face of complex medical data.",
        "arxiv_id": "2505.02703",
        "ARXIVID": "2505.02703",
        "COMMENT": "Matches criterion 2 as it integrates large language models (LLMs) with causal models for medical visual question answering, which is a novel application.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.02746": {
        "authors": [
            "Simon Ging",
            "Sebastian Walter",
            "Jelena Bratuli\\'c",
            "Johannes Dienert",
            "Hannah Bast",
            "Thomas Brox"
        ],
        "title": "Using Knowledge Graphs to harvest datasets for efficient CLIP model training",
        "abstract": "arXiv:2505.02746v1 Announce Type: new  Abstract: Training high-quality CLIP models typically requires enormous datasets, which limits the development of domain-specific models -- especially in areas that even the largest CLIP models do not cover well -- and drives up training costs. This poses challenges for scientific research that needs fine-grained control over the training procedure of CLIP models. In this work, we show that by employing smart web search strategies enhanced with knowledge graphs, a robust CLIP model can be trained from scratch with considerably less data. Specifically, we demonstrate that an expert foundation model for living organisms can be built using just 10M images. Moreover, we introduce EntityNet, a dataset comprising 33M images paired with 46M text descriptions, which enables the training of a generic CLIP model in significantly reduced time.",
        "arxiv_id": "2505.02746",
        "ARXIVID": "2505.02746",
        "COMMENT": "Matches criterion 4 as it discusses a vision foundation model (CLIP) and its applications, specifically in training with reduced data using knowledge graphs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.01571": {
        "authors": [
            "Stefanos Gkikas",
            "Raul Fernandez Rojas",
            "Manolis Tsiknakis"
        ],
        "title": "PainFormer: a Vision Foundation Model for Automatic Pain Assessment",
        "abstract": "arXiv:2505.01571v1 Announce Type: new  Abstract: Pain is a manifold condition that impacts a significant percentage of the population. Accurate and reliable pain evaluation for the people suffering is crucial to developing effective and advanced pain management protocols. Automatic pain assessment systems provide continuous monitoring and support decision-making processes, ultimately aiming to alleviate distress and prevent functionality decline. This study introduces PainFormer, a vision foundation model based on multi-task learning principles trained simultaneously on 14 tasks/datasets with a total of 10.9 million samples. Functioning as an embedding extractor for various input modalities, the foundation model provides feature representations to the Embedding-Mixer, a transformer-based module that performs the final pain assessment. Extensive experiments employing behavioral modalities-including RGB, synthetic thermal, and estimated depth videos-and physiological modalities such as ECG, EMG, GSR, and fNIRS revealed that PainFormer effectively extracts high-quality embeddings from diverse input modalities. The proposed framework is evaluated on two pain datasets, BioVid and AI4Pain, and directly compared to 73 different methodologies documented in the literature. Experiments conducted in unimodal and multimodal settings demonstrate state-of-the-art performances across modalities and pave the way toward general-purpose models for automatic pain assessment.",
        "arxiv_id": "2505.01571",
        "ARXIVID": "2505.01571",
        "COMMENT": "Matches criterion 4 as it introduces a vision foundation model (PainFormer) and demonstrates its application in pain assessment.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.02393": {
        "authors": [
            "Sungheon Jeong",
            "Jihong Park",
            "Mohsen Imani"
        ],
        "title": "Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection",
        "abstract": "arXiv:2505.02393v1 Announce Type: new  Abstract: Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that synthesizes event representations directly from RGB videos and fuses them with image features through a principled, uncertainty-aware process. The system (i) models heavy-tailed sensor noise with a Student`s-t likelihood, deriving value-level inverse-variance weights via a Laplace approximation; (ii) applies Kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new state of the art across multiple real-world anomaly detection benchmarks. These findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in RGB frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. Code and models are available at https://github.com/EavnJeong/IEF-VAD.",
        "arxiv_id": "2505.02393",
        "ARXIVID": "2505.02393",
        "COMMENT": "Matches criterion 3. Proposes a novel multimodal fusion framework for video anomaly detection, emphasizing synthetic event representations.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.01851": {
        "authors": [
            "Chaomeng Chen",
            "Zitong Yu",
            "Junhao Dong",
            "Sen Su",
            "Linlin Shen",
            "Shutao Xia",
            "Xiaochun Cao"
        ],
        "title": "Mitigating Group-Level Fairness Disparities in Federated Visual Language Models",
        "abstract": "arXiv:2505.01851v1 Announce Type: new  Abstract: Visual language models (VLMs) have shown remarkable capabilities in multimodal tasks but face challenges in maintaining fairness across demographic groups, particularly when deployed in federated learning (FL) environments. This paper addresses the critical issue of group fairness in federated VLMs by introducing FVL-FP, a novel framework that combines FL with fair prompt tuning techniques. We focus on mitigating demographic biases while preserving model performance through three innovative components: (1) Cross-Layer Demographic Fair Prompting (CDFP), which adjusts potentially biased embeddings through counterfactual regularization; (2) Demographic Subspace Orthogonal Projection (DSOP), which removes demographic bias in image representations by mapping fair prompt text to group subspaces; and (3) Fair-aware Prompt Fusion (FPF), which dynamically balances client contributions based on both performance and fairness metrics. Extensive evaluations across four benchmark datasets demonstrate that our approach reduces demographic disparity by an average of 45\\% compared to standard FL approaches, while maintaining task performance within 6\\% of state-of-the-art results. FVL-FP effectively addresses the challenges of non-IID data distributions in federated settings and introduces minimal computational overhead while providing significant fairness benefits. Our work presents a parameter-efficient solution to the critical challenge of ensuring equitable performance across demographic groups in privacy-preserving multimodal systems.",
        "arxiv_id": "2505.01851",
        "ARXIVID": "2505.01851",
        "COMMENT": "Matches criterion 2 as it focuses on visual language models (VLMs) and introduces a novel framework for fairness in federated learning settings.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2505.02448": {
        "authors": [
            "Chaohua Li",
            "Enhao Zhang",
            "Chuanxing Geng",
            "Songcan Chen"
        ],
        "title": "Recent Advances in Out-of-Distribution Detection with CLIP-Like Models: A Survey",
        "abstract": "arXiv:2505.02448v1 Announce Type: new  Abstract: Out-of-distribution detection (OOD) is a pivotal task for real-world applications that trains models to identify samples that are distributionally different from the in-distribution (ID) data during testing. Recent advances in AI, particularly Vision-Language Models (VLMs) like CLIP, have revolutionized OOD detection by shifting from traditional unimodal image detectors to multimodal image-text detectors. This shift has inspired extensive research; however, existing categorization schemes (e.g., few- or zero-shot types) still rely solely on the availability of ID images, adhering to a unimodal paradigm. To better align with CLIP's cross-modal nature, we propose a new categorization framework rooted in both image and text modalities. Specifically, we categorize existing methods based on how visual and textual information of OOD data is utilized within image + text modalities, and further divide them into four groups: OOD Images (i.e., outliers) Seen or Unseen, and OOD Texts (i.e., learnable vectors or class names) Known or Unknown, across two training strategies (i.e., train-free or training-required). More importantly, we discuss open problems in CLIP-like OOD detection and highlight promising directions for future research, including cross-domain integration, practical applications, and theoretical understanding.",
        "arxiv_id": "2505.02448",
        "ARXIVID": "2505.02448",
        "COMMENT": "Matches criterion 4. Surveys advances in out-of-distribution detection using vision-language models like CLIP.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2505.02331": {
        "authors": [
            "Hao Cheng",
            "Zhiwei Zhao",
            "Yichao He",
            "Zhenzhen Hu",
            "Jia Li",
            "Meng Wang",
            "Richang Hong"
        ],
        "title": "VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection",
        "abstract": "arXiv:2505.02331v1 Announce Type: new  Abstract: Audiovisual emotion recognition (AVER) aims to infer human emotions from nonverbal visual-audio (VA) cues, offering modality-complementary and language-agnostic advantages. However, AVER remains challenging due to the inherent ambiguity of emotional expressions, cross-modal expressive disparities, and the scarcity of reliably annotated data. Recent self-supervised AVER approaches have introduced strong multimodal representations, yet they predominantly rely on modality-specific encoders and coarse content-level alignment, limiting fine-grained emotional semantic modeling. To address these issues, we propose VAEmo, an efficient two-stage framework for emotion-centric joint VA representation learning with external knowledge injection. In Stage 1, a unified and lightweight representation network is pre-trained on large-scale speaker-centric VA corpora via masked reconstruction and contrastive objectives, mitigating the modality gap and learning expressive, complementary representations without emotion labels. In Stage 2, multimodal large language models automatically generate detailed affective descriptions according to our well-designed chain-of-thought prompting for only a small subset of VA samples; these rich textual semantics are then injected by aligning their corresponding embeddings with VA representations through dual-path contrastive learning, further bridging the emotion gap. Extensive experiments on multiple downstream AVER benchmarks show that VAEmo achieves state-of-the-art performance with a compact design, highlighting the benefit of unified cross-modal encoding and emotion-aware semantic guidance for efficient, generalizable VA emotion representations.",
        "arxiv_id": "2505.02331",
        "ARXIVID": "2505.02331",
        "COMMENT": "Matches criterion 2. Proposes a new framework for visual-audio emotion recognition with multimodal large language models and knowledge injection.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2505.01583": {
        "authors": [
            "Jen-Hao Cheng",
            "Vivian Wang",
            "Huayu Wang",
            "Huapeng Zhou",
            "Yi-Hao Peng",
            "Hou-I Liu",
            "Hsiang-Wei Huang",
            "Kuang-Ming Chen",
            "Cheng-Yen Yang",
            "Wenhao Chai",
            "Yi-Ling Chen",
            "Vibhav Vineet",
            "Qin Cai",
            "Jenq-Neng Hwang"
        ],
        "title": "TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action",
        "abstract": "arXiv:2505.01583v1 Announce Type: new  Abstract: Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event Masked Prediction and Understanding for Reasoning in Action), a two-stage training framework that enhances video temporal understanding. TEMPURA first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. TEMPURA then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. We train TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training instances and 500K videos with temporally aligned event descriptions and structured reasoning steps. Experiments on temporal grounding and highlight detection benchmarks demonstrate that TEMPURA outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding.",
        "arxiv_id": "2505.01583",
        "ARXIVID": "2505.01583",
        "COMMENT": "This paper introduces TEMPURA, a framework for temporal event understanding in videos, which aligns with criterion 2 on vision-language models with surprising empirical results.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2505.02109": {
        "authors": [
            "Yingkai Zhang",
            "Zeqiang Lai",
            "Tao Zhang",
            "Ying Fu",
            "Chenghu Zhou"
        ],
        "title": "Unaligned RGB Guided Hyperspectral Image Super-Resolution with Spatial-Spectral Concordance",
        "abstract": "arXiv:2505.02109v1 Announce Type: new  Abstract: Hyperspectral images super-resolution aims to improve the spatial resolution, yet its performance is often limited at high-resolution ratios. The recent adoption of high-resolution reference images for super-resolution is driven by the poor spatial detail found in low-resolution HSIs, presenting it as a favorable method. However, these approaches cannot effectively utilize information from the reference image, due to the inaccuracy of alignment and its inadequate interaction between alignment and fusion modules. In this paper, we introduce a Spatial-Spectral Concordance Hyperspectral Super-Resolution (SSC-HSR) framework for unaligned reference RGB guided HSI SR to address the issues of inaccurate alignment and poor interactivity of the previous approaches. Specifically, to ensure spatial concordance, i.e., align images more accurately across resolutions and refine textures, we construct a Two-Stage Image Alignment with a synthetic generation pipeline in the image alignment module, where the fine-tuned optical flow model can produce a more accurate optical flow in the first stage and warp model can refine damaged textures in the second stage. To enhance the interaction between alignment and fusion modules and ensure spectral concordance during reconstruction, we propose a Feature Aggregation module and an Attention Fusion module. In the feature aggregation module, we introduce an Iterative Deformable Feature Aggregation block to achieve significant feature matching and texture aggregation with the fusion multi-scale results guidance, iteratively generating learnable offset. Besides, we introduce two basic spectral-wise attention blocks in the attention fusion module to model the inter-spectra interactions. Extensive experiments on three natural or remote-sensing datasets show that our method outperforms state-of-the-art approaches on both quantitative and qualitative evaluations.",
        "arxiv_id": "2505.02109",
        "ARXIVID": "2505.02109",
        "COMMENT": "This paper aligns with criterion 4 as it discusses hyperspectral image super-resolution with spatial-spectral concordance, which is related to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.01743": {
        "authors": [
            "Siyang Jiang",
            "Bufang Yang",
            "Lilin Xu",
            "Mu Yuan",
            "Yeerzhati Abudunuer",
            "Kaiwei Liu",
            "Liekang Zeng",
            "Hongkai Chen",
            "Zhenyu Yan",
            "Xiaofan Jiang",
            "Guoliang Xing"
        ],
        "title": "An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding",
        "abstract": "arXiv:2505.01743v1 Announce Type: new  Abstract: The rapid advancements in Large Vision Language Models (LVLMs) offer the potential to surpass conventional labeling by generating richer, more detailed descriptions of on-device human behavior understanding (HBU) in low-resolution vision systems, such as depth, thermal, and infrared. However, existing large vision language model (LVLM) approaches are unable to understand low-resolution data well as they are primarily designed for high-resolution data, such as RGB images. A quick fixing approach is to caption a large amount of low-resolution data, but it requires a significant amount of labor-intensive annotation efforts. In this paper, we propose a novel, labor-saving system, Llambda, designed to support low-resolution HBU. The core idea is to leverage limited labeled data and a large amount of unlabeled data to guide LLMs in generating informative captions, which can be combined with raw data to effectively fine-tune LVLM models for understanding low-resolution videos in HBU. First, we propose a Contrastive-Oriented Data Labeler, which can capture behavior-relevant information from long, low-resolution videos and generate high-quality pseudo labels for unlabeled data via contrastive learning. Second, we propose a Physical-Knowledge Guided Captioner, which utilizes spatial and temporal consistency checks to mitigate errors in pseudo labels. Therefore, it can improve LLMs' understanding of sequential data and then generate high-quality video captions. Finally, to ensure on-device deployability, we employ LoRA-based efficient fine-tuning to adapt LVLMs for low-resolution data. We evaluate Llambda using a region-scale real-world testbed and three distinct low-resolution datasets, and the experiments show that Llambda outperforms several state-of-the-art LVLM systems up to $40.03\\%$ on average Bert-Score.",
        "arxiv_id": "2505.01743",
        "ARXIVID": "2505.01743",
        "COMMENT": "This paper aligns with criterion 2 as it discusses adapting large vision-language models (LVLMs) for low-resolution human behavior understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.02005": {
        "authors": [
            "Zhenxing Mi",
            "Ping Yin",
            "Xue Xiao",
            "Dan Xu"
        ],
        "title": "Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields",
        "abstract": "arXiv:2505.02005v1 Announce Type: new  Abstract: Recent NeRF methods on large-scale scenes have underlined the importance of scene decomposition for scalable NeRFs. Although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within a unified framework. It is a highly scalable NeRF that learns heterogeneous decomposition and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end manner. In our framework, a gating network learns to decomposes scenes and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts, by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. We incorporate a hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets and a new dataset with very large-scale scenes ($>6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to Switch-NeRF. Codes will be released in https://github.com/MiZhenxing/Switch-NeRF.",
        "arxiv_id": "2505.02005",
        "ARXIVID": "2505.02005",
        "COMMENT": "This paper aligns with criterion 3 as it introduces a novel method for large-scale scene modeling in NeRFs, which could be relevant to embodied AI benchmarks or simulators.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.02413": {
        "authors": [
            "Baoxia Du",
            "Hongyang Du",
            "Dusit Niyato",
            "Ruidong Li"
        ],
        "title": "Task-Oriented Semantic Communication in Large Multimodal Models-based Vehicle Networks",
        "abstract": "arXiv:2505.02413v1 Announce Type: new  Abstract: Task-oriented semantic communication has emerged as a fundamental approach for enhancing performance in various communication scenarios. While recent advances in Generative Artificial Intelligence (GenAI), such as Large Language Models (LLMs), have been applied to semantic communication designs, the potential of Large Multimodal Models (LMMs) remains largely unexplored. In this paper, we investigate an LMM-based vehicle AI assistant using a Large Language and Vision Assistant (LLaVA) and propose a task-oriented semantic communication framework to facilitate efficient interaction between users and cloud servers. To reduce computational demands and shorten response time, we optimize LLaVA's image slicing to selectively focus on areas of utmost interest to users. Additionally, we assess the importance of image patches by combining objective and subjective user attention, adjusting energy usage for transmitting semantic information. This strategy optimizes resource utilization, ensuring precise transmission of critical information. We construct a Visual Question Answering (VQA) dataset for traffic scenarios to evaluate effectiveness. Experimental results show that our semantic communication framework significantly increases accuracy in answering questions under the same channel conditions, performing particularly well in environments with poor Signal-to-Noise Ratios (SNR). Accuracy can be improved by 13.4% at an SNR of 12dB and 33.1% at 10dB, respectively.",
        "arxiv_id": "2505.02413",
        "ARXIVID": "2505.02413",
        "COMMENT": "This paper explores task-oriented semantic communication in vehicle networks using large multimodal models, which aligns with criterion 2 on multi-modal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.02654": {
        "authors": [
            "Clara Tomasini",
            "Luis Riazuelo",
            "Ana C. Murillo"
        ],
        "title": "Sim2Real in endoscopy segmentation with a novel structure aware image translation",
        "abstract": "arXiv:2505.02654v1 Announce Type: new  Abstract: Automatic segmentation of anatomical landmarks in endoscopic images can provide assistance to doctors and surgeons for diagnosis, treatments or medical training. However, obtaining the annotations required to train commonly used supervised learning methods is a tedious and difficult task, in particular for real images. While ground truth annotations are easier to obtain for synthetic data, models trained on such data often do not generalize well to real data. Generative approaches can add realistic texture to it, but face difficulties to maintain the structure of the original scene. The main contribution in this work is a novel image translation model that adds realistic texture to simulated endoscopic images while keeping the key scene layout information. Our approach produces realistic images in different endoscopy scenarios. We demonstrate these images can effectively be used to successfully train a model for a challenging end task without any real labeled data. In particular, we demonstrate our approach for the task of fold segmentation in colonoscopy images. Folds are key anatomical landmarks that can occlude parts of the colon mucosa and possible polyps. Our approach generates realistic images maintaining the shape and location of the original folds, after the image-style-translation, better than existing methods. We run experiments both on a novel simulated dataset for fold segmentation, and real data from the EndoMapper (EM) dataset. All our new generated data and new EM metadata is being released to facilitate further research, as no public benchmark is currently available for the task of fold segmentation.",
        "arxiv_id": "2505.02654",
        "ARXIVID": "2505.02654",
        "COMMENT": "This paper introduces a novel image translation model for sim2real in endoscopy segmentation, which aligns with criterion 3 on embodied AI with new methods and benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.01430": {
        "authors": [
            "Muna Numan Said",
            "Aarib Zaidi",
            "Rabia Usman",
            "Sonia Okon",
            "Praneeth Medepalli",
            "Kevin Zhu",
            "Vasu Sharma",
            "Sean O'Brien"
        ],
        "title": "Deconstructing Bias: A Multifaceted Framework for Diagnosing Cultural and Compositional Inequities in Text-to-Image Generative Models",
        "abstract": "arXiv:2505.01430v1 Announce Type: new  Abstract: The transformative potential of text-to-image (T2I) models hinges on their ability to synthesize culturally diverse, photorealistic images from textual prompts. However, these models often perpetuate cultural biases embedded within their training data, leading to systemic misrepresentations. This paper benchmarks the Component Inclusion Score (CIS), a metric designed to evaluate the fidelity of image generation across cultural contexts. Through extensive analysis involving 2,400 images, we quantify biases in terms of compositional fragility and contextual misalignment, revealing significant performance gaps between Western and non-Western cultural prompts. Our findings underscore the impact of data imbalance, attention entropy, and embedding superposition on model fairness. By benchmarking models like Stable Diffusion with CIS, we provide insights into architectural and data-centric interventions for enhancing cultural inclusivity in AI-generated imagery. This work advances the field by offering a comprehensive tool for diagnosing and mitigating biases in T2I generation, advocating for more equitable AI systems.",
        "arxiv_id": "2505.01430",
        "ARXIVID": "2505.01430",
        "COMMENT": "This paper addresses biases in text-to-image generative models, which aligns with criterion 4 on vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.02735": {
        "authors": [
            "Zhouliang Yu",
            "Ruotian Peng",
            "Keyi Ding",
            "Yizhe Li",
            "Zhongyuan Peng",
            "Minghao Liu",
            "Yifan Zhang",
            "Zheng Yuan",
            "Huajian Xin",
            "Wenhao Huang",
            "Yandong Wen",
            "Ge Zhang",
            "Weiyang Liu"
        ],
        "title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models",
        "abstract": "arXiv:2505.02735v1 Announce Type: new  Abstract: Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olympiad challenges to undergraduate-level theorems across diverse domains (e.g., algebra, applied mathematics, calculus, number theory, and discrete mathematics). To mitigate the inefficiency of manual formalization, we introduce a novel human-in-the-loop autoformalization pipeline that integrates: (1) specialized large language models (LLMs) for statement autoformalization, (2) multi-LLM semantic verification, and (3) negation-based disproof filtering strategies using off-the-shelf LLM-based provers. This approach reduces expert annotation costs by retaining 72.09% of statements before manual verification while ensuring fidelity to the original natural-language problems. Our evaluation of state-of-the-art LLM-based theorem provers reveals significant limitations: even the strongest models achieve only 16.46% success rate under practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling in algebra but failing in calculus) and over-reliance on simplified automation tactics. Notably, we identify a counterintuitive inverse relationship between natural-language solution guidance and proof success in chain-of-thought reasoning scenarios, suggesting that human-written informal reasoning introduces noise rather than clarity in the formal reasoning settings. We believe that FormalMATH provides a robust benchmark for benchmarking formal mathematical reasoning.",
        "arxiv_id": "2505.02735",
        "ARXIVID": "2505.02735",
        "COMMENT": "Does not match any specific criteria. Focuses on formal mathematical reasoning benchmarks for large language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.02118": {
        "authors": [
            "Wei Liu",
            "Zhongyu Niu",
            "Lang Gao",
            "Zhiying Deng",
            "Jun Wang",
            "Haozhao Wang",
            "Ruixuan Li"
        ],
        "title": "Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets",
        "abstract": "arXiv:2505.02118v1 Announce Type: new  Abstract: This study investigates the self-rationalization framework constructed with a cooperative game, where a generator initially extracts the most informative segment from raw input, and a subsequent predictor utilizes the selected subset for its input. The generator and predictor are trained collaboratively to maximize prediction accuracy. In this paper, we first uncover a potential caveat: such a cooperative game could unintentionally introduce a sampling bias during rationale extraction. Specifically, the generator might inadvertently create an incorrect correlation between the selected rationale candidate and the label, even when they are semantically unrelated in the original dataset. Subsequently, we elucidate the origins of this bias using both detailed theoretical analysis and empirical evidence. Our findings suggest a direction for inspecting these correlations through attacks, based on which we further introduce an instruction to prevent the predictor from learning the correlations. Through experiments on six text classification datasets and two graph classification datasets using three network architectures (GRUs, BERT, and GCN), we show that our method not only significantly outperforms recent rationalization methods, but also achieves comparable or even better results than a representative LLM (llama3.1-8b-instruct).",
        "arxiv_id": "2505.02118",
        "ARXIVID": "2505.02118",
        "COMMENT": "Does not match any specific criterion but is related to rationalization and spurious correlations, which are tangentially relevant to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.01699": {
        "authors": [
            "Yifan Liu",
            "Ruichen Yao",
            "Yaokun Liu",
            "Ruohan Zong",
            "Zelin Li",
            "Yang Zhang",
            "Dong Wang"
        ],
        "title": "Component-Based Fairness in Face Attribute Classification with Bayesian Network-informed Meta Learning",
        "abstract": "arXiv:2505.01699v1 Announce Type: new  Abstract: The widespread integration of face recognition technologies into various applications (e.g., access control and personalized advertising) necessitates a critical emphasis on fairness. While previous efforts have focused on demographic fairness, the fairness of individual biological face components remains unexplored. In this paper, we focus on face component fairness, a fairness notion defined by biological face features. To our best knowledge, our work is the first work to mitigate bias of face attribute prediction at the biological feature level. In this work, we identify two key challenges in optimizing face component fairness: attribute label scarcity and attribute inter-dependencies, both of which limit the effectiveness of bias mitigation from previous approaches. To address these issues, we propose \\textbf{B}ayesian \\textbf{N}etwork-informed \\textbf{M}eta \\textbf{R}eweighting (BNMR), which incorporates a Bayesian Network calibrator to guide an adaptive meta-learning-based sample reweighting process. During the training process of our approach, the Bayesian Network calibrator dynamically tracks model bias and encodes prior probabilities for face component attributes to overcome the above challenges. To demonstrate the efficacy of our approach, we conduct extensive experiments on a large-scale real-world human face dataset. Our results show that BNMR is able to consistently outperform recent face bias mitigation baselines. Moreover, our results suggest a positive impact of face component fairness on the commonly considered demographic fairness (e.g., \\textit{gender}). Our findings pave the way for new research avenues on face component fairness, suggesting that face component fairness could serve as a potential surrogate objective for demographic fairness. The code for our work is publicly available~\\footnote{https://github.com/yliuaa/BNMR-FairCompFace.git}.",
        "arxiv_id": "2505.01699",
        "ARXIVID": "2505.01699",
        "COMMENT": "Does not match any specific criterion but is related to fairness in machine learning, which is tangentially relevant to the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.02824": {
        "authors": [
            "Kuofeng Gao",
            "Yufei Zhu",
            "Yiming Li",
            "Jiawang Bai",
            "Yong Yang",
            "Zhifeng Li",
            "Shu-Tao Xia"
        ],
        "title": "Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models",
        "abstract": "arXiv:2505.02824v1 Announce Type: new  Abstract: Text-to-image (T2I) diffusion models have rapidly advanced, enabling high-quality image generation conditioned on textual prompts. However, the growing trend of fine-tuning pre-trained models for personalization raises serious concerns about unauthorized dataset usage. To combat this, dataset ownership verification (DOV) has emerged as a solution, embedding watermarks into the fine-tuning datasets using backdoor techniques. These watermarks remain inactive under benign samples but produce owner-specified outputs when triggered. Despite the promise of DOV for T2I diffusion models, its robustness against copyright evasion attacks (CEA) remains unexplored. In this paper, we explore how attackers can bypass these mechanisms through CEA, allowing models to circumvent watermarks even when trained on watermarked datasets. We propose the first copyright evasion attack (i.e., CEAT2I) specifically designed to undermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three stages: watermarked sample detection, trigger identification, and efficient watermark mitigation. A key insight driving our approach is that T2I models exhibit faster convergence on watermarked samples during the fine-tuning, evident through intermediate feature deviation. Leveraging this, CEAT2I can reliably detect the watermarked samples. Then, we iteratively ablate tokens from the prompts of detected watermarked samples and monitor shifts in intermediate features to pinpoint the exact trigger tokens. Finally, we adopt a closed-form concept erasure method to remove the injected watermark. Extensive experiments show that our CEAT2I effectively evades DOV mechanisms while preserving model performance.",
        "arxiv_id": "2505.02824",
        "ARXIVID": "2505.02824",
        "COMMENT": "Does not match any specific criterion but is related to the general interest area of generative modeling and text-to-image diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.02722": {
        "authors": [
            "Junu Kim",
            "Chaeeun Shim",
            "Sungjin Park",
            "Su Yeon Lee",
            "Gee Young Suh",
            "Chae-Man Lim",
            "Seong Jin Choi",
            "Song Mi Moon",
            "Kyoung-Ho Song",
            "Eu Suk Kim",
            "Hong Bin Kim",
            "Sejoong Kim",
            "Chami Im",
            "Dong-Wan Kang",
            "Yong Soo Kim",
            "Hee-Joon Bae",
            "Sung Yoon Lim",
            "Han-Gil Jeong",
            "Edward Choi"
        ],
        "title": "Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry",
        "abstract": "arXiv:2505.02722v1 Announce Type: new  Abstract: Although large language models (LLMs) have demonstrated impressive reasoning capabilities across general domains, their effectiveness in real-world clinical practice remains limited. This is likely due to their insufficient exposure to real-world clinical data during training, as such data is typically not included due to privacy concerns. To address this, we propose enhancing the clinical reasoning capabilities of LLMs by leveraging real-world clinical data. We constructed reasoning-intensive questions from a nationwide sepsis registry and fine-tuned Phi-4 on these questions using reinforcement learning, resulting in C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the in-domain test set, as evidenced by both quantitative metrics and expert evaluations. Furthermore, its enhanced reasoning capabilities generalized to a sepsis dataset involving different tasks and patient cohorts, an open-ended consultations on antibiotics use task, and other diseases. Future research should focus on training LLMs with large-scale, multi-disease clinical datasets to develop more powerful, general-purpose clinical reasoning models.",
        "arxiv_id": "2505.02722",
        "ARXIVID": "2505.02722",
        "COMMENT": "Does not match any specific criterion but is related to the general interest area of large language models and their applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.01928": {
        "authors": [
            "Anushka Agarwal",
            "Muhammad Yusuf Hassan",
            "Talha Chafekar"
        ],
        "title": "GenSync: A Generalized Talking Head Framework for Audio-driven Multi-Subject Lip-Sync using 3D Gaussian Splatting",
        "abstract": "arXiv:2505.01928v1 Announce Type: new  Abstract: We introduce GenSync, a novel framework for multi-identity lip-synced video synthesis using 3D Gaussian Splatting. Unlike most existing 3D methods that require training a new model for each identity , GenSync learns a unified network that synthesizes lip-synced videos for multiple speakers. By incorporating a Disentanglement Module, our approach separates identity-specific features from audio representations, enabling efficient multi-identity video synthesis. This design reduces computational overhead and achieves 6.8x faster training compared to state-of-the-art models, while maintaining high lip-sync accuracy and visual quality.",
        "arxiv_id": "2505.01928",
        "ARXIVID": "2505.01928",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling for lip-synced video synthesis, which is tangentially relevant to general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.02130": {
        "authors": [
            "Zhong Guan",
            "Likang Wu",
            "Hongke Zhao",
            "Ming He",
            "Jianpin Fan"
        ],
        "title": "Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data",
        "abstract": "arXiv:2505.02130v1 Announce Type: new  Abstract: Attention mechanisms are critical to the success of large language models (LLMs), driving significant advancements in multiple fields. However, for graph-structured data, which requires emphasis on topological connections, they fall short compared to message-passing mechanisms on fixed links, such as those employed by Graph Neural Networks (GNNs). This raises a question: ``Does attention fail for graphs in natural language settings?'' Motivated by these observations, we embarked on an empirical study from the perspective of attention mechanisms to explore how LLMs process graph-structured data. The goal is to gain deeper insights into the attention behavior of LLMs over graph structures. We uncovered unique phenomena regarding how LLMs apply attention to graph-structured data and analyzed these findings to improve the modeling of such data by LLMs. The primary findings of our research are: 1) While LLMs can recognize graph data and capture text-node interactions, they struggle to model inter-node relationships within graph structures due to inherent architectural constraints. 2) The attention distribution of LLMs across graph nodes does not align with ideal structural patterns, indicating a failure to adapt to graph topology nuances. 3) Neither fully connected attention nor fixed connectivity is optimal; each has specific limitations in its application scenarios. Instead, intermediate-state attention windows improve LLM training performance and seamlessly transition to fully connected windows during inference. Source code: \\href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}",
        "arxiv_id": "2505.02130",
        "ARXIVID": "2505.02130",
        "COMMENT": "Does not match any specific criteria but explores attention mechanisms in LLMs for graph-structured data, which is tangentially relevant to general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.01468": {
        "authors": [
            "Filippo Betello",
            "Antonio Purificato",
            "Vittoria Vineis",
            "Gabriele Tolomei",
            "Fabrizio Silvestri"
        ],
        "title": "One Search Fits All: Pareto-Optimal Eco-Friendly Model Selection",
        "abstract": "arXiv:2505.01468v1 Announce Type: new  Abstract: The environmental impact of Artificial Intelligence (AI) is emerging as a significant global concern, particularly regarding model training. In this paper, we introduce GREEN (Guided Recommendations of Energy-Efficient Networks), a novel, inference-time approach for recommending Pareto-optimal AI model configurations that optimize validation performance and energy consumption across diverse AI domains and tasks. Our approach directly addresses the limitations of current eco-efficient neural architecture search methods, which are often restricted to specific architectures or tasks. Central to this work is EcoTaskSet, a dataset comprising training dynamics from over 1767 experiments across computer vision, natural language processing, and recommendation systems using both widely used and cutting-edge architectures. Leveraging this dataset and a prediction model, our approach demonstrates effectiveness in selecting the best model configuration based on user preferences. Experimental results show that our method successfully identifies energy-efficient configurations while ensuring competitive performance.",
        "arxiv_id": "2505.01468",
        "ARXIVID": "2505.01468",
        "COMMENT": "Does not match any specific criteria but is related to eco-efficient AI model selection, which is tangentially relevant to general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.01837": {
        "authors": [
            "Xiangru Li",
            "Wei Song",
            "Yingda Huang",
            "Wei Meng",
            "Le Chang"
        ],
        "title": "CVVNet: A Cross-Vertical-View Network for Gait Recognition",
        "abstract": "arXiv:2505.01837v1 Announce Type: new  Abstract: Gait recognition enables contact-free, long-range person identification that is robust to clothing variations and non-cooperative scenarios. While existing methods perform well in controlled indoor environments, they struggle with cross-vertical view scenarios, where surveillance angles vary significantly in elevation. Our experiments show up to 60\\% accuracy degradation in low-to-high vertical view settings due to severe deformations and self-occlusions of key anatomical features. Current CNN and self-attention-based methods fail to effectively handle these challenges, due to their reliance on single-scale convolutions or simplistic attention mechanisms that lack effective multi-frequency feature integration. To tackle this challenge, we propose CVVNet (Cross-Vertical-View Network), a frequency aggregation architecture specifically designed for robust cross-vertical-view gait recognition. CVVNet employs a High-Low Frequency Extraction module (HLFE) that adopts parallel multi-scale convolution/max-pooling path and self-attention path as high- and low-frequency mixers for effective multi-frequency feature extraction from input silhouettes. We also introduce the Dynamic Gated Aggregation (DGA) mechanism to adaptively adjust the fusion ratio of high- and low-frequency features. The integration of our core Multi-Scale Attention Gated Aggregation (MSAGA) module, HLFE and DGA enables CVVNet to effectively handle distortions from view changes, significantly improving the recognition robustness across different vertical views. Experimental results show that our CVVNet achieves state-of-the-art performance, with $8.6\\%$ improvement on DroneGait and $2\\%$ on Gait3D compared with the best existing methods.",
        "arxiv_id": "2505.01837",
        "ARXIVID": "2505.01837",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and machine learning, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.02099": {
        "authors": [
            "Zeyu Zhang",
            "Quanyu Dai",
            "Xu Chen",
            "Rui Li",
            "Zhongyang Li",
            "Zhenhua Dong"
        ],
        "title": "MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents",
        "abstract": "arXiv:2505.02099v1 Announce Type: new  Abstract: Recently, large language model based (LLM-based) agents have been widely applied across various fields. As a critical part, their memory capabilities have captured significant interest from both industrial and academic communities. Despite the proposal of many advanced memory models in recent research, however, there remains a lack of unified implementations under a general framework. To address this issue, we develop a unified and modular library for developing advanced memory models of LLM-based agents, called MemEngine. Based on our framework, we implement abundant memory models from recent research works. Additionally, our library facilitates convenient and extensible memory development, and offers user-friendly and pluggable memory usage. For benefiting our community, we have made our project publicly available at https://github.com/nuster1128/MemEngine.",
        "arxiv_id": "2505.02099",
        "ARXIVID": "2505.02099",
        "COMMENT": "Does not match any specific criteria. Focuses on a library for memory development in LLM-based agents.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.02529": {
        "authors": [
            "Aiman Farooq",
            "Azad Singh",
            "Deepak Mishra",
            "Santanu Chaudhury"
        ],
        "title": "RobSurv: Vector Quantization-Based Multi-Modal Learning for Robust Cancer Survival Prediction",
        "abstract": "arXiv:2505.02529v1 Announce Type: new  Abstract: Cancer survival prediction using multi-modal medical imaging presents a critical challenge in oncology, mainly due to the vulnerability of deep learning models to noise and protocol variations across imaging centers. Current approaches struggle to extract consistent features from heterogeneous CT and PET images, limiting their clinical applicability. We address these challenges by introducing RobSurv, a robust deep-learning framework that leverages vector quantization for resilient multi-modal feature learning. The key innovation of our approach lies in its dual-path architecture: one path maps continuous imaging features to learned discrete codebooks for noise-resistant representation, while the parallel path preserves fine-grained details through continuous feature processing. This dual representation is integrated through a novel patch-wise fusion mechanism that maintains local spatial relationships while capturing global context via Transformer-based processing. In extensive evaluations across three diverse datasets (HECKTOR, H\\&N1, and NSCLC Radiogenomics), RobSurv demonstrates superior performance, achieving concordance index of 0.771, 0.742, and 0.734 respectively - significantly outperforming existing methods. Most notably, our model maintains robust performance even under severe noise conditions, with performance degradation of only 3.8-4.5\\% compared to 8-12\\% in baseline methods. These results, combined with strong generalization across different cancer types and imaging protocols, establish RobSurv as a promising solution for reliable clinical prognosis that can enhance treatment planning and patient care.",
        "arxiv_id": "2505.02529",
        "ARXIVID": "2505.02529",
        "COMMENT": "Does not match any specific criteria. Focuses on cancer survival prediction using multi-modal medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.02665": {
        "authors": [
            "Qianjun Pan",
            "Wenkai Ji",
            "Yuyang Ding",
            "Junsong Li",
            "Shilian Chen",
            "Junyi Wang",
            "Jie Zhou",
            "Qin Chen",
            "Min Zhang",
            "Yulan Wu",
            "Liang He"
        ],
        "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law",
        "abstract": "arXiv:2505.02665v1 Announce Type: new  Abstract: This survey explores recent advancements in reasoning large language models (LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by human cognition, as described in Kahneman's Thinking, Fast and Slow. These models, like OpenAI's o1, focus on scaling computational resources dynamically during complex tasks, such as math reasoning, visual reasoning, medical diagnosis, and multi-agent debates. We present the development of reasoning LLMs and list their key technologies. By synthesizing over 100 studies, it charts a path toward LLMs that combine human-like deep thinking with scalable efficiency for reasoning. The review breaks down methods into three categories: (1) test-time scaling dynamically adjusts computation based on task complexity via search and sampling, dynamic verification; (2) reinforced learning refines decision-making through iterative improvement leveraging policy networks, reward models, and self-evolution strategies; and (3) slow-thinking frameworks (e.g., long CoT, hierarchical processes) that structure problem-solving with manageable steps. The survey highlights the challenges and further directions of this domain. Understanding and advancing the reasoning abilities of LLMs is crucial for unlocking their full potential in real-world applications, from scientific discovery to decision support systems.",
        "arxiv_id": "2505.02665",
        "ARXIVID": "2505.02665",
        "COMMENT": "This paper does not match any specific criteria but is related to reasoning in large language models, which is of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.01746": {
        "authors": [
            "Xingqun Qi",
            "Yatian Wang",
            "Hengyuan Zhang",
            "Jiahao Pan",
            "Wei Xue",
            "Shanghang Zhang",
            "Wenhan Luo",
            "Qifeng Liu",
            "Yike Guo"
        ],
        "title": "Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion",
        "abstract": "arXiv:2505.01746v1 Announce Type: new  Abstract: Generating gestures from human speech has gained tremendous progress in animating virtual avatars. While the existing methods enable synthesizing gestures cooperated by individual self-talking, they overlook the practicality of concurrent gesture modeling with two-person interactive conversations. Moreover, the lack of high-quality datasets with concurrent co-speech gestures also limits handling this issue. To fulfill this goal, we first construct a large-scale concurrent co-speech gesture dataset that contains more than 7M frames for diverse two-person interactive posture sequences, dubbed GES-Inter. Additionally, we propose Co$^3$Gesture, a novel framework that enables coherent concurrent co-speech gesture synthesis including two-person interactive movements. Considering the asymmetric body dynamics of two speakers, our framework is built upon two cooperative generation branches conditioned on separated speaker audio. Specifically, to enhance the coordination of human postures with respect to corresponding speaker audios while interacting with the conversational partner, we present a Temporal Interaction Module (TIM). TIM can effectively model the temporal association representation between two speakers' gesture sequences as interaction guidance and fuse it into the concurrent gesture generation. Then, we devise a mutual attention mechanism to further holistically boost learning dependencies of interacted concurrent motions, thereby enabling us to generate vivid and coherent gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected GES-Inter dataset. The dataset and source code are publicly available at \\href{https://mattie-e.github.io/Co3/}{\\textit{https://mattie-e.github.io/Co3/}}.",
        "arxiv_id": "2505.01746",
        "ARXIVID": "2505.01746",
        "COMMENT": "This paper does not match any specific criteria but is related to generative modeling for gesture synthesis, which is of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.01766": {
        "authors": [
            "Long Bai",
            "Boyi Ma",
            "Ruohan Wang",
            "Guankun Wang",
            "Beilei Cui",
            "Zhongliang Jiang",
            "Mobarakol Islam",
            "Zhe Min",
            "Jiewen Lai",
            "Nassir Navab",
            "Hongliang Ren"
        ],
        "title": "Multimodal Graph Representation Learning for Robust Surgical Workflow Recognition with Adversarial Feature Disentanglement",
        "abstract": "arXiv:2505.01766v1 Announce Type: new  Abstract: Surgical workflow recognition is vital for automating tasks, supporting decision-making, and training novice surgeons, ultimately improving patient safety and standardizing procedures. However, data corruption can lead to performance degradation due to issues like occlusion from bleeding or smoke in surgical scenes and problems with data storage and transmission. In this case, we explore a robust graph-based multimodal approach to integrating vision and kinematic data to enhance accuracy and reliability. Vision data captures dynamic surgical scenes, while kinematic data provides precise movement information, overcoming limitations of visual recognition under adverse conditions. We propose a multimodal Graph Representation network with Adversarial feature Disentanglement (GRAD) for robust surgical workflow recognition in challenging scenarios with domain shifts or corrupted data. Specifically, we introduce a Multimodal Disentanglement Graph Network that captures fine-grained visual information while explicitly modeling the complex relationships between vision and kinematic embeddings through graph-based message modeling. To align feature spaces across modalities, we propose a Vision-Kinematic Adversarial framework that leverages adversarial training to reduce modality gaps and improve feature consistency. Furthermore, we design a Contextual Calibrated Decoder, incorporating temporal and contextual priors to enhance robustness against domain shifts and corrupted data. Extensive comparative and ablation experiments demonstrate the effectiveness of our model and proposed modules. Moreover, our robustness experiments show that our method effectively handles data corruption during storage and transmission, exhibiting excellent stability and robustness. Our approach aims to advance automated surgical workflow recognition, addressing the complexities and dynamism inherent in surgical procedures.",
        "arxiv_id": "2505.01766",
        "ARXIVID": "2505.01766",
        "COMMENT": "This paper does not match any specific criteria but is related to multimodal learning and robust recognition, which are of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.02823": {
        "authors": [
            "Zinan Guo",
            "Pengze Zhang",
            "Yanze Wu",
            "Chong Mou",
            "Songtao Zhao",
            "Qian He"
        ],
        "title": "MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset via Attention Routing",
        "abstract": "arXiv:2505.02823v1 Announce Type: new  Abstract: Current multi-subject customization approaches encounter two critical challenges: the difficulty in acquiring diverse multi-subject training data, and attribute entanglement across different subjects. To bridge these gaps, we propose MUSAR - a simple yet effective framework to achieve robust multi-subject customization while requiring only single-subject training data. Firstly, to break the data limitation, we introduce debiased diptych learning. It constructs diptych training pairs from single-subject images to facilitate multi-subject learning, while actively correcting the distribution bias introduced by diptych construction via static attention routing and dual-branch LoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic attention routing mechanism, which adaptively establishes bijective mappings between generated images and conditional subjects. This design not only achieves decoupling of multi-subject representations but also maintains scalable generalization performance with increasing reference subjects. Comprehensive experiments demonstrate that our MUSAR outperforms existing methods - even those trained on multi-subject dataset - in image quality, subject consistency, and interaction naturalness, despite requiring only single-subject dataset.",
        "arxiv_id": "2505.02823",
        "ARXIVID": "2505.02823",
        "COMMENT": "This paper does not match any of the specific criteria but is related to generative modeling and image quality improvements.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.02322": {
        "authors": [
            "Runquan Gui",
            "Zhihai Wang",
            "Jie Wang",
            "Chi Ma",
            "Huiling Zhen",
            "Mingxuan Yuan",
            "Jianye Hao",
            "Defu Lian",
            "Enhong Chen",
            "Feng Wu"
        ],
        "title": "HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking",
        "abstract": "arXiv:2505.02322v1 Announce Type: new  Abstract: Recent advancements have significantly enhanced the performance of large language models (LLMs) in tackling complex reasoning tasks, achieving notable success in domains like mathematical and logical reasoning. However, these methods encounter challenges with complex planning tasks, primarily due to extended reasoning steps, diverse constraints, and the challenge of handling multiple distinct sub-tasks. To address these challenges, we propose HyperTree Planning (HTP), a novel reasoning paradigm that constructs hypertree-structured planning outlines for effective planning. The hypertree structure enables LLMs to engage in hierarchical thinking by flexibly employing the divide-and-conquer strategy, effectively breaking down intricate reasoning steps, accommodating diverse constraints, and managing multiple distinct sub-tasks in a well-organized manner. We further introduce an autonomous planning framework that completes the planning process by iteratively refining and expanding the hypertree-structured planning outlines. Experiments demonstrate the effectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner benchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvement over o1-preview.",
        "arxiv_id": "2505.02322",
        "ARXIVID": "2505.02322",
        "COMMENT": "This paper proposes a hierarchical reasoning paradigm for LLMs, which does not directly match any specific criterion but is tangentially related to reasoning in generative models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.01969": {
        "authors": [
            "Jiayi Cheng",
            "Can Gao",
            "Jie Zhou",
            "Jiajun Wen",
            "Tao Dai",
            "Jinbao Wang"
        ],
        "title": "MC3D-AD: A Unified Geometry-aware Reconstruction Model for Multi-category 3D Anomaly Detection",
        "abstract": "arXiv:2505.01969v1 Announce Type: new  Abstract: 3D Anomaly Detection (AD) is a promising means of controlling the quality of manufactured products. However, existing methods typically require carefully training a task-specific model for each category independently, leading to high cost, low efficiency, and weak generalization. Therefore, this paper presents a novel unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) that aims to utilize both local and global geometry-aware information to reconstruct normal representations of all categories. First, to learn robust and generalized features of different categories, we propose an adaptive geometry-aware masked attention module that extracts geometry variation information to guide mask attention. Then, we introduce a local geometry-aware encoder reinforced by the improved mask attention to encode group-level feature tokens. Finally, we design a global query decoder that utilizes point cloud position embeddings to improve the decoding process and reconstruction ability. This leads to local and global geometry-aware reconstructed feature tokens for the AD task. MC3D-AD is evaluated on two publicly available Real3D-AD and Anomaly-ShapeNet datasets, and exhibits significant superiority over current state-of-the-art single-category methods, achieving 3.1\\% and 9.3\\% improvement in object-level AUROC over Real3D-AD and Anomaly-ShapeNet, respectively. The source code will be released upon acceptance.",
        "arxiv_id": "2505.01969",
        "ARXIVID": "2505.01969",
        "COMMENT": "This paper introduces a novel geometry-aware model for 3D anomaly detection, which could be tangentially related to spatial understanding but does not directly match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.02539": {
        "authors": [
            "Nahuel Garcia-D'Urso",
            "Bernabe Sanchez-Sos",
            "Jorge Azorin-Lopez",
            "Andres Fuster-Guillo",
            "Antonio Macia-Lillo",
            "Higinio Mora-Mora"
        ],
        "title": "Marker-Based Extrinsic Calibration Method for Accurate Multi-Camera 3D Reconstruction",
        "abstract": "arXiv:2505.02539v1 Announce Type: new  Abstract: Accurate 3D reconstruction using multi-camera RGB-D systems critically depends on precise extrinsic calibration to achieve proper alignment between captured views. In this paper, we introduce an iterative extrinsic calibration method that leverages the geometric constraints provided by a three-dimensional marker to significantly improve calibration accuracy. Our proposed approach systematically segments and refines marker planes through clustering, regression analysis, and iterative reassignment techniques, ensuring robust geometric correspondence across camera views. We validate our method comprehensively in both controlled environments and practical real-world settings within the Tech4Diet project, aimed at modeling the physical progression of patients undergoing nutritional treatments. Experimental results demonstrate substantial reductions in alignment errors, facilitating accurate and reliable 3D reconstructions.",
        "arxiv_id": "2505.02539",
        "ARXIVID": "2505.02539",
        "COMMENT": "Does not match any specific criteria but is related to 3D reconstruction and calibration, which is tangentially relevant to general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.01615": {
        "authors": [
            "Dimitrios Dagdilelis",
            "Panagiotis Grigoriadis",
            "Roberto Galeazzi"
        ],
        "title": "Multimodal and Multiview Deep Fusion for Autonomous Marine Navigation",
        "abstract": "arXiv:2505.01615v1 Announce Type: new  Abstract: We propose a cross attention transformer based method for multimodal sensor fusion to build a birds eye view of a vessels surroundings supporting safer autonomous marine navigation. The model deeply fuses multiview RGB and long wave infrared images with sparse LiDAR point clouds. Training also integrates X band radar and electronic chart data to inform predictions. The resulting view provides a detailed reliable scene representation improving navigational accuracy and robustness. Real world sea trials confirm the methods effectiveness even in adverse weather and complex maritime settings.",
        "arxiv_id": "2505.01615",
        "ARXIVID": "2505.01615",
        "COMMENT": "This paper does not match any specific criteria but is related to multimodal sensor fusion, which is of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.02825": {
        "authors": [
            "Alex Hoi Hang Chan",
            "Otto Brookes",
            "Urs Waldmann",
            "Hemal Naik",
            "Iain D. Couzin",
            "Majid Mirmehdi",
            "No\\\"el Adiko Houa",
            "Emmanuelle Normand",
            "Christophe Boesch",
            "Lukas Boesch",
            "Mimi Arandjelovic",
            "Hjalmar K\\\"uhl",
            "Tilo Burghardt",
            "Fumihiro Kano"
        ],
        "title": "Towards Application-Specific Evaluation of Vision Models: Case Studies in Ecology and Biology",
        "abstract": "arXiv:2505.02825v1 Announce Type: new  Abstract: Computer vision methods have demonstrated considerable potential to streamline ecological and biological workflows, with a growing number of datasets and models becoming available to the research community. However, these resources focus predominantly on evaluation using machine learning metrics, with relatively little emphasis on how their application impacts downstream analysis. We argue that models should be evaluated using application-specific metrics that directly represent model performance in the context of its final use case. To support this argument, we present two disparate case studies: (1) estimating chimpanzee abundance and density with camera trap distance sampling when using a video-based behaviour classifier and (2) estimating head rotation in pigeons using a 3D posture estimator. We show that even models with strong machine learning performance (e.g., 87% mAP) can yield data that leads to discrepancies in abundance estimates compared to expert-derived data. Similarly, the highest-performing models for posture estimation do not produce the most accurate inferences of gaze direction in pigeons. Motivated by these findings, we call for researchers to integrate application-specific metrics in ecological/biological datasets, allowing for models to be benchmarked in the context of their downstream application and to facilitate better integration of models into application workflows.",
        "arxiv_id": "2505.02825",
        "ARXIVID": "2505.02825",
        "COMMENT": "This paper discusses application-specific evaluation of vision models in ecology and biology, which does not match any specific criterion but is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.01539": {
        "authors": [
            "Cor Steging",
            "Silja Renooij",
            "Bart Verheij"
        ],
        "title": "Parameterized Argumentation-based Reasoning Tasks for Benchmarking Generative Language Models",
        "abstract": "arXiv:2505.01539v1 Announce Type: new  Abstract: Generative large language models as tools in the legal domain have the potential to improve the justice system. However, the reasoning behavior of current generative models is brittle and poorly understood, hence cannot be responsibly applied in the domains of law and evidence. In this paper, we introduce an approach for creating benchmarks that can be used to evaluate the reasoning capabilities of generative language models. These benchmarks are dynamically varied, scalable in their complexity, and have formally unambiguous interpretations. In this study, we illustrate the approach on the basis of witness testimony, focusing on the underlying argument attack structure. We dynamically generate both linear and non-linear argument attack graphs of varying complexity and translate these into reasoning puzzles about witness testimony expressed in natural language. We show that state-of-the-art large language models often fail in these reasoning puzzles, already at low complexity. Obvious mistakes are made by the models, and their inconsistent performance indicates that their reasoning capabilities are brittle. Furthermore, at higher complexity, even state-of-the-art models specifically presented for reasoning capabilities make mistakes. We show the viability of using a parametrized benchmark with varying complexity to evaluate the reasoning capabilities of generative language models. As such, the findings contribute to a better understanding of the limitations of the reasoning capabilities of generative models, which is essential when designing responsible AI systems in the legal domain.",
        "arxiv_id": "2505.01539",
        "ARXIVID": "2505.01539",
        "COMMENT": "This paper focuses on benchmarking generative language models for reasoning tasks, which does not match any specific criterion but is tangentially related to your friend's interest in generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.01869": {
        "authors": [
            "Guoxi Huang",
            "Haoran Wang",
            "Brett Seymour",
            "Evan Kovacs",
            "John Ellerbrock",
            "Dave Blackham",
            "Nantheera Anantrasirichai"
        ],
        "title": "Visual enhancement and 3D representation for underwater scenes: a review",
        "abstract": "arXiv:2505.01869v1 Announce Type: new  Abstract: Underwater visual enhancement (UVE) and underwater 3D reconstruction pose significant challenges in   computer vision and AI-based tasks due to complex imaging conditions in aquatic environments. Despite   the development of numerous enhancement algorithms, a comprehensive and systematic review covering both   UVE and underwater 3D reconstruction remains absent. To advance research in these areas, we present an   in-depth review from multiple perspectives. First, we introduce the fundamental physical models, highlighting the   peculiarities that challenge conventional techniques. We survey advanced methods for visual enhancement and   3D reconstruction specifically designed for underwater scenarios. The paper assesses various approaches from   non-learning methods to advanced data-driven techniques, including Neural Radiance Fields and 3D Gaussian   Splatting, discussing their effectiveness in handling underwater distortions. Finally, we conduct both quantitative   and qualitative evaluations of state-of-the-art UVE and underwater 3D reconstruction algorithms across multiple   benchmark datasets. Finally, we highlight key research directions for future advancements in underwater vision.",
        "arxiv_id": "2505.01869",
        "ARXIVID": "2505.01869",
        "COMMENT": "This is a review paper on underwater visual enhancement and 3D reconstruction, which does not match any specific criterion but is tangentially related to spatial understanding.",
        "RELEVANCE": 3,
        "NOVELTY": 3
    }
}