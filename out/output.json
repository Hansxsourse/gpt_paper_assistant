{
    "2509.07530": {
        "authors": [
            "Kiet T. Nguyen",
            "Chanhuyk Lee",
            "Donggyun Kim",
            "Dong Hoon Lee",
            "Seunghoon Hong"
        ],
        "title": "Universal Few-Shot Spatial Control for Diffusion Models",
        "abstract": "arXiv:2509.07530v1 Announce Type: new  Abstract: Spatial conditioning in pretrained text-to-image diffusion models has significantly improved fine-grained control over the structure of generated images. However, existing control adapters exhibit limited adaptability and incur high training costs when encountering novel spatial control conditions that differ substantially from the training tasks. To address this limitation, we propose Universal Few-Shot Control (UFC), a versatile few-shot control adapter capable of generalizing to novel spatial conditions. Given a few image-condition pairs of an unseen task and a query condition, UFC leverages the analogy between query and support conditions to construct task-specific control features, instantiated by a matching mechanism and an update on a small set of task-specific parameters. Experiments on six novel spatial control tasks show that UFC, fine-tuned with only 30 annotated examples of novel tasks, achieves fine-grained control consistent with the spatial conditions. Notably, when fine-tuned with 0.1% of the full training data, UFC achieves competitive performance with the fully supervised baselines in various control tasks. We also show that UFC is applicable agnostically to various diffusion backbones and demonstrate its effectiveness on both UNet and DiT architectures. Code is available at https://github.com/kietngt00/UFC.",
        "arxiv_id": "2509.07530",
        "ARXIVID": "2509.07530",
        "COMMENT": "No criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.07920": {
        "authors": [
            "Ao Li",
            "Jinpeng Liu",
            "Yixuan Zhu",
            "Yansong Tang"
        ],
        "title": "ScoreHOI: Physically Plausible Reconstruction of Human-Object Interaction via Score-Guided Diffusion",
        "abstract": "arXiv:2509.07920v1 Announce Type: new  Abstract: Joint reconstruction of human-object interaction marks a significant milestone in comprehending the intricate interrelations between humans and their surrounding environment. Nevertheless, previous optimization methods often struggle to achieve physically plausible reconstruction results due to the lack of prior knowledge about human-object interactions. In this paper, we introduce ScoreHOI, an effective diffusion-based optimizer that introduces diffusion priors for the precise recovery of human-object interactions. By harnessing the controllability within score-guided sampling, the diffusion model can reconstruct a conditional distribution of human and object pose given the image observation and object feature. During inference, the ScoreHOI effectively improves the reconstruction results by guiding the denoising process with specific physical constraints. Furthermore, we propose a contact-driven iterative refinement approach to enhance the contact plausibility and improve the reconstruction accuracy. Extensive evaluations on standard benchmarks demonstrate ScoreHOI's superior performance over state-of-the-art methods, highlighting its ability to achieve a precise and robust improvement in joint human-object interaction reconstruction.",
        "arxiv_id": "2509.07920",
        "ARXIVID": "2509.07920",
        "COMMENT": "No criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.07435": {
        "authors": [
            "Ze-Xin Yin",
            "Jiaxiong Qiu",
            "Liu Liu",
            "Xinjie Wang",
            "Wei Sui",
            "Zhizhong Su",
            "Jian Yang",
            "Jin Xie"
        ],
        "title": "DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation",
        "abstract": "arXiv:2509.07435v1 Announce Type: new  Abstract: The labor- and experience-intensive creation of 3D assets with physically based rendering (PBR) materials demands an autonomous 3D asset creation pipeline. However, most existing 3D generation methods focus on geometry modeling, either baking textures into simple vertex colors or leaving texture synthesis to post-processing with image diffusion models. To achieve end-to-end PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter (LGAA), a novel framework that unifies the modeling of geometry and PBR materials by exploiting multi-view (MV) diffusion priors from a novel perspective. The LGAA features a modular design with three components. Specifically, the LGAA Wrapper reuses and adapts network layers from MV diffusion models, which encapsulate knowledge acquired from billions of images, enabling better convergence in a data-efficient manner. To incorporate multiple diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated post-processing procedure to effectively extract high-quality, relightable mesh assets from the resulting 2DGS. Extensive quantitative and qualitative experiments demonstrate the superior performance of LGAA with both text-and image-conditioned MV diffusion models. Additionally, the modular design enables flexible incorporation of multiple diffusion priors, and the knowledge-preserving scheme leads to efficient convergence trained on merely 69k multi-view instances. Our code, pre-trained weights, and the dataset used will be publicly available via our project page: https://zx-yin.github.io/dreamlifting/.",
        "arxiv_id": "2509.07435",
        "ARXIVID": "2509.07435",
        "COMMENT": "No criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}