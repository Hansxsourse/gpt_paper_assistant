{
    "2504.06148": {
        "authors": [
            "Xiangxi Zheng",
            "Linjie Li",
            "Zhengyuan Yang",
            "Ping Yu",
            "Alex Jinpeng Wang",
            "Rui Yan",
            "Yuan Yao",
            "Lijuan Wang"
        ],
        "title": "V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric Capabilities in Multimodal Large Language Models",
        "abstract": "arXiv:2504.06148v1 Announce Type: new  Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current game-based benchmarks remain inadequate because they lack visual-centric tasks and fail to assess the diverse reasoning skills required for real-world decision-making. To address this, we introduce Visual-centric Multiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework designed to assess visual reasoning capabilities of MLLMs. V-MAGE features five diverse games with 30+ handcrafted levels, testing models on core visual skills such as positioning, trajectory tracking, timing, and visual memory, alongside higher-level reasoning like long-term planning and deliberation. We use V-MAGE to evaluate leading MLLMs, revealing significant challenges in their visual perception and reasoning. In all game environments, the top-performing MLLMs, as determined by Elo rating comparisons, exhibit a substantial performance gap compared to humans. Our findings highlight critical limitations, including various types of perceptual errors made by the models, and suggest potential avenues for improvement from an agent-centric perspective, such as refining agent strategies and addressing perceptual inaccuracies. Code is available at https://github.com/CSU-JPG/V-MAGE.",
        "arxiv_id": "2504.06148",
        "ARXIVID": "2504.06148",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (V-MAGE) for evaluating MLLMs in visual-centric tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2504.05741": {
        "authors": [
            "Shuai Wang",
            "Zhi Tian",
            "Weilin Huang",
            "Limin Wang"
        ],
        "title": "DDT: Decoupled Diffusion Transformer",
        "abstract": "arXiv:2504.05741v1 Announce Type: new  Abstract: Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new \\textbf{\\color{ddt}D}ecoupled \\textbf{\\color{ddt}D}iffusion \\textbf{\\color{ddt}T}ransformer~(\\textbf{\\color{ddt}DDT}), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet $256\\times256$, Our DDT-XL/2 achieves a new state-of-the-art performance of {1.31 FID}~(nearly $4\\times$ faster training convergence compared to previous diffusion transformers). For ImageNet $512\\times512$, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies.",
        "arxiv_id": "2504.05741",
        "ARXIVID": "2504.05741",
        "COMMENT": "Matches criterion 4 as it introduces a novel diffusion transformer architecture for image generation, which is relevant to vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2504.05786": {
        "authors": [
            "Jirong Zha",
            "Yuxuan Fan",
            "Xiao Yang",
            "Chen Gao",
            "Xinlei Chen"
        ],
        "title": "How to Enable LLM with 3D Capacity? A Survey of Spatial Reasoning in LLM",
        "abstract": "arXiv:2504.05786v1 Announce Type: new  Abstract: 3D spatial understanding is essential in real-world applications such as robotics, autonomous vehicles, virtual reality, and medical imaging. Recently, Large Language Models (LLMs), having demonstrated remarkable success across various domains, have been leveraged to enhance 3D understanding tasks, showing potential to surpass traditional computer vision methods. In this survey, we present a comprehensive review of methods integrating LLMs with 3D spatial understanding. We propose a taxonomy that categorizes existing methods into three branches: image-based methods deriving 3D understanding from 2D visual data, point cloud-based methods working directly with 3D representations, and hybrid modality-based methods combining multiple data streams. We systematically review representative methods along these categories, covering data representations, architectural modifications, and training strategies that bridge textual and 3D modalities. Finally, we discuss current limitations, including dataset scarcity and computational challenges, while highlighting promising research directions in spatial perception, multi-modal fusion, and real-world applications.",
        "arxiv_id": "2504.05786",
        "ARXIVID": "2504.05786",
        "COMMENT": "Matches criterion 1 as it focuses on new methodological improvements to spatial understanding in LLMs, specifically integrating 3D spatial reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2504.06232": {
        "authors": [
            "Jiazi Bu",
            "Pengyang Ling",
            "Yujie Zhou",
            "Pan Zhang",
            "Tong Wu",
            "Xiaoyi Dong",
            "Yuhang Zang",
            "Yuhang Cao",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "title": "HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned Guidance",
        "abstract": "arXiv:2504.06232v1 Announce Type: new  Abstract: Text-to-image (T2I) diffusion/flow models have drawn considerable attention recently due to their remarkable ability to deliver flexible visual creations. Still, high-resolution image synthesis presents formidable challenges due to the scarcity and complexity of high-resolution content. To this end, we present HiFlow, a training-free and model-agnostic framework to unlock the resolution potential of pre-trained flow models. Specifically, HiFlow establishes a virtual reference flow within the high-resolution space that effectively captures the characteristics of low-resolution flow information, offering guidance for high-resolution generation through three key aspects: initialization alignment for low-frequency consistency, direction alignment for structure preservation, and acceleration alignment for detail fidelity. By leveraging this flow-aligned guidance, HiFlow substantially elevates the quality of high-resolution image synthesis of T2I models and demonstrates versatility across their personalized variants. Extensive experiments validate HiFlow's superiority in achieving superior high-resolution image quality over current state-of-the-art methods.",
        "arxiv_id": "2504.06232",
        "ARXIVID": "2504.06232",
        "COMMENT": "Matches criterion 4 as it focuses on improving high-resolution image generation using flow-aligned guidance, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.05782": {
        "authors": [
            "Pengfei Zhou",
            "Fanrui Zhang",
            "Xiaopeng Peng",
            "Zhaopan Xu",
            "Jiaxin Ai",
            "Yansheng Qiu",
            "Chuanhao Li",
            "Zhen Li",
            "Ming Li",
            "Yukang Feng",
            "Jianwen Sun",
            "Haoquan Zhang",
            "Zizhen Li",
            "Xiaofeng Mao",
            "Wangbo Zhao",
            "Kai Wang",
            "Xiaojun Chang",
            "Wenqi Shao",
            "Yang You",
            "Kaipeng Zhang"
        ],
        "title": "MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models",
        "abstract": "arXiv:2504.05782v1 Announce Type: new  Abstract: Multimodal reasoning, which integrates language and visual cues into problem solving and decision making, is a fundamental aspect of human intelligence and a crucial step toward artificial general intelligence. However, the evaluation of multimodal reasoning capabilities in Multimodal Large Language Models (MLLMs) remains inadequate. Most existing reasoning benchmarks are constrained by limited data size, narrow domain coverage, and unstructured knowledge distribution. To close these gaps, we introduce MDK12-Bench, a multi-disciplinary benchmark assessing the reasoning capabilities of MLLMs via real-world K-12 examinations. Spanning six disciplines (math, physics, chemistry, biology, geography, and information science), our benchmark comprises 140K reasoning instances across diverse difficulty levels from primary school to 12th grade. It features 6,827 instance-level knowledge point annotations based on a well-organized knowledge structure, detailed answer explanations, difficulty labels and cross-year partitions, providing a robust platform for comprehensive evaluation. Additionally, we present a novel dynamic evaluation framework to mitigate data contamination issues by bootstrapping question forms, question types, and image styles during evaluation. Extensive experiment on MDK12-Bench reveals the significant limitation of current MLLMs in multimodal reasoning. The findings on our benchmark provide insights into the development of the next-generation models. Our data and codes are available at https://github.com/LanceZPF/MDK12.",
        "arxiv_id": "2504.05782",
        "ARXIVID": "2504.05782",
        "COMMENT": "Matches criterion 3. Proposes a multi-disciplinary benchmark for evaluating reasoning in MLLMs, which aligns with the interest in embodied AI benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.06260": {
        "authors": [
            "Nayantara Mudur",
            "Hao Cui",
            "Subhashini Venugopalan",
            "Paul Raccuglia",
            "Michael P. Brenner",
            "Peter Norgaard"
        ],
        "title": "FEABench: Evaluating Language Models on Multiphysics Reasoning Ability",
        "abstract": "arXiv:2504.06260v1 Announce Type: new  Abstract: Building precise simulations of the real world and invoking numerical solvers to answer quantitative problems is an essential requirement in engineering and science. We present FEABench, a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). We introduce a comprehensive evaluation scheme to investigate the ability of LLMs to solve these problems end-to-end by reasoning over natural language problem descriptions and operating COMSOL Multiphysics$^\\circledR$, an FEA software, to compute the answers. We additionally design a language model agent equipped with the ability to interact with the software through its Application Programming Interface (API), examine its outputs and use tools to improve its solutions over multiple iterations. Our best performing strategy generates executable API calls 88% of the time. LLMs that can successfully interact with and operate FEA software to solve problems such as those in our benchmark would push the frontiers of automation in engineering. Acquiring this capability would augment LLMs' reasoning skills with the precision of numerical solvers and advance the development of autonomous systems that can tackle complex problems in the real world. The code is available at https://github.com/google/feabench",
        "arxiv_id": "2504.06260",
        "ARXIVID": "2504.06260",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (FEABench) for evaluating LLMs in physics and engineering tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.06263": {
        "authors": [
            "Yiying Yang",
            "Wei Cheng",
            "Sijin Chen",
            "Xianfang Zeng",
            "Jiaxu Zhang",
            "Liao Wang",
            "Gang Yu",
            "Xingjun Ma",
            "Yu-Gang Jiang"
        ],
        "title": "OmniSVG: A Unified Scalable Vector Graphics Generation Model",
        "abstract": "arXiv:2504.06263v1 Announce Type: new  Abstract: Scalable Vector Graphics (SVG) is an important image format widely adopted in graphic design because of their resolution independence and editability. The study of generating high-quality SVG has continuously drawn attention from both designers and researchers in the AIGC community. However, existing methods either produces unstructured outputs with huge computational cost or is limited to generating monochrome icons of over-simplified structures. To produce high-quality and complex SVG, we propose OmniSVG, a unified framework that leverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal SVG generation. By parameterizing SVG commands and coordinates into discrete tokens, OmniSVG decouples structural logic from low-level geometry for efficient training while maintaining the expressiveness of complex SVG structure. To further advance the development of SVG synthesis, we introduce MMSVG-2M, a multimodal dataset with two million richly annotated SVG assets, along with a standardized evaluation protocol for conditional SVG generation tasks. Extensive experiments show that OmniSVG outperforms existing methods and demonstrates its potential for integration into professional SVG design workflows.",
        "arxiv_id": "2504.06263",
        "ARXIVID": "2504.06263",
        "COMMENT": "Matches criterion 2 as it leverages vision-language models for SVG generation, aligning with VLLM advancements.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.06264": {
        "authors": [
            "Jisang Han",
            "Honggyu An",
            "Jaewoo Jung",
            "Takuya Narihira",
            "Junyoung Seo",
            "Kazumi Fukuda",
            "Chaehyun Kim",
            "Sunghwan Hong",
            "Yuki Mitsufuji",
            "Seungryong Kim"
        ],
        "title": "D^2USt3R: Enhancing 3D Reconstruction with 4D Pointmaps for Dynamic Scenes",
        "abstract": "arXiv:2504.06264v1 Announce Type: new  Abstract: We address the task of 3D reconstruction in dynamic scenes, where object motions degrade the quality of previous 3D pointmap regression methods, such as DUSt3R, originally designed for static 3D scene reconstruction. Although these methods provide an elegant and powerful solution in static settings, they struggle in the presence of dynamic motions that disrupt alignment based solely on camera poses. To overcome this, we propose D^2USt3R that regresses 4D pointmaps that simultaneiously capture both static and dynamic 3D scene geometry in a feed-forward manner. By explicitly incorporating both spatial and temporal aspects, our approach successfully encapsulates spatio-temporal dense correspondence to the proposed 4D pointmaps, enhancing downstream tasks. Extensive experimental evaluations demonstrate that our proposed approach consistently achieves superior reconstruction performance across various datasets featuring complex motions.",
        "arxiv_id": "2504.06264",
        "ARXIVID": "2504.06264",
        "COMMENT": "Matches criterion 1 as it proposes a novel method for enhancing spatial understanding in dynamic 3D scenes.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.05463": {
        "authors": [
            "Sofian Chaybouti",
            "Walid Bousselham",
            "Moritz Wolter",
            "Hilde Kuehne"
        ],
        "title": "REVEAL: Relation-based Video Representation Learning for Video-Question-Answering",
        "abstract": "arXiv:2504.05463v1 Announce Type: new  Abstract: Video-Question-Answering (VideoQA) comprises the capturing of complex visual relation changes over time, remaining a challenge even for advanced Video Language Models (VLM), i.a., because of the need to represent the visual content to a reasonably sized input for those models. To address this problem, we propose   RElation-based Video rEpresentAtion Learning (REVEAL), a framework designed to capture visual relation information by encoding them into structured, decomposed representations. Specifically, inspired by spatiotemporal scene graphs, we propose to encode video sequences as sets of relation triplets in the form of (\\textit{subject-predicate-object}) over time via their language embeddings. To this end, we extract explicit relations from video captions and introduce a Many-to-Many Noise Contrastive Estimation (MM-NCE) together with a Q-Former architecture to align an unordered set of video-derived queries with corresponding text-based relation descriptions. At inference, the resulting Q-former produces an efficient token representation that can serve as input to a VLM for VideoQA.   We evaluate the proposed framework on five challenging benchmarks: NeXT-QA, Intent-QA, STAR, VLEP, and TVQA. It shows that the resulting query-based video representation is able to outperform global alignment-based CLS or patch token representations and achieves competitive results against state-of-the-art models, particularly on tasks requiring temporal reasoning and relation comprehension. The code and models will be publicly released.",
        "arxiv_id": "2504.05463",
        "ARXIVID": "2504.05463",
        "COMMENT": "Matches criterion 4 as it proposes a relation-based video representation learning framework for video-question-answering, which is relevant to vision-language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.05400": {
        "authors": [
            "Sihang Li",
            "Zeyu Jiang",
            "Grace Chen",
            "Chenyang Xu",
            "Siqi Tan",
            "Xue Wang",
            "Irving Fang",
            "Kristof Zyskowski",
            "Shannon P. McPherron",
            "Radu Iovita",
            "Chen Feng",
            "Jing Zhang"
        ],
        "title": "GARF: Learning Generalizable 3D Reassembly for Real-World Fractures",
        "abstract": "arXiv:2504.05400v1 Announce Type: new  Abstract: 3D reassembly is a challenging spatial intelligence task with broad applications across scientific domains. While large-scale synthetic datasets have fueled promising learning-based approaches, their generalizability to different domains is limited. Critically, it remains uncertain whether models trained on synthetic datasets can generalize to real-world fractures where breakage patterns are more complex. To bridge this gap, we propose GARF, a generalizable 3D reassembly framework for real-world fractures. GARF leverages fracture-aware pretraining to learn fracture features from individual fragments, with flow matching enabling precise 6-DoF alignments. At inference time, we introduce one-step preassembly, improving robustness to unseen objects and varying numbers of fractures. In collaboration with archaeologists, paleoanthropologists, and ornithologists, we curate Fractura, a diverse dataset for vision and learning communities, featuring real-world fracture types across ceramics, bones, eggshells, and lithics. Comprehensive experiments have shown our approach consistently outperforms state-of-the-art methods on both synthetic and real-world datasets, achieving 82.87\\% lower rotation error and 25.15\\% higher part accuracy. This sheds light on training on synthetic data to advance real-world 3D puzzle solving, demonstrating its strong generalization across unseen object shapes and diverse fracture types.",
        "arxiv_id": "2504.05400",
        "ARXIVID": "2504.05400",
        "COMMENT": "Matches criterion 1 as it focuses on 3D reassembly, a spatial intelligence task, with a novel framework for real-world fractures.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.05621": {
        "authors": [
            "Bing Han",
            "Feifei Zhao",
            "Yinqian Sun",
            "Wenxuan Pan",
            "Yi Zeng"
        ],
        "title": "Continual Learning of Multiple Cognitive Functions with Brain-inspired Temporal Development Mechanism",
        "abstract": "arXiv:2504.05621v1 Announce Type: new  Abstract: Cognitive functions in current artificial intelligence networks are tied to the exponential increase in network scale, whereas the human brain can continuously learn hundreds of cognitive functions with remarkably low energy consumption. This advantage is in part due to the brain cross-regional temporal development mechanisms, where the progressive formation, reorganization, and pruning of connections from basic to advanced regions, facilitate knowledge transfer and prevent network redundancy. Inspired by these, we propose the Continual Learning of Multiple Cognitive Functions with Brain-inspired Temporal Development Mechanism(TD-MCL), enabling cognitive enhancement from simple to complex in Perception-Motor-Interaction(PMI) multiple cognitive task scenarios. The TD-MCL model proposes the sequential evolution of long-range connections between different cognitive modules to promote positive knowledge transfer, while using feedback-guided local connection inhibition and pruning to effectively eliminate redundancies in previous tasks, reducing energy consumption while preserving acquired knowledge. Experiments show that the proposed method can achieve continual learning capabilities while reducing network scale, without introducing regularization, replay, or freezing strategies, and achieving superior accuracy on new tasks compared to direct learning. The proposed method shows that the brain's developmental mechanisms offer a valuable reference for exploring biologically plausible, low-energy enhancements of general cognitive abilities.",
        "arxiv_id": "2504.05621",
        "ARXIVID": "2504.05621",
        "COMMENT": "Matches criterion 3 as it introduces a biologically inspired continual learning mechanism for embodied AI tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2504.06178": {
        "authors": [
            "Yujia Hu",
            "Songhua Liu",
            "Xingyi Yang",
            "Xinchao Wang"
        ],
        "title": "Flash Sculptor: Modular 3D Worlds from Objects",
        "abstract": "arXiv:2504.06178v1 Announce Type: new  Abstract: Existing text-to-3D and image-to-3D models often struggle with complex scenes involving multiple objects and intricate interactions. Although some recent attempts have explored such compositional scenarios, they still require an extensive process of optimizing the entire layout, which is highly cumbersome if not infeasible at all. To overcome these challenges, we propose Flash Sculptor in this paper, a simple yet effective framework for compositional 3D scene/object reconstruction from a single image. At the heart of Flash Sculptor lies a divide-and-conquer strategy, which decouples compositional scene reconstruction into a sequence of sub-tasks, including handling the appearance, rotation, scale, and translation of each individual instance. Specifically, for rotation, we introduce a coarse-to-fine scheme that brings the best of both worlds--efficiency and accuracy--while for translation, we develop an outlier-removal-based algorithm that ensures robust and precise parameters in a single step, without any iterative optimization. Extensive experiments demonstrate that Flash Sculptor achieves at least a 3 times speedup over existing compositional 3D methods, while setting new benchmarks in compositional 3D reconstruction performance. Codes are available at https://github.com/YujiaHu1109/Flash-Sculptor.",
        "arxiv_id": "2504.06178",
        "ARXIVID": "2504.06178",
        "COMMENT": "Matches criterion 1 as it proposes a novel framework for compositional 3D scene reconstruction, which involves spatial intelligence and understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.06003": {
        "authors": [
            "Can Zhang",
            "Gim Hee Lee"
        ],
        "title": "econSG: Efficient and Multi-view Consistent Open-Vocabulary 3D Semantic Gaussians",
        "abstract": "arXiv:2504.06003v1 Announce Type: new  Abstract: The primary focus of most recent works on open-vocabulary neural fields is extracting precise semantic features from the VLMs and then consolidating them efficiently into a multi-view consistent 3D neural fields representation. However, most existing works over-trusted SAM to regularize image-level CLIP without any further refinement. Moreover, several existing works improved efficiency by dimensionality reduction of semantic features from 2D VLMs before fusing with 3DGS semantic fields, which inevitably leads to multi-view inconsistency. In this work, we propose econSG for open-vocabulary semantic segmentation with 3DGS. Our econSG consists of: 1) A Confidence-region Guided Regularization (CRR) that mutually refines SAM and CLIP to get the best of both worlds for precise semantic features with complete and precise boundaries. 2) A low dimensional contextual space to enforce 3D multi-view consistency while improving computational efficiency by fusing backprojected multi-view 2D features and follow by dimensional reduction directly on the fused 3D features instead of operating on each 2D view separately. Our econSG shows state-of-the-art performance on four benchmark datasets compared to the existing methods. Furthermore, we are also the most efficient training among all the methods.",
        "arxiv_id": "2504.06003",
        "ARXIVID": "2504.06003",
        "COMMENT": "Matches criterion 4 as it discusses open-vocabulary 3D semantic segmentation with a focus on multi-view consistency and efficiency, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.05950": {
        "authors": [
            "Zhuoli Zhuang",
            "Cheng-You Lu",
            "Yu-Cheng Fred Chang",
            "Yu-Kai Wang",
            "Thomas Do",
            "Chin-Teng Lin"
        ],
        "title": "AEGIS: Human Attention-based Explainable Guidance for Intelligent Vehicle Systems",
        "abstract": "arXiv:2504.05950v1 Announce Type: new  Abstract: Improving decision-making capabilities in Autonomous Intelligent Vehicles (AIVs) has been a heated topic in recent years. Despite advancements, training machines to capture regions of interest for comprehensive scene understanding, like human perception and reasoning, remains a significant challenge. This study introduces a novel framework, Human Attention-based Explainable Guidance for Intelligent Vehicle Systems (AEGIS). AEGIS utilizes human attention, converted from eye-tracking, to guide reinforcement learning (RL) models to identify critical regions of interest for decision-making. AEGIS uses a pre-trained human attention model to guide RL models to identify critical regions of interest for decision-making. By collecting 1.2 million frames from 20 participants across six scenarios, AEGIS pre-trains a model to predict human attention patterns.",
        "arxiv_id": "2504.05950",
        "ARXIVID": "2504.05950",
        "COMMENT": "Matches criterion 1 as it introduces a novel framework (AEGIS) for improving decision-making in autonomous vehicles using human attention, which relates to spatial understanding in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.05491": {
        "authors": [
            "Sakib Reza",
            "Xiyun Song",
            "Heather Yu",
            "Zongfang Lin",
            "Mohsen Moghaddam",
            "Octavia Camps"
        ],
        "title": "REEF: Relevance-Aware and Efficient LLM Adapter for Video Understanding",
        "abstract": "arXiv:2504.05491v1 Announce Type: new  Abstract: Integrating vision models into large language models (LLMs) has sparked significant interest in creating vision-language foundation models, especially for video understanding. Recent methods often utilize memory banks to handle untrimmed videos for video-level understanding. However, they typically compress visual memory using similarity-based greedy approaches, which can overlook the contextual importance of individual tokens. To address this, we introduce an efficient LLM adapter designed for video-level understanding of untrimmed videos that prioritizes the contextual relevance of spatio-temporal tokens. Our framework leverages scorer networks to selectively compress the visual memory bank and filter spatial tokens based on relevance, using a differentiable Top-K operator for end-to-end training. Across three key video-level understanding tasks$\\unicode{x2013}$ untrimmed video classification, video question answering, and video captioning$\\unicode{x2013}$our method achieves competitive or superior results on four large-scale datasets while reducing computational overhead by up to 34%. The code will be available soon on GitHub.",
        "arxiv_id": "2504.05491",
        "ARXIVID": "2504.05491",
        "COMMENT": "Matches criterion 2. Proposes an efficient LLM adapter for video understanding, which aligns with the interest in VLLMs/MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2504.05979": {
        "authors": [
            "Sixiang Chen",
            "Jinbin Bai",
            "Zhuoran Zhao",
            "Tian Ye",
            "Qingyu Shi",
            "Donghao Zhou",
            "Wenhao Chai",
            "Xin Lin",
            "Jianzong Wu",
            "Chao Tang",
            "Shilin Xu",
            "Tao Zhang",
            "Haobo Yuan",
            "Yikang Zhou",
            "Wei Chow",
            "Linfeng Li",
            "Xiangtai Li",
            "Lei Zhu",
            "Lu Qi"
        ],
        "title": "An Empirical Study of GPT-4o Image Generation Capabilities",
        "abstract": "arXiv:2504.05979v1 Announce Type: new  Abstract: The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into a unified framework for those methods. In this work, we conduct an empirical study of GPT-4o's image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling.",
        "arxiv_id": "2504.05979",
        "ARXIVID": "2504.05979",
        "COMMENT": "Matches criterion 2. Conducts an empirical study on GPT-4o's image generation capabilities, which aligns with the interest in VLLMs/MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2504.05422": {
        "authors": [
            "Yue Yao",
            "Mohamed-Khalil Bouzidi",
            "Daniel Goehring",
            "Joerg Reichardt"
        ],
        "title": "EP-Diffuser: An Efficient Diffusion Model for Traffic Scene Generation and Prediction via Polynomial Representations",
        "abstract": "arXiv:2504.05422v1 Announce Type: new  Abstract: As the prediction horizon increases, predicting the future evolution of traffic scenes becomes increasingly difficult due to the multi-modal nature of agent motion. Most state-of-the-art (SotA) prediction models primarily focus on forecasting the most likely future. However, for the safe operation of autonomous vehicles, it is equally important to cover the distribution for plausible motion alternatives. To address this, we introduce EP-Diffuser, a novel parameter-efficient diffusion-based generative model designed to capture the distribution of possible traffic scene evolutions. Conditioned on road layout and agent history, our model acts as a predictor and generates diverse, plausible scene continuations. We benchmark EP-Diffuser against two SotA models in terms of accuracy and plausibility of predictions on the Argoverse 2 dataset. Despite its significantly smaller model size, our approach achieves both highly accurate and plausible traffic scene predictions. We further evaluate model generalization ability in an out-of-distribution (OoD) test setting using Waymo Open dataset and show superior robustness of our approach. The code and model checkpoints can be found here: https://github.com/continental/EP-Diffuser.",
        "arxiv_id": "2504.05422",
        "ARXIVID": "2504.05422",
        "COMMENT": "Matches criterion 3. Introduces a novel diffusion-based generative model for traffic scene prediction, which is a new method in embodied AI with a focus on traffic scenarios.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2504.06088": {
        "authors": [
            "Divyanshu Mishra",
            "Pramit Saha",
            "He Zhao",
            "Netzahualcoyotl Hernandez-Cruz",
            "Olga Patey",
            "Aris Papageorghiou",
            "J. Alison Noble"
        ],
        "title": "MCAT: Visual Query-Based Localization of Standard Anatomical Clips in Fetal Ultrasound Videos Using Multi-Tier Class-Aware Token Transformer",
        "abstract": "arXiv:2504.06088v1 Announce Type: new  Abstract: Accurate standard plane acquisition in fetal ultrasound (US) videos is crucial for fetal growth assessment, anomaly detection, and adherence to clinical guidelines. However, manually selecting standard frames is time-consuming and prone to intra- and inter-sonographer variability. Existing methods primarily rely on image-based approaches that capture standard frames and then classify the input frames across different anatomies. This ignores the dynamic nature of video acquisition and its interpretation. To address these challenges, we introduce Multi-Tier Class-Aware Token Transformer (MCAT), a visual query-based video clip localization (VQ-VCL) method, to assist sonographers by enabling them to capture a quick US sweep. By then providing a visual query of the anatomy they wish to analyze, MCAT returns the video clip containing the standard frames for that anatomy, facilitating thorough screening for potential anomalies. We evaluate MCAT on two ultrasound video datasets and a natural image VQ-VCL dataset based on Ego4D. Our model outperforms state-of-the-art methods by 10% and 13% mIoU on the ultrasound datasets and by 5.35% mIoU on the Ego4D dataset, using 96% fewer tokens. MCAT's efficiency and accuracy have significant potential implications for public health, especially in low- and middle-income countries (LMICs), where it may enhance prenatal care by streamlining standard plane acquisition, simplifying US-based screening, diagnosis and allowing sonographers to examine more patients.",
        "arxiv_id": "2504.06088",
        "ARXIVID": "2504.06088",
        "COMMENT": "Matches criterion 3. Proposes a novel method for video-based localization in fetal ultrasound using a transformer, which is a new angle in embodied AI benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2504.05673": {
        "authors": [
            "Dongjun Qian",
            "Kai Su",
            "Yiming Tan",
            "Qishuai Diao",
            "Xian Wu",
            "Chang Liu",
            "Bingyue Peng",
            "Zehuan Yuan"
        ],
        "title": "VC-LLM: Automated Advertisement Video Creation from Raw Footage using Multi-modal LLMs",
        "abstract": "arXiv:2504.05673v1 Announce Type: new  Abstract: As short videos have risen in popularity, the role of video content in advertising has become increasingly significant. Typically, advertisers record a large amount of raw footage about the product and then create numerous different short-form advertisement videos based on this raw footage. Creating such videos mainly involves editing raw footage and writing advertisement scripts, which requires a certain level of creative ability. It is usually challenging to create many different video contents for the same product, and manual efficiency is often low. In this paper, we present VC-LLM, a framework powered by Large Language Models for the automatic creation of high-quality short-form advertisement videos. Our approach leverages high-resolution spatial input and low-resolution temporal input to represent video clips more effectively, capturing both fine-grained visual details and broader temporal dynamics. In addition, during training, we incorporate supplementary information generated by rewriting the ground truth text, ensuring that all key output information can be directly traced back to the input, thereby reducing model hallucinations. We also designed a benchmark to evaluate the quality of the created videos. Experiments show that VC-LLM based on GPT-4o can produce videos comparable to those created by humans. Furthermore, we collected numerous high-quality short advertisement videos to create a pre-training dataset and manually cleaned a portion of the data to construct a high-quality fine-tuning dataset. Experiments indicate that, on the benchmark, the VC-LLM based on fine-tuned LLM can produce videos with superior narrative logic compared to those created by the VC-LLM based on GPT-4o.",
        "arxiv_id": "2504.05673",
        "ARXIVID": "2504.05673",
        "COMMENT": "Matches criterion 2. The paper introduces VC-LLM, a multi-modal large language model for automated video creation, which aligns with the interest in VLLMs/MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2504.05579": {
        "authors": [
            "Artem Zholus",
            "Carl Doersch",
            "Yi Yang",
            "Skanda Koppula",
            "Viorica Patraucean",
            "Xu Owen He",
            "Ignacio Rocco",
            "Mehdi S. M. Sajjadi",
            "Sarath Chandar",
            "Ross Goroshin"
        ],
        "title": "TAPNext: Tracking Any Point (TAP) as Next Token Prediction",
        "abstract": "arXiv:2504.05579v1 Announce Type: new  Abstract: Tracking Any Point (TAP) in a video is a challenging computer vision problem with many demonstrated applications in robotics, video editing, and 3D reconstruction. Existing methods for TAP rely heavily on complex tracking-specific inductive biases and heuristics, limiting their generality and potential for scaling. To address these challenges, we present TAPNext, a new approach that casts TAP as sequential masked token decoding. Our model is causal, tracks in a purely online fashion, and removes tracking-specific inductive biases. This enables TAPNext to run with minimal latency, and removes the temporal windowing required by many existing state of art trackers. Despite its simplicity, TAPNext achieves a new state-of-the-art tracking performance among both online and offline trackers. Finally, we present evidence that many widely used tracking heuristics emerge naturally in TAPNext through end-to-end training.",
        "arxiv_id": "2504.05579",
        "ARXIVID": "2504.05579",
        "COMMENT": "Matches criterion 4 as it introduces a novel approach to tracking any point in video using a token prediction framework, which could be relevant to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2504.05830": {
        "authors": [
            "Shiao Wang",
            "Xiao Wang",
            "Bo Jiang",
            "Lin Zhu",
            "Guoqi Li",
            "Yaowei Wang",
            "Yonghong Tian",
            "Jin Tang"
        ],
        "title": "Human Activity Recognition using RGB-Event based Sensors: A Multi-modal Heat Conduction Model and A Benchmark Dataset",
        "abstract": "arXiv:2504.05830v1 Announce Type: new  Abstract: Human Activity Recognition (HAR) primarily relied on traditional RGB cameras to achieve high-performance activity recognition. However, the challenging factors in real-world scenarios, such as insufficient lighting and rapid movements, inevitably degrade the performance of RGB cameras. To address these challenges, biologically inspired event cameras offer a promising solution to overcome the limitations of traditional RGB cameras. In this work, we rethink human activity recognition by combining the RGB and event cameras. The first contribution is the proposed large-scale multi-modal RGB-Event human activity recognition benchmark dataset, termed HARDVS 2.0, which bridges the dataset gaps. It contains 300 categories of everyday real-world actions with a total of 107,646 paired videos covering various challenging scenarios. Inspired by the physics-informed heat conduction model, we propose a novel multi-modal heat conduction operation framework for effective activity recognition, termed MMHCO-HAR. More in detail, given the RGB frames and event streams, we first extract the feature embeddings using a stem network. Then, multi-modal Heat Conduction blocks are designed to fuse the dual features, the key module of which is the multi-modal Heat Conduction Operation layer. We integrate RGB and event embeddings through a multi-modal DCT-IDCT layer while adaptively incorporating the thermal conductivity coefficient via FVEs into this module. After that, we propose an adaptive fusion module based on a policy routing strategy for high-performance classification. Comprehensive experiments demonstrate that our method consistently performs well, validating its effectiveness and robustness. The source code and benchmark dataset will be released on https://github.com/Event-AHU/HARDVS/tree/HARDVSv2",
        "arxiv_id": "2504.05830",
        "ARXIVID": "2504.05830",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset (HARDVS 2.0) for human activity recognition using RGB and event cameras, which is relevant to embodied AI benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2504.06256": {
        "authors": [
            "Xichen Pan",
            "Satya Narayan Shukla",
            "Aashu Singh",
            "Zhuokai Zhao",
            "Shlok Kumar Mishra",
            "Jialiang Wang",
            "Zhiyang Xu",
            "Jiuhai Chen",
            "Kunpeng Li",
            "Felix Juefei-Xu",
            "Ji Hou",
            "Saining Xie"
        ],
        "title": "Transfer between Modalities with MetaQueries",
        "abstract": "arXiv:2504.06256v1 Announce Type: new  Abstract: Unified multimodal models aim to integrate understanding (text output) and generation (pixel output), but aligning these different modalities within a single architecture often demands complex training recipes and careful data balancing. We introduce MetaQueries, a set of learnable queries that act as an efficient interface between autoregressive multimodal LLMs (MLLMs) and diffusion models. MetaQueries connects the MLLM's latents to the diffusion decoder, enabling knowledge-augmented image generation by leveraging the MLLM's deep understanding and reasoning capabilities. Our method simplifies training, requiring only paired image-caption data and standard diffusion objectives. Notably, this transfer is effective even when the MLLM backbone remains frozen, thereby preserving its state-of-the-art multimodal understanding capabilities while achieving strong generative performance. Additionally, our method is flexible and can be easily instruction-tuned for advanced applications such as image editing and subject-driven generation.",
        "arxiv_id": "2504.06256",
        "ARXIVID": "2504.06256",
        "COMMENT": "Matches criterion 2 as it introduces MetaQueries, a novel interface between multimodal LLMs and diffusion models for knowledge-augmented image generation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2504.05706": {
        "authors": [
            "Fida Mohammad Thoker",
            "Letian Jiang",
            "Chen Zhao",
            "Piyush Bagad",
            "Hazel Doughty",
            "Bernard Ghanem",
            "Cees G. M. Snoek"
        ],
        "title": "SEVERE++: Evaluating Benchmark Sensitivity in Generalization of Video Representation Learning",
        "abstract": "arXiv:2504.05706v1 Announce Type: new  Abstract: Continued advances in self-supervised learning have led to significant progress in video representation learning, offering a scalable alternative to supervised approaches by removing the need for manual annotations. Despite strong performance on standard action recognition benchmarks, video self-supervised learning methods are largely evaluated under narrow protocols, typically pretraining on Kinetics-400 and fine-tuning on similar datasets, limiting our understanding of their generalization in real world scenarios. In this work, we present a comprehensive evaluation of modern video self-supervised models, focusing on generalization across four key downstream factors: domain shift, sample efficiency, action granularity, and task diversity. Building on our prior work analyzing benchmark sensitivity in CNN-based contrastive learning, we extend the study to cover state-of-the-art transformer-based video-only and video-text models. Specifically, we benchmark 12 transformer-based methods (7 video-only, 5 video-text) and compare them to 10 CNN-based methods, totaling over 1100 experiments across 8 datasets and 7 downstream tasks. Our analysis shows that, despite architectural advances, transformer-based models remain sensitive to downstream conditions. No method generalizes consistently across all factors, video-only transformers perform better under domain shifts, CNNs outperform for fine-grained tasks, and video-text models often underperform despite large scale pretraining. We also find that recent transformer models do not consistently outperform earlier approaches. Our findings provide a detailed view of the strengths and limitations of current video SSL methods and offer a unified benchmark for evaluating generalization in video representation learning.",
        "arxiv_id": "2504.05706",
        "ARXIVID": "2504.05706",
        "COMMENT": "Matches criterion 4 as it evaluates video representation learning methods, which are foundational for vision tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.05682": {
        "authors": [
            "Xiaxu Chen",
            "Wei Li",
            "Chunxu Liu",
            "Chi Xie",
            "Xiaoyan Hu",
            "Chengqian Ma",
            "Feng Zhu",
            "Rui Zhao"
        ],
        "title": "On the Suitability of Reinforcement Fine-Tuning to Visual Tasks",
        "abstract": "arXiv:2504.05682v1 Announce Type: new  Abstract: Reinforcement Fine-Tuning (RFT) is proved to be greatly valuable for enhancing the reasoning ability of LLMs. Researchers have been starting to apply RFT to MLLMs, hoping it will also enhance the capabilities of visual understanding. However, these works are at a very early stage and have not examined how suitable RFT actually is for visual tasks. In this work, we endeavor to understand the suitabilities and limitations of RFT for visual tasks, through experimental analysis and observations. We start by quantitative comparisons on various tasks, which shows RFT is generally better than SFT on visual tasks. %especially when the number of training samples are limited. To check whether such advantages are brought up by the reasoning process, we design a new reward that encourages the model to ``think'' more, whose results show more thinking can be beneficial for complicated tasks but harmful for simple tasks. We hope this study can provide more insight for the rapid advancements on this topic.",
        "arxiv_id": "2504.05682",
        "ARXIVID": "2504.05682",
        "COMMENT": "Matches criterion 2 as it explores reinforcement fine-tuning for MLLMs and their visual reasoning capabilities.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.05882": {
        "authors": [
            "Luca Barco",
            "Giacomo Blanco",
            "Gaetano Chiriaco",
            "Alessia Intini",
            "Luigi La Riccia",
            "Vittorio Scolamiero",
            "Piero Boccardo",
            "Paolo Garza",
            "Fabrizio Dominici"
        ],
        "title": "Turin3D: Evaluating Adaptation Strategies under Label Scarcity in Urban LiDAR Segmentation with Semi-Supervised Techniques",
        "abstract": "arXiv:2504.05882v1 Announce Type: new  Abstract: 3D semantic segmentation plays a critical role in urban modelling, enabling detailed understanding and mapping of city environments. In this paper, we introduce Turin3D: a new aerial LiDAR dataset for point cloud semantic segmentation covering an area of around 1.43 km2 in the city centre of Turin with almost 70M points. We describe the data collection process and compare Turin3D with others previously proposed in the literature. We did not fully annotate the dataset due to the complexity and time-consuming nature of the process; however, a manual annotation process was performed on the validation and test sets, to enable a reliable evaluation of the proposed techniques. We first benchmark the performances of several point cloud semantic segmentation models, trained on the existing datasets, when tested on Turin3D, and then improve their performances by applying a semi-supervised learning technique leveraging the unlabelled training set. The dataset will be publicly available to support research in outdoor point cloud segmentation, with particular relevance for self-supervised and semi-supervised learning approaches given the absence of ground truth annotations for the training set.",
        "arxiv_id": "2504.05882",
        "ARXIVID": "2504.05882",
        "COMMENT": "Matches criterion 3 as it introduces a new dataset (Turin3D) for urban LiDAR segmentation and explores semi-supervised learning techniques, which is relevant to embodied AI benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.05794": {
        "authors": [
            "Leiye Liu",
            "Miao Zhang",
            "Jihao Yin",
            "Tingwei Liu",
            "Wei Ji",
            "Yongri Piao",
            "Huchuan Lu"
        ],
        "title": "DefMamba: Deformable Visual State Space Model",
        "abstract": "arXiv:2504.05794v1 Announce Type: new  Abstract: Recently, state space models (SSM), particularly Mamba, have attracted significant attention from scholars due to their ability to effectively balance computational efficiency and performance. However, most existing visual Mamba methods flatten images into 1D sequences using predefined scan orders, which results the model being less capable of utilizing the spatial structural information of the image during the feature extraction process. To address this issue, we proposed a novel visual foundation model called DefMamba. This model includes a multi-scale backbone structure and deformable mamba (DM) blocks, which dynamically adjust the scanning path to prioritize important information, thus enhancing the capture and processing of relevant input features. By combining a deformable scanning(DS) strategy, this model significantly improves its ability to learn image structures and detects changes in object details. Numerous experiments have shown that DefMamba achieves state-of-the-art performance in various visual tasks, including image classification, object detection, instance segmentation, and semantic segmentation. The code is open source on DefMamba.",
        "arxiv_id": "2504.05794",
        "ARXIVID": "2504.05794",
        "COMMENT": "Matches criterion 4 as it introduces DefMamba, a visual foundation model with deformable scanning for various vision tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.06220": {
        "authors": [
            "Xiaoxing Hu",
            "Ziyang Gong",
            "Yupei Wang",
            "Yuru Jia",
            "Gen Luo",
            "Xue Yang"
        ],
        "title": "Earth-Adapter: Bridge the Geospatial Domain Gaps with Mixture of Frequency Adaptation",
        "abstract": "arXiv:2504.06220v1 Announce Type: new  Abstract: Parameter-Efficient Fine-Tuning (PEFT) is a technique that allows us to adapt powerful Foundation Models (FMs) to diverse downstream tasks while preserving and unleashing their inherent capabilities. However, we have observed that existing PEFT methods, which are often designed with natural imagery in mind, struggle when applied to Remote Sensing (RS) scenarios. This is primarily due to their inability to handle artifact influences, a problem particularly severe in RS image features. To tackle this challenge, we introduce Earth-Adapter, the first PEFT method specifically designed for RS artifacts conquering. Earth-Adapter introduces a novel Mixture of Frequency Adaptation process that combines a Mixture of Adapter (MoA) with Discrete Fourier Transformation (DFT). By utilizing DFT, Earth-Adapter can decompose features into different frequency components, precisely separating artifacts from original features. The MoA then dynamically assigns weights to each adapter expert, allowing for the combination of features across various frequency domains. These simple-yet-effective approaches enable Earth-Adapter to more efficiently overcome the disturbances caused by artifacts than previous PEFT methods, significantly enhancing the FMs' performance on RS scenarios. Experiments on Domain Adaptation (DA), and Domain Generalization (DG) semantic segmentation benchmarks showcase the Earth-Adapter's effectiveness. Compared with baseline Rein, Earth-Adapter significantly improves 9.0% mIoU in DA and 3.1% mIoU in DG benchmarks. Our code will be released at https://github.com/VisionXLab/Earth-Adapter.",
        "arxiv_id": "2504.06220",
        "ARXIVID": "2504.06220",
        "COMMENT": "Partially matches criterion 4 as it discusses foundation models and their adaptation to remote sensing, but not directly related to vision foundation models in general applications.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2504.06153": {
        "authors": [
            "Akash Kumar",
            "Ashlesha Kumar",
            "Vibhav Vineet",
            "Yogesh S Rawat"
        ],
        "title": "A Large-Scale Analysis on Contextual Self-Supervised Video Representation Learning",
        "abstract": "arXiv:2504.06153v1 Announce Type: new  Abstract: Self-supervised learning has emerged as a powerful paradigm for label-free model pretraining, particularly in the video domain, where manual annotation is costly and time-intensive. However, existing self-supervised approaches employ diverse experimental setups, making direct comparisons challenging due to the absence of a standardized benchmark. In this work, we establish a unified benchmark that enables fair comparisons across different methods. Additionally, we systematically investigate five critical aspects of self-supervised learning in videos: (1) dataset size, (2) model complexity, (3) data distribution, (4) data noise, and (5) feature representations. To facilitate this study, we evaluate six self-supervised learning methods across six network architectures, conducting extensive experiments on five benchmark datasets and assessing performance on two distinct downstream tasks. Our analysis reveals key insights into the interplay between pretraining strategies, dataset characteristics, pretext tasks, and model architectures. Furthermore, we extend these findings to Video Foundation Models (ViFMs), demonstrating their relevance in large-scale video representation learning. Finally, leveraging these insights, we propose a novel approach that significantly reduces training data requirements while surpassing state-of-the-art methods that rely on 10% more pretraining data. We believe this work will guide future research toward a deeper understanding of self-supervised video representation learning and its broader implications.",
        "arxiv_id": "2504.06153",
        "ARXIVID": "2504.06153",
        "COMMENT": "Matches criterion 4 as it discusses Video Foundation Models (ViFMs) and their role in self-supervised video representation learning.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2504.05613": {
        "authors": [
            "Xiao Zhang",
            "Xiangyu Han",
            "Xiwen Lai",
            "Yao Sun",
            "Pei Zhang",
            "Konrad Kording"
        ],
        "title": "Falcon: Fractional Alternating Cut with Overcoming Minima in Unsupervised Segmentation",
        "abstract": "arXiv:2504.05613v1 Announce Type: new  Abstract: Today's unsupervised image segmentation algorithms often segment suboptimally. Modern graph-cut based approaches rely on high-dimensional attention maps from Transformer-based foundation models, typically employing a relaxed Normalized Cut solved recursively via the Fiedler vector (the eigenvector of the second smallest eigenvalue). Consequently, they still lag behind supervised methods in both mask generation speed and segmentation accuracy. We present a regularized fractional alternating cut (Falcon), an optimization-based K-way Normalized Cut without relying on recursive eigenvector computations, achieving substantially improved speed and accuracy. Falcon operates in two stages: (1) a fast K-way Normalized Cut solved by extending into a fractional quadratic transformation, with an alternating iterative procedure and regularization to avoid local minima; and (2) refinement of the resulting masks using complementary low-level information, producing high-quality pixel-level segmentations. Experiments show that Falcon not only surpasses existing state-of-the-art methods by an average of 2.5% across six widely recognized benchmarks (reaching up to 4.3\\% improvement on Cityscapes), but also reduces runtime by around 30% compared to prior graph-based approaches. These findings demonstrate that the semantic information within foundation-model attention can be effectively harnessed by a highly parallelizable graph cut framework. Consequently, Falcon can narrow the gap between unsupervised and supervised segmentation, enhancing scalability in real-world applications and paving the way for dense prediction-based vision pre-training in various downstream tasks. The code is released in https://github.com/KordingLab/Falcon.",
        "arxiv_id": "2504.05613",
        "ARXIVID": "2504.05613",
        "COMMENT": "Matches criterion 4 as it introduces Falcon, a novel unsupervised segmentation method leveraging foundation model attention maps.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2504.05541": {
        "authors": [
            "Yunlong Tang",
            "Jing Bi",
            "Chao Huang",
            "Susan Liang",
            "Daiki Shimada",
            "Hang Hua",
            "Yunzhong Xiao",
            "Yizhi Song",
            "Pinxin Liu",
            "Mingqian Feng",
            "Junjia Guo",
            "Zhuo Liu",
            "Luchuan Song",
            "Ali Vosoughi",
            "Jinxi He",
            "Liu He",
            "Zeliang Zhang",
            "Jiebo Luo",
            "Chenliang Xu"
        ],
        "title": "Caption Anything in Video: Fine-grained Object-centric Captioning via Spatiotemporal Multimodal Prompting",
        "abstract": "arXiv:2504.05541v1 Announce Type: new  Abstract: We present CAT-V (Caption AnyThing in Video), a training-free framework for fine-grained object-centric video captioning that enables detailed descriptions of user-selected objects through time. CAT-V integrates three key components: a Segmenter based on SAMURAI for precise object segmentation across frames, a Temporal Analyzer powered by TRACE-Uni for accurate event boundary detection and temporal analysis, and a Captioner using InternVL-2.5 for generating detailed object-centric descriptions. Through spatiotemporal visual prompts and chain-of-thought reasoning, our framework generates detailed, temporally-aware descriptions of objects' attributes, actions, statuses, interactions, and environmental contexts without requiring additional training data. CAT-V supports flexible user interactions through various visual prompts (points, bounding boxes, and irregular regions) and maintains temporal sensitivity by tracking object states and interactions across different time segments. Our approach addresses limitations of existing video captioning methods, which either produce overly abstract descriptions or lack object-level precision, enabling fine-grained, object-specific descriptions while maintaining temporal coherence and spatial accuracy. The GitHub repository for this project is available at https://github.com/yunlong10/CAT-V",
        "arxiv_id": "2504.05541",
        "ARXIVID": "2504.05541",
        "COMMENT": "Matches criterion 2 as it introduces CAT-V, a framework for fine-grained object-centric video captioning using multimodal prompting.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2504.06010": {
        "authors": [
            "Stefanos-Iordanis Papadopoulos",
            "Christos Koutlis",
            "Symeon Papadopoulos",
            "Panagiotis C. Petrantonakis"
        ],
        "title": "Latent Multimodal Reconstruction for Misinformation Detection",
        "abstract": "arXiv:2504.06010v1 Announce Type: new  Abstract: Multimodal misinformation, such as miscaptioned images, where captions misrepresent an image's origin, context, or meaning, poses a growing challenge in the digital age. To support fact-checkers, researchers have been focusing on creating datasets and developing methods for multimodal misinformation detection (MMD). Due to the scarcity of large-scale annotated MMD datasets, recent studies leverage synthetic training data via out-of-context image-caption pairs or named entity manipulations; altering names, dates, and locations. However, these approaches often produce simplistic misinformation that fails to reflect real-world complexity, limiting the robustness of detection models trained on them. Meanwhile, despite recent advancements, Large Vision-Language Models (LVLMs) remain underutilized for generating diverse, realistic synthetic training data for MMD. To address this gap, we introduce \"MisCaption This!\", a training dataset comprising LVLM-generated miscaptioned images. Additionally, we introduce \"Latent Multimodal Reconstruction\" (LAMAR), a network trained to reconstruct the embeddings of truthful captions, providing a strong auxiliary signal to the detection process. To optimize LAMAR, we explore different training strategies (end-to-end training and large-scale pre-training) and integration approaches (direct, mask, gate, and attention). Extensive experiments show that models trained on \"MisCaption This!\" generalize better on real-world misinformation, while LAMAR sets new state-of-the-art on both NewsCLIPpings and VERITE benchmarks; highlighting the potential of LVLM-generated data and reconstruction-based approaches for advancing MMD. We release our code at: https://github.com/stevejpapad/miscaptioned-image-reconstruction",
        "arxiv_id": "2504.06010",
        "ARXIVID": "2504.06010",
        "COMMENT": "Partially matches criterion 2 as it explores LVLMs for generating synthetic data and improving multimodal misinformation detection, but not focused on new VLLMs or MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.05575": {
        "authors": [
            "Belal Alsinglawi",
            "Chris McCarthy",
            "Sara Webb",
            "Christopher Fluke",
            "Navid Toosy Saidy"
        ],
        "title": "A Lightweight Large Vision-language Model for Multimodal Medical Images",
        "abstract": "arXiv:2504.05575v1 Announce Type: new  Abstract: Medical Visual Question Answering (VQA) enhances clinical decision-making by enabling systems to interpret medical images and answer clinical queries. However, developing efficient, high-performance VQA models is challenging due to the complexity of medical imagery and diverse modalities. In this paper, we introduce a lightweight, multimodal VQA model integrating BiomedCLIP for image feature extraction and LLaMA-3 for text processing. Designed for medical VQA tasks, our model achieves state-of-the-art performance on the OmniMedVQA dataset. With approximately 8 billion parameters, it requires only two NVIDIA 40 GB A100 GPUs, demonstrating superior efficiency over larger models. Our results show 73.4% accuracy for open-end questions, surpassing existing models and validating its potential for real-world medical applications. Key contributions include a specialized multimodal VQA model, a resource-efficient architecture, and strong performance in answering open-ended clinical questions.",
        "arxiv_id": "2504.05575",
        "ARXIVID": "2504.05575",
        "COMMENT": "Matches criterion 2 as it introduces a lightweight multimodal VQA model for medical images, integrating BiomedCLIP and LLaMA-3.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.06196": {
        "authors": [
            "Eric Wang",
            "Samuel Schmidgall",
            "Paul F. Jaeger",
            "Fan Zhang",
            "Rory Pilgrim",
            "Yossi Matias",
            "Joelle Barral",
            "David Fleet",
            "Shekoofeh Azizi"
        ],
        "title": "TxGemma: Efficient and Agentic LLMs for Therapeutics",
        "abstract": "arXiv:2504.06196v1 Announce Type: new  Abstract: Therapeutic development is a costly and high-risk endeavor that is often plagued by high failure rates. To address this, we introduce TxGemma, a suite of efficient, generalist large language models (LLMs) capable of therapeutic property prediction as well as interactive reasoning and explainability. Unlike task-specific models, TxGemma synthesizes information from diverse sources, enabling broad application across the therapeutic development pipeline. The suite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a comprehensive dataset of small molecules, proteins, nucleic acids, diseases, and cell lines. Across 66 therapeutic development tasks, TxGemma achieved superior or comparable performance to the state-of-the-art generalist model on 64 (superior on 45), and against state-of-the-art specialist models on 50 (superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks, such as clinical trial adverse event prediction, requires less training data than fine-tuning base LLMs, making TxGemma suitable for data-limited applications. Beyond these predictive capabilities, TxGemma features conversational models that bridge the gap between general LLMs and specialized property predictors. These allow scientists to interact in natural language, provide mechanistic reasoning for predictions based on molecular structure, and engage in scientific discussions. Building on this, we further introduce Agentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that reasons, acts, manages diverse workflows, and acquires external domain knowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last Exam benchmark (Chemistry & Biology) with 52.3% relative improvement over o3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels with improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over o3-mini (high).",
        "arxiv_id": "2504.06196",
        "ARXIVID": "2504.06196",
        "COMMENT": "Does not match any specific criteria. Focuses on therapeutic development using LLMs, which is not directly related to spatial intelligence, VLLMs, MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.05774": {
        "authors": [
            "Enming Zhang",
            "Zhengyu Li",
            "Yanru Wu",
            "Jingge Wang",
            "Yang Tan",
            "Ruizhe Zhao",
            "Guan Wang",
            "Yang Li"
        ],
        "title": "Transferable Mask Transformer: Cross-domain Semantic Segmentation with Region-adaptive Transferability Estimation",
        "abstract": "arXiv:2504.05774v1 Announce Type: new  Abstract: Recent advances in Vision Transformers (ViTs) have set new benchmarks in semantic segmentation. However, when adapting pretrained ViTs to new target domains, significant performance degradation often occurs due to distribution shifts, resulting in suboptimal global attention. Since self-attention mechanisms are inherently data-driven, they may fail to effectively attend to key objects when source and target domains exhibit differences in texture, scale, or object co-occurrence patterns. While global and patch-level domain adaptation methods provide partial solutions, region-level adaptation with dynamically shaped regions is crucial due to spatial heterogeneity in transferability across different image areas. We present Transferable Mask Transformer (TMT), a novel region-level adaptation framework for semantic segmentation that aligns cross-domain representations through spatial transferability analysis. TMT consists of two key components: (1) An Adaptive Cluster-based Transferability Estimator (ACTE) that dynamically segments images into structurally and semantically coherent regions for localized transferability assessment, and (2) A Transferable Masked Attention (TMA) module that integrates region-specific transferability maps into ViTs' attention mechanisms, prioritizing adaptation in regions with low transferability and high semantic uncertainty. Comprehensive evaluations across 20 cross-domain pairs demonstrate TMT's superiority, achieving an average 2% MIoU improvement over vanilla fine-tuning and a 1.28% increase compared to state-of-the-art baselines. The source code will be publicly available.",
        "arxiv_id": "2504.05774",
        "ARXIVID": "2504.05774",
        "COMMENT": "Does not match any specific criterion but is related to semantic segmentation and transfer learning, which are tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.06027": {
        "authors": [
            "Xiaochen Wei",
            "Weiwei Guo",
            "Wenxian Yu",
            "Feiming Wei",
            "Dongying Li"
        ],
        "title": "OSDM-MReg: Multimodal Image Registration based One Step Diffusion Model",
        "abstract": "arXiv:2504.06027v1 Announce Type: new  Abstract: Multimodal remote sensing image registration aligns images from different sensors for data fusion and analysis. However, current methods often fail to extract modality-invariant features when aligning image pairs with large nonlinear radiometric differences. To address this issues, we propose OSDM-MReg, a novel multimodal image registration framework based image-to-image translation to eliminate the gap of multimodal images. Firstly, we propose a novel one-step unaligned target-guided conditional denoising diffusion probabilistic models(UTGOS-CDDPM)to translate multimodal images into a unified domain. In the inference stage, traditional conditional DDPM generate translated source image by a large number of iterations, which severely slows down the image registration task. To address this issues, we use the unaligned traget image as a condition to promote the generation of low-frequency features of the translated source image. Furthermore, during the training stage, we add the inverse process of directly predicting the translated image to ensure that the translated source image can be generated in one step during the testing stage. Additionally, to supervised the detail features of translated source image, we propose a new perceptual loss that focuses on the high-frequency feature differences between the translated and ground-truth images. Finally, a multimodal multiscale image registration network (MM-Reg) fuse the multimodal feature of the unimodal images and multimodal images by proposed multimodal feature fusion strategy. Experiments demonstrate superior accuracy and efficiency across various multimodal registration tasks, particularly for SAR-optical image pairs.",
        "arxiv_id": "2504.06027",
        "ARXIVID": "2504.06027",
        "COMMENT": "Does not match any specific criterion but is related to multimodal image registration, which is tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.05649": {
        "authors": [
            "Yining Shi",
            "Kun Jiang",
            "Xin Zhao",
            "Kangan Qian",
            "Chuchu Xie",
            "Tuopu Wen",
            "Mengmeng Yang",
            "Diange Yang"
        ],
        "title": "POD: Predictive Object Detection with Single-Frame FMCW LiDAR Point Cloud",
        "abstract": "arXiv:2504.05649v1 Announce Type: new  Abstract: LiDAR-based 3D object detection is a fundamental task in the field of autonomous driving. This paper explores the unique advantage of Frequency Modulated Continuous Wave (FMCW) LiDAR in autonomous perception. Given a single frame FMCW point cloud with radial velocity measurements, we expect that our object detector can detect the short-term future locations of objects using only the current frame sensor data and demonstrate a fast ability to respond to intermediate danger. To achieve this, we extend the standard object detection task to a novel task named predictive object detection (POD), which aims to predict the short-term future location and dimensions of objects based solely on current observations. Typically, a motion prediction task requires historical sensor information to process the temporal contexts of each object, while our detector's avoidance of multi-frame historical information enables a much faster response time to potential dangers. The core advantage of FMCW LiDAR lies in the radial velocity associated with every reflected point. We propose a novel POD framework, the core idea of which is to generate a virtual future point using a ray casting mechanism, create virtual two-frame point clouds with the current and virtual future frames, and encode these two-frame voxel features with a sparse 4D encoder. Subsequently, the 4D voxel features are separated by temporal indices and remapped into two Bird's Eye View (BEV) features: one decoded for standard current frame object detection and the other for future predictive object detection. Extensive experiments on our in-house dataset demonstrate the state-of-the-art standard and predictive detection performance of the proposed POD framework.",
        "arxiv_id": "2504.05649",
        "ARXIVID": "2504.05649",
        "COMMENT": "Does not match any specific criteria. Focuses on predictive object detection using FMCW LiDAR, which is not directly related to embodied AI or spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.05601": {
        "authors": [
            "Zhenteng Li",
            "Sheng Lian",
            "Dengfeng Pan",
            "Youlin Wang",
            "Wei Liu"
        ],
        "title": "AD-Det: Boosting Object Detection in UAV Images with Focused Small Objects and Balanced Tail Classes",
        "abstract": "arXiv:2504.05601v1 Announce Type: new  Abstract: Object detection in Unmanned Aerial Vehicle (UAV) images poses significant challenges due to complex scale variations and class imbalance among objects. Existing methods often address these challenges separately, overlooking the intricate nature of UAV images and the potential synergy between them. In response, this paper proposes AD-Det, a novel framework employing a coherent coarse-to-fine strategy that seamlessly integrates two pivotal components: Adaptive Small Object Enhancement (ASOE) and Dynamic Class-balanced Copy-paste (DCC). ASOE utilizes a high-resolution feature map to identify and cluster regions containing small objects. These regions are subsequently enlarged and processed by a fine-grained detector. On the other hand, DCC conducts object-level resampling by dynamically pasting tail classes around the cluster centers obtained by ASOE, main-taining a dynamic memory bank for each tail class. This approach enables AD-Det to not only extract regions with small objects for precise detection but also dynamically perform reasonable resampling for tail-class objects. Consequently, AD-Det enhances the overall detection performance by addressing the challenges of scale variations and class imbalance in UAV images through a synergistic and adaptive framework. We extensively evaluate our approach on two public datasets, i.e., VisDrone and UAVDT, and demonstrate that AD-Det significantly outperforms existing competitive alternatives. Notably, AD-Det achieves a 37.5% Average Precision (AP) on the VisDrone dataset, surpassing its counterparts by at least 3.1%.",
        "arxiv_id": "2504.05601",
        "ARXIVID": "2504.05601",
        "COMMENT": "Does not match any specific criteria. Focuses on object detection in UAV images with a novel framework for small objects and class imbalance.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.05451": {
        "authors": [
            "Arjun Somayazulu",
            "Efi Mavroudi",
            "Changan Chen",
            "Lorenzo Torresani",
            "Kristen Grauman"
        ],
        "title": "Learning Activity View-invariance Under Extreme Viewpoint Changes via Curriculum Knowledge Distillation",
        "abstract": "arXiv:2504.05451v1 Announce Type: new  Abstract: Traditional methods for view-invariant learning from video rely on controlled multi-view settings with minimal scene clutter. However, they struggle with in-the-wild videos that exhibit extreme viewpoint differences and share little visual content. We introduce a method for learning rich video representations in the presence of such severe view-occlusions. We first define a geometry-based metric that ranks views at a fine-grained temporal scale by their likely occlusion level. Then, using those rankings, we formulate a knowledge distillation objective that preserves action-centric semantics with a novel curriculum learning procedure that pairs incrementally more challenging views over time, thereby allowing smooth adaptation to extreme viewpoint differences. We evaluate our approach on two tasks, outperforming SOTA models on both temporal keystep grounding and fine-grained keystep recognition benchmarks - particularly on views that exhibit severe occlusion.",
        "arxiv_id": "2504.05451",
        "ARXIVID": "2504.05451",
        "COMMENT": "Does not match any specific criteria. Focuses on view-invariant learning for videos, which is tangentially related to spatial intelligence but not embodied agents.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.05583": {
        "authors": [
            "Jiahang Li",
            "Shibo Xue",
            "Yong Su"
        ],
        "title": "Gaze-Guided Learning: Avoiding Shortcut Bias in Visual Classification",
        "abstract": "arXiv:2504.05583v1 Announce Type: new  Abstract: Inspired by human visual attention, deep neural networks have widely adopted attention mechanisms to learn locally discriminative attributes for challenging visual classification tasks. However, existing approaches primarily emphasize the representation of such features while neglecting their precise localization, which often leads to misclassification caused by shortcut biases. This limitation becomes even more pronounced when models are evaluated on transfer or out-of-distribution datasets. In contrast, humans are capable of leveraging prior object knowledge to quickly localize and compare fine-grained attributes, a capability that is especially crucial in complex and high-variance classification scenarios. Motivated by this, we introduce Gaze-CIFAR-10, a human gaze time-series dataset, along with a dual-sequence gaze encoder that models the precise sequential localization of human attention on distinct local attributes. In parallel, a Vision Transformer (ViT) is employed to learn the sequential representation of image content. Through cross-modal fusion, our framework integrates human gaze priors with machine-derived visual sequences, effectively correcting inaccurate localization in image feature representations. Extensive qualitative and quantitative experiments demonstrate that gaze-guided cognitive cues significantly enhance classification accuracy.",
        "arxiv_id": "2504.05583",
        "ARXIVID": "2504.05583",
        "COMMENT": "Does not directly match any specific criterion but is tangentially related to computer vision and machine learning through gaze-guided learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.05795": {
        "authors": [
            "Hao Zhang",
            "Yanping Zha",
            "Qingwei Zhuang",
            "Zhenfeng Shao",
            "Jiayi Ma"
        ],
        "title": "Robust Fusion Controller: Degradation-aware Image Fusion with Fine-grained Language Instructions",
        "abstract": "arXiv:2504.05795v1 Announce Type: new  Abstract: Current image fusion methods struggle to adapt to real-world environments encompassing diverse degradations with spatially varying characteristics. To address this challenge, we propose a robust fusion controller (RFC) capable of achieving degradation-aware image fusion through fine-grained language instructions, ensuring its reliable application in adverse environments. Specifically, RFC first parses language instructions to innovatively derive the functional condition and the spatial condition, where the former specifies the degradation type to remove, while the latter defines its spatial coverage. Then, a composite control priori is generated through a multi-condition coupling network, achieving a seamless transition from abstract language instructions to latent control variables. Subsequently, we design a hybrid attention-based fusion network to aggregate multi-modal information, in which the obtained composite control priori is deeply embedded to linearly modulate the intermediate fused features. To ensure the alignment between language instructions and control outcomes, we introduce a novel language-feature alignment loss, which constrains the consistency between feature-level gains and the composite control priori. Extensive experiments on publicly available datasets demonstrate that our RFC is robust against various composite degradations, particularly in highly challenging flare scenarios.",
        "arxiv_id": "2504.05795",
        "ARXIVID": "2504.05795",
        "COMMENT": "Does not match any specific criteria. Focuses on degradation-aware image fusion with language instructions, which is not directly related to spatial intelligence, VLLMs, MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.05746": {
        "authors": [
            "Zhihua Xu",
            "Tianshui Chen",
            "Zhijing Yang",
            "Siyuan Peng",
            "Keze Wang",
            "Liang Lin"
        ],
        "title": "Exploiting Temporal Audio-Visual Correlation Embedding for Audio-Driven One-Shot Talking Head Animation",
        "abstract": "arXiv:2504.05746v1 Announce Type: new  Abstract: The paramount challenge in audio-driven One-shot Talking Head Animation (ADOS-THA) lies in capturing subtle imperceptible changes between adjacent video frames. Inherently, the temporal relationship of adjacent audio clips is highly correlated with that of the corresponding adjacent video frames, offering supplementary information that can be pivotal for guiding and supervising talking head animations. In this work, we propose to learn audio-visual correlations and integrate the correlations to help enhance feature representation and regularize final generation by a novel Temporal Audio-Visual Correlation Embedding (TAVCE) framework. Specifically, it first learns an audio-visual temporal correlation metric, ensuring the temporal audio relationships of adjacent clips are aligned with the temporal visual relationships of corresponding adjacent video frames. Since the temporal audio relationship contains aligned information about the visual frame, we first integrate it to guide learning more representative features via a simple yet effective channel attention mechanism. During training, we also use the alignment correlations as an additional objective to supervise generating visual frames. We conduct extensive experiments on several publicly available benchmarks (i.e., HDTF, LRW, VoxCeleb1, and VoxCeleb2) to demonstrate its superiority over existing leading algorithms.",
        "arxiv_id": "2504.05746",
        "ARXIVID": "2504.05746",
        "COMMENT": "Does not match any specific criterion but is tangentially related to generative modeling in audio-visual tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.05700": {
        "authors": [
            "Seth Z. Zhao",
            "Reza Ghoddoosian",
            "Isht Dwivedi",
            "Nakul Agarwal",
            "Behzad Dariush"
        ],
        "title": "Pose-Aware Weakly-Supervised Action Segmentation",
        "abstract": "arXiv:2504.05700v1 Announce Type: new  Abstract: Understanding human behavior is an important problem in the pursuit of visual intelligence. A challenge in this endeavor is the extensive and costly effort required to accurately label action segments. To address this issue, we consider learning methods that demand minimal supervision for segmentation of human actions in long instructional videos. Specifically, we introduce a weakly-supervised framework that uniquely incorporates pose knowledge during training while omitting its use during inference, thereby distilling pose knowledge pertinent to each action component. We propose a pose-inspired contrastive loss as a part of the whole weakly-supervised framework which is trained to distinguish action boundaries more effectively. Our approach, validated through extensive experiments on representative datasets, outperforms previous state-of-the-art (SOTA) in segmenting long instructional videos under both online and offline settings. Additionally, we demonstrate the framework's adaptability to various segmentation backbones and pose extractors across different datasets.",
        "arxiv_id": "2504.05700",
        "ARXIVID": "2504.05700",
        "COMMENT": "Does not match any specific criterion but is tangentially related to visual intelligence and segmentation tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.05594": {
        "authors": [
            "Qi Mao",
            "Lan Chen",
            "Yuchao Gu",
            "Mike Zheng Shou",
            "Ming-Hsuan Yang"
        ],
        "title": "Tuning-Free Image Editing with Fidelity and Editability via Unified Latent Diffusion Model",
        "abstract": "arXiv:2504.05594v1 Announce Type: new  Abstract: Balancing fidelity and editability is essential in text-based image editing (TIE), where failures commonly lead to over- or under-editing issues. Existing methods typically rely on attention injections for structure preservation and leverage the inherent text alignment capabilities of pre-trained text-to-image (T2I) models for editability, but they lack explicit and unified mechanisms to properly balance these two objectives. In this work, we introduce UnifyEdit, a tuning-free method that performs diffusion latent optimization to enable a balanced integration of fidelity and editability within a unified framework. Unlike direct attention injections, we develop two attention-based constraints: a self-attention (SA) preservation constraint for structural fidelity, and a cross-attention (CA) alignment constraint to enhance text alignment for improved editability. However, simultaneously applying both constraints can lead to gradient conflicts, where the dominance of one constraint results in over- or under-editing. To address this challenge, we introduce an adaptive time-step scheduler that dynamically adjusts the influence of these constraints, guiding the diffusion latent toward an optimal balance. Extensive quantitative and qualitative experiments validate the effectiveness of our approach, demonstrating its superiority in achieving a robust balance between structure preservation and text alignment across various editing tasks, outperforming other state-of-the-art methods. The source code will be available at https://github.com/CUC-MIPG/UnifyEdit.",
        "arxiv_id": "2504.05594",
        "ARXIVID": "2504.05594",
        "COMMENT": "Does not match any specific criteria. Focuses on text-based image editing with diffusion latent optimization.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.06144": {
        "authors": [
            "Jihun Park",
            "Jongmin Gim",
            "Kyoungmin Lee",
            "Minseok Oh",
            "Minwoo Choi",
            "Jaeyeul Kim",
            "Woo Chool Park",
            "Sunghoon Im"
        ],
        "title": "A Training-Free Style-aligned Image Generation with Scale-wise Autoregressive Model",
        "abstract": "arXiv:2504.06144v1 Announce Type: new  Abstract: We present a training-free style-aligned image generation method that leverages a scale-wise autoregressive model. While large-scale text-to-image (T2I) models, particularly diffusion-based methods, have demonstrated impressive generation quality, they often suffer from style misalignment across generated image sets and slow inference speeds, limiting their practical usability. To address these issues, we propose three key components: initial feature replacement to ensure consistent background appearance, pivotal feature interpolation to align object placement, and dynamic style injection, which reinforces style consistency using a schedule function. Unlike previous methods requiring fine-tuning or additional training, our approach maintains fast inference while preserving individual content details. Extensive experiments show that our method achieves generation quality comparable to competing approaches, significantly improves style alignment, and delivers inference speeds over six times faster than the fastest model.",
        "arxiv_id": "2504.06144",
        "ARXIVID": "2504.06144",
        "COMMENT": "Does not match any specific criteria. Focuses on style-aligned image generation and speed improvements in T2I models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.06122": {
        "authors": [
            "Jingyuan Zhang",
            "Qi Wang",
            "Xingguang Ji",
            "Yahui Liu",
            "Yang Yue",
            "Fuzheng Zhang",
            "Di Zhang",
            "Guorui Zhou",
            "Kun Gai"
        ],
        "title": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning",
        "abstract": "arXiv:2504.06122v1 Announce Type: new  Abstract: Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages.To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details.",
        "arxiv_id": "2504.06122",
        "ARXIVID": "2504.06122",
        "COMMENT": "Does not match any specific criteria. Focuses on posttraining scaling in formal reasoning, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.05419": {
        "authors": [
            "Anqi Zhang",
            "Yulin Chen",
            "Jane Pan",
            "Chen Zhao",
            "Aurojit Panda",
            "Jinyang Li",
            "He He"
        ],
        "title": "Reasoning Models Know When They're Right: Probing Hidden States for Self-Verification",
        "abstract": "arXiv:2504.05419v1 Announce Type: new  Abstract: Reasoning models have achieved remarkable performance on tasks like math and logical reasoning thanks to their ability to search during reasoning. However, they still suffer from overthinking, often performing unnecessary reasoning steps even after reaching the correct answer. This raises the question: can models evaluate the correctness of their intermediate answers during reasoning? In this work, we study whether reasoning models encode information about answer correctness through probing the model's hidden states. The resulting probe can verify intermediate answers with high accuracy and produces highly calibrated scores. Additionally, we find models' hidden states encode correctness of future answers, enabling early prediction of the correctness before the intermediate answer is fully formulated. We then use the probe as a verifier to decide whether to exit reasoning at intermediate answers during inference, reducing the number of inference tokens by 24\\% without compromising performance. These findings confirm that reasoning models do encode a notion of correctness yet fail to exploit it, revealing substantial untapped potential to enhance their efficiency.",
        "arxiv_id": "2504.05419",
        "ARXIVID": "2504.05419",
        "COMMENT": "Does not match any specific criteria. Focuses on reasoning models and self-verification, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.05783": {
        "authors": [
            "Zijie Song",
            "Zhenzhen Hu",
            "Yixiao Ma",
            "Jia Li",
            "Richang Hong"
        ],
        "title": "Video Flow as Time Series: Discovering Temporal Consistency and Variability for VideoQA",
        "abstract": "arXiv:2504.05783v1 Announce Type: new  Abstract: Video Question Answering (VideoQA) is a complex video-language task that demands a sophisticated understanding of both visual content and temporal dynamics. Traditional Transformer-style architectures, while effective in integrating multimodal data, often simplify temporal dynamics through positional encoding and fail to capture non-linear interactions within video sequences. In this paper, we introduce the Temporal Trio Transformer (T3T), a novel architecture that models time consistency and time variability. The T3T integrates three key components: Temporal Smoothing (TS), Temporal Difference (TD), and Temporal Fusion (TF). The TS module employs Brownian Bridge for capturing smooth, continuous temporal transitions, while the TD module identifies and encodes significant temporal variations and abrupt changes within the video content. Subsequently, the TF module synthesizes these temporal features with textual cues, facilitating a deeper contextual understanding and response accuracy. The efficacy of the T3T is demonstrated through extensive testing on multiple VideoQA benchmark datasets. Our results underscore the importance of a nuanced approach to temporal modeling in improving the accuracy and depth of video-based question answering.",
        "arxiv_id": "2504.05783",
        "ARXIVID": "2504.05783",
        "COMMENT": "Does not match any specific criteria but introduces a novel architecture for VideoQA focusing on temporal dynamics, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.05977": {
        "authors": [
            "Jakob L{\\o}nborg Christensen",
            "Morten Rieger Hannemose",
            "Anders Bjorholm Dahl",
            "Vedrana Andersen Dahl"
        ],
        "title": "Diffusion Based Ambiguous Image Segmentation",
        "abstract": "arXiv:2504.05977v1 Announce Type: new  Abstract: Medical image segmentation often involves inherent uncertainty due to variations in expert annotations. Capturing this uncertainty is an important goal and previous works have used various generative image models for the purpose of representing the full distribution of plausible expert ground truths. In this work, we explore the design space of diffusion models for generative segmentation, investigating the impact of noise schedules, prediction types, and loss weightings. Notably, we find that making the noise schedule harder with input scaling significantly improves performance. We conclude that x- and v-prediction outperform epsilon-prediction, likely because the diffusion process is in the discrete segmentation domain. Many loss weightings achieve similar performance as long as they give enough weight to the end of the diffusion process. We base our experiments on the LIDC-IDRI lung lesion dataset and obtain state-of-the-art (SOTA) performance. Additionally, we introduce a randomly cropped variant of the LIDC-IDRI dataset that is better suited for uncertainty in image segmentation. Our model also achieves SOTA in this harder setting.",
        "arxiv_id": "2504.05977",
        "ARXIVID": "2504.05977",
        "COMMENT": "Does not match any specific criteria but discusses diffusion models for ambiguous medical image segmentation, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.05499": {
        "authors": [
            "Ruoyu Xue",
            "Jingyi Xu",
            "Sounak Mondal",
            "Hieu Le",
            "Gregory Zelinsky",
            "Minh Hoai",
            "Dimitris Samaras"
        ],
        "title": "Few-shot Personalized Scanpath Prediction",
        "abstract": "arXiv:2504.05499v1 Announce Type: new  Abstract: A personalized model for scanpath prediction provides insights into the visual preferences and attention patterns of individual subjects. However, existing methods for training scanpath prediction models are data-intensive and cannot be effectively personalized to new individuals with only a few available examples. In this paper, we propose few-shot personalized scanpath prediction task (FS-PSP) and a novel method to address it, which aims to predict scanpaths for an unseen subject using minimal support data of that subject's scanpath behavior. The key to our method's adaptability is the Subject-Embedding Network (SE-Net), specifically designed to capture unique, individualized representations for each subject's scanpaths. SE-Net generates subject embeddings that effectively distinguish between subjects while minimizing variability among scanpaths from the same individual. The personalized scanpath prediction model is then conditioned on these subject embeddings to produce accurate, personalized results. Experiments on multiple eye-tracking datasets demonstrate that our method excels in FS-PSP settings and does not require any fine-tuning steps at test time. Code is available at: https://github.com/cvlab-stonybrook/few-shot-scanpath",
        "arxiv_id": "2504.05499",
        "ARXIVID": "2504.05499",
        "COMMENT": "Does not match any specific criteria but introduces a few-shot personalized scanpath prediction model, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.05393": {
        "authors": [
            "Yotam Amitai",
            "Ofra Amir",
            "Guy Avni"
        ],
        "title": "Interactive Explanations for Reinforcement-Learning Agents",
        "abstract": "arXiv:2504.05393v1 Announce Type: new  Abstract: As reinforcement learning methods increasingly amass accomplishments, the need for comprehending their solutions becomes more crucial. Most explainable reinforcement learning (XRL) methods generate a static explanation depicting their developers' intuition of what should be explained and how. In contrast, literature from the social sciences proposes that meaningful explanations are structured as a dialog between the explainer and the explainee, suggesting a more active role for the user and her communication with the agent. In this paper, we present ASQ-IT -- an interactive explanation system that presents video clips of the agent acting in its environment based on queries given by the user that describe temporal properties of behaviors of interest. Our approach is based on formal methods: queries in ASQ-IT's user interface map to a fragment of Linear Temporal Logic over finite traces (LTLf), which we developed, and our algorithm for query processing is based on automata theory. User studies show that end-users can understand and formulate queries in ASQ-IT and that using ASQ-IT assists users in identifying faulty agent behaviors.",
        "arxiv_id": "2504.05393",
        "ARXIVID": "2504.05393",
        "COMMENT": "Does not match any specific criteria but discusses interactive explanations for reinforcement learning agents, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}