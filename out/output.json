{
    "2504.06863": {
        "authors": [
            "Chang Nie",
            "Yiqing Xu",
            "Guangming Wang",
            "Zhe Liu",
            "Yanzi Miao",
            "Hesheng Wang"
        ],
        "title": "MovSAM: A Single-image Moving Object Segmentation Framework Based on Deep Thinking",
        "abstract": "arXiv:2504.06863v1 Announce Type: new  Abstract: Moving object segmentation plays a vital role in understanding dynamic visual environments. While existing methods rely on multi-frame image sequences to identify moving objects, single-image MOS is critical for applications like motion intention prediction and handling camera frame drops. However, segmenting moving objects from a single image remains challenging for existing methods due to the absence of temporal cues. To address this gap, we propose MovSAM, the first framework for single-image moving object segmentation. MovSAM leverages a Multimodal Large Language Model (MLLM) enhanced with Chain-of-Thought (CoT) prompting to search the moving object and generate text prompts based on deep thinking for segmentation. These prompts are cross-fused with visual features from the Segment Anything Model (SAM) and a Vision-Language Model (VLM), enabling logic-driven moving object segmentation. The segmentation results then undergo a deep thinking refinement loop, allowing MovSAM to iteratively improve its understanding of the scene context and inter-object relationships with logical reasoning. This innovative approach enables MovSAM to segment moving objects in single images by considering scene understanding. We implement MovSAM in the real world to validate its practical application and effectiveness for autonomous driving scenarios where the multi-frame methods fail. Furthermore, despite the inherent advantage of multi-frame methods in utilizing temporal information, MovSAM achieves state-of-the-art performance across public MOS benchmarks, reaching 92.5\\% on J\\&F. Our implementation will be available at https://github.com/IRMVLab/MovSAM.",
        "arxiv_id": "2504.06863",
        "ARXIVID": "2504.06863",
        "COMMENT": "Matches criteria 2 and 3. The paper introduces MovSAM, a novel framework leveraging Multimodal Large Language Models (MLLMs) and Vision-Language Models (VLMs) for single-image moving object segmentation, which is a new methodological improvement for embodied AI and involves innovative use of VLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2504.06827": {
        "authors": [
            "Can Zhang",
            "Gim Hee Lee"
        ],
        "title": "IAAO: Interactive Affordance Learning for Articulated Objects in 3D Environments",
        "abstract": "arXiv:2504.06827v1 Announce Type: new  Abstract: This work presents IAAO, a novel framework that builds an explicit 3D model for intelligent agents to gain understanding of articulated objects in their environment through interaction. Unlike prior methods that rely on task-specific networks and assumptions about movable parts, our IAAO leverages large foundation models to estimate interactive affordances and part articulations in three stages. We first build hierarchical features and label fields for each object state using 3D Gaussian Splatting (3DGS) by distilling mask features and view-consistent labels from multi-view images. We then perform object- and part-level queries on the 3D Gaussian primitives to identify static and articulated elements, estimating global transformations and local articulation parameters along with affordances. Finally, scenes from different states are merged and refined based on the estimated transformations, enabling robust affordance-based interaction and manipulation of objects. Experimental results demonstrate the effectiveness of our method.",
        "arxiv_id": "2504.06827",
        "ARXIVID": "2504.06827",
        "COMMENT": "Matches criterion 1 and 3 as it proposes a novel framework for embodied agents to understand articulated objects in 3D environments, leveraging foundation models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2504.06982": {
        "authors": [
            "Yuhang Yang",
            "Fengqi Liu",
            "Yixing Lu",
            "Qin Zhao",
            "Pingyu Wu",
            "Wei Zhai",
            "Ran Yi",
            "Yang Cao",
            "Lizhuang Ma",
            "Zheng-Jun Zha",
            "Junting Dong"
        ],
        "title": "SIGMAN:Scaling 3D Human Gaussian Generation with Millions of Assets",
        "abstract": "arXiv:2504.06982v1 Announce Type: new  Abstract: 3D human digitization has long been a highly pursued yet challenging task. Existing methods aim to generate high-quality 3D digital humans from single or multiple views, but remain primarily constrained by current paradigms and the scarcity of 3D human assets. Specifically, recent approaches fall into several paradigms: optimization-based and feed-forward (both single-view regression and multi-view generation with reconstruction). However, they are limited by slow speed, low quality, cascade reasoning, and ambiguity in mapping low-dimensional planes to high-dimensional space due to occlusion and invisibility, respectively. Furthermore, existing 3D human assets remain small-scale, insufficient for large-scale training. To address these challenges, we propose a latent space generation paradigm for 3D human digitization, which involves compressing multi-view images into Gaussians via a UV-structured VAE, along with DiT-based conditional generation, we transform the ill-posed low-to-high-dimensional mapping problem into a learnable distribution shift, which also supports end-to-end inference. In addition, we employ the multi-view optimization approach combined with synthetic data to construct the HGS-1M dataset, which contains $1$ million 3D Gaussian assets to support the large-scale training. Experimental results demonstrate that our paradigm, powered by large-scale training, produces high-quality 3D human Gaussians with intricate textures, facial details, and loose clothing deformation.",
        "arxiv_id": "2504.06982",
        "ARXIVID": "2504.06982",
        "COMMENT": "Matches criterion 4 as it proposes a new paradigm for 3D human digitization using large-scale training, which is related to vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.06801": {
        "authors": [
            "Rishubh Parihar",
            "Srinjay Sarkar",
            "Sarthak Vora",
            "Jogendra Kundu",
            "R. Venkatesh Babu"
        ],
        "title": "MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection",
        "abstract": "arXiv:2504.06801v1 Announce Type: new  Abstract: Current monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, it's particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation focus on realistic object appearance through improved rendering techniques. However, we show that where and how objects are positioned is just as crucial for training effective 3D monocular detectors. The key obstacle lies in automatically determining realistic object placement parameters - including position, dimensions, and directional alignment when introducing synthetic objects into actual scenes. To address this, we introduce MonoPlace3D, a novel system that considers the 3D scene content to create realistic augmentations. Specifically, given a background scene, MonoPlace3D learns a distribution over plausible 3D bounding boxes. Subsequently, we render realistic objects and place them according to the locations sampled from the learned distribution. Our comprehensive evaluation on two standard datasets KITTI and NuScenes, demonstrates that MonoPlace3D significantly improves the accuracy of multiple existing monocular 3D detectors while being highly data efficient.",
        "arxiv_id": "2504.06801",
        "ARXIVID": "2504.06801",
        "COMMENT": "Matches criterion 3 as it introduces a novel system for realistic 3D object placement in monocular 3D detection, addressing a previously overlooked aspect of data augmentation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.07079": {
        "authors": [
            "Boyuan Zheng",
            "Michael Y. Fatemi",
            "Xiaolong Jin",
            "Zora Zhiruo Wang",
            "Apurva Gandhi",
            "Yueqi Song",
            "Yu Gu",
            "Jayanth Srinivasa",
            "Gaowen Liu",
            "Graham Neubig",
            "Yu Su"
        ],
        "title": "SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills",
        "abstract": "arXiv:2504.07079v1 Announce Type: new  Abstract: To survive and thrive in complex environments, humans have evolved sophisticated self-improvement mechanisms through environment exploration, hierarchical abstraction of experiences into reuseable skills, and collaborative construction of an ever-growing skill repertoire. Despite recent advancements, autonomous web agents still lack crucial self-improvement capabilities, struggling with procedural knowledge abstraction, refining skills, and skill composition. In this work, we introduce SkillWeaver, a skill-centric framework enabling agents to self-improve by autonomously synthesizing reusable skills as APIs. Given a new website, the agent autonomously discovers skills, executes them for practice, and distills practice experiences into robust APIs. Iterative exploration continually expands a library of lightweight, plug-and-play APIs, significantly enhancing the agent's capabilities. Experiments on WebArena and real-world websites demonstrate the efficacy of SkillWeaver, achieving relative success rate improvements of 31.8% and 39.8%, respectively. Additionally, APIs synthesized by strong agents substantially enhance weaker agents through transferable skills, yielding improvements of up to 54.3% on WebArena. These results demonstrate the effectiveness of honing diverse website interactions into APIs, which can be seamlessly shared among various web agents.",
        "arxiv_id": "2504.07079",
        "ARXIVID": "2504.07079",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for embodied agents to self-improve by synthesizing reusable skills, which is a new method for embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.06835": {
        "authors": [
            "Ziyi Wang",
            "Haoran Wu",
            "Yiming Rong",
            "Deyang Jiang",
            "Yixin Zhang",
            "Yunlong Zhao",
            "Shuang Xu",
            "Bo XU"
        ],
        "title": "LVC: A Lightweight Compression Framework for Enhancing VLMs in Long Video Understanding",
        "abstract": "arXiv:2504.06835v1 Announce Type: new  Abstract: Long video understanding is a complex task that requires both spatial detail and temporal awareness. While Vision-Language Models (VLMs) obtain frame-level understanding capabilities through multi-frame input, they suffer from information loss due to the sparse sampling strategy. In contrast, Video Large Language Models (Video-LLMs) capture temporal relationships within visual features but are limited by the scarcity of high-quality video-text datasets. To transfer long video understanding capabilities to VLMs with minimal data and computational cost, we propose Lightweight Video Compression (LVC), a novel method featuring the Query-Attention Video Compression mechanism, which effectively tackles the sparse sampling problem in VLMs. By training only the alignment layer with 10k short video-text pairs, LVC significantly enhances the temporal reasoning abilities of VLMs. Extensive experiments show that LVC provides consistent performance improvements across various models, including the InternVL2 series and Phi-3.5-Vision. Notably, the InternVL2-40B-LVC achieves scores of 68.2 and 65.9 on the long video understanding benchmarks MLVU and Video-MME, respectively, with relative improvements of 14.6% and 7.7%. The enhanced models and code will be publicly available soon.",
        "arxiv_id": "2504.06835",
        "ARXIVID": "2504.06835",
        "COMMENT": "Matches criterion 2 as it enhances Vision-Language Models (VLMs) for long video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.06606": {
        "authors": [
            "Minghe Gao",
            "Xuqi Liu",
            "Zhongqi Yue",
            "Yang Wu",
            "Shuang Chen",
            "Juncheng Li",
            "Siliang Tang",
            "Fei Wu",
            "Tat-Seng Chua",
            "Yueting Zhuang"
        ],
        "title": "Benchmarking Multimodal CoT Reward Model Stepwise by Visual Program",
        "abstract": "arXiv:2504.06606v1 Announce Type: new  Abstract: Recent advancements in reward signal usage for Large Language Models (LLMs) are remarkable. However, significant challenges exist when transitioning reward signal to the multimodal domain, including labor-intensive annotations, over-reliance on one-step rewards, and inadequate evaluation. To address these issues, we propose SVIP, a novel approach to train a step-level multi-dimensional Chain-of-Thought~(CoT) reward model automatically. It generates code for solving visual tasks and transforms the analysis of code blocks into the evaluation of CoT step as training samples. Then, we train SVIP-Reward model using a multi-head attention mechanism called TriAtt-CoT. The advantages of SVIP-Reward are evident throughout the entire process of MLLM. We also introduce a benchmark for CoT reward model training and testing. Experimental results demonstrate that SVIP-Reward improves MLLM performance across training and inference-time scaling, yielding better results on benchmarks while reducing hallucinations and enhancing reasoning ability.",
        "arxiv_id": "2504.06606",
        "ARXIVID": "2504.06606",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for CoT reward model training and testing in MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.06958": {
        "authors": [
            "Xinhao Li",
            "Ziang Yan",
            "Desen Meng",
            "Lu Dong",
            "Xiangyu Zeng",
            "Yinan He",
            "Yali Wang",
            "Yu Qiao",
            "Yi Wang",
            "Limin Wang"
        ],
        "title": "VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning",
        "abstract": "arXiv:2504.06958v1 Announce Type: new  Abstract: Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs.",
        "arxiv_id": "2504.06958",
        "ARXIVID": "2504.06958",
        "COMMENT": "Matches criterion 2 as it discusses reinforcement fine-tuning for video multimodal large language models (MLLMs) to enhance spatio-temporal perception.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2504.06815": {
        "authors": [
            "Hanxiao Sun",
            "YuPeng Gao",
            "Jin Xie",
            "Jian Yang",
            "Beibei Wang"
        ],
        "title": "SVG-IR: Spatially-Varying Gaussian Splatting for Inverse Rendering",
        "abstract": "arXiv:2504.06815v1 Announce Type: new  Abstract: Reconstructing 3D assets from images, known as inverse rendering (IR), remains a challenging task due to its ill-posed nature. 3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities for novel view synthesis (NVS) tasks. Methods apply it to relighting by separating radiance into BRDF parameters and lighting, yet produce inferior relighting quality with artifacts and unnatural indirect illumination due to the limited capability of each Gaussian, which has constant material parameters and normal, alongside the absence of physical constraints for indirect lighting. In this paper, we present a novel framework called Spatially-vayring Gaussian Inverse Rendering (SVG-IR), aimed at enhancing both NVS and relighting quality. To this end, we propose a new representation-Spatially-varying Gaussian (SVG)-that allows per-Gaussian spatially varying parameters. This enhanced representation is complemented by a SVG splatting scheme akin to vertex/fragment shading in traditional graphics pipelines. Furthermore, we integrate a physically-based indirect lighting model, enabling more realistic relighting. The proposed SVG-IR framework significantly improves rendering quality, outperforming state-of-the-art NeRF-based methods by 2.5 dB in peak signal-to-noise ratio (PSNR) and surpassing existing Gaussian-based techniques by 3.5 dB in relighting tasks, all while maintaining a real-time rendering speed.",
        "arxiv_id": "2504.06815",
        "ARXIVID": "2504.06815",
        "COMMENT": "Matches criterion 4 as it proposes a novel representation for inverse rendering using spatially-varying Gaussian splatting.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2504.06897": {
        "authors": [
            "Jiawei Mao",
            "Yuhan Wang",
            "Yucheng Tang",
            "Daguang Xu",
            "Kang Wang",
            "Yang Yang",
            "Zongwei Zhou",
            "Yuyin Zhou"
        ],
        "title": "MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs",
        "abstract": "arXiv:2504.06897v1 Announce Type: new  Abstract: This paper presents MedSegFactory, a versatile medical synthesis framework that generates high-quality paired medical images and segmentation masks across modalities and tasks. It aims to serve as an unlimited data repository, supplying image-mask pairs to enhance existing segmentation tools. The core of MedSegFactory is a dual-stream diffusion model, where one stream synthesizes medical images and the other generates corresponding segmentation masks. To ensure precise alignment between image-mask pairs, we introduce Joint Cross-Attention (JCA), enabling a collaborative denoising paradigm by dynamic cross-conditioning between streams. This bidirectional interaction allows both representations to guide each other's generation, enhancing consistency between generated pairs. MedSegFactory unlocks on-demand generation of paired medical images and segmentation masks through user-defined prompts that specify the target labels, imaging modalities, anatomical regions, and pathological conditions, facilitating scalable and high-quality data generation. This new paradigm of medical image synthesis enables seamless integration into diverse medical imaging workflows, enhancing both efficiency and accuracy. Extensive experiments show that MedSegFactory generates data of superior quality and usability, achieving competitive or state-of-the-art performance in 2D and 3D segmentation tasks while addressing data scarcity and regulatory constraints.",
        "arxiv_id": "2504.06897",
        "ARXIVID": "2504.06897",
        "COMMENT": "Matches criterion 2 as it introduces a dual-stream diffusion model for generating medical image-mask pairs, which is relevant to multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.07093": {
        "authors": [
            "Gene Chou",
            "Wenqi Xian",
            "Guandao Yang",
            "Mohamed Abdelfattah",
            "Bharath Hariharan",
            "Noah Snavely",
            "Ning Yu",
            "Paul Debevec"
        ],
        "title": "FlashDepth: Real-time Streaming Video Depth Estimation at 2K Resolution",
        "abstract": "arXiv:2504.07093v1 Announce Type: new  Abstract: A versatile video depth estimation model should (1) be accurate and consistent across frames, (2) produce high-resolution depth maps, and (3) support real-time streaming. We propose FlashDepth, a method that satisfies all three requirements, performing depth estimation on a 2044x1148 streaming video at 24 FPS. We show that, with careful modifications to pretrained single-image depth models, these capabilities are enabled with relatively little data and training. We evaluate our approach across multiple unseen datasets against state-of-the-art depth models, and find that ours outperforms them in terms of boundary sharpness and speed by a significant margin, while maintaining competitive accuracy. We hope our model will enable various applications that require high-resolution depth, such as video editing, and online decision-making, such as robotics.",
        "arxiv_id": "2504.07093",
        "ARXIVID": "2504.07093",
        "COMMENT": "Matches criterion 4 as it focuses on real-time video depth estimation, which is a vision foundation model application.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.07029": {
        "authors": [
            "Ran Zhang",
            "Xuanhua He",
            "Ke Cao",
            "Liu Liu",
            "Li Zhang",
            "Man Zhou",
            "Jie Zhang"
        ],
        "title": "Distilling Textual Priors from LLM to Efficient Image Fusion",
        "abstract": "arXiv:2504.07029v1 Announce Type: new  Abstract: Multi-modality image fusion aims to synthesize a single, comprehensive image from multiple source inputs. Traditional approaches, such as CNNs and GANs, offer efficiency but struggle to handle low-quality or complex inputs. Recent advances in text-guided methods leverage large model priors to overcome these limitations, but at the cost of significant computational overhead, both in memory and inference time. To address this challenge, we propose a novel framework for distilling large model priors, eliminating the need for text guidance during inference while dramatically reducing model size. Our framework utilizes a teacher-student architecture, where the teacher network incorporates large model priors and transfers this knowledge to a smaller student network via a tailored distillation process. Additionally, we introduce spatial-channel cross-fusion module to enhance the model's ability to leverage textual priors across both spatial and channel dimensions. Our method achieves a favorable trade-off between computational efficiency and fusion quality. The distilled network, requiring only 10\\% of the parameters and inference time of the teacher network, retains 90\\% of its performance and outperforms existing SOTA methods. Extensive experiments demonstrate the effectiveness of our approach. The implementation will be made publicly available as an open-source resource.",
        "arxiv_id": "2504.07029",
        "ARXIVID": "2504.07029",
        "COMMENT": "Matches criterion 2 as it focuses on distilling large model priors for multi-modality image fusion, which is relevant to VLLMs or MLLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.06925": {
        "authors": [
            "Sergio Romero-Tapiador",
            "Ruben Tolosana",
            "Blanca Lacruz-Pleguezuelos",
            "Laura Judith Marcos Zambrano",
            "Guadalupe X. Baz\\'an",
            "Isabel Espinosa-Salinas",
            "Julian Fierrez",
            "Javier Ortega-Garcia",
            "Enrique Carrillo de Santa Pau",
            "Aythami Morales"
        ],
        "title": "Are Vision-Language Models Ready for Dietary Assessment? Exploring the Next Frontier in AI-Powered Food Image Recognition",
        "abstract": "arXiv:2504.06925v1 Announce Type: new  Abstract: Automatic dietary assessment based on food images remains a challenge, requiring precise food detection, segmentation, and classification. Vision-Language Models (VLMs) offer new possibilities by integrating visual and textual reasoning. In this study, we evaluate six state-of-the-art VLMs (ChatGPT, Gemini, Claude, Moondream, DeepSeek, and LLaVA), analyzing their capabilities in food recognition at different levels. For the experimental framework, we introduce the FoodNExTDB, a unique food image database that contains 9,263 expert-labeled images across 10 categories (e.g., \"protein source\"), 62 subcategories (e.g., \"poultry\"), and 9 cooking styles (e.g., \"grilled\"). In total, FoodNExTDB includes 50k nutritional labels generated by seven experts who manually annotated all images in the database. Also, we propose a novel evaluation metric, Expert-Weighted Recall (EWR), that accounts for the inter-annotator variability. Results show that closed-source models outperform open-source ones, achieving over 90% EWR in recognizing food products in images containing a single product. Despite their potential, current VLMs face challenges in fine-grained food recognition, particularly in distinguishing subtle differences in cooking styles and visually similar food items, which limits their reliability for automatic dietary assessment. The FoodNExTDB database is publicly available at https://github.com/AI4Food/FoodNExtDB.",
        "arxiv_id": "2504.06925",
        "ARXIVID": "2504.06925",
        "COMMENT": "Matches criterion 2 as it evaluates state-of-the-art Vision-Language Models (VLMs) for dietary assessment.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.06504": {
        "authors": [
            "Xiaohang Yang",
            "Qing Wang",
            "Jiahao Yang",
            "Gregory Slabaugh",
            "Shanxin Yuan"
        ],
        "title": "STaR: Seamless Spatial-Temporal Aware Motion Retargeting with Penetration and Consistency Constraints",
        "abstract": "arXiv:2504.06504v1 Announce Type: new  Abstract: Motion retargeting seeks to faithfully replicate the spatio-temporal motion characteristics of a source character onto a target character with a different body shape. Apart from motion semantics preservation, ensuring geometric plausibility and maintaining temporal consistency are also crucial for effective motion retargeting. However, many existing methods prioritize either geometric plausibility or temporal consistency. Neglecting geometric plausibility results in interpenetration while neglecting temporal consistency leads to motion jitter. In this paper, we propose a novel sequence-to-sequence model for seamless Spatial-Temporal aware motion Retargeting (STaR), with penetration and consistency constraints. STaR consists of two modules: (1) a spatial module that incorporates dense shape representation and a novel limb penetration constraint to ensure geometric plausibility while preserving motion semantics, and (2) a temporal module that utilizes a temporal transformer and a novel temporal consistency constraint to predict the entire motion sequence at once while enforcing multi-level trajectory smoothness. The seamless combination of the two modules helps us achieve a good balance between the semantic, geometric, and temporal targets. Extensive experiments on the Mixamo and ScanRet datasets demonstrate that our method produces plausible and coherent motions while significantly reducing interpenetration rates compared with other approaches.",
        "arxiv_id": "2504.06504",
        "ARXIVID": "2504.06504",
        "COMMENT": "Matches criterion 1 as it proposes a novel spatial-temporal aware motion retargeting method, which is relevant to spatial intelligence in embodied agents.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2504.06527": {
        "authors": [
            "Xinyu Liu",
            "Xiaoguang Lin",
            "Xiang Liu",
            "Yong Yang",
            "Hongqian Wang",
            "Qilong Sun"
        ],
        "title": "TSP-OCS: A Time-Series Prediction for Optimal Camera Selection in Multi-Viewpoint Surgical Video Analysis",
        "abstract": "arXiv:2504.06527v1 Announce Type: new  Abstract: Recording the open surgery process is essential for educational and medical evaluation purposes; however, traditional single-camera methods often face challenges such as occlusions caused by the surgeon's head and body, as well as limitations due to fixed camera angles, which reduce comprehensibility of the video content. This study addresses these limitations by employing a multi-viewpoint camera recording system, capturing the surgical procedure from six different angles to mitigate occlusions. We propose a fully supervised learning-based time series prediction method to choose the best shot sequences from multiple simultaneously recorded video streams, ensuring optimal viewpoints at each moment. Our time series prediction model forecasts future camera selections by extracting and fusing visual and semantic features from surgical videos using pre-trained models. These features are processed by a temporal prediction network with TimeBlocks to capture sequential dependencies. A linear embedding layer reduces dimensionality, and a Softmax classifier selects the optimal camera view based on the highest probability. In our experiments, we created five groups of open thyroidectomy videos, each with simultaneous recordings from six different angles. The results demonstrate that our method achieves competitive accuracy compared to traditional supervised methods, even when predicting over longer time horizons. Furthermore, our approach outperforms state-of-the-art time series prediction techniques on our dataset. This manuscript makes a unique contribution by presenting an innovative framework that advances surgical video analysis techniques, with significant implications for improving surgical education and patient safety.",
        "arxiv_id": "2504.06527",
        "ARXIVID": "2504.06527",
        "COMMENT": "Matches criterion 3 as it proposes a new method for optimal camera selection in multi-viewpoint surgical video analysis, which could be relevant to embodied AI benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.07089": {
        "authors": [
            "Yiting Lu",
            "Jiakang Yuan",
            "Zhen Li",
            "Shitian Zhao",
            "Qi Qin",
            "Xinyue Li",
            "Le Zhuo",
            "Licheng Wen",
            "Dongyang Liu",
            "Yuewen Cao",
            "Xiangchao Yan",
            "Xin Li",
            "Botian Shi",
            "Tao Chen",
            "Zhibo Chen",
            "Lei Bai",
            "Bo Zhang",
            "Peng Gao"
        ],
        "title": "OmniCaptioner: One Captioner to Rule Them All",
        "abstract": "arXiv:2504.07089v1 Announce Type: new  Abstract: We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities.",
        "arxiv_id": "2504.07089",
        "ARXIVID": "2504.07089",
        "COMMENT": "Matches criterion 2 as it introduces OmniCaptioner, a versatile visual captioning framework that bridges visual and textual modalities.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.06672": {
        "authors": [
            "Elia Peruzzo",
            "Dejia Xu",
            "Xingqian Xu",
            "Humphrey Shi",
            "Nicu Sebe"
        ],
        "title": "RAGME: Retrieval Augmented Video Generation for Enhanced Motion Realism",
        "abstract": "arXiv:2504.06672v1 Announce Type: new  Abstract: Video generation is experiencing rapid growth, driven by advances in diffusion models and the development of better and larger datasets. However, producing high-quality videos remains challenging due to the high-dimensional data and the complexity of the task. Recent efforts have primarily focused on enhancing visual quality and addressing temporal inconsistencies, such as flickering. Despite progress in these areas, the generated videos often fall short in terms of motion complexity and physical plausibility, with many outputs either appearing static or exhibiting unrealistic motion. In this work, we propose a framework to improve the realism of motion in generated videos, exploring a complementary direction to much of the existing literature. Specifically, we advocate for the incorporation of a retrieval mechanism during the generation phase. The retrieved videos act as grounding signals, providing the model with demonstrations of how the objects move. Our pipeline is designed to apply to any text-to-video diffusion model, conditioning a pretrained model on the retrieved samples with minimal fine-tuning. We demonstrate the superiority of our approach through established metrics, recently proposed benchmarks, and qualitative results, and we highlight additional applications of the framework.",
        "arxiv_id": "2504.06672",
        "ARXIVID": "2504.06672",
        "COMMENT": "Matches criterion 2 as it proposes a retrieval-augmented framework for video generation, which could be relevant to multi-modal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.07092": {
        "authors": [
            "Alexander Rubinstein",
            "Ameya Prabhu",
            "Matthias Bethge",
            "Seong Joon Oh"
        ],
        "title": "Are We Done with Object-Centric Learning?",
        "abstract": "arXiv:2504.07092v1 Announce Type: new  Abstract: Object-centric learning (OCL) seeks to learn representations that only encode an object, isolated from other objects or background cues in a scene. This approach underpins various aims, including out-of-distribution (OOD) generalization, sample-efficient composition, and modeling of structured environments. Most research has focused on developing unsupervised mechanisms that separate objects into discrete slots in the representation space, evaluated using unsupervised object discovery. However, with recent sample-efficient segmentation models, we can separate objects in the pixel space and encode them independently. This achieves remarkable zero-shot performance on OOD object discovery benchmarks, is scalable to foundation models, and can handle a variable number of slots out-of-the-box. Hence, the goal of OCL methods to obtain object-centric representations has been largely achieved. Despite this progress, a key question remains: How does the ability to separate objects within a scene contribute to broader OCL objectives, such as OOD generalization? We address this by investigating the OOD generalization challenge caused by spurious background cues through the lens of OCL. We propose a novel, training-free probe called $\\textbf{Object-Centric Classification with Applied Masks (OCCAM)}$, demonstrating that segmentation-based encoding of individual objects significantly outperforms slot-based OCL methods. However, challenges in real-world applications remain. We provide the toolbox for the OCL community to use scalable object-centric representations, and focus on practical applications and fundamental questions, such as understanding object perception in human cognition. Our code is available $\\href{https://github.com/AlexanderRubinstein/OCCAM}{here}$.",
        "arxiv_id": "2504.07092",
        "ARXIVID": "2504.07092",
        "COMMENT": "Matches criterion 4 as it discusses object-centric learning and its implications for OOD generalization, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.07046": {
        "authors": [
            "Jifang Wang",
            "Xue Yang",
            "Longyue Wang",
            "Zhenran Xu",
            "Yiyu Wang",
            "Yaowei Wang",
            "Weihua Luo",
            "Kaifu Zhang",
            "Baotian Hu",
            "Min Zhang"
        ],
        "title": "A Unified Agentic Framework for Evaluating Conditional Image Generation",
        "abstract": "arXiv:2504.07046v1 Announce Type: new  Abstract: Conditional image generation has gained significant attention for its ability to personalize content. However, the field faces challenges in developing task-agnostic, reliable, and explainable evaluation metrics. This paper introduces CIGEval, a unified agentic framework for comprehensive evaluation of conditional image generation tasks. CIGEval utilizes large multimodal models (LMMs) as its core, integrating a multi-functional toolbox and establishing a fine-grained evaluation framework. Additionally, we synthesize evaluation trajectories for fine-tuning, empowering smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. Experiments across seven prominent conditional image generation tasks demonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K training trajectories, CIGEval surpasses the previous GPT-4o-based state-of-the-art method. Case studies on GPT-4o image generation highlight CIGEval's capability in identifying subtle issues related to subject consistency and adherence to control guidance, indicating its great potential for automating evaluation of image generation tasks with human-level reliability.",
        "arxiv_id": "2504.07046",
        "ARXIVID": "2504.07046",
        "COMMENT": "Matches criterion 4 as it introduces a unified framework for evaluating conditional image generation, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.06785": {
        "authors": [
            "Shuoshuo Xu",
            "Kai Zhao",
            "James Loney",
            "Zili Li",
            "Andrea Visentin"
        ],
        "title": "Zero-Shot Image-Based Large Language Model Approach to Road Pavement Monitoring",
        "abstract": "arXiv:2504.06785v1 Announce Type: new  Abstract: Effective and rapid evaluation of pavement surface condition is critical for prioritizing maintenance, ensuring transportation safety, and minimizing vehicle wear and tear. While conventional manual inspections suffer from subjectivity, existing machine learning-based methods are constrained by their reliance on large and high-quality labeled datasets, which require significant resources and limit adaptability across varied road conditions. The revolutionary advancements in Large Language Models (LLMs) present significant potential for overcoming these challenges. In this study, we propose an innovative automated zero-shot learning approach that leverages the image recognition and natural language understanding capabilities of LLMs to assess road conditions effectively. Multiple LLM-based assessment models were developed, employing prompt engineering strategies aligned with the Pavement Surface Condition Index (PSCI) standards. These models' accuracy and reliability were evaluated against official PSCI results, with an optimized model ultimately selected. Extensive tests benchmarked the optimized model against evaluations from various levels experts using Google Street View road images. The results reveal that the LLM-based approach can effectively assess road conditions, with the optimized model -employing comprehensive and structured prompt engineering strategies -outperforming simpler configurations by achieving high accuracy and consistency, even surpassing expert evaluations. Moreover, successfully applying the optimized model to Google Street View images demonstrates its potential for future city-scale deployments. These findings highlight the transformative potential of LLMs in automating road damage evaluations and underscore the pivotal role of detailed prompt engineering in achieving reliable assessments.",
        "arxiv_id": "2504.06785",
        "ARXIVID": "2504.06785",
        "COMMENT": "Matches criterion 2 as it discusses leveraging large language models (LLMs) for zero-shot image-based road pavement monitoring, which aligns with VLLMs/MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.06962": {
        "authors": [
            "Thomas Kerdreux",
            "Alexandre Tuel",
            "Quentin Febvre",
            "Alexis Mouche",
            "Bertrand Chapron"
        ],
        "title": "Efficient Self-Supervised Learning for Earth Observation via Dynamic Dataset Curation",
        "abstract": "arXiv:2504.06962v1 Announce Type: new  Abstract: Self-supervised learning (SSL) has enabled the development of vision foundation models for Earth Observation (EO), demonstrating strong transferability across diverse remote sensing tasks. While prior work has focused on network architectures and training strategies, the role of dataset curation, especially in balancing and diversifying pre-training datasets, remains underexplored. In EO, this challenge is amplified by the redundancy and heavy-tailed distributions common in satellite imagery, which can lead to biased representations and inefficient training.   In this work, we propose a dynamic dataset pruning strategy designed to improve SSL pre-training by maximizing dataset diversity and balance. Our method iteratively refines the training set without requiring a pre-existing feature extractor, making it well-suited for domains where curated datasets are limited or unavailable. We demonstrate our approach on the Sentinel-1 Wave Mode (WV) Synthetic Aperture Radar (SAR) archive, a challenging dataset dominated by ocean observations. We train models from scratch on the entire Sentinel-1 WV archive spanning 10 years. Across three downstream tasks, our results show that dynamic pruning improves both computational efficiency and representation quality, leading to stronger transferability.   We also release the weights of Nereus-SAR-1, the first model in the Nereus family, a series of foundation models for ocean observation and analysis using SAR imagery, at github.com/galeio-research/nereus-sar-models/.",
        "arxiv_id": "2504.06962",
        "ARXIVID": "2504.06962",
        "COMMENT": "Matches criterion 4 as it discusses vision foundation models for Earth Observation and proposes a novel dataset curation strategy to improve self-supervised learning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.06742": {
        "authors": [
            "Alexandra Ertl",
            "Shuhan Xiao",
            "Stefan Denner",
            "Robin Peretzke",
            "David Zimmerer",
            "Peter Neher",
            "Fabian Isensee",
            "Klaus Maier-Hein"
        ],
        "title": "nnLandmark: A Self-Configuring Method for 3D Medical Landmark Detection",
        "abstract": "arXiv:2504.06742v1 Announce Type: new  Abstract: Landmark detection plays a crucial role in medical imaging tasks that rely on precise spatial localization, including specific applications in diagnosis, treatment planning, image registration, and surgical navigation. However, manual annotation is labor-intensive and requires expert knowledge. While deep learning shows promise in automating this task, progress is hindered by limited public datasets, inconsistent benchmarks, and non-standardized baselines, restricting reproducibility, fair comparisons, and model generalizability.This work introduces nnLandmark, a self-configuring deep learning framework for 3D medical landmark detection, adapting nnU-Net to perform heatmap-based regression. By leveraging nnU-Net's automated configuration, nnLandmark eliminates the need for manual parameter tuning, offering out-of-the-box usability. It achieves state-of-the-art accuracy across two public datasets, with a mean radial error (MRE) of 1.5 mm on the Mandibular Molar Landmark (MML) dental CT dataset and 1.2 mm for anatomical fiducials on a brain MRI dataset (AFIDs), where nnLandmark aligns with the inter-rater variability of 1.5 mm. With its strong generalization, reproducibility, and ease of deployment, nnLandmark establishes a reliable baseline for 3D landmark detection, supporting research in anatomical localization and clinical workflows that depend on precise landmark identification. The code will be available soon.",
        "arxiv_id": "2504.06742",
        "ARXIVID": "2504.06742",
        "COMMENT": "Does not match any specific criteria but is tangentially related to spatial understanding in medical imaging. It introduces a self-configuring framework for 3D landmark detection, which is relevant to spatial intelligence but not directly tied to embodied agents or vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.06675": {
        "authors": [
            "Qingtao Yu",
            "Jaskirat Singh",
            "Zhaoyuan Yang",
            "Peter Henry Tu",
            "Jing Zhang",
            "Hongdong Li",
            "Richard Hartley",
            "Dylan Campbell"
        ],
        "title": "Probability Density Geodesics in Image Diffusion Latent Space",
        "abstract": "arXiv:2504.06675v1 Announce Type: new  Abstract: Diffusion models indirectly estimate the probability density over a data space, which can be used to study its structure. In this work, we show that geodesics can be computed in diffusion latent space, where the norm induced by the spatially-varying inner product is inversely proportional to the probability density. In this formulation, a path that traverses a high density (that is, probable) region of image latent space is shorter than the equivalent path through a low density region. We present algorithms for solving the associated initial and boundary value problems and show how to compute the probability density along the path and the geodesic distance between two points. Using these techniques, we analyze how closely video clips approximate geodesics in a pre-trained image diffusion space. Finally, we demonstrate how these techniques can be applied to training-free image sequence interpolation and extrapolation, given a pre-trained image diffusion model.",
        "arxiv_id": "2504.06675",
        "ARXIVID": "2504.06675",
        "COMMENT": "Does not match any specific criterion but is related to diffusion models and geodesics, which are tangentially relevant to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.07083": {
        "authors": [
            "Mengchen Zhang",
            "Tong Wu",
            "Jing Tan",
            "Ziwei Liu",
            "Gordon Wetzstein",
            "Dahua Lin"
        ],
        "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography",
        "abstract": "arXiv:2504.07083v1 Announce Type: new  Abstract: Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, a large-scale multi-modal dataset containing 29K real-world shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movement generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes a new standard for learning-based cinematography, paving the way for future advancements in camera control and filmmaking. Our project website: https://kszpxxzmc.github.io/GenDoP/.",
        "arxiv_id": "2504.07083",
        "ARXIVID": "2504.07083",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling for camera trajectory design, which is tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.06608": {
        "authors": [
            "Jiajun Chen",
            "Hongpeng Yin",
            "Yifu Yang"
        ],
        "title": "A Cross-Domain Few-Shot Learning Method Based on Domain Knowledge Mapping",
        "abstract": "arXiv:2504.06608v1 Announce Type: new  Abstract: In task-based few-shot learning paradigms, it is commonly assumed that different tasks are independently and identically distributed (i.i.d.). However, in real-world scenarios, the distribution encountered in few-shot learning can significantly differ from the distribution of existing data. Thus, how to effectively leverage existing data knowledge to enable models to quickly adapt to class variations under non-i.i.d. assumptions has emerged as a key research challenge. To address this challenge, this paper proposes a new cross-domain few-shot learning approach based on domain knowledge mapping, applied consistently throughout the pre-training, training, and testing phases. In the pre-training phase, our method integrates self-supervised and supervised losses by maximizing mutual information, thereby mitigating mode collapse. During the training phase, the domain knowledge mapping layer collaborates with a domain classifier to learn both domain mapping capabilities and the ability to assess domain adaptation difficulty. Finally, this approach is applied during the testing phase, rapidly adapting to domain variations through meta-training tasks on support sets, consequently enhancing the model's capability to transfer domain knowledge effectively. Experimental validation conducted across six datasets from diverse domains demonstrates the effectiveness of the proposed method.",
        "arxiv_id": "2504.06608",
        "ARXIVID": "2504.06608",
        "COMMENT": "Does not match any specific criteria. Focuses on cross-domain few-shot learning, which is not directly related to spatial understanding, VLLMs, MLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.06634": {
        "authors": [
            "Junyoung Kim",
            "Youngrok Kim",
            "Siyeol Jung",
            "Donghyun Min"
        ],
        "title": "Crafting Query-Aware Selective Attention for Single Image Super-Resolution",
        "abstract": "arXiv:2504.06634v1 Announce Type: new  Abstract: Single Image Super-Resolution (SISR) reconstructs high-resolution images from low-resolution inputs, enhancing image details. While Vision Transformer (ViT)-based models improve SISR by capturing long-range dependencies, they suffer from quadratic computational costs or employ selective attention mechanisms that do not explicitly focus on query-relevant regions. Despite these advancements, prior work has overlooked how selective attention mechanisms should be effectively designed for SISR. We propose SSCAN, which dynamically selects the most relevant key-value windows based on query similarity, ensuring focused feature extraction while maintaining efficiency. In contrast to prior approaches that apply attention globally or heuristically, our method introduces a query-aware window selection strategy that better aligns attention computation with important image regions. By incorporating fixed-sized windows, SSCAN reduces memory usage and enforces linear token-to-token complexity, making it scalable for large images. Our experiments demonstrate that SSCAN outperforms existing attention-based SISR methods, achieving up to 0.14 dB PSNR improvement on urban datasets, guaranteeing both computational efficiency and reconstruction quality in SISR.",
        "arxiv_id": "2504.06634",
        "ARXIVID": "2504.06634",
        "COMMENT": "Does not match any specific criteria. Focuses on single image super-resolution with selective attention, which is not directly related to spatial understanding, VLLMs, MLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.07060": {
        "authors": [
            "Ruoyu Chen",
            "Hua Zhang",
            "Jingzhi Li",
            "Li Liu",
            "Zhen Huang",
            "Xiaochun Cao"
        ],
        "title": "Generalized Semantic Contrastive Learning via Embedding Side Information for Few-Shot Object Detection",
        "abstract": "arXiv:2504.07060v1 Announce Type: new  Abstract: The objective of few-shot object detection (FSOD) is to detect novel objects with few training samples. The core challenge of this task is how to construct a generalized feature space for novel categories with limited data on the basis of the base category space, which could adapt the learned detection model to unknown scenarios. However, limited by insufficient samples for novel categories, two issues still exist: (1) the features of the novel category are easily implicitly represented by the features of the base category, leading to inseparable classifier boundaries, (2) novel categories with fewer data are not enough to fully represent the distribution, where the model fine-tuning is prone to overfitting. To address these issues, we introduce the side information to alleviate the negative influences derived from the feature space and sample viewpoints and formulate a novel generalized feature representation learning method for FSOD. Specifically, we first utilize embedding side information to construct a knowledge matrix to quantify the semantic relationship between the base and novel categories. Then, to strengthen the discrimination between semantically similar categories, we further develop contextual semantic supervised contrastive learning which embeds side information. Furthermore, to prevent overfitting problems caused by sparse samples, a side-information guided region-aware masked module is introduced to augment the diversity of samples, which finds and abandons biased information that discriminates between similar categories via counterfactual explanation, and refines the discriminative representation space further. Extensive experiments using ResNet and ViT backbones on PASCAL VOC, MS COCO, LVIS V1, FSOD-1K, and FSVOD-500 benchmarks demonstrate that our model outperforms the previous state-of-the-art methods, significantly improving the ability of FSOD in most shots/splits.",
        "arxiv_id": "2504.07060",
        "ARXIVID": "2504.07060",
        "COMMENT": "Does not match any specific criteria. Focuses on few-shot object detection with semantic contrastive learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.06572": {
        "authors": [
            "Shaocong Long",
            "Qianyu Zhou",
            "Xikun Jiang",
            "Chenhao Ying",
            "Lizhuang Ma",
            "Yuan Luo"
        ],
        "title": "Domain Generalization via Discrete Codebook Learning",
        "abstract": "arXiv:2504.06572v1 Announce Type: new  Abstract: Domain generalization (DG) strives to address distribution shifts across diverse environments to enhance model's generalizability. Current DG approaches are confined to acquiring robust representations with continuous features, specifically training at the pixel level. However, this DG paradigm may struggle to mitigate distribution gaps in dealing with a large space of continuous features, rendering it susceptible to pixel details that exhibit spurious correlations or noise. In this paper, we first theoretically demonstrate that the domain gaps in continuous representation learning can be reduced by the discretization process. Based on this inspiring finding, we introduce a novel learning paradigm for DG, termed Discrete Domain Generalization (DDG). DDG proposes to use a codebook to quantize the feature map into discrete codewords, aligning semantic-equivalent information in a shared discrete representation space that prioritizes semantic-level information over pixel-level intricacies. By learning at the semantic level, DDG diminishes the number of latent features, optimizing the utilization of the representation space and alleviating the risks associated with the wide-ranging space of continuous features. Extensive experiments across widely employed benchmarks in DG demonstrate DDG's superior performance compared to state-of-the-art approaches, underscoring its potential to reduce the distribution gaps and enhance the model's generalizability.",
        "arxiv_id": "2504.06572",
        "ARXIVID": "2504.06572",
        "COMMENT": "Does not match any specific criteria. Focuses on domain generalization via discrete codebook learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.06895": {
        "authors": [
            "Dingkun Yan",
            "Xinrui Wang",
            "Yusuke Iwasawa",
            "Yutaka Matsuo",
            "Suguru Saito",
            "Jiaxian Guo"
        ],
        "title": "ColorizeDiffusion v2: Enhancing Reference-based Sketch Colorization Through Separating Utilities",
        "abstract": "arXiv:2504.06895v1 Announce Type: new  Abstract: Reference-based sketch colorization methods have garnered significant attention due to their potential applications in the animation production industry. However, most existing methods are trained with image triplets of sketch, reference, and ground truth that are semantically and spatially well-aligned, while real-world references and sketches often exhibit substantial misalignment. This mismatch in data distribution between training and inference leads to overfitting, consequently resulting in spatial artifacts and significant degradation in overall colorization quality, limiting potential applications of current methods for general purposes. To address this limitation, we conduct an in-depth analysis of the \\textbf{carrier}, defined as the latent representation facilitating information transfer from reference to sketch. Based on this analysis, we propose a novel workflow that dynamically adapts the carrier to optimize distinct aspects of colorization. Specifically, for spatially misaligned artifacts, we introduce a split cross-attention mechanism with spatial masks, enabling region-specific reference injection within the diffusion process. To mitigate semantic neglect of sketches, we employ dedicated background and style encoders to transfer detailed reference information in the latent feature space, achieving enhanced spatial control and richer detail synthesis. Furthermore, we propose character-mask merging and background bleaching as preprocessing steps to improve foreground-background integration and background generation. Extensive qualitative and quantitative evaluations, including a user study, demonstrate the superior performance of our proposed method compared to existing approaches. An ablation study further validates the efficacy of each proposed component.",
        "arxiv_id": "2504.06895",
        "ARXIVID": "2504.06895",
        "COMMENT": "Does not match any specific criteria. Focuses on sketch colorization with diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.06580": {
        "authors": [
            "Joochan Kim",
            "Minjoon Jung",
            "Byoung-Tak Zhang"
        ],
        "title": "Exploring Ordinal Bias in Action Recognition for Instructional Videos",
        "abstract": "arXiv:2504.06580v1 Announce Type: new  Abstract: Action recognition models have achieved promising results in understanding instructional videos. However, they often rely on dominant, dataset-specific action sequences rather than true video comprehension, a problem that we define as ordinal bias. To address this issue, we propose two effective video manipulation methods: Action Masking, which masks frames of frequently co-occurring actions, and Sequence Shuffling, which randomizes the order of action segments. Through comprehensive experiments, we demonstrate that current models exhibit significant performance drops when confronted with nonstandard action sequences, underscoring their vulnerability to ordinal bias. Our findings emphasize the importance of rethinking evaluation strategies and developing models capable of generalizing beyond fixed action patterns in diverse instructional videos.",
        "arxiv_id": "2504.06580",
        "ARXIVID": "2504.06580",
        "COMMENT": "Does not match any specific criteria. Focuses on ordinal bias in action recognition for instructional videos.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.06978": {
        "authors": [
            "Daiwei Zhang",
            "Joaquin Gajardo",
            "Tomislav Medic",
            "Isinsu Katircioglu",
            "Mike Boss",
            "Norbert Kirchgessner",
            "Achim Walter",
            "Lukas Roth"
        ],
        "title": "Wheat3DGS: In-field 3D Reconstruction, Instance Segmentation and Phenotyping of Wheat Heads with Gaussian Splatting",
        "abstract": "arXiv:2504.06978v1 Announce Type: new  Abstract: Automated extraction of plant morphological traits is crucial for supporting crop breeding and agricultural management through high-throughput field phenotyping (HTFP). Solutions based on multi-view RGB images are attractive due to their scalability and affordability, enabling volumetric measurements that 2D approaches cannot directly capture. While advanced methods like Neural Radiance Fields (NeRFs) have shown promise, their application has been limited to counting or extracting traits from only a few plants or organs. Furthermore, accurately measuring complex structures like individual wheat heads-essential for studying crop yields-remains particularly challenging due to occlusions and the dense arrangement of crop canopies in field conditions. The recent development of 3D Gaussian Splatting (3DGS) offers a promising alternative for HTFP due to its high-quality reconstructions and explicit point-based representation. In this paper, we present Wheat3DGS, a novel approach that leverages 3DGS and the Segment Anything Model (SAM) for precise 3D instance segmentation and morphological measurement of hundreds of wheat heads automatically, representing the first application of 3DGS to HTFP. We validate the accuracy of wheat head extraction against high-resolution laser scan data, obtaining per-instance mean absolute percentage errors of 15.1%, 18.3%, and 40.2% for length, width, and volume. We provide additional comparisons to NeRF-based approaches and traditional Muti-View Stereo (MVS), demonstrating superior results. Our approach enables rapid, non-destructive measurements of key yield-related traits at scale, with significant implications for accelerating crop breeding and improving our understanding of wheat development.",
        "arxiv_id": "2504.06978",
        "ARXIVID": "2504.06978",
        "COMMENT": "Does not match any specific criteria. Focuses on 3D reconstruction and phenotyping of wheat heads using Gaussian Splatting, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.06629": {
        "authors": [
            "MinKyu Lee",
            "Sangeek Hyun",
            "Woojin Jun",
            "Hyunjun Kim",
            "Jiwoo Chung",
            "Jae-Pil Heo"
        ],
        "title": "Rethinking LayerNorm in Image Restoration Transformers",
        "abstract": "arXiv:2504.06629v1 Announce Type: new  Abstract: This work investigates abnormal feature behaviors observed in image restoration (IR) Transformers. Specifically, we identify two critical issues: feature entropy becoming excessively small and feature magnitudes diverging up to a million-fold scale. We pinpoint the root cause to the per-token normalization aspect of conventional LayerNorm, which disrupts essential spatial correlations and internal feature statistics. To address this, we propose a simple normalization strategy tailored for IR Transformers. Our approach applies normalization across the entire spatio-channel dimension, effectively preserving spatial correlations. Additionally, we introduce an input-adaptive rescaling method that aligns feature statistics to the unique statistical requirements of each input. Experimental results verify that this combined strategy effectively resolves feature divergence, significantly enhancing both the stability and performance of IR Transformers across various IR tasks.",
        "arxiv_id": "2504.06629",
        "ARXIVID": "2504.06629",
        "COMMENT": "Does not match any specific criteria. Focuses on normalization strategies for image restoration transformers, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.06755": {
        "authors": [
            "Li Yu",
            "Zhihui Li",
            "Jimin Xiao",
            "Moncef Gabbouj"
        ],
        "title": "FANeRV: Frequency Separation and Augmentation based Neural Representation for Video",
        "abstract": "arXiv:2504.06755v1 Announce Type: new  Abstract: Neural representations for video (NeRV) have gained considerable attention for their strong performance across various video tasks. However, existing NeRV methods often struggle to capture fine spatial details, resulting in vague reconstructions. In this paper, we present a Frequency Separation and Augmentation based Neural Representation for video (FANeRV), which addresses these limitations with its core Wavelet Frequency Upgrade Block.This block explicitly separates input frames into high and low-frequency components using discrete wavelet transform, followed by targeted enhancement using specialized modules. Finally, a specially designed gated network effectively fuses these frequency components for optimal reconstruction. Additionally, convolutional residual enhancement blocks are integrated into the later stages of the network to balance parameter distribution and improve the restoration of high-frequency details. Experimental results demonstrate that FANeRV significantly improves reconstruction performance and excels in multiple tasks, including video compression, inpainting, and interpolation, outperforming existing NeRV methods.",
        "arxiv_id": "2504.06755",
        "ARXIVID": "2504.06755",
        "COMMENT": "Does not match any specific criteria. Focuses on video neural representations and frequency separation, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.06632": {
        "authors": [
            "Yifan Gao",
            "Zihang Lin",
            "Chuanbin Liu",
            "Min Zhou",
            "Tiezheng Ge",
            "Bo Zheng",
            "Hongtao Xie"
        ],
        "title": "PosterMaker: Towards High-Quality Product Poster Generation with Accurate Text Rendering",
        "abstract": "arXiv:2504.06632v1 Announce Type: new  Abstract: Product posters, which integrate subject, scene, and text, are crucial promotional tools for attracting customers. Creating such posters using modern image generation methods is valuable, while the main challenge lies in accurately rendering text, especially for complex writing systems like Chinese, which contains over 10,000 individual characters. In this work, we identify the key to precise text rendering as constructing a character-discriminative visual feature as a control signal. Based on this insight, we propose a robust character-wise representation as control and we develop TextRenderNet, which achieves a high text rendering accuracy of over 90%. Another challenge in poster generation is maintaining the fidelity of user-specific products. We address this by introducing SceneGenNet, an inpainting-based model, and propose subject fidelity feedback learning to further enhance fidelity. Based on TextRenderNet and SceneGenNet, we present PosterMaker, an end-to-end generation framework. To optimize PosterMaker efficiently, we implement a two-stage training strategy that decouples text rendering and background generation learning. Experimental results show that PosterMaker outperforms existing baselines by a remarkable margin, which demonstrates its effectiveness.",
        "arxiv_id": "2504.06632",
        "ARXIVID": "2504.06632",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling for product poster creation, which is tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.06781": {
        "authors": [
            "Reiji Saito",
            "Kazuhiro Hotta"
        ],
        "title": "Domain Generalization through Attenuation of Domain-Specific Information",
        "abstract": "arXiv:2504.06781v1 Announce Type: new  Abstract: In this paper, we propose a new evaluation metric called Domain Independence (DI) and Attenuation of Domain-Specific Information (ADSI) which is specifically designed for domain-generalized semantic segmentation in automotive images. DI measures the presence of domain-specific information: a lower DI value indicates strong domain dependence, while a higher DI value suggests greater domain independence. This makes it roughly where domain-specific information exists and up to which frequency range it is present. As a result, it becomes possible to effectively suppress only the regions in the image that contain domain-specific information, enabling feature extraction independent of the domain. ADSI uses a Butterworth filter to remove the low-frequency components of images that contain inherent domain-specific information such as sensor characteristics and lighting conditions. However, since low-frequency components also contain important information such as color, we should not remove them completely. Thus, a scalar value (ranging from 0 to 1) is multiplied by the low-frequency components to retain essential information. This helps the model learn more domain-independent features. In experiments, GTA5 (synthetic dataset) was used as training images, and a real-world dataset was used for evaluation, and the proposed method outperformed conventional approaches. Similarly, in experiments that the Cityscapes (real-world dataset) was used for training and various environment datasets such as rain and nighttime were used for evaluation, the proposed method demonstrated its robustness under nighttime conditions.",
        "arxiv_id": "2504.06781",
        "ARXIVID": "2504.06781",
        "COMMENT": "Does not match any specific criterion but is related to domain generalization, which is tangentially relevant to spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.06881": {
        "authors": [
            "Mingbo Li",
            "Liying Liu",
            "Ye Luo"
        ],
        "title": "Compound and Parallel Modes of Tropical Convolutional Neural Networks",
        "abstract": "arXiv:2504.06881v1 Announce Type: new  Abstract: Convolutional neural networks have become increasingly deep and complex, leading to higher computational costs. While tropical convolutional neural networks (TCNNs) reduce multiplications, they underperform compared to standard CNNs. To address this, we propose two new variants - compound TCNN (cTCNN) and parallel TCNN (pTCNN)-that use combinations of tropical min-plus and max-plus kernels to replace traditional convolution kernels. This reduces multiplications and balances efficiency with performance. Experiments on various datasets show that cTCNN and pTCNN match or exceed the performance of other CNN methods. Combining these with conventional CNNs in deeper architectures also improves performance. We are further exploring simplified TCNN architectures that reduce parameters and multiplications with minimal accuracy loss, aiming for efficient and effective models.",
        "arxiv_id": "2504.06881",
        "ARXIVID": "2504.06881",
        "COMMENT": "Does not match any specific criteria. Focuses on tropical CNNs, which are not directly related to spatial understanding, VLLMs, MLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.06544": {
        "authors": [
            "Weiwei Xing",
            "Yue Cheng",
            "Hongzhu Yi",
            "Xiaohui Gao",
            "Xiang Wei",
            "Xiaoyu Guo",
            "Yuming Zhang",
            "Xinyu Pang"
        ],
        "title": "LCGC: Learning from Consistency Gradient Conflicting for Class-Imbalanced Semi-Supervised Debiasing",
        "abstract": "arXiv:2504.06544v1 Announce Type: new  Abstract: Classifiers often learn to be biased corresponding to the class-imbalanced dataset, especially under the semi-supervised learning (SSL) set. While previous work tries to appropriately re-balance the classifiers by subtracting a class-irrelevant image's logit, but lacks a firm theoretical basis. We theoretically analyze why exploiting a baseline image can refine pseudo-labels and prove that the black image is the best choice. We also indicated that as the training process deepens, the pseudo-labels before and after refinement become closer. Based on this observation, we propose a debiasing scheme dubbed LCGC, which Learning from Consistency Gradient Conflicting, by encouraging biased class predictions during training. We intentionally update the pseudo-labels whose gradient conflicts with the debiased logits, representing the optimization direction offered by the over-imbalanced classifier predictions. Then, we debiased the predictions by subtracting the baseline image logits during testing. Extensive experiments demonstrate that LCGC can significantly improve the prediction accuracy of existing CISSL models on public benchmarks.",
        "arxiv_id": "2504.06544",
        "ARXIVID": "2504.06544",
        "COMMENT": "Does not match any specific criteria. Focuses on class-imbalanced semi-supervised learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.06358": {
        "authors": [
            "Yupeng Cheng",
            "Zi Pong Lim",
            "Sarthak Ketanbhai Modi",
            "Yon Shin Teo",
            "Yushi Cao",
            "Shang-Wei Lin"
        ],
        "title": "Towards Calibration Enhanced Network by Inverse Adversarial Attack",
        "abstract": "arXiv:2504.06358v1 Announce Type: new  Abstract: Test automation has become increasingly important as the complexity of both design and content in Human Machine Interface (HMI) software continues to grow. Current standard practice uses Optical Character Recognition (OCR) techniques to automatically extract textual information from HMI screens for validation. At present, one of the key challenges faced during the automation of HMI screen validation is the noise handling for the OCR models. In this paper, we propose to utilize adversarial training techniques to enhance OCR models in HMI testing scenarios. More specifically, we design a new adversarial attack objective for OCR models to discover the decision boundaries in the context of HMI testing. We then adopt adversarial training to optimize the decision boundaries towards a more robust and accurate OCR model. In addition, we also built an HMI screen dataset based on real-world requirements and applied multiple types of perturbation onto the clean HMI dataset to provide a more complete coverage for the potential scenarios. We conduct experiments to demonstrate how using adversarial training techniques yields more robust OCR models against various kinds of noises, while still maintaining high OCR model accuracy. Further experiments even demonstrate that the adversarial training models exhibit a certain degree of robustness against perturbations from other patterns.",
        "arxiv_id": "2504.06358",
        "ARXIVID": "2504.06358",
        "COMMENT": "Does not match any specific criteria. Focuses on adversarial training for OCR models in HMI testing.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.06514": {
        "authors": [
            "Chenrui Fan",
            "Ming Li",
            "Lichao Sun",
            "Tianyi Zhou"
        ],
        "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?",
        "abstract": "arXiv:2504.06514v1 Announce Type: new  Abstract: We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general overthinking issue to a large extent, which we name as the MiP-Overthinking. Such failures are against the ``test-time scaling law'' but have been widely observed on multiple datasets we curated with MiP, indicating the harm of cheap overthinking and a lack of critical thinking. Surprisingly, LLMs not specifically trained for reasoning exhibit much better performance on the MiP scenario, producing much shorter responses that quickly identify ill-posed queries. This implies a critical flaw of the current training recipe for reasoning LLMs, which does not encourage efficient thinking adequately, leading to the abuse of thinking patterns. To further investigate the reasons behind such failures, we conduct fine-grained analyses of the reasoning length, overthinking patterns, and location of critical thinking on different types of LLMs. Moreover, our extended ablation study reveals that the overthinking is contagious through the distillation of reasoning models' responses. These results improve the understanding of overthinking and shed novel insights into mitigating the problem.",
        "arxiv_id": "2504.06514",
        "ARXIVID": "2504.06514",
        "COMMENT": "Does not match any specific criteria. Focuses on reasoning models and overthinking in LLMs, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.06578": {
        "authors": [
            "Rahul Singh Maharjan",
            "Marta Romeo",
            "Angelo Cangelosi"
        ],
        "title": "Attributes-aware Visual Emotion Representation Learning",
        "abstract": "arXiv:2504.06578v1 Announce Type: new  Abstract: Visual emotion analysis or recognition has gained considerable attention due to the growing interest in understanding how images can convey rich semantics and evoke emotions in human perception. However, visual emotion analysis poses distinctive challenges compared to traditional vision tasks, especially due to the intricate relationship between general visual features and the different affective states they evoke, known as the affective gap. Researchers have used deep representation learning methods to address this challenge of extracting generalized features from entire images. However, most existing methods overlook the importance of specific emotional attributes such as brightness, colorfulness, scene understanding, and facial expressions. Through this paper, we introduce A4Net, a deep representation network to bridge the affective gap by leveraging four key attributes: brightness (Attribute 1), colorfulness (Attribute 2), scene context (Attribute 3), and facial expressions (Attribute 4). By fusing and jointly training all aspects of attribute recognition and visual emotion analysis, A4Net aims to provide a better insight into emotional content in images. Experimental results show the effectiveness of A4Net, showcasing competitive performance compared to state-of-the-art methods across diverse visual emotion datasets. Furthermore, visualizations of activation maps generated by A4Net offer insights into its ability to generalize across different visual emotion datasets.",
        "arxiv_id": "2504.06578",
        "ARXIVID": "2504.06578",
        "COMMENT": "Does not match any specific criteria. Focuses on visual emotion representation learning, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.07078": {
        "authors": [
            "Meien Li",
            "Mark Stamp"
        ],
        "title": "Detecting AI-generated Artwork",
        "abstract": "arXiv:2504.07078v1 Announce Type: new  Abstract: The high efficiency and quality of artwork generated by Artificial Intelligence (AI) has created new concerns and challenges for human artists. In particular, recent improvements in generative AI have made it difficult for people to distinguish between human-generated and AI-generated art. In this research, we consider the potential utility of various types of Machine Learning (ML) and Deep Learning (DL) models in distinguishing AI-generated artwork from human-generated artwork. We focus on three challenging artistic styles, namely, baroque, cubism, and expressionism. The learning models we test are Logistic Regression (LR), Support Vector Machine (SVM), Multilayer Perceptron (MLP), and Convolutional Neural Network (CNN). Our best experimental results yield a multiclass accuracy of 0.8208 over six classes, and an impressive accuracy of 0.9758 for the binary classification problem of distinguishing AI-generated from human-generated art.",
        "arxiv_id": "2504.07078",
        "ARXIVID": "2504.07078",
        "COMMENT": "Does not match any specific criterion but is related to distinguishing AI-generated artwork, which is tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}