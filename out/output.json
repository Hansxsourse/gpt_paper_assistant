{
    "2507.17388": {
        "authors": [
            "Xinyu Liu",
            "Hengyu Liu",
            "Cheng Wang",
            "Tianming Liu",
            "Yixuan Yuan"
        ],
        "title": "EndoGen: Conditional Autoregressive Endoscopic Video Generation",
        "abstract": "arXiv:2507.17388v1 Announce Type: new  Abstract: Endoscopic video generation is crucial for advancing medical imaging and enhancing diagnostic capabilities. However, prior efforts in this field have either focused on static images, lacking the dynamic context required for practical applications, or have relied on unconditional generation that fails to provide meaningful references for clinicians. Therefore, in this paper, we propose the first conditional endoscopic video generation framework, namely EndoGen. Specifically, we build an autoregressive model with a tailored Spatiotemporal Grid-Frame Patterning (SGP) strategy. It reformulates the learning of generating multiple frames as a grid-based image generation pattern, which effectively capitalizes the inherent global dependency modeling capabilities of autoregressive architectures. Furthermore, we propose a Semantic-Aware Token Masking (SAT) mechanism, which enhances the model's ability to produce rich and diverse content by selectively focusing on semantically meaningful regions during the generation process. Through extensive experiments, we demonstrate the effectiveness of our framework in generating high-quality, conditionally guided endoscopic content, and improves the performance of downstream task of polyp segmentation. Code released at https://www.github.com/CUHK-AIM-Group/EndoGen.",
        "arxiv_id": "2507.17388",
        "ARXIVID": "2507.17388",
        "COMMENT": "Matches criteria 1 as it proposes a framework for conditional video generation and segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.17613": {
        "authors": [
            "Xiaoxue Chen",
            "Bhargav Chandaka",
            "Chih-Hao Lin",
            "Ya-Qin Zhang",
            "David Forsyth",
            "Hao Zhao",
            "Shenlong Wang"
        ],
        "title": "InvRGB+L: Inverse Rendering of Complex Scenes with Unified Color and LiDAR Reflectance Modeling",
        "abstract": "arXiv:2507.17613v1 Announce Type: new  Abstract: We present InvRGB+L, a novel inverse rendering model that reconstructs large, relightable, and dynamic scenes from a single RGB+LiDAR sequence. Conventional inverse graphics methods rely primarily on RGB observations and use LiDAR mainly for geometric information, often resulting in suboptimal material estimates due to visible light interference. We find that LiDAR's intensity values-captured with active illumination in a different spectral range-offer complementary cues for robust material estimation under variable lighting. Inspired by this, InvRGB+L leverages LiDAR intensity cues to overcome challenges inherent in RGB-centric inverse graphics through two key innovations: (1) a novel physics-based LiDAR shading model and (2) RGB-LiDAR material consistency losses. The model produces novel-view RGB and LiDAR renderings of urban and indoor scenes and supports relighting, night simulations, and dynamic object insertions, achieving results that surpass current state-of-the-art methods in both scene-level urban inverse rendering and LiDAR simulation.",
        "arxiv_id": "2507.17613",
        "ARXIVID": "2507.17613",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.17661": {
        "authors": [
            "Xuzhi Wang",
            "Xinran Wu",
            "Song Wang",
            "Lingdong Kong",
            "Ziping Zhao"
        ],
        "title": "Monocular Semantic Scene Completion via Masked Recurrent Networks",
        "abstract": "arXiv:2507.17661v1 Announce Type: new  Abstract: Monocular Semantic Scene Completion (MSSC) aims to predict the voxel-wise occupancy and semantic category from a single-view RGB image. Existing methods adopt a single-stage framework that aims to simultaneously achieve visible region segmentation and occluded region hallucination, while also being affected by inaccurate depth estimation. Such methods often achieve suboptimal performance, especially in complex scenes. We propose a novel two-stage framework that decomposes MSSC into coarse MSSC followed by the Masked Recurrent Network. Specifically, we propose the Masked Sparse Gated Recurrent Unit (MS-GRU) which concentrates on the occupied regions by the proposed mask updating mechanism, and a sparse GRU design is proposed to reduce the computation cost. Additionally, we propose the distance attention projection to reduce projection errors by assigning different attention scores according to the distance to the observed surface. Experimental results demonstrate that our proposed unified framework, MonoMRN, effectively supports both indoor and outdoor scenes and achieves state-of-the-art performance on the NYUv2 and SemanticKITTI datasets. Furthermore, we conduct robustness analysis under various disturbances, highlighting the role of the Masked Recurrent Network in enhancing the model's resilience to such challenges. The source code is publicly available.",
        "arxiv_id": "2507.17661",
        "ARXIVID": "2507.17661",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}