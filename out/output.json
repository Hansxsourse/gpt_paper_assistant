{
    "2504.21356": {
        "authors": [
            "Hong Zhang",
            "Zhongjie Duan",
            "Xingjun Wang",
            "Yingda Chen",
            "Yuze Zhao",
            "Yu Zhang"
        ],
        "title": "Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing",
        "abstract": "arXiv:2504.21356v1 Announce Type: new  Abstract: Unified multimodal large language models (MLLMs) aim to integrate multimodal understanding and generation abilities through a single framework. Despite their versatility, existing open-source unified models exhibit performance gaps against domain-specific architectures. To bridge this gap, we present Nexus-Gen, a unified model that synergizes the language reasoning capabilities of LLMs with the image synthesis power of diffusion models. To align the embedding space of the LLM and diffusion model, we conduct a dual-phase alignment training process. (1) The autoregressive LLM learns to predict image embeddings conditioned on multimodal inputs, while (2) the vision decoder is trained to reconstruct high-fidelity images from these embeddings. During training the LLM, we identified a critical discrepancy between the autoregressive paradigm's training and inference phases, where error accumulation in continuous embedding space severely degrades generation quality. To avoid this issue, we introduce a prefilled autoregression strategy that prefills input sequence with position-embedded special tokens instead of continuous embeddings. Through dual-phase training, Nexus-Gen has developed the integrated capability to comprehensively address the image understanding, generation and editing tasks. All models, datasets, and codes are published at https://github.com/modelscope/Nexus-Gen.git to facilitate further advancements across the field.",
        "arxiv_id": "2504.21356",
        "ARXIVID": "2504.21356",
        "COMMENT": "Matches criterion 2 as it introduces a unified multimodal large language model (MLLM) for image understanding, generation, and editing.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2504.21447": {
        "authors": [
            "Haoran Chen",
            "Junyan Lin",
            "Xinhao Chen",
            "Yue Fan",
            "Xin Jin",
            "Hui Su",
            "Jianfeng Dong",
            "Jinlan Fu",
            "Xiaoyu Shen"
        ],
        "title": "Rethinking Visual Layer Selection in Multimodal LLMs",
        "abstract": "arXiv:2504.21447v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have achieved impressive performance across a wide range of tasks, typically using CLIP-ViT as their visual encoder due to its strong text-image alignment capabilities. While prior studies suggest that different CLIP-ViT layers capture different types of information, with shallower layers focusing on fine visual details and deeper layers aligning more closely with textual semantics, most MLLMs still select visual features based on empirical heuristics rather than systematic analysis. In this work, we propose a Layer-wise Representation Similarity approach to group CLIP-ViT layers with similar behaviors into {shallow, middle, and deep} categories and assess their impact on MLLM performance. Building on this foundation, we revisit the visual layer selection problem in MLLMs at scale, training LLaVA-style models ranging from 1.4B to 7B parameters. Through extensive experiments across 10 datasets and 4 tasks, we find that: (1) deep layers are essential for OCR tasks; (2) shallow and middle layers substantially outperform deep layers on reasoning tasks involving counting, positioning, and object localization; (3) a lightweight fusion of features across shallow, middle, and deep layers consistently outperforms specialized fusion baselines and single-layer selections, achieving gains on 9 out of 10 datasets. Our work offers the first principled study of visual layer selection in MLLMs, laying the groundwork for deeper investigations into visual representation learning for MLLMs.",
        "arxiv_id": "2504.21447",
        "ARXIVID": "2504.21447",
        "COMMENT": "Matches criterion 2 as it systematically studies visual layer selection in multimodal large language models (MLLMs), which aligns with the interest in VLLMs/MLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2504.21650": {
        "authors": [
            "Haiyang Zhou",
            "Wangbo Yu",
            "Jiawen Guan",
            "Xinhua Cheng",
            "Yonghong Tian",
            "Li Yuan"
        ],
        "title": "HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation",
        "abstract": "arXiv:2504.21650v1 Announce Type: new  Abstract: The rapid advancement of diffusion models holds the promise of revolutionizing the application of VR and AR technologies, which typically require scene-level 4D assets for user experience. Nonetheless, existing diffusion models predominantly concentrate on modeling static 3D scenes or object-level dynamics, constraining their capacity to provide truly immersive experiences. To address this issue, we propose HoloTime, a framework that integrates video diffusion models to generate panoramic videos from a single prompt or reference image, along with a 360-degree 4D scene reconstruction method that seamlessly transforms the generated panoramic video into 4D assets, enabling a fully immersive 4D experience for users. Specifically, to tame video diffusion models for generating high-fidelity panoramic videos, we introduce the 360World dataset, the first comprehensive collection of panoramic videos suitable for downstream 4D scene reconstruction tasks. With this curated dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion model that can convert panoramic images into high-quality panoramic videos. Following this, we present Panoramic Space-Time Reconstruction, which leverages a space-time depth estimation method to transform the generated panoramic videos into 4D point clouds, enabling the optimization of a holistic 4D Gaussian Splatting representation to reconstruct spatially and temporally consistent 4D scenes. To validate the efficacy of our method, we conducted a comparative analysis with existing approaches, revealing its superiority in both panoramic video generation and 4D scene reconstruction. This demonstrates our method's capability to create more engaging and realistic immersive environments, thereby enhancing user experiences in VR and AR applications.",
        "arxiv_id": "2504.21650",
        "ARXIVID": "2504.21650",
        "COMMENT": "Matches criterion 4 as it introduces a novel framework (HoloTime) for panoramic 4D scene generation using video diffusion models, with applications in VR and AR.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.21559": {
        "authors": [
            "Sangmin Woo",
            "Kang Zhou",
            "Yun Zhou",
            "Shuai Wang",
            "Sheng Guan",
            "Haibo Ding",
            "Lin Lee Cheong"
        ],
        "title": "Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models",
        "abstract": "arXiv:2504.21559v1 Announce Type: new  Abstract: Large Vision Language Models (LVLMs) often suffer from object hallucination, which undermines their reliability. Surprisingly, we find that simple object-based visual prompting -- overlaying visual cues (e.g., bounding box, circle) on images -- can significantly mitigate such hallucination; however, different visual prompts (VPs) vary in effectiveness. To address this, we propose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify optimal VPs that enhance LVLM responses without needing access to model internals. Our approach employs a pool of candidate VPs and trains a router model to dynamically select the most effective VP for a given input image. This black-box approach is model-agnostic, making it applicable to both open-source and proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR demonstrate that BBVPE effectively reduces object hallucination.",
        "arxiv_id": "2504.21559",
        "ARXIVID": "2504.21559",
        "COMMENT": "Matches criterion 2 as it addresses object hallucination in large vision-language models (LVLMs) using a novel visual prompt engineering framework.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2504.21561": {
        "authors": [
            "Pengxiang Li",
            "Zhi Gao",
            "Bofei Zhang",
            "Yapeng Mi",
            "Xiaojian Ma",
            "Chenrui Shi",
            "Tao Yuan",
            "Yuwei Wu",
            "Yunde Jia",
            "Song-Chun Zhu",
            "Qing Li"
        ],
        "title": "Iterative Trajectory Exploration for Multimodal Agents",
        "abstract": "arXiv:2504.21561v1 Announce Type: new  Abstract: Multimodal agents, which integrate a controller (e.g., a large language model) with external tools, have demonstrated remarkable capabilities in tackling complex tasks. However, existing agents need to collect a large number of expert data for fine-tuning to adapt to new environments. In this paper, we propose an online self-exploration method for multimodal agents, namely SPORT, via step-wise preference optimization to refine the trajectories of agents, which automatically generates tasks and learns from solving the generated tasks, without any expert annotation. SPORT operates through four iterative components: task synthesis, step sampling, step verification, and preference tuning. First, we synthesize multi-modal tasks using language models. Then, we introduce a novel search scheme, where step sampling and step verification are executed alternately to solve each generated task. We employ a verifier to provide AI feedback to construct step-wise preference data. The data is subsequently used to update the controller's policy through preference tuning, producing a SPORT Agent. By interacting with real environments, the SPORT Agent evolves into a more refined and capable system. Evaluation in the GTA and GAIA benchmarks show that the SPORT Agent achieves 6.41\\% and 3.64\\% improvements, underscoring the generalization and effectiveness introduced by our method. The project page is https://SPORT-Agents.github.io.",
        "arxiv_id": "2504.21561",
        "ARXIVID": "2504.21561",
        "COMMENT": "Matches criterion 3 as it proposes a novel self-exploration method (SPORT) for multimodal agents, focusing on online task generation and learning without expert annotation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.21544": {
        "authors": [
            "Uzair Shah",
            "Marco Agus",
            "Daniya Boges",
            "Vanessa Chiappini",
            "Mahmood Alzubaidi",
            "Jens Schneider",
            "Markus Hadwiger",
            "Pierre J. Magistretti",
            "Mowafa Househ",
            "Corrado Cal{\\i}"
        ],
        "title": "SAM4EM: Efficient memory-based two stage prompt-free segment anything model adapter for complex 3D neuroscience electron microscopy stacks",
        "abstract": "arXiv:2504.21544v1 Announce Type: new  Abstract: We present SAM4EM, a novel approach for 3D segmentation of complex neural structures in electron microscopy (EM) data by leveraging the Segment Anything Model (SAM) alongside advanced fine-tuning strategies. Our contributions include the development of a prompt-free adapter for SAM using two stage mask decoding to automatically generate prompt embeddings, a dual-stage fine-tuning method based on Low-Rank Adaptation (LoRA) for enhancing segmentation with limited annotated data, and a 3D memory attention mechanism to ensure segmentation consistency across 3D stacks. We further release a unique benchmark dataset for the segmentation of astrocytic processes and synapses. We evaluated our method on challenging neuroscience segmentation benchmarks, specifically targeting mitochondria, glia, and synapses, with significant accuracy improvements over state-of-the-art (SOTA) methods, including recent SAM-based adapters developed for the medical domain and other vision transformer-based approaches. Experimental results indicate that our approach outperforms existing solutions in the segmentation of complex processes like glia and post-synaptic densities. Our code and models are available at https://github.com/Uzshah/SAM4EM.",
        "arxiv_id": "2504.21544",
        "ARXIVID": "2504.21544",
        "COMMENT": "Matches criterion 4 as it adapts the Segment Anything Model (SAM) for 3D neuroscience segmentation, which is an application of vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.21344": {
        "authors": [
            "Luoting Zhuang",
            "Seyed Mohammad Hossein Tabatabaei",
            "Ramin Salehi-Rad",
            "Linh M. Tran",
            "Denise R. Aberle",
            "Ashley E. Prosper",
            "William Hsu"
        ],
        "title": "Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early Lung Cancer Detection",
        "abstract": "arXiv:2504.21344v1 Announce Type: new  Abstract: Objective: A number of machine learning models have utilized semantic features, deep features, or both to assess lung nodule malignancy. However, their reliance on manual annotation during inference, limited interpretability, and sensitivity to imaging variations hinder their application in real-world clinical settings. Thus, this research aims to integrate semantic features derived from radiologists' assessments of nodules, allowing the model to learn clinically relevant, robust, and explainable features for predicting lung cancer. Methods: We obtained 938 low-dose CT scans from the National Lung Screening Trial with 1,246 nodules and semantic features. The Lung Image Database Consortium dataset contains 1,018 CT scans, with 2,625 lesions annotated for nodule characteristics. Three external datasets were obtained from UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We finetuned a pretrained Contrastive Language-Image Pretraining model with a parameter-efficient fine-tuning approach to align imaging and semantic features and predict the one-year lung cancer diagnosis. Results: We evaluated the performance of the one-year diagnosis of lung cancer with AUROC and AUPRC and compared it to three state-of-the-art models. Our model demonstrated an AUROC of 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on external datasets. Using CLIP, we also obtained predictions on semantic features, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and pleural attachment (0.84), that can be used to explain model predictions. Conclusion: Our approach accurately classifies lung nodules as benign or malignant, providing explainable outputs, aiding clinicians in comprehending the underlying meaning of model predictions. This approach also prevents the model from learning shortcuts and generalizes across clinical settings.",
        "arxiv_id": "2504.21344",
        "ARXIVID": "2504.21344",
        "COMMENT": "Matches criterion 2 as it uses a vision-language model (CLIP) for semantic-guided imaging biomarker prediction, which aligns with VLLMs/MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2504.21435": {
        "authors": [
            "Chenkai Zhang",
            "Yiming Lei",
            "Zeming Liu",
            "Haitao Leng",
            "ShaoGuo Liu",
            "Tingting Gao",
            "Qingjie Liu",
            "Yunhong Wang"
        ],
        "title": "SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding",
        "abstract": "arXiv:2504.21435v1 Announce Type: new  Abstract: With the rapid development of Multi-modal Large Language Models (MLLMs), an increasing number of benchmarks have been established to evaluate the video understanding capabilities of these models. However, these benchmarks focus on \\textbf{standalone} videos and mainly assess ``visual elements'' like human actions and object states. In reality, contemporary videos often encompass complex and continuous narratives, typically presented as a \\textbf{series}. To address this challenge, we propose \\textbf{SeriesBench}, a benchmark consisting of 105 carefully curated narrative-driven series, covering 28 specialized tasks that require deep narrative understanding. Specifically, we first select a diverse set of drama series spanning various genres. Then, we introduce a novel long-span narrative annotation method, combined with a full-information transformation approach to convert manual annotations into diverse task formats. To further enhance model capacity for detailed analysis of plot structures and character relationships within series, we propose a novel narrative reasoning framework, \\textbf{PC-DCoT}. Extensive results on \\textbf{SeriesBench} indicate that existing MLLMs still face significant challenges in understanding narrative-driven series, while \\textbf{PC-DCoT} enables these MLLMs to achieve performance improvements. Overall, our \\textbf{SeriesBench} and \\textbf{PC-DCoT} highlight the critical necessity of advancing model capabilities to understand narrative-driven series, guiding the future development of MLLMs. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025.",
        "arxiv_id": "2504.21435",
        "ARXIVID": "2504.21435",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (SeriesBench) for narrative-driven video understanding and proposes a novel narrative reasoning framework (PC-DCoT).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.21318": {
        "authors": [
            "Marah Abdin",
            "Sahaj Agarwal",
            "Ahmed Awadallah",
            "Vidhisha Balachandran",
            "Harkirat Behl",
            "Lingjiao Chen",
            "Gustavo de Rosa",
            "Suriya Gunasekar",
            "Mojan Javaheripi",
            "Neel Joshi",
            "Piero Kauffmann",
            "Yash Lara",
            "Caio C\\'esar Teodoro Mendes",
            "Arindam Mitra",
            "Besmira Nushi",
            "Dimitris Papailiopoulos",
            "Olli Saarikivi",
            "Shital Shah",
            "Vaishnavi Shrivastava",
            "Vibhav Vineet",
            "Yue Wu",
            "Safoora Yousefi",
            "Guoqing Zheng"
        ],
        "title": "Phi-4-reasoning Technical Report",
        "abstract": "arXiv:2504.21318v1 Announce Type: new  Abstract: We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.",
        "arxiv_id": "2504.21318",
        "ARXIVID": "2504.21318",
        "COMMENT": "Matches criterion 4 as it evaluates a reasoning model (Phi-4-reasoning) on spatial understanding tasks, which may align with vision foundation models and their applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.21263": {
        "authors": [
            "Jinpeng Wang",
            "Tianci Luo",
            "Yaohua Zha",
            "Yan Feng",
            "Ruisheng Luo",
            "Bin Chen",
            "Tao Dai",
            "Long Chen",
            "Yaowei Wang",
            "Shu-Tao Xia"
        ],
        "title": "Embracing Collaboration Over Competition: Condensing Multiple Prompts for Visual In-Context Learning",
        "abstract": "arXiv:2504.21263v1 Announce Type: new  Abstract: Visual In-Context Learning (VICL) enables adaptively solving vision tasks by leveraging pixel demonstrations, mimicking human-like task completion through analogy. Prompt selection is critical in VICL, but current methods assume the existence of a single \"ideal\" prompt in a pool of candidates, which in practice may not hold true. Multiple suitable prompts may exist, but individually they often fall short, leading to difficulties in selection and the exclusion of useful context. To address this, we propose a new perspective: prompt condensation. Rather than relying on a single prompt, candidate prompts collaborate to efficiently integrate informative contexts without sacrificing resolution. We devise Condenser, a lightweight external plugin that compresses relevant fine-grained context across multiple prompts. Optimized end-to-end with the backbone, Condenser ensures accurate integration of contextual cues. Experiments demonstrate Condenser outperforms state-of-the-arts across benchmark tasks, showing superior context compression, scalability with more prompts, and enhanced computational efficiency compared to ensemble methods, positioning it as a highly competitive solution for VICL. Code is open-sourced at https://github.com/gimpong/CVPR25-Condenser.",
        "arxiv_id": "2504.21263",
        "ARXIVID": "2504.21263",
        "COMMENT": "Matches criterion 4 as it focuses on visual in-context learning and proposes a novel method for prompt condensation, which is related to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.21403": {
        "authors": [
            "Yumeng Shi",
            "Quanyu Long",
            "Wenya Wang"
        ],
        "title": "Static or Dynamic: Towards Query-Adaptive Token Selection for Video Question Answering",
        "abstract": "arXiv:2504.21403v1 Announce Type: new  Abstract: Video question answering benefits from the rich information available in videos, enabling a wide range of applications. However, the large volume of tokens generated from longer videos presents significant challenges to memory efficiency and model performance. To alleviate this issue, existing works propose to compress video inputs, but usually overlooking the varying importance of static and dynamic information across different queries, leading to inefficient token usage within limited budgets. To tackle this, we propose a novel token selection strategy, EXPLORE-THEN-SELECT, that adaptively adjust static and dynamic information needed based on question requirements. Our framework first explores different token allocations between static frames, which preserve spatial details, and dynamic frames, which capture temporal changes. Next, it employs a query-aware attention-based metric to select the optimal token combination without model updates. Our proposed framework is plug-and-play that can be seamlessly integrated within diverse video-language models. Extensive experiments show that our method achieves significant performance improvements (up to 5.8%) among various video question answering benchmarks.",
        "arxiv_id": "2504.21403",
        "ARXIVID": "2504.21403",
        "COMMENT": "Matches criterion 3 as it proposes a novel token selection strategy for video question answering, focusing on query-adaptive token selection.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.21643": {
        "authors": [
            "Luca Marzari",
            "Francesco Trotti",
            "Enrico Marchesini",
            "Alessandro Farinelli"
        ],
        "title": "Designing Control Barrier Function via Probabilistic Enumeration for Safe Reinforcement Learning Navigation",
        "abstract": "arXiv:2504.21643v1 Announce Type: new  Abstract: Achieving safe autonomous navigation systems is critical for deploying robots in dynamic and uncertain real-world environments. In this paper, we propose a hierarchical control framework leveraging neural network verification techniques to design control barrier functions (CBFs) and policy correction mechanisms that ensure safe reinforcement learning navigation policies. Our approach relies on probabilistic enumeration to identify unsafe regions of operation, which are then used to construct a safe CBF-based control layer applicable to arbitrary policies. We validate our framework both in simulation and on a real robot, using a standard mobile robot benchmark and a highly dynamic aquatic environmental monitoring task. These experiments demonstrate the ability of the proposed solution to correct unsafe actions while preserving efficient navigation behavior. Our results show the promise of developing hierarchical verification-based systems to enable safe and robust navigation behaviors in complex scenarios.",
        "arxiv_id": "2504.21643",
        "ARXIVID": "2504.21643",
        "COMMENT": "Matches criterion 3 as it proposes a novel hierarchical control framework for safe reinforcement learning navigation, focusing on safety in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.21682": {
        "authors": [
            "Yan Shu",
            "Weichao Zeng",
            "Fangmin Zhao",
            "Zeyu Chen",
            "Zhenhang Li",
            "Xiaomeng Yang",
            "Yu Zhou",
            "Paolo Rota",
            "Xiang Bai",
            "Lianwen Jin",
            "Xu-Cheng Yin",
            "Nicu Sebe"
        ],
        "title": "Visual Text Processing: A Comprehensive Review and Unified Evaluation",
        "abstract": "arXiv:2504.21682v1 Announce Type: new  Abstract: Visual text is a crucial component in both document and scene images, conveying rich semantic information and attracting significant attention in the computer vision community. Beyond traditional tasks such as text detection and recognition, visual text processing has witnessed rapid advancements driven by the emergence of foundation models, including text image reconstruction and text image manipulation. Despite significant progress, challenges remain due to the unique properties that differentiate text from general objects. Effectively capturing and leveraging these distinct textual characteristics is essential for developing robust visual text processing models. In this survey, we present a comprehensive, multi-perspective analysis of recent advancements in visual text processing, focusing on two key questions: (1) What textual features are most suitable for different visual text processing tasks? (2) How can these distinctive text features be effectively incorporated into processing frameworks? Furthermore, we introduce VTPBench, a new benchmark that encompasses a broad range of visual text processing datasets. Leveraging the advanced visual quality assessment capabilities of multimodal large language models (MLLMs), we propose VTPScore, a novel evaluation metric designed to ensure fair and reliable evaluation. Our empirical study with more than 20 specific models reveals substantial room for improvement in the current techniques. Our aim is to establish this work as a fundamental resource that fosters future exploration and innovation in the dynamic field of visual text processing. The relevant repository is available at https://github.com/shuyansy/Visual-Text-Processing-survey.",
        "arxiv_id": "2504.21682",
        "ARXIVID": "2504.21682",
        "COMMENT": "Matches criterion 4 as it provides a comprehensive review of visual text processing advancements and introduces a new benchmark (VTPBench) and evaluation metric (VTPScore).",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2504.21308": {
        "authors": [
            "Yunhao Li",
            "Sijing Wu",
            "Wei Sun",
            "Zhichao Zhang",
            "Yucheng Zhu",
            "Zicheng Zhang",
            "Huiyu Duan",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ],
        "title": "AGHI-QA: A Subjective-Aligned Dataset and Metric for AI-Generated Human Images",
        "abstract": "arXiv:2504.21308v1 Announce Type: new  Abstract: The rapid development of text-to-image (T2I) generation approaches has attracted extensive interest in evaluating the quality of generated images, leading to the development of various quality assessment methods for general-purpose T2I outputs. However, existing image quality assessment (IQA) methods are limited to providing global quality scores, failing to deliver fine-grained perceptual evaluations for structurally complex subjects like humans, which is a critical challenge considering the frequent anatomical and textural distortions in AI-generated human images (AGHIs). To address this gap, we introduce AGHI-QA, the first large-scale benchmark specifically designed for quality assessment of AGHIs. The dataset comprises 4,000 images generated from 400 carefully crafted text prompts using 10 state of-the-art T2I models. We conduct a systematic subjective study to collect multidimensional annotations, including perceptual quality scores, text-image correspondence scores, visible and distorted body part labels. Based on AGHI-QA, we evaluate the strengths and weaknesses of current T2I methods in generating human images from multiple dimensions. Furthermore, we propose AGHI-Assessor, a novel quality metric that integrates the large multimodal model (LMM) with domain-specific human features for precise quality prediction and identification of visible and distorted body parts in AGHIs. Extensive experimental results demonstrate that AGHI-Assessor showcases state-of-the-art performance, significantly outperforming existing IQA methods in multidimensional quality assessment and surpassing leading LMMs in detecting structural distortions in AGHIs.",
        "arxiv_id": "2504.21308",
        "ARXIVID": "2504.21308",
        "COMMENT": "Matches criterion 2 as it proposes a novel quality metric (AGHI-Assessor) leveraging large multimodal models for assessing AI-generated human images.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.21614": {
        "authors": [
            "Daniel Bogdoll",
            "Rajanikant Patnaik Ananta",
            "Abeyankar Giridharan",
            "Isabel Moore",
            "Gregory Stevens",
            "Henry X. Liu"
        ],
        "title": "Mcity Data Engine: Iterative Model Improvement Through Open-Vocabulary Data Selection",
        "abstract": "arXiv:2504.21614v1 Announce Type: new  Abstract: With an ever-increasing availability of data, it has become more and more challenging to select and label appropriate samples for the training of machine learning models. It is especially difficult to detect long-tail classes of interest in large amounts of unlabeled data. This holds especially true for Intelligent Transportation Systems (ITS), where vehicle fleets and roadside perception systems generate an abundance of raw data. While industrial, proprietary data engines for such iterative data selection and model training processes exist, researchers and the open-source community suffer from a lack of an openly available system. We present the Mcity Data Engine, which provides modules for the complete data-based development cycle, beginning at the data acquisition phase and ending at the model deployment stage. The Mcity Data Engine focuses on rare and novel classes through an open-vocabulary data selection process. All code is publicly available on GitHub under an MIT license: https://github.com/mcity/mcity_data_engine",
        "arxiv_id": "2504.21614",
        "ARXIVID": "2504.21614",
        "COMMENT": "Matches criterion 3 as it introduces a new open-source benchmark system for iterative data selection and model training, focusing on rare and novel classes.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2504.21855": {
        "authors": [
            "Qihao Liu",
            "Ju He",
            "Qihang Yu",
            "Liang-Chieh Chen",
            "Alan Yuille"
        ],
        "title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction",
        "abstract": "arXiv:2504.21855v1 Announce Type: new  Abstract: In recent years, video generation has seen significant advancements. However, challenges still persist in generating complex motions and interactions. To address these challenges, we introduce ReVision, a plug-and-play framework that explicitly integrates parameterized 3D physical knowledge into a pretrained conditional video generation model, significantly enhancing its ability to generate high-quality videos with complex motion and interactions. Specifically, ReVision consists of three stages. First, a video diffusion model is used to generate a coarse video. Next, we extract a set of 2D and 3D features from the coarse video to construct a 3D object-centric representation, which is then refined by our proposed parameterized physical prior model to produce an accurate 3D motion sequence. Finally, this refined motion sequence is fed back into the same video diffusion model as additional conditioning, enabling the generation of motion-consistent videos, even in scenarios involving complex actions and interactions. We validate the effectiveness of our approach on Stable Video Diffusion, where ReVision significantly improves motion fidelity and coherence. Remarkably, with only 1.5B parameters, it even outperforms a state-of-the-art video generation model with over 13B parameters on complex video generation by a substantial margin. Our results suggest that, by incorporating 3D physical knowledge, even a relatively small video diffusion model can generate complex motions and interactions with greater realism and controllability, offering a promising solution for physically plausible video generation.",
        "arxiv_id": "2504.21855",
        "ARXIVID": "2504.21855",
        "COMMENT": "This paper does not match any specific criteria but is relevant to video generation and 3D modeling, which may align with your friend's general interest in generative modeling.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2504.21302": {
        "authors": [
            "Zhelun Shen",
            "Zhuo Li",
            "Chenming Wu",
            "Zhibo Rao",
            "Lina Liu",
            "Yuchao Dai",
            "Liangjun Zhang"
        ],
        "title": "CMD: Constraining Multimodal Distribution for Domain Adaptation in Stereo Matching",
        "abstract": "arXiv:2504.21302v1 Announce Type: new  Abstract: Recently, learning-based stereo matching methods have achieved great improvement in public benchmarks, where soft argmin and smooth L1 loss play a core contribution to their success. However, in unsupervised domain adaptation scenarios, we observe that these two operations often yield multimodal disparity probability distributions in target domains, resulting in degraded generalization. In this paper, we propose a novel approach, Constrain Multi-modal Distribution (CMD), to address this issue. Specifically, we introduce \\textit{uncertainty-regularized minimization} and \\textit{anisotropic soft argmin} to encourage the network to produce predominantly unimodal disparity distributions in the target domain, thereby improving prediction accuracy. Experimentally, we apply the proposed method to multiple representative stereo-matching networks and conduct domain adaptation from synthetic data to unlabeled real-world scenes. Results consistently demonstrate improved generalization in both top-performing and domain-adaptable stereo-matching models. The code for CMD will be available at: \\href{https://github.com/gallenszl/CMD}{https://github.com/gallenszl/CMD}.",
        "arxiv_id": "2504.21302",
        "ARXIVID": "2504.21302",
        "COMMENT": "This paper does not match any specific criteria. It focuses on domain adaptation in stereo matching, which is outside the scope of the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.21659": {
        "authors": [
            "Haotian Luo",
            "Haiying He",
            "Yibo Wang",
            "Jinluan Yang",
            "Rui Liu",
            "Naiqiang Tan",
            "Xiaochun Cao",
            "Dacheng Tao",
            "Li Shen"
        ],
        "title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization",
        "abstract": "arXiv:2504.21659v1 Announce Type: new  Abstract: Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1",
        "arxiv_id": "2504.21659",
        "ARXIVID": "2504.21659",
        "COMMENT": "This paper does not match any specific criteria. It focuses on adaptive reasoning strategies for large language models, which is outside the scope of the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.21478": {
        "authors": [
            "Zherui Zhang",
            "Changwei Wang",
            "Rongtao Xu",
            "Wenhao Xu",
            "Shibiao Xu",
            "Yu Zhang",
            "Li Guo"
        ],
        "title": "CAE-DFKD: Bridging the Transferability Gap in Data-Free Knowledge Distillation",
        "abstract": "arXiv:2504.21478v1 Announce Type: new  Abstract: Data-Free Knowledge Distillation (DFKD) enables the knowledge transfer from the given pre-trained teacher network to the target student model without access to the real training data. Existing DFKD methods focus primarily on improving image recognition performance on associated datasets, often neglecting the crucial aspect of the transferability of learned representations. In this paper, we propose Category-Aware Embedding Data-Free Knowledge Distillation (CAE-DFKD), which addresses at the embedding level the limitations of previous rely on image-level methods to improve model generalization but fail when directly applied to DFKD. The superiority and flexibility of CAE-DFKD are extensively evaluated, including: \\textit{\\textbf{i.)}} Significant efficiency advantages resulting from altering the generator training paradigm; \\textit{\\textbf{ii.)}} Competitive performance with existing DFKD state-of-the-art methods on image recognition tasks; \\textit{\\textbf{iii.)}} Remarkable transferability of data-free learned representations demonstrated in downstream tasks.",
        "arxiv_id": "2504.21478",
        "ARXIVID": "2504.21478",
        "COMMENT": "This paper does not match any specific criteria. It focuses on data-free knowledge distillation, which is outside the scope of the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.21414": {
        "authors": [
            "Qi Fan",
            "Kaiqi Liu",
            "Nian Liu",
            "Hisham Cholakkal",
            "Rao Muhammad Anwer",
            "Wenbin Li",
            "Yang Gao"
        ],
        "title": "Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining",
        "abstract": "arXiv:2504.21414v1 Announce Type: new  Abstract: Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel classes in new domains, which is often challenging due to the diverse characteristics of target domains and the limited availability of support data. Most CD-FSS methods redesign and retrain in-domain FSS models using various domain-generalization techniques, which are effective but costly to train. To address these issues, we propose adapting informative model structures of the well-trained FSS model for target domains by learning domain characteristics from few-shot labeled support samples during inference, thereby eliminating the need for retraining. Specifically, we first adaptively identify domain-specific model structures by measuring parameter importance using a novel structure Fisher score in a data-dependent manner. Then, we progressively train the selected informative model structures with hierarchically constructed training samples, progressing from fewer to more support shots. The resulting Informative Structure Adaptation (ISA) method effectively addresses domain shifts and equips existing well-trained in-domain FSS models with flexible adaptation capabilities for new domains, eliminating the need to redesign or retrain CD-FSS models on base data. Extensive experiments validate the effectiveness of our method, demonstrating superior performance across multiple CD-FSS benchmarks.",
        "arxiv_id": "2504.21414",
        "ARXIVID": "2504.21414",
        "COMMENT": "Does not match any specific criteria. Focuses on cross-domain few-shot segmentation, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.21292": {
        "authors": [
            "ZiYi Dong",
            "Chengxing Zhou",
            "Weijian Deng",
            "Pengxu Wei",
            "Xiangyang Ji",
            "Liang Lin"
        ],
        "title": "Can We Achieve Efficient Diffusion without Self-Attention? Distilling Self-Attention into Convolutions",
        "abstract": "arXiv:2504.21292v1 Announce Type: new  Abstract: Contemporary diffusion models built upon U-Net or Diffusion Transformer (DiT) architectures have revolutionized image generation through transformer-based attention mechanisms. The prevailing paradigm has commonly employed self-attention with quadratic computational complexity to handle global spatial relationships in complex images, thereby synthesizing high-fidelity images with coherent visual semantics.Contrary to conventional wisdom, our systematic layer-wise analysis reveals an interesting discrepancy: self-attention in pre-trained diffusion models predominantly exhibits localized attention patterns, closely resembling convolutional inductive biases. This suggests that global interactions in self-attention may be less critical than commonly assumed.Driven by this, we propose \\(\\Delta\\)ConvFusion to replace conventional self-attention modules with Pyramid Convolution Blocks (\\(\\Delta\\)ConvBlocks).By distilling attention patterns into localized convolutional operations while keeping other components frozen, \\(\\Delta\\)ConvFusion achieves performance comparable to transformer-based counterparts while reducing computational cost by 6929$\\times$ and surpassing LinFusion by 5.42$\\times$ in efficiency--all without compromising generative fidelity.",
        "arxiv_id": "2504.21292",
        "ARXIVID": "2504.21292",
        "COMMENT": "Does not match any specific criteria. Focuses on replacing self-attention with convolution in diffusion models, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.21487": {
        "authors": [
            "Hebaixu Wang",
            "Jing Zhang",
            "Haonan Guo",
            "Di Wang",
            "Jiayi Ma",
            "Bo Du"
        ],
        "title": "DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling for Image Restoration",
        "abstract": "arXiv:2504.21487v1 Announce Type: new  Abstract: Diffusion models have achieved remarkable progress in universal image restoration. While existing methods speed up inference by reducing sampling steps, substantial step intervals often introduce cumulative errors. Moreover, they struggle to balance the commonality of degradation representations and restoration quality. To address these challenges, we introduce \\textbf{DGSolver}, a diffusion generalist solver with universal posterior sampling. We first derive the exact ordinary differential equations for generalist diffusion models and tailor high-order solvers with a queue-based accelerated sampling strategy to improve both accuracy and efficiency. We then integrate universal posterior sampling to better approximate manifold-constrained gradients, yielding a more accurate noise estimation and correcting errors in inverse inference. Extensive experiments show that DGSolver outperforms state-of-the-art methods in restoration accuracy, stability, and scalability, both qualitatively and quantitatively. Code and models will be available at https://github.com/MiliLab/DGSolver.",
        "arxiv_id": "2504.21487",
        "ARXIVID": "2504.21487",
        "COMMENT": "Does not match any specific criteria. Focuses on image restoration using diffusion models, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.21472": {
        "authors": [
            "Jingjing Liu",
            "Nian Wu",
            "Xianchao Xiu",
            "Jianhua Zhang"
        ],
        "title": "Robust Orthogonal NMF with Label Propagation for Image Clustering",
        "abstract": "arXiv:2504.21472v1 Announce Type: new  Abstract: Non-negative matrix factorization (NMF) is a popular unsupervised learning approach widely used in image clustering. However, in real-world clustering scenarios, most existing NMF methods are highly sensitive to noise corruption and are unable to effectively leverage limited supervised information. To overcome these drawbacks, we propose a unified non-convex framework with label propagation called robust orthogonal nonnegative matrix factorization (RONMF). This method not only considers the graph Laplacian and label propagation as regularization terms but also introduces a more effective non-convex structure to measure the reconstruction error and imposes orthogonal constraints on the basis matrix to reduce the noise corruption, thereby achieving higher robustness. To solve RONMF, we develop an alternating direction method of multipliers (ADMM)-based optimization algorithm. In particular, all subproblems have closed-form solutions, which ensures its efficiency. Experimental evaluations on eight public image datasets demonstrate that the proposed RONMF outperforms state-of-the-art NMF methods across various standard metrics and shows excellent robustness. The code will be available at https://github.com/slinda-liu.",
        "arxiv_id": "2504.21472",
        "ARXIVID": "2504.21472",
        "COMMENT": "Does not match any specific criterion but focuses on robust image clustering using NMF, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.21433": {
        "authors": [
            "Zhicong Li",
            "Hangyu Mao",
            "Jiangjin Yin",
            "Mingzhe Xing",
            "Zhiwei Xu",
            "Yuanxing Zhang",
            "Yang Xiao"
        ],
        "title": "NGENT: Next-Generation AI Agents Must Integrate Multi-Domain Abilities to Achieve Artificial General Intelligence",
        "abstract": "arXiv:2504.21433v1 Announce Type: new  Abstract: This paper argues that the next generation of AI agent (NGENT) should integrate across-domain abilities to advance toward Artificial General Intelligence (AGI). Although current AI agents are effective in specialized tasks such as robotics, role-playing, and tool-using, they remain confined to narrow domains. We propose that future AI agents should synthesize the strengths of these specialized systems into a unified framework capable of operating across text, vision, robotics, reinforcement learning, emotional intelligence, and beyond. This integration is not only feasible but also essential for achieving the versatility and adaptability that characterize human intelligence. The convergence of technologies across AI domains, coupled with increasing user demand for cross-domain capabilities, suggests that such integration is within reach. Ultimately, the development of these versatile agents is a critical step toward realizing AGI. This paper explores the rationale for this shift, potential pathways for achieving it.",
        "arxiv_id": "2504.21433",
        "ARXIVID": "2504.21433",
        "COMMENT": "Does not directly match any specific criterion but discusses the integration of multi-domain abilities for AGI, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.21347": {
        "authors": [
            "Seonghee Lee",
            "Denae Ford",
            "John Tang",
            "Sasa Junuzovic",
            "Asta Roseway",
            "Ed Cutrell",
            "Kori Inkpen"
        ],
        "title": "IRL Dittos: Embodied Multimodal AI Agent Interactions in Open Spaces",
        "abstract": "arXiv:2504.21347v1 Announce Type: new  Abstract: We introduce the In Real Life (IRL) Ditto, an AI-driven embodied agent designed to represent remote colleagues in shared office spaces, creating opportunities for real-time exchanges even in their absence. IRL Ditto offers a unique hybrid experience by allowing in-person colleagues to encounter a digital version of their remote teammates, initiating greetings, updates, or small talk as they might in person. Our research question examines: How can the IRL Ditto influence interactions and relationships among colleagues in a shared office space? Through a four-day study, we assessed IRL Ditto's ability to strengthen social ties by simulating presence and enabling meaningful interactions across different levels of social familiarity. We find that enhancing social relationships depended deeply on the foundation of the relationship participants had with the source of the IRL Ditto. This study provides insights into the role of embodied agents in enriching workplace dynamics for distributed teams.",
        "arxiv_id": "2504.21347",
        "ARXIVID": "2504.21347",
        "COMMENT": "This paper does not match any of the specific criteria. It focuses on embodied agents for workplace dynamics, which is outside the scope of the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}