{
    "2507.07982": {
        "authors": [
            "Haoyu Wu",
            "Diankun Wu",
            "Tianyu He",
            "Junliang Guo",
            "Yang Ye",
            "Yueqi Duan",
            "Jiang Bian"
        ],
        "title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling",
        "abstract": "arXiv:2507.07982v1 Announce Type: new  Abstract: Videos inherently represent 2D projections of a dynamic 3D world. However, our analysis suggests that video diffusion models trained solely on raw video data often fail to capture meaningful geometric-aware structure in their learned representations. To bridge this gap between video diffusion models and the underlying 3D nature of the physical world, we propose Geometry Forcing, a simple yet effective method that encourages video diffusion models to internalize latent 3D representations. Our key insight is to guide the model's intermediate representations toward geometry-aware structure by aligning them with features from a pretrained geometric foundation model. To this end, we introduce two complementary alignment objectives: Angular Alignment, which enforces directional consistency via cosine similarity, and Scale Alignment, which preserves scale-related information by regressing unnormalized geometric features from normalized diffusion representation. We evaluate Geometry Forcing on both camera view-conditioned and action-conditioned video generation tasks. Experimental results demonstrate that our method substantially improves visual quality and 3D consistency over the baseline methods. Project page: https://GeometryForcing.github.io.",
        "arxiv_id": "2507.07982",
        "ARXIVID": "2507.07982",
        "COMMENT": "The paper matches criterion 2 closely. It proposes a method for video diffusion models to internalize latent 3D representations, aligning with the idea of a unified diffusion model handling multiple vision tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2507.07978": {
        "authors": [
            "Longfei Li",
            "Zhiwen Fan",
            "Wenyan Cong",
            "Xinhang Liu",
            "Yuyang Yin",
            "Matt Foutter",
            "Panwang Pan",
            "Chenyu You",
            "Yue Wang",
            "Zhangyang Wang",
            "Yao Zhao",
            "Marco Pavone",
            "Yunchao Wei"
        ],
        "title": "Martian World Models: Controllable Video Synthesis with Physically Accurate 3D Reconstructions",
        "abstract": "arXiv:2507.07978v1 Announce Type: new  Abstract: Synthesizing realistic Martian landscape videos is crucial for mission rehearsal and robotic simulation. However, this task poses unique challenges due to the scarcity of high-quality Martian data and the significant domain gap between Martian and terrestrial imagery. To address these challenges, we propose a holistic solution composed of two key components: 1) A data curation pipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian environments from real stereo navigation images, sourced from NASA's Planetary Data System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A Martian terrain video generator, MarsGen, which synthesizes novel videos visually realistic and geometrically consistent with the 3D structure encoded in the data. Our M3arsSynth engine spans a wide range of Martian terrains and acquisition dates, enabling the generation of physically accurate 3D surface models at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data, synthesizes videos conditioned on an initial image frame and, optionally, camera trajectories or textual prompts, allowing for video generation in novel environments. Experimental results show that our approach outperforms video synthesis models trained on terrestrial datasets, achieving superior visual fidelity and 3D structural consistency.",
        "arxiv_id": "2507.07978",
        "ARXIVID": "2507.07978",
        "COMMENT": "The paper does not match any specific criteria closely. It focuses on video synthesis with 3D reconstructions for Martian landscapes, which does not involve joint generation and segmentation or multi-task diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2507.07443": {
        "authors": [
            "Ling Zhou",
            "Runtian Yuan",
            "Yi Liu",
            "Yuejie Zhang",
            "Rui Feng",
            "Shang Gao"
        ],
        "title": "Dual Semantic-Aware Network for Noise Suppressed Ultrasound Video Segmentation",
        "abstract": "arXiv:2507.07443v1 Announce Type: new  Abstract: Ultrasound imaging is a prevalent diagnostic tool known for its simplicity and non-invasiveness. However, its inherent characteristics often introduce substantial noise, posing considerable challenges for automated lesion or organ segmentation in ultrasound video sequences. To address these limitations, we propose the Dual Semantic-Aware Network (DSANet), a novel framework designed to enhance noise robustness in ultrasound video segmentation by fostering mutual semantic awareness between local and global features. Specifically, we introduce an Adjacent-Frame Semantic-Aware (AFSA) module, which constructs a channel-wise similarity matrix to guide feature fusion across adjacent frames, effectively mitigating the impact of random noise without relying on pixel-level relationships. Additionally, we propose a Local-and-Global Semantic-Aware (LGSA) module that reorganizes and fuses temporal unconditional local features, which capture spatial details independently at each frame, with conditional global features that incorporate temporal context from adjacent frames. This integration facilitates multi-level semantic representation, significantly improving the model's resilience to noise interference. Extensive evaluations on four benchmark datasets demonstrate that DSANet substantially outperforms state-of-the-art methods in segmentation accuracy. Moreover, since our model avoids pixel-level feature dependencies, it achieves significantly higher inference FPS than video-based methods, and even surpasses some image-based models. Code can be found in \\href{https://github.com/ZhouL2001/DSANet}{DSANet}",
        "arxiv_id": "2507.07443",
        "ARXIVID": "2507.07443",
        "COMMENT": "The paper does not match any specific criteria closely. It proposes a framework for noise-suppressed ultrasound video segmentation, but does not involve joint generation and segmentation or multi-task diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2507.07578": {
        "authors": [
            "Chunyan Wang",
            "Dong Zhang",
            "Jinhui Tang"
        ],
        "title": "Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-Light Semantic Segmentation",
        "abstract": "arXiv:2507.07578v1 Announce Type: new  Abstract: Weakly-supervised semantic segmentation aims to assign category labels to each pixel using weak annotations, significantly reducing manual annotation costs. Although existing methods have achieved remarkable progress in well-lit scenarios, their performance significantly degrades in low-light environments due to two fundamental limitations: severe image quality degradation (e.g., low contrast, noise, and color distortion) and the inherent constraints of weak supervision. These factors collectively lead to unreliable class activation maps and semantically ambiguous pseudo-labels, ultimately compromising the model's ability to learn discriminative feature representations. To address these problems, we propose Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-light Semantic Segmentation (DGKD-WLSS), a novel framework that synergistically combines Diffusion-Guided Knowledge Distillation (DGKD) with Depth-Guided Feature Fusion (DGF2). DGKD aligns normal-light and low-light features via diffusion-based denoising and knowledge distillation, while DGF2 integrates depth maps as illumination-invariant geometric priors to enhance structural feature learning. Extensive experiments demonstrate the effectiveness of DGKD-WLSS, which achieves state-of-the-art performance in weakly supervised semantic segmentation tasks under low-light conditions. The source codes have been released at:https://github.com/ChunyanWang1/DGKD-WLSS.",
        "arxiv_id": "2507.07578",
        "ARXIVID": "2507.07578",
        "COMMENT": "The paper does not match any specific criteria closely. It focuses on weakly-supervised semantic segmentation with diffusion-guided knowledge distillation, but does not propose a unified model for generation and segmentation or a multi-task diffusion model.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}