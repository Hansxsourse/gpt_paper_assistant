{
    "2505.18129": {
        "authors": [
            "Yan Ma",
            "Linge Du",
            "Xuyang Shen",
            "Shaoxiang Chen",
            "Pengfei Li",
            "Qibing Ren",
            "Lizhuang Ma",
            "Yuchao Dai",
            "Pengfei Liu",
            "Junjie Yan"
        ],
        "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning",
        "abstract": "arXiv:2505.18129v1 Announce Type: new  Abstract: Reinforcement learning (RL) has significantly advanced the reasoning capabilities of vision-language models (VLMs). However, the use of RL beyond reasoning tasks remains largely unexplored, especially for perceptionintensive tasks like object detection and grounding. We propose V-Triune, a Visual Triple Unified Reinforcement Learning system that enables VLMs to jointly learn visual reasoning and perception tasks within a single training pipeline. V-Triune comprises triple complementary components: Sample-Level Data Formatting (to unify diverse task inputs), Verifier-Level Reward Computation (to deliver custom rewards via specialized verifiers) , and Source-Level Metric Monitoring (to diagnose problems at the data-source level). We further introduce a novel Dynamic IoU reward, which provides adaptive, progressive, and definite feedback for perception tasks handled by V-Triune. Our approach is instantiated within off-the-shelf RL training framework using open-source 7B and 32B backbone models. The resulting model, dubbed Orsta (One RL to See Them All), demonstrates consistent improvements across both reasoning and perception tasks. This broad capability is significantly shaped by its training on a diverse dataset, constructed around four representative visual reasoning tasks (Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding, Detection, Counting, and OCR). Subsequently, Orsta achieves substantial gains on MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1 across its various 7B and 32B model variants, with performance benefits extending to a wide range of downstream tasks. These results highlight the effectiveness and scalability of our unified RL approach for VLMs. The V-Triune system, along with the Orsta models, is publicly available at https://github.com/MiniMax-AI.",
        "arxiv_id": "2505.18129",
        "ARXIVID": "2505.18129",
        "COMMENT": "Matches criterion 2 as it introduces a new visual large language model (VLM) with reinforcement learning for reasoning and perception tasks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2505.17726": {
        "authors": [
            "Donghwan Chi",
            "Hyomin Kim",
            "Yoonjin Oh",
            "Yongjin Kim",
            "Donghoon Lee",
            "Daejin Jo",
            "Jongmin Kim",
            "Junyeob Baek",
            "Sungjin Ahn",
            "Sungwoong Kim"
        ],
        "title": "Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM",
        "abstract": "arXiv:2505.17726v1 Announce Type: new  Abstract: Recently, multimodal large language models (MLLMs) have emerged as a key approach in achieving artificial general intelligence. In particular, vision-language MLLMs have been developed to generate not only text but also visual outputs from multimodal inputs. This advancement requires efficient image tokens that LLMs can process effectively both in input and output. However, existing image tokenization methods for MLLMs typically capture only global abstract concepts or uniformly segmented image patches, restricting MLLMs' capability to effectively understand or generate detailed visual content, particularly at the object level. To address this limitation, we propose an object-centric visual tokenizer based on Slot Attention specifically for MLLMs. In particular, based on the Q-Former encoder, diffusion decoder, and residual vector quantization, our proposed discretized slot tokens can encode local visual details while maintaining high-level semantics, and also align with textual data to be integrated seamlessly within a unified next-token prediction framework of LLMs. The resulting Slot-MLLM demonstrates significant performance improvements over baselines with previous visual tokenizers across various vision-language tasks that entail local detailed comprehension and generation. Notably, this work is the first demonstration of the feasibility of object-centric slot attention performed with MLLMs and in-the-wild natural images.",
        "arxiv_id": "2505.17726",
        "ARXIVID": "2505.17726",
        "COMMENT": "Matches criterion 2 as it proposes Slot-MLLM, a novel object-centric visual tokenizer for multimodal large language models, improving detailed visual comprehension and generation.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2505.18134": {
        "authors": [
            "Alex L. Zhang",
            "Thomas L. Griffiths",
            "Karthik R. Narasimhan",
            "Ofir Press"
        ],
        "title": "VideoGameBench: Can Vision-Language Models complete popular video games?",
        "abstract": "arXiv:2505.18134v1 Announce Type: new  Abstract: Vision-language models (VLMs) have achieved strong results on coding and math benchmarks that are challenging for humans, yet their ability to perform tasks that come naturally to humans--such as perception, spatial navigation, and memory management--remains understudied. Real video games are crafted to be intuitive for humans to learn and master by leveraging innate inductive biases, making them an ideal testbed for evaluating such capabilities in VLMs. To this end, we introduce VideoGameBench, a benchmark consisting of 10 popular video games from the 1990s that VLMs directly interact with in real-time. VideoGameBench challenges models to complete entire games with access to only raw visual inputs and a high-level description of objectives and controls, a significant departure from existing setups that rely on game-specific scaffolding and auxiliary information. We keep three of the games secret to encourage solutions that generalize to unseen environments. Our experiments show that frontier vision-language models struggle to progress beyond the beginning of each game. We find inference latency to be a major limitation of frontier models in the real-time setting; therefore, we introduce VideoGameBench Lite, a setting where the game pauses while waiting for the LM's next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization of the human skills mentioned above into this benchmark motivates progress in these research directions.",
        "arxiv_id": "2505.18134",
        "ARXIVID": "2505.18134",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (VideoGameBench) for evaluating vision-language models in real-time video game environments, focusing on spatial navigation and perception.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2505.17534": {
        "authors": [
            "Jingjing Jiang",
            "Chongjie Si",
            "Jun Luo",
            "Hanwang Zhang",
            "Chao Ma"
        ],
        "title": "Co-Reinforcement Learning for Unified Multimodal Understanding and Generation",
        "abstract": "arXiv:2505.17534v1 Announce Type: new  Abstract: This paper presents a pioneering exploration of reinforcement learning (RL) via group relative policy optimization for unified multimodal large language models (ULMs), aimed at simultaneously reinforcing generation and understanding capabilities. Through systematic pilot studies, we uncover the significant potential of ULMs to enable the synergistic co-evolution of dual capabilities within a shared policy optimization framework. Building on this insight, we introduce \\textbf{CoRL}, a co-reinforcement learning framework comprising a unified RL stage for joint optimization and a refined RL stage for task-specific enhancement. With the proposed CoRL, our resulting model, \\textbf{ULM-R1}, achieves average improvements of \\textbf{7%} on three text-to-image generation datasets and \\textbf{23%} on nine multimodal understanding benchmarks. These results demonstrate the effectiveness of CoRL and highlight the substantial benefit of reinforcement learning in facilitating cross-task synergy and optimization for ULMs.",
        "arxiv_id": "2505.17534",
        "ARXIVID": "2505.17534",
        "COMMENT": "Matches criterion 2 as it explores reinforcement learning for unified multimodal large language models (ULMs).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2505.18053": {
        "authors": [
            "Zherui Zhang",
            "Jiaxin Wu",
            "Changwei Wang",
            "Rongtao Xu",
            "Longzhao Huang",
            "Wenhao Xu",
            "Wenbo Xu",
            "Li Guo",
            "Shibiao Xu"
        ],
        "title": "FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware Vision-Language Models Adaptation",
        "abstract": "arXiv:2505.18053v1 Announce Type: new  Abstract: Prompt learning as a parameter-efficient method that has been widely adopted to adapt Vision-Language Models (VLMs) to downstream tasks. While hard-prompt design requires domain expertise and iterative optimization, soft-prompt methods rely heavily on task-specific hard labels, limiting their generalization to unseen categories. Recent popular distillation-based prompt learning methods improve generalization by exploiting larger teacher VLMs and unsupervised knowledge transfer, yet their repetitive teacher model online inference sacrifices the inherent training efficiency advantage of prompt learning. In this paper, we propose {{\\large {\\textbf{F}}}}aster {{\\large {\\textbf{D}}}}istillation-{{\\large {\\textbf{B}}}}ased {{\\large {\\textbf{P}}}}rompt {{\\large {\\textbf{L}}}}earning (\\textbf{FDBPL}), which addresses these issues by sharing soft supervision contexts across multiple training stages and implementing accelerated I/O. Furthermore, FDBPL introduces a region-aware prompt learning paradigm with dual positive-negative prompt spaces to fully exploit randomly cropped regions that containing multi-level information. We propose a positive-negative space mutual learning mechanism based on similarity-difference learning, enabling student CLIP models to recognize correct semantics while learning to reject weakly related concepts, thereby improving zero-shot performance. Unlike existing distillation-based prompt learning methods that sacrifice parameter efficiency for generalization, FDBPL maintains dual advantages of parameter efficiency and strong downstream generalization. Comprehensive evaluations across 11 datasets demonstrate superior performance in base-to-new generalization, cross-dataset transfer, and robustness tests, achieving $2.2\\times$ faster training speed.",
        "arxiv_id": "2505.18053",
        "ARXIVID": "2505.18053",
        "COMMENT": "Matches criterion 2 as it proposes a novel distillation-based prompt learning method for adapting vision-language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2505.17994": {
        "authors": [
            "Zhihua Liu",
            "Amrutha Saseendran",
            "Lei Tong",
            "Xilin He",
            "Fariba Yousefi",
            "Nikolay Burlutskiy",
            "Dino Oglic",
            "Tom Diethe",
            "Philip Teare",
            "Huiyu Zhou",
            "Chen Jin"
        ],
        "title": "Segment Anyword: Mask Prompt Inversion for Open-Set Grounded Segmentation",
        "abstract": "arXiv:2505.17994v1 Announce Type: new  Abstract: Open-set image segmentation poses a significant challenge because existing methods often demand extensive training or fine-tuning and generally struggle to segment unified objects consistently across diverse text reference expressions. Motivated by this, we propose Segment Anyword, a novel training-free visual concept prompt learning approach for open-set language grounded segmentation that relies on token-level cross-attention maps from a frozen diffusion model to produce segmentation surrogates or mask prompts, which are then refined into targeted object masks. Initial prompts typically lack coherence and consistency as the complexity of the image-text increases, resulting in suboptimal mask fragments. To tackle this issue, we further introduce a novel linguistic-guided visual prompt regularization that binds and clusters visual prompts based on sentence dependency and syntactic structural information, enabling the extraction of robust, noise-tolerant mask prompts, and significant improvements in segmentation accuracy. The proposed approach is effective, generalizes across different open-set segmentation tasks, and achieves state-of-the-art results of 52.5 (+6.8 relative) mIoU on Pascal Context 59, 67.73 (+25.73 relative) cIoU on gRefCOCO, and 67.4 (+1.1 relative to fine-tuned methods) mIoU on GranDf, which is the most complex open-set grounded segmentation task in the field.",
        "arxiv_id": "2505.17994",
        "ARXIVID": "2505.17994",
        "COMMENT": "Matches criterion 2 as it introduces a novel training-free visual concept prompt learning approach for open-set language grounded segmentation, leveraging a frozen diffusion model, which aligns with advancements in VLLMs/MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.17574": {
        "authors": [
            "Xueji Fang",
            "Liyuan Ma",
            "Zhiyang Chen",
            "Mingyuan Zhou",
            "Guo-jun Qi"
        ],
        "title": "InfLVG: Reinforce Inference-Time Consistent Long Video Generation with GRPO",
        "abstract": "arXiv:2505.17574v1 Announce Type: new  Abstract: Recent advances in text-to-video generation, particularly with autoregressive models, have enabled the synthesis of high-quality videos depicting individual scenes. However, extending these models to generate long, cross-scene videos remains a significant challenge. As the context length grows during autoregressive decoding, computational costs rise sharply, and the model's ability to maintain consistency and adhere to evolving textual prompts deteriorates. We introduce InfLVG, an inference-time framework that enables coherent long video generation without requiring additional long-form video data. InfLVG leverages a learnable context selection policy, optimized via Group Relative Policy Optimization (GRPO), to dynamically identify and retain the most semantically relevant context throughout the generation process. Instead of accumulating the entire generation history, the policy ranks and selects the top-$K$ most contextually relevant tokens, allowing the model to maintain a fixed computational budget while preserving content consistency and prompt alignment. To optimize the policy, we design a hybrid reward function that jointly captures semantic alignment, cross-scene consistency, and artifact reduction. To benchmark performance, we introduce the Cross-scene Video Benchmark (CsVBench) along with an Event Prompt Set (EPS) that simulates complex multi-scene transitions involving shared subjects and varied actions/backgrounds. Experimental results show that InfLVG can extend video length by up to 9$\\times$, achieving strong consistency and semantic fidelity across scenes. Our code is available at https://github.com/MAPLE-AIGC/InfLVG.",
        "arxiv_id": "2505.17574",
        "ARXIVID": "2505.17574",
        "COMMENT": "Matches criterion 2 as it introduces a framework for long video generation using multimodal models, which aligns with advancements in VLLMs/MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.17768": {
        "authors": [
            "Dong Zhang",
            "Lingfeng He",
            "Rui Yan",
            "Fei Shen",
            "Jinhui Tang"
        ],
        "title": "R-Genie: Reasoning-Guided Generative Image Editing",
        "abstract": "arXiv:2505.17768v1 Announce Type: new  Abstract: While recent advances in image editing have enabled impressive visual synthesis capabilities, current methods remain constrained by explicit textual instructions and limited editing operations, lacking deep comprehension of implicit user intentions and contextual reasoning. In this work, we introduce a new image editing paradigm: reasoning-guided generative editing, which synthesizes images based on complex, multi-faceted textual queries accepting world knowledge and intention inference. To facilitate this task, we first construct a comprehensive dataset featuring over 1,000 image-instruction-edit triples that incorporate rich reasoning contexts and real-world knowledge. We then propose R-Genie: a reasoning-guided generative image editor, which synergizes the generation power of diffusion models with advanced reasoning capabilities of multimodal large language models. R-Genie incorporates a reasoning-attention mechanism to bridge linguistic understanding with visual synthesis, enabling it to handle intricate editing requests involving abstract user intentions and contextual reasoning relations. Extensive experimental results validate that R-Genie can equip diffusion models with advanced reasoning-based editing capabilities, unlocking new potentials for intelligent image synthesis.",
        "arxiv_id": "2505.17768",
        "ARXIVID": "2505.17768",
        "COMMENT": "Matches criterion 2 as it introduces a multimodal large language model (R-Genie) for reasoning-guided generative image editing.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.17818": {
        "authors": [
            "Daeun Kyung",
            "Hyunseung Chung",
            "Seongsu Bae",
            "Jiho Kim",
            "Jae Ho Sohn",
            "Taerim Kim",
            "Soo Kyung Kim",
            "Edward Choi"
        ],
        "title": "PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions",
        "abstract": "arXiv:2505.17818v1 Announce Type: new  Abstract: Doctor-patient consultations require multi-turn, context-aware communication tailored to diverse patient personas. Training or evaluating doctor LLMs in such settings requires realistic patient interaction systems. However, existing simulators often fail to reflect the full range of personas seen in clinical practice. To address this, we introduce PatientSim, a patient simulator that generates realistic and diverse patient personas for clinical scenarios, grounded in medical expertise. PatientSim operates using: 1) clinical profiles, including symptoms and medical history, derived from real-world data in the MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes: personality, language proficiency, medical history recall level, and cognitive confusion level, resulting in 37 unique combinations. We evaluated eight LLMs for factual accuracy and persona consistency. The top-performing open-source model, Llama 3.3, was validated by four clinicians to confirm the robustness of our framework. As an open-source, customizable platform, PatientSim provides a reproducible and scalable solution that can be customized for specific training needs. Offering a privacy-compliant environment, it serves as a robust testbed for evaluating medical dialogue systems across diverse patient presentations and shows promise as an educational tool for healthcare.",
        "arxiv_id": "2505.17818",
        "ARXIVID": "2505.17818",
        "COMMENT": "Matches criterion 3 as it introduces a new simulator (PatientSim) for embodied AI in medical dialogue systems.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.17127": {
        "authors": [
            "Michal Golovanevsky",
            "William Rudman",
            "Michael Lepori",
            "Amir Bar",
            "Ritambhara Singh",
            "Carsten Eickhoff"
        ],
        "title": "Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts",
        "abstract": "arXiv:2505.17127v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) perform well on tasks such as visual question answering, but it remains unclear whether their reasoning relies more on memorized world knowledge or on the visual information present in the input image. To investigate this, we introduce Visual CounterFact, a new dataset of visually-realistic counterfactuals that put world knowledge priors (e.g, red strawberry) into direct conflict with visual input (e.g, blue strawberry). Using Visual CounterFact, we show that model predictions initially reflect memorized priors, but shift toward visual evidence in mid-to-late layers. This dynamic reveals a competition between the two modalities, with visual input ultimately overriding priors during evaluation. To control this behavior, we propose Pixels Versus Priors (PvP) steering vectors, a mechanism for controlling model outputs toward either world knowledge or visual input through activation-level interventions. On average, PvP successfully shifts 92.5% of color and 74.6% of size predictions from priors to counterfactuals. Together, these findings offer new tools for interpreting and controlling factual behavior in multimodal models.",
        "arxiv_id": "2505.17127",
        "ARXIVID": "2505.17127",
        "COMMENT": "Matches criterion 2 as it introduces Visual CounterFact, a dataset for studying the interplay of visual input and world knowledge in multimodal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.17316": {
        "authors": [
            "Jiachen Jiang",
            "Jinxin Zhou",
            "Bo Peng",
            "Xia Ning",
            "Zhihui Zhu"
        ],
        "title": "Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models",
        "abstract": "arXiv:2505.17316v1 Announce Type: new  Abstract: Achieving better alignment between vision embeddings and Large Language Models (LLMs) is crucial for enhancing the abilities of Multimodal LLMs (MLLMs), particularly for recent models that rely on powerful pretrained vision encoders and LLMs. A common approach to connect the pretrained vision encoder and LLM is through a projector applied after the vision encoder. However, the projector is often trained to enable the LLM to generate captions, and hence the mechanism by which LLMs understand each vision token remains unclear. In this work, we first investigate the role of the projector in compressing vision embeddings and aligning them with word embeddings. We show that the projector significantly compresses visual information, removing redundant details while preserving essential elements necessary for the LLM to understand visual content. We then examine patch-level alignment -- the alignment between each vision patch and its corresponding semantic words -- and propose a *multi-semantic alignment hypothesis*. Our analysis indicates that the projector trained by caption loss improves patch-level alignment but only to a limited extent, resulting in weak and coarse alignment. To address this issue, we propose *patch-aligned training* to efficiently enhance patch-level alignment. Our experiments show that patch-aligned training (1) achieves stronger compression capability and improved patch-level alignment, enabling the MLLM to generate higher-quality captions, (2) improves the MLLM's performance by 16% on referring expression grounding tasks, 4% on question-answering tasks, and 3% on modern instruction-following benchmarks when using the same supervised fine-tuning (SFT) setting. The proposed method can be easily extended to other multimodal models.",
        "arxiv_id": "2505.17316",
        "ARXIVID": "2505.17316",
        "COMMENT": "Matches criterion 2 as it analyzes fine-grained alignment in multimodal language models and proposes methods to enhance vision understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.18115": {
        "authors": [
            "Jacob Hansen",
            "Wei Lin",
            "Junmo Kang",
            "Muhammad Jehanzeb Mirza",
            "Hongyin Luo",
            "Rogerio Feris",
            "Alan Ritter",
            "James Glass",
            "Leonid Karlinsky"
        ],
        "title": "Instructify: Demystifying Metadata to Visual Instruction Tuning Data Conversion",
        "abstract": "arXiv:2505.18115v1 Announce Type: new  Abstract: Visual Instruction Tuning (VisIT) data, commonly available as human-assistant conversations with images interleaved in the human turns, are currently the most widespread vehicle for aligning strong LLMs to understand visual inputs, converting them to strong LMMs. While many VisIT datasets are available, most are constructed using ad-hoc techniques developed independently by different groups. They are often poorly documented, lack reproducible code, and rely on paid, closed-source model APIs such as GPT-4, Gemini, or Claude to convert image metadata (labels) into VisIT instructions. This leads to high costs and makes it challenging to scale, enhance quality, or generate VisIT data for new datasets. In this work, we address these challenges and propose an open and unified recipe and approach,~\\textbf{\\method}, for converting available metadata to VisIT instructions using open LLMs. Our multi-stage \\method features an efficient framework for metadata grouping, quality control, data and prompt organization, and conversation sampling. We show that our approach can reproduce or enhance the data quality of available VisIT datasets when applied to the same image data and metadata sources, improving GPT-4 generated VisIT instructions by ~3\\% on average and up to 12\\% on individual benchmarks using open models, such as Gemma 2 27B and LLaMa 3.1 70B. Additionally, our approach enables effective performance scaling - both in quantity and quality - by enhancing the resulting LMM performance across a wide range of benchmarks. We also analyze the impact of various factors, including conversation format, base model selection, and resampling strategies. Our code, which supports the reproduction of equal or higher-quality VisIT datasets and facilities future metadata-to-VisIT data conversion for niche domains, is released at https://github.com/jacob-hansen/Instructify.",
        "arxiv_id": "2505.18115",
        "ARXIVID": "2505.18115",
        "COMMENT": "Matches criterion 2 as it discusses Instructify, a framework for visual instruction tuning data conversion, enhancing visual large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.17540": {
        "authors": [
            "Mingrui Wu",
            "Lu Wang",
            "Pu Zhao",
            "Fangkai Yang",
            "Jianjin Zhang",
            "Jianfeng Liu",
            "Yuefeng Zhan",
            "Weihao Han",
            "Hao Sun",
            "Jiayi Ji",
            "Xiaoshuai Sun",
            "Qingwei Lin",
            "Weiwei Deng",
            "Dongmei Zhang",
            "Feng Sun",
            "Qi Zhang",
            "Rongrong Ji"
        ],
        "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning",
        "abstract": "arXiv:2505.17540v1 Announce Type: new  Abstract: Despite recent progress in text-to-image (T2I) generation, existing models often struggle to faithfully capture user intentions from short and under-specified prompts. While prior work has attempted to enhance prompts using large language models (LLMs), these methods frequently generate stylistic or unrealistic content due to insufficient grounding in visual semantics and real-world composition. Inspired by recent advances in reasoning for language model, we propose RePrompt, a novel reprompting framework that introduces explicit reasoning into the prompt enhancement process via reinforcement learning. Instead of relying on handcrafted rules or stylistic rewrites, our method trains a language model to generate structured, self-reflective prompts by optimizing for image-level outcomes. The tailored reward models assesse the generated images in terms of human preference, semantic alignment, and visual composition, providing indirect supervision to refine prompt generation. Our approach enables end-to-end training without human-annotated data. Experiments on GenEval and T2I-Compbench show that RePrompt significantly boosts spatial layout fidelity and compositional generalization across diverse T2I backbones, establishing new state-of-the-art results.",
        "arxiv_id": "2505.17540",
        "ARXIVID": "2505.17540",
        "COMMENT": "Matches criterion 2 as it discusses a novel framework (RePrompt) for reasoning-augmented prompt enhancement in text-to-image generation, which aligns with visual large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.18096": {
        "authors": [
            "Ziqiao Peng",
            "Yanbo Fan",
            "Haoyu Wu",
            "Xuan Wang",
            "Hongyan Liu",
            "Jun He",
            "Zhaoxin Fan"
        ],
        "title": "DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations",
        "abstract": "arXiv:2505.18096v1 Announce Type: new  Abstract: In face-to-face conversations, individuals need to switch between speaking and listening roles seamlessly. Existing 3D talking head generation models focus solely on speaking or listening, neglecting the natural dynamics of interactive conversation, which leads to unnatural interactions and awkward transitions. To address this issue, we propose a new task -- multi-round dual-speaker interaction for 3D talking head generation -- which requires models to handle and generate both speaking and listening behaviors in continuous conversation. To solve this task, we introduce DualTalk, a novel unified framework that integrates the dynamic behaviors of speakers and listeners to simulate realistic and coherent dialogue interactions. This framework not only synthesizes lifelike talking heads when speaking but also generates continuous and vivid non-verbal feedback when listening, effectively capturing the interplay between the roles. We also create a new dataset featuring 50 hours of multi-round conversations with over 1,000 characters, where participants continuously switch between speaking and listening roles. Extensive experiments demonstrate that our method significantly enhances the naturalness and expressiveness of 3D talking heads in dual-speaker conversations. We recommend watching the supplementary video: https://ziqiaopeng.github.io/dualtalk.",
        "arxiv_id": "2505.18096",
        "ARXIVID": "2505.18096",
        "COMMENT": "Matches criterion 3 as it introduces a new task and dataset for dual-speaker 3D talking head generation, which is a novel angle in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.18087": {
        "authors": [
            "Hyungyung Lee",
            "Geon Choi",
            "Jung-Oh Lee",
            "Hangyul Yoon",
            "Hyuk Gi Hong",
            "Edward Choi"
        ],
        "title": "CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays",
        "abstract": "arXiv:2505.18087v1 Announce Type: new  Abstract: Recent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, a structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning. The benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at https://github.com/ttumyche/CXReasonBench",
        "arxiv_id": "2505.18087",
        "ARXIVID": "2505.18087",
        "COMMENT": "Matches criterion 4 as it introduces a benchmark (CXReasonBench) for evaluating reasoning in vision-language models for medical applications.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2505.18025": {
        "authors": [
            "Evangelos Sariyanidi",
            "Claudio Ferrari",
            "Federico Nocentini",
            "Stefano Berretti",
            "Andrea Cavallaro",
            "Birkan Tunc"
        ],
        "title": "3D Face Reconstruction Error Decomposed: A Modular Benchmark for Fair and Fast Method Evaluation",
        "abstract": "arXiv:2505.18025v1 Announce Type: new  Abstract: Computing the standard benchmark metric for 3D face reconstruction, namely geometric error, requires a number of steps, such as mesh cropping, rigid alignment, or point correspondence. Current benchmark tools are monolithic (they implement a specific combination of these steps), even though there is no consensus on the best way to measure error. We present a toolkit for a Modularized 3D Face reconstruction Benchmark (M3DFB), where the fundamental components of error computation are segregated and interchangeable, allowing one to quantify the effect of each. Furthermore, we propose a new component, namely correction, and present a computationally efficient approach that penalizes for mesh topology inconsistency. Using this toolkit, we test 16 error estimators with 10 reconstruction methods on two real and two synthetic datasets. Critically, the widely used ICP-based estimator provides the worst benchmarking performance, as it significantly alters the true ranking of the top-5 reconstruction methods. Notably, the correlation of ICP with the true error can be as low as 0.41. Moreover, non-rigid alignment leads to significant improvement (correlation larger than 0.90), highlighting the importance of annotating 3D landmarks on datasets. Finally, the proposed correction scheme, together with non-rigid warping, leads to an accuracy on a par with the best non-rigid ICP-based estimators, but runs an order of magnitude faster. Our open-source codebase is designed for researchers to easily compare alternatives for each component, thus helping accelerating progress in benchmarking for 3D face reconstruction and, furthermore, supporting the improvement of learned reconstruction methods, which depend on accurate error estimation for effective training.",
        "arxiv_id": "2505.18025",
        "ARXIVID": "2505.18025",
        "COMMENT": "Matches criterion 3 as it introduces a modular benchmark for 3D face reconstruction error evaluation, which is a novel benchmarking approach in the embodied AI domain.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.17090": {
        "authors": [
            "Phoebe Chua",
            "Cathy Mengying Fang",
            "Takehiko Ohkawa",
            "Raja Kushalnagar",
            "Suranga Nanayakkara",
            "Pattie Maes"
        ],
        "title": "EmoSign: A Multimodal Dataset for Understanding Emotions in American Sign Language",
        "abstract": "arXiv:2505.17090v1 Announce Type: new  Abstract: Unlike spoken languages where the use of prosodic features to convey emotion is well studied, indicators of emotion in sign language remain poorly understood, creating communication barriers in critical settings. Sign languages present unique challenges as facial expressions and hand movements simultaneously serve both grammatical and emotional functions. To address this gap, we introduce EmoSign, the first sign video dataset containing sentiment and emotion labels for 200 American Sign Language (ASL) videos. We also collect open-ended descriptions of emotion cues. Annotations were done by 3 Deaf ASL signers with professional interpretation experience. Alongside the annotations, we include baseline models for sentiment and emotion classification. This dataset not only addresses a critical gap in existing sign language research but also establishes a new benchmark for understanding model capabilities in multimodal emotion recognition for sign languages. The dataset is made available at https://huggingface.co/datasets/catfang/emosign.",
        "arxiv_id": "2505.17090",
        "ARXIVID": "2505.17090",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for multimodal emotion recognition in sign language, which is a novel angle in embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2505.17801": {
        "authors": [
            "B\\'alint Gyevn\\'ar",
            "Christopher G. Lucas",
            "Stefano V. Albrecht",
            "Shay B. Cohen"
        ],
        "title": "Integrating Counterfactual Simulations with Language Models for Explaining Multi-Agent Behaviour",
        "abstract": "arXiv:2505.17801v1 Announce Type: new  Abstract: Autonomous multi-agent systems (MAS) are useful for automating complex tasks but raise trust concerns due to risks like miscoordination and goal misalignment. Explainability is vital for trust calibration, but explainable reinforcement learning for MAS faces challenges in state/action space complexity, stakeholder needs, and evaluation. Using the counterfactual theory of causation and LLMs' summarisation capabilities, we propose Agentic eXplanations via Interrogative Simulation (AXIS). AXIS generates intelligible causal explanations for pre-trained multi-agent policies by having an LLM interrogate an environment simulator using queries like 'whatif' and 'remove' to observe and synthesise counterfactual information over multiple rounds. We evaluate AXIS on autonomous driving across 10 scenarios for 5 LLMs with a novel evaluation methodology combining subjective preference, correctness, and goal/action prediction metrics, and an external LLM as evaluator. Compared to baselines, AXIS improves perceived explanation correctness by at least 7.7% across all models and goal prediction accuracy by 23% for 4 models, with improved or comparable action prediction accuracy, achieving the highest scores overall.",
        "arxiv_id": "2505.17801",
        "ARXIVID": "2505.17801",
        "COMMENT": "Matches criterion 3 as it introduces a novel method (AXIS) for explainability in multi-agent systems using counterfactual simulations.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.17735": {
        "authors": [
            "Xueyang Zhou",
            "Weidong Wang",
            "Lin Lu",
            "Jiawen Shi",
            "Guiyao Tie",
            "Yongtian Xu",
            "Lixing Chen",
            "Pan Zhou",
            "Neil Zhenqiang Gong",
            "Lichao Sun"
        ],
        "title": "Automating Safety Enhancement for LLM-based Agents with Synthetic Risk Scenarios",
        "abstract": "arXiv:2505.17735v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents are increasingly deployed in real-world applications such as \"digital assistants, autonomous customer service, and decision-support systems\", where their ability to \"interact in multi-turn, tool-augmented environments\" makes them indispensable. However, ensuring the safety of these agents remains a significant challenge due to the diverse and complex risks arising from dynamic user interactions, external tool usage, and the potential for unintended harmful behaviors. To address this critical issue, we propose AutoSafe, the first framework that systematically enhances agent safety through fully automated synthetic data generation. Concretely, 1) we introduce an open and extensible threat model, OTS, which formalizes how unsafe behaviors emerge from the interplay of user instructions, interaction contexts, and agent actions. This enables precise modeling of safety risks across diverse scenarios. 2) we develop a fully automated data generation pipeline that simulates unsafe user behaviors, applies self-reflective reasoning to generate safe responses, and constructs a large-scale, diverse, and high-quality safety training dataset-eliminating the need for hazardous real-world data collection. To evaluate the effectiveness of our framework, we design comprehensive experiments on both synthetic and real-world safety benchmarks. Results demonstrate that AutoSafe boosts safety scores by 45% on average and achieves a 28.91% improvement on real-world tasks, validating the generalization ability of our learned safety strategies. These results highlight the practical advancement and scalability of AutoSafe in building safer LLM-based agents for real-world deployment. We have released the project page at https://auto-safe.github.io/.",
        "arxiv_id": "2505.17735",
        "ARXIVID": "2505.17735",
        "COMMENT": "Matches criterion 3 as it proposes a framework for enhancing safety in LLM-based agents, which is relevant to embodied AI and novel methods.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.18111": {
        "authors": [
            "Cheng-Yen Yang",
            "Hsiang-Wei Huang",
            "Pyong-Kun Kim",
            "Chien-Kai Kuo",
            "Jui-Wei Chang",
            "Kwang-Ju Kim",
            "Chung-I Huang",
            "Jenq-Neng Hwang"
        ],
        "title": "Adapting SAM 2 for Visual Object Tracking: 1st Place Solution for MMVPR Challenge Multi-Modal Tracking",
        "abstract": "arXiv:2505.18111v1 Announce Type: new  Abstract: We present an effective approach for adapting the Segment Anything Model 2 (SAM2) to the Visual Object Tracking (VOT) task. Our method leverages the powerful pre-trained capabilities of SAM2 and incorporates several key techniques to enhance its performance in VOT applications. By combining SAM2 with our proposed optimizations, we achieved a first place AUC score of 89.4 on the 2024 ICPR Multi-modal Object Tracking challenge, demonstrating the effectiveness of our approach. This paper details our methodology, the specific enhancements made to SAM2, and a comprehensive analysis of our results in the context of VOT solutions along with the multi-modality aspect of the dataset.",
        "arxiv_id": "2505.18111",
        "ARXIVID": "2505.18111",
        "COMMENT": "Matches criterion 3 as it adapts SAM2 for visual object tracking, which involves embodied AI and multi-modal tracking benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.17436": {
        "authors": [
            "Cheng Peng",
            "Kai Zhang",
            "Mengxian Lyu",
            "Hongfang Liu",
            "Lichao Sun",
            "Yonghui Wu"
        ],
        "title": "Scaling Up Biomedical Vision-Language Models: Fine-Tuning, Instruction Tuning, and Multi-Modal Learning",
        "abstract": "arXiv:2505.17436v1 Announce Type: new  Abstract: To advance biomedical vison-language model capabilities through scaling up, fine-tuning, and instruction tuning, develop vision-language models with improved performance in handling long text, explore strategies to efficiently adopt vision language models for diverse multi-modal biomedical tasks, and examine the zero-shot learning performance.   We developed two biomedical vision language models, BiomedGPT-Large and BiomedGPT-XLarge, based on an encoder-decoder-based transformer architecture. We fine-tuned the two models on 23 benchmark datasets from 6 multi-modal biomedical tasks including one image-only task (image classification), three language-only tasks (text understanding, text summarization and question answering), and two vision-language tasks (visual question answering and image captioning). We compared the developed scaled models with our previous BiomedGPT-Base model and existing prestigious models reported in the literature. We instruction-tuned the two models using a large-scale multi-modal biomedical instruction-tuning dataset and assessed the zero-shot learning performance and alignment accuracy.",
        "arxiv_id": "2505.17436",
        "ARXIVID": "2505.17436",
        "COMMENT": "Matches criterion 2 as it discusses scaling up vision-language models (VLLMs) for biomedical applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.17223": {
        "authors": [
            "Siyang Song",
            "Micol Spitale",
            "Xiangyu Kong",
            "Hengde Zhu",
            "Cheng Luo",
            "Cristina Palmero",
            "German Barquero",
            "Sergio Escalera",
            "Michel Valstar",
            "Mohamed Daoudi",
            "Tobias Baur",
            "Fabien Ringeval",
            "Andrew Howes",
            "Elisabeth Andre",
            "Hatice Gunes"
        ],
        "title": "REACT 2025: the Third Multiple Appropriate Facial Reaction Generation Challenge",
        "abstract": "arXiv:2505.17223v1 Announce Type: new  Abstract: In dyadic interactions, a broad spectrum of human facial reactions might be appropriate for responding to each human speaker behaviour. Following the successful organisation of the REACT 2023 and REACT 2024 challenges, we are proposing the REACT 2025 challenge encouraging the development and benchmarking of Machine Learning (ML) models that can be used to generate multiple appropriate, diverse, realistic and synchronised human-style facial reactions expressed by human listeners in response to an input stimulus (i.e., audio-visual behaviours expressed by their corresponding speakers). As a key of the challenge, we provide challenge participants with the first natural and large-scale multi-modal MAFRG dataset (called MARS) recording 137 human-human dyadic interactions containing a total of 2856 interaction sessions covering five different topics. In addition, this paper also presents the challenge guidelines and the performance of our baselines on the two proposed sub-challenges: Offline MAFRG and Online MAFRG, respectively. The challenge baseline code is publicly available at https://github.com/reactmultimodalchallenge/baseline_react2025",
        "arxiv_id": "2505.17223",
        "ARXIVID": "2505.17223",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset and challenge for facial reaction generation, focusing on multimodal interactions.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2505.17931": {
        "authors": [
            "Xingjian Li",
            "Qifeng Wu",
            "Colleen Que",
            "Yiran Ding",
            "Adithya S. Ubaradka",
            "Jianhua Xing",
            "Tianyang Wang",
            "Min Xu"
        ],
        "title": "AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models",
        "abstract": "arXiv:2505.17931v1 Announce Type: new  Abstract: Medical image segmentation is vital for clinical diagnosis, yet current deep learning methods often demand extensive expert effort, i.e., either through annotating large training datasets or providing prompts at inference time for each new case. This paper introduces a zero-shot and automatic segmentation pipeline that combines off-the-shelf vision-language and segmentation foundation models. Given a medical image and a task definition (e.g., \"segment the optic disc in an eye fundus image\"), our method uses a grounding model to generate an initial bounding box, followed by a visual prompt boosting module that enhance the prompts, which are then processed by a promptable segmentation model to produce the final mask. To address the challenges of domain gap and result verification, we introduce a test-time adaptation framework featuring a set of learnable adaptors that align the medical inputs with foundation model representations. Its hyperparameters are optimized via Bayesian Optimization, guided by a proxy validation model without requiring ground-truth labels. Our pipeline offers an annotation-efficient and scalable solution for zero-shot medical image segmentation across diverse tasks. Our pipeline is evaluated on seven diverse medical imaging datasets and shows promising results. By proper decomposition and test-time adaptation, our fully automatic pipeline performs competitively with weakly-prompted interactive foundation models.",
        "arxiv_id": "2505.17931",
        "ARXIVID": "2505.17931",
        "COMMENT": "Matches criterion 4 as it proposes AutoMiSeg, a pipeline combining vision-language and segmentation foundation models for zero-shot medical image segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2505.18051": {
        "authors": [
            "Anthony Fuller",
            "Yousef Yassin",
            "Junfeng Wen",
            "Daniel G. Kyrollos",
            "Tarek Ibrahim",
            "James R. Green",
            "Evan Shelhamer"
        ],
        "title": "LookWhere? Efficient Visual Recognition by Learning Where to Look and What to See from Self-Supervision",
        "abstract": "arXiv:2505.18051v1 Announce Type: new  Abstract: Vision transformers are ever larger, more accurate, and more expensive to compute. The expense is even more extreme at high resolution as the number of tokens grows quadratically with the image size. We turn to adaptive computation to cope with this cost by learning to predict where to compute. Our LookWhere method divides the computation between a low-resolution selector and a high-resolution extractor without ever processing the full high-resolution input. We jointly pretrain the selector and extractor without task supervision by distillation from a self-supervised teacher, in effect, learning where and what to compute simultaneously. Unlike prior token reduction methods, which pay to save by pruning already-computed tokens, and prior token selection methods, which require complex and expensive per-task optimization, LookWhere economically and accurately selects and extracts transferrable representations of images. We show that LookWhere excels at sparse recognition on high-resolution inputs (Traffic Signs), maintaining accuracy while reducing FLOPs by up to 34x and time by 6x. It also excels at standard recognition tasks that are global (ImageNet classification) or local (ADE20K segmentation), improving accuracy while reducing time by 1.36x.",
        "arxiv_id": "2505.18051",
        "ARXIVID": "2505.18051",
        "COMMENT": "Matches criterion 4 as it introduces LookWhere, a method for efficient visual recognition using adaptive computation and self-supervised learning.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2505.17613": {
        "authors": [
            "Jihan Yao",
            "Yushi Hu",
            "Yujie Yi",
            "Bin Han",
            "Shangbin Feng",
            "Guang Yang",
            "Bingbing Wen",
            "Ranjay Krishna",
            "Lucy Lu Wang",
            "Yulia Tsvetkov",
            "Noah A. Smith",
            "Banghua Zhu"
        ],
        "title": "MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation",
        "abstract": "arXiv:2505.17613v1 Announce Type: new  Abstract: Automatically evaluating multimodal generation presents a significant challenge, as automated metrics often struggle to align reliably with human evaluation, especially for complex tasks that involve multiple modalities. To address this, we present MMMG, a comprehensive and human-aligned benchmark for multimodal generation across 4 modality combinations (image, audio, interleaved text and image, interleaved text and audio), with a focus on tasks that present significant challenges for generation models, while still enabling reliable automatic evaluation through a combination of models and programs. MMMG encompasses 49 tasks (including 29 newly developed ones), each with a carefully designed evaluation pipeline, and 937 instructions to systematically assess reasoning, controllability, and other key capabilities of multimodal generation models. Extensive validation demonstrates that MMMG is highly aligned with human evaluation, achieving an average agreement of 94.3%. Benchmarking results on 24 multimodal generation models reveal that even though the state-of-the-art model, GPT Image, achieves 78.3% accuracy for image generation, it falls short on multimodal reasoning and interleaved generation. Furthermore, results suggest considerable headroom for improvement in audio generation, highlighting an important direction for future research.",
        "arxiv_id": "2505.17613",
        "ARXIVID": "2505.17613",
        "COMMENT": "Matches criterion 2 as it introduces MMMG, a comprehensive evaluation suite for multimodal generation models, focusing on reasoning and controllability.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2505.17649": {
        "authors": [
            "Junhang Li",
            "Yu Guo",
            "Chuhua Xian",
            "Shengfeng He"
        ],
        "title": "Instruct2See: Learning to Remove Any Obstructions Across Distributions",
        "abstract": "arXiv:2505.17649v1 Announce Type: new  Abstract: Images are often obstructed by various obstacles due to capture limitations, hindering the observation of objects of interest. Most existing methods address occlusions from specific elements like fences or raindrops, but are constrained by the wide range of real-world obstructions, making comprehensive data collection impractical. To overcome these challenges, we propose Instruct2See, a novel zero-shot framework capable of handling both seen and unseen obstacles. The core idea of our approach is to unify obstruction removal by treating it as a soft-hard mask restoration problem, where any obstruction can be represented using multi-modal prompts, such as visual semantics and textual instructions, processed through a cross-attention unit to enhance contextual understanding and improve mode control. Additionally, a tunable mask adapter allows for dynamic soft masking, enabling real-time adjustment of inaccurate masks. Extensive experiments on both in-distribution and out-of-distribution obstacles show that Instruct2See consistently achieves strong performance and generalization in obstruction removal, regardless of whether the obstacles were present during the training phase. Code and dataset are available at https://jhscut.github.io/Instruct2See.",
        "arxiv_id": "2505.17649",
        "ARXIVID": "2505.17649",
        "COMMENT": "Matches criterion 2 as it proposes a novel multi-modal framework (Instruct2See) for obstruction removal using visual and textual prompts.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2505.17256": {
        "authors": [
            "Liang Shi",
            "Yun Fu"
        ],
        "title": "ExpertGen: Training-Free Expert Guidance for Controllable Text-to-Face Generation",
        "abstract": "arXiv:2505.17256v1 Announce Type: new  Abstract: Recent advances in diffusion models have significantly improved text-to-face generation, but achieving fine-grained control over facial features remains a challenge. Existing methods often require training additional modules to handle specific controls such as identity, attributes, or age, making them inflexible and resource-intensive. We propose ExpertGen, a training-free framework that leverages pre-trained expert models such as face recognition, facial attribute recognition, and age estimation networks to guide generation with fine control. Our approach uses a latent consistency model to ensure realistic and in-distribution predictions at each diffusion step, enabling accurate guidance signals to effectively steer the diffusion process. We show qualitatively and quantitatively that expert models can guide the generation process with high precision, and multiple experts can collaborate to enable simultaneous control over diverse facial aspects. By allowing direct integration of off-the-shelf expert models, our method transforms any such model into a plug-and-play component for controllable face generation.",
        "arxiv_id": "2505.17256",
        "ARXIVID": "2505.17256",
        "COMMENT": "Matches criterion 4 as it proposes a training-free framework for controllable text-to-face generation, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2505.17910": {
        "authors": [
            "Bin Wu",
            "Wei Wang",
            "Yahui Liu",
            "Zixiang Li",
            "Yao Zhao"
        ],
        "title": "DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learning",
        "abstract": "arXiv:2505.17910v1 Announce Type: new  Abstract: Reward Feedback Learning (ReFL) has recently shown great potential in aligning model outputs with human preferences across various generative tasks. In this work, we introduce a ReFL framework, named DiffusionReward, to the Blind Face Restoration task for the first time. DiffusionReward effectively overcomes the limitations of diffusion-based methods, which often fail to generate realistic facial details and exhibit poor identity consistency. The core of our framework is the Face Reward Model (FRM), which is trained using carefully annotated data. It provides feedback signals that play a pivotal role in steering the optimization process of the restoration network. In particular, our ReFL framework incorporates a gradient flow into the denoising process of off-the-shelf face restoration methods to guide the update of model parameters. The guiding gradient is collaboratively determined by three aspects: (i) the FRM to ensure the perceptual quality of the restored faces; (ii) a regularization term that functions as a safeguard to preserve generative diversity; and (iii) a structural consistency constraint to maintain facial fidelity. Furthermore, the FRM undergoes dynamic optimization throughout the process. It not only ensures that the restoration network stays precisely aligned with the real face manifold, but also effectively prevents reward hacking. Experiments on synthetic and wild datasets demonstrate that our method outperforms state-of-the-art methods, significantly improving identity consistency and facial details. The source codes, data, and models are available at: https://github.com/01NeuralNinja/DiffusionReward.",
        "arxiv_id": "2505.17910",
        "ARXIVID": "2505.17910",
        "COMMENT": "Matches criterion 4 as it introduces a reward feedback learning framework for blind face restoration, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2505.17476": {
        "authors": [
            "Yuchen Zhang",
            "Yaxiong Wang",
            "Yujiao Wu",
            "Lianwei Wu",
            "Li Zhu"
        ],
        "title": "The Coherence Trap: When MLLM-Crafted Narratives Exploit Manipulated Visual Contexts",
        "abstract": "arXiv:2505.17476v1 Announce Type: new  Abstract: The detection and grounding of multimedia manipulation has emerged as a critical challenge in combating AI-generated disinformation. While existing methods have made progress in recent years, we identify two fundamental limitations in current approaches: (1) Underestimation of MLLM-driven deception risk: prevailing techniques primarily address rule-based text manipulations, yet fail to account for sophisticated misinformation synthesized by multimodal large language models (MLLMs) that can dynamically generate semantically coherent, contextually plausible yet deceptive narratives conditioned on manipulated images; (2) Unrealistic misalignment artifacts: currently focused scenarios rely on artificially misaligned content that lacks semantic coherence, rendering them easily detectable. To address these gaps holistically, we propose a new adversarial pipeline that leverages MLLMs to generate high-risk disinformation. Our approach begins with constructing the MLLM-Driven Synthetic Multimodal (MDSM) dataset, where images are first altered using state-of-the-art editing techniques and then paired with MLLM-generated deceptive texts that maintain semantic consistency with the visual manipulations. Building upon this foundation, we present the Artifact-aware Manipulation Diagnosis via MLLM (AMD) framework featuring two key innovations: Artifact Pre-perception Encoding strategy and Manipulation-Oriented Reasoning, to tame MLLMs for the MDSM problem. Comprehensive experiments validate our framework's superior generalization capabilities as a unified architecture for detecting MLLM-powered multimodal deceptions.",
        "arxiv_id": "2505.17476",
        "ARXIVID": "2505.17476",
        "COMMENT": "Matches criterion 2 as it addresses MLLM-driven deception and introduces a new dataset (MDSM) and framework (AMD) for detecting multimodal manipulations.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.17619": {
        "authors": [
            "Bo Wang",
            "De-Xing Huang",
            "Xiao-Hu Zhou",
            "Mei-Jiang Gui",
            "Nu-Fang Xiao",
            "Jian-Long Hao",
            "Ming-Yuan Liu",
            "Zeng-Guang Hou"
        ],
        "title": "CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography Quality Assessment",
        "abstract": "arXiv:2505.17619v1 Announce Type: new  Abstract: Synthetic X-ray angiographies generated by modern generative models hold great potential to reduce the use of contrast agents in vascular interventional procedures. However, low-quality synthetic angiographies can significantly increase procedural risk, underscoring the need for reliable image quality assessment (IQA) methods. Existing IQA models, however, fail to leverage auxiliary images as references during evaluation and lack fine-grained, task-specific metrics necessary for clinical relevance. To address these limitations, this paper proposes CAS-IQA, a vision-language model (VLM)-based framework that predicts fine-grained quality scores by effectively incorporating auxiliary information from related images. In the absence of angiography datasets, CAS-3K is constructed, comprising 3,565 synthetic angiographies along with score annotations. To ensure clinically meaningful assessment, three task-specific evaluation metrics are defined. Furthermore, a Multi-path featUre fuSion and rouTing (MUST) module is designed to enhance image representations by adaptively fusing and routing visual tokens to metric-specific branches. Extensive experiments on the CAS-3K dataset demonstrate that CAS-IQA significantly outperforms state-of-the-art IQA methods by a considerable margin.",
        "arxiv_id": "2505.17619",
        "ARXIVID": "2505.17619",
        "COMMENT": "Matches criterion 4 as it applies vision-language models to synthetic angiography quality assessment, introducing task-specific metrics and a new dataset (CAS-3K).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.17473": {
        "authors": [
            "Jiangning Zhu",
            "Yuxing Zhou",
            "Zheng Wang",
            "Juntao Yao",
            "Yima Gu",
            "Yuhui Yuan",
            "Shixia Liu"
        ],
        "title": "OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics",
        "abstract": "arXiv:2505.17473v1 Announce Type: new  Abstract: Given the central role of charts in scientific, business, and communication contexts, enhancing the chart understanding capabilities of vision-language models (VLMs) has become increasingly critical. A key limitation of existing VLMs lies in their inaccurate visual grounding of infographic elements, including charts and human-recognizable objects (HROs) such as icons and images. However, chart understanding often requires identifying relevant elements and reasoning over them. To address this limitation, we introduce OrionBench, a benchmark designed to support the development of accurate object detection models for charts and HROs in infographics. It contains 26,250 real and 78,750 synthetic infographics, with over 6.9 million bounding box annotations. These annotations are created by combining the model-in-the-loop and programmatic methods. We demonstrate the usefulness of OrionBench through three applications: 1) constructing a Thinking-with-Boxes scheme to boost the chart understanding performance of VLMs, 2) comparing existing object detection models, and 3) applying the developed detection model to document layout and UI element detection.",
        "arxiv_id": "2505.17473",
        "ARXIVID": "2505.17473",
        "COMMENT": "Matches criterion 3 as it introduces OrionBench, a new benchmark for chart and object detection in infographics, addressing limitations in visual grounding for VLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.17609": {
        "authors": [
            "Zixian Guo",
            "Ming Liu",
            "Zhilong Ji",
            "Jinfeng Bai",
            "Lei Zhang",
            "Wangmeng Zuo"
        ],
        "title": "Decoupled Visual Interpretation and Linguistic Reasoning for Math Problem Solving",
        "abstract": "arXiv:2505.17609v1 Announce Type: new  Abstract: Current large vision-language models (LVLMs) typically employ a connector module to link visual features with text embeddings of large language models (LLMs) and use end-to-end training to achieve multi-modal understanding in a unified process. Well alignment needs high-quality pre-training data and a carefully designed training process. Current LVLMs face challenges when addressing complex vision-language reasoning tasks, with their reasoning capabilities notably lagging behind those of LLMs. This paper proposes a paradigm shift: instead of training end-to-end vision-language reasoning models, we advocate for developing a decoupled reasoning framework based on existing visual interpretation specialists and text-based reasoning LLMs. Our approach leverages (1) a dedicated vision-language model to transform the visual content of images into textual descriptions and (2) an LLM to perform reasoning according to the visual-derived text and the original question. This method presents a cost-efficient solution for multi-modal model development by optimizing existing models to work collaboratively, avoiding end-to-end development of vision-language models from scratch. By transforming images into language model-compatible text representations, it facilitates future low-cost and flexible upgrades to upcoming powerful LLMs. We introduce an outcome-rewarded joint-tuning strategy to optimize the cooperation between the visual interpretation and linguistic reasoning model. Evaluation results on vision-language benchmarks demonstrate that the decoupled reasoning framework outperforms recent LVLMs. Our approach yields particularly significant performance gains on visually intensive geometric mathematics problems. The code is available: https://github.com/guozix/DVLR.",
        "arxiv_id": "2505.17609",
        "ARXIVID": "2505.17609",
        "COMMENT": "Matches criterion 2 as it proposes a decoupled reasoning framework for vision-language models, focusing on improving reasoning capabilities by leveraging existing models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.17512": {
        "authors": [
            "Shuhang Xu",
            "Weijian Deng",
            "Yixuan Zhou",
            "Fangwei Zhong"
        ],
        "title": "Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge in LLMs",
        "abstract": "arXiv:2505.17512v1 Announce Type: new  Abstract: Concepts represent generalized abstractions that enable humans to categorize and reason efficiently, yet it is unclear to what extent Large Language Models (LLMs) comprehend these semantic relationships. Existing benchmarks typically focus on factual recall and isolated tasks, failing to evaluate the ability of LLMs to understand conceptual boundaries. To address this gap, we introduce CK-Arena, a multi-agent interaction game built upon the Undercover game, designed to evaluate the capacity of LLMs to reason with concepts in interactive settings. CK-Arena challenges models to describe, differentiate, and infer conceptual boundaries based on partial information, encouraging models to explore commonalities and distinctions between closely related concepts. By simulating real-world interaction, CK-Arena provides a scalable and realistic benchmark for assessing conceptual reasoning in dynamic environments. Experimental results show that LLMs' understanding of conceptual knowledge varies significantly across different categories and is not strictly aligned with parameter size or general model capabilities. The data and code are available at the project homepage: https://ck-arena.site.",
        "arxiv_id": "2505.17512",
        "ARXIVID": "2505.17512",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (CK-Arena) for assessing conceptual reasoning in LLMs, focusing on interactive and dynamic environments.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.17425": {
        "authors": [
            "Wei Jie Yeo",
            "Rui Mao",
            "Moloud Abdar",
            "Erik Cambria",
            "Ranjan Satapathy"
        ],
        "title": "Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads",
        "abstract": "arXiv:2505.17425v1 Announce Type: new  Abstract: Multimodal models like CLIP have gained significant attention due to their remarkable zero-shot performance across various tasks. However, studies have revealed that CLIP can inadvertently learn spurious associations between target variables and confounding factors. To address this, we introduce \\textsc{Locate-Then-Correct} (LTC), a contrastive framework that identifies spurious attention heads in Vision Transformers via mechanistic insights and mitigates them through targeted ablation. Furthermore, LTC identifies salient, task-relevant attention heads, enabling the integration of discriminative features through orthogonal projection to improve classification performance. We evaluate LTC on benchmarks with inherent background and gender biases, achieving over a $>50\\%$ gain in worst-group accuracy compared to non-training post-hoc baselines. Additionally, we visualize the representation of selected heads and find that the presented interpretation corroborates our contrastive mechanism for identifying both spurious and salient attention heads. Code available at https://github.com/wj210/CLIP_LTC.",
        "arxiv_id": "2505.17425",
        "ARXIVID": "2505.17425",
        "COMMENT": "Matches criterion 2 as it focuses on debiasing CLIP, a multimodal model, which is relevant to VLLMs/MLLMs.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2505.17955": {
        "authors": [
            "Yujin Jeong",
            "Arnas Uselis",
            "Seong Joon Oh",
            "Anna Rohrbach"
        ],
        "title": "Diffusion Classifiers Understand Compositionality, but Conditions Apply",
        "abstract": "arXiv:2505.17955v1 Announce Type: new  Abstract: Understanding visual scenes is fundamental to human intelligence. While discriminative models have significantly advanced computer vision, they often struggle with compositional understanding. In contrast, recent generative text-to-image diffusion models excel at synthesizing complex scenes, suggesting inherent compositional capabilities. Building on this, zero-shot diffusion classifiers have been proposed to repurpose diffusion models for discriminative tasks. While prior work offered promising results in discriminative compositional scenarios, these results remain preliminary due to a small number of benchmarks and a relatively shallow analysis of conditions under which the models succeed. To address this, we present a comprehensive study of the discriminative capabilities of diffusion classifiers on a wide range of compositional tasks. Specifically, our study covers three diffusion models (SD 1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks. Further, we shed light on the role that target dataset domains play in respective performance; to isolate the domain effects, we introduce a new diagnostic benchmark Self-Bench comprised of images created by diffusion models themselves. Finally, we explore the importance of timestep weighting and uncover a relationship between domain gap and timestep sensitivity, particularly for SD3-m. To sum up, diffusion classifiers understand compositionality, but conditions apply! Code and dataset are available at https://github.com/eugene6923/Diffusion-Classifiers-Compositionality.",
        "arxiv_id": "2505.17955",
        "ARXIVID": "2505.17955",
        "COMMENT": "Matches criterion 4 as it explores diffusion classifiers and their compositional understanding, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.18142": {
        "authors": [
            "Junfeng Wu",
            "Dongliang Luo",
            "Weizhi Zhao",
            "Zhihao Xie",
            "Yuanhao Wang",
            "Junyi Li",
            "Xudong Xie",
            "Yuliang Liu",
            "Xiang Bai"
        ],
        "title": "TokBench: Evaluating Your Visual Tokenizer before Visual Generation",
        "abstract": "arXiv:2505.18142v1 Announce Type: new  Abstract: In this work, we reveal the limitations of visual tokenizers and VAEs in preserving fine-grained features, and propose a benchmark to evaluate reconstruction performance for two challenging visual contents: text and face. Image tokenization has significantly advanced visual generation and multimodal modeling, particularly with autoregressive models due to the modeling simplicity of discrete tokens. Autoregressive models typically rely on image tokenizers to compress images into discrete tokens for sequential prediction, whereas diffusion models often operate on continuous latent space to reduce computational costs. However, both visual compression approaches inevitably lose visual information, thereby limiting the upper bound of visual generation quality. To evaluate how these compression losses affect text and faces, the most human-sensitive visual elements, we first collect and curate a collection of text and faces images from existing datasets, ensuring clarity and diversity. For text reconstruction, we employ OCR models to assess the recognition accuracy of the reconstructed text, and then we measure feature similarity between original and reconstructed faces thereby quantifying faces reconstruction fidelity. Our method is highly lightweight, requiring just 2GB memory and 4 minutes to complete evaluations. With our benchmark, we analyze the reconstruction quality of text and faces at various scales across different image tokenizers and VAEs. Our results demonstrate that modern visual tokenizers still struggle to preserve fine-grained features, particularly at smaller scales. Furthermore, we extend this evaluation framework to the video, conducting a comprehensive analysis of video tokenizers. Additionally, we find that traditional metrics fail to accurately reflect the reconstruction performance for faces and text, while our proposed metrics serve as an effective complement.",
        "arxiv_id": "2505.18142",
        "ARXIVID": "2505.18142",
        "COMMENT": "Matches criterion 4 as it focuses on visual tokenizers and their limitations, which are relevant to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.17333": {
        "authors": [
            "Xin You",
            "Minghui Zhang",
            "Hanxiao Zhang",
            "Jie Yang",
            "Nassir Navab"
        ],
        "title": "Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis",
        "abstract": "arXiv:2505.17333v1 Announce Type: new  Abstract: Temporal modeling on regular respiration-induced motions is crucial to image-guided clinical applications. Existing methods cannot simulate temporal motions unless high-dose imaging scans including starting and ending frames exist simultaneously. However, in the preoperative data acquisition stage, the slight movement of patients may result in dynamic backgrounds between the first and last frames in a respiratory period. This additional deviation can hardly be removed by image registration, thus affecting the temporal modeling. To address that limitation, we pioneeringly simulate the regular motion process via the image-to-video (I2V) synthesis framework, which animates with the first frame to forecast future frames of a given length. Besides, to promote the temporal consistency of animated videos, we devise the Temporal Differential Diffusion Model to generate temporal differential fields, which measure the relative differential representations between adjacent frames. The prompt attention layer is devised for fine-grained differential fields, and the field augmented layer is adopted to better interact these fields with the I2V framework, promoting more accurate temporal variation of synthesized videos. Extensive results on ACDC cardiac and 4D Lung datasets reveal that our approach simulates 4D videos along the intrinsic motion trajectory, rivaling other competitive methods on perceptual similarity and temporal consistency. Codes will be available soon.",
        "arxiv_id": "2505.17333",
        "ARXIVID": "2505.17333",
        "COMMENT": "Does not match any specific criteria. Focuses on temporal modeling and image-to-video synthesis for clinical applications, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.17618": {
        "authors": [
            "Haoran He",
            "Jiajun Liang",
            "Xintao Wang",
            "Pengfei Wan",
            "Di Zhang",
            "Kun Gai",
            "Ling Pan"
        ],
        "title": "Scaling Image and Video Generation via Test-Time Evolutionary Search",
        "abstract": "arXiv:2505.17618v1 Announce Type: new  Abstract: As the marginal cost of scaling computation (data and parameters) during model pre-training continues to increase substantially, test-time scaling (TTS) has emerged as a promising direction for improving generative model performance by allocating additional computation at inference time. While TTS has demonstrated significant success across multiple language tasks, there remains a notable gap in understanding the test-time scaling behaviors of image and video generative models (diffusion-based or flow-based models). Although recent works have initiated exploration into inference-time strategies for vision tasks, these approaches face critical limitations: being constrained to task-specific domains, exhibiting poor scalability, or falling into reward over-optimization that sacrifices sample diversity. In this paper, we propose \\textbf{Evo}lutionary \\textbf{Search} (EvoSearch), a novel, generalist, and efficient TTS method that effectively enhances the scalability of both image and video generation across diffusion and flow models, without requiring additional training or model expansion. EvoSearch reformulates test-time scaling for diffusion and flow models as an evolutionary search problem, leveraging principles from biological evolution to efficiently explore and refine the denoising trajectory. By incorporating carefully designed selection and mutation mechanisms tailored to the stochastic differential equation denoising process, EvoSearch iteratively generates higher-quality offspring while preserving population diversity. Through extensive evaluation across both diffusion and flow architectures for image and video generation tasks, we demonstrate that our method consistently outperforms existing approaches, achieves higher diversity, and shows strong generalizability to unseen evaluation metrics. Our project is available at the website https://tinnerhrhe.github.io/evosearch.",
        "arxiv_id": "2505.17618",
        "ARXIVID": "2505.17618",
        "COMMENT": "This paper does not match any specific criteria but is related to generative modeling for image and video generation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.17501": {
        "authors": [
            "Yuehan Jin",
            "Xiaoqing Liu",
            "Yiyuan Yang",
            "Zhiwen Yu",
            "Tong Zhang",
            "Kaixiang Yang"
        ],
        "title": "RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition",
        "abstract": "arXiv:2505.17501v1 Announce Type: new  Abstract: Multimodal emotion recognition analyzes emotions by combining data from multiple sources. However, real-world noise or sensor failures often cause missing or corrupted data, creating the Incomplete Multimodal Emotion Recognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion Recovery (RoHyDR), a novel framework that performs missing-modality recovery at unimodal, multimodal, feature, and semantic levels. For unimodal representation recovery of missing modalities, RoHyDR exploits a diffusion-based generator to generate distribution-consistent and semantically aligned representations from Gaussian noise, using available modalities as conditioning. For multimodal fusion recovery, we introduce adversarial learning to produce a realistic fused multimodal representation and recover missing semantic content. We further propose a multi-stage optimization strategy that enhances training stability and efficiency. In contrast to previous work, the hybrid diffusion and adversarial learning-based recovery mechanism in RoHyDR allows recovery of missing information in both unimodal representation and multimodal fusion, at both feature and semantic levels, effectively mitigating performance degradation caused by suboptimal optimization. Comprehensive experiments conducted on two widely used multimodal emotion recognition benchmarks demonstrate that our proposed method outperforms state-of-the-art IMER methods, achieving robust recognition performance under various missing-modality scenarios. Our code will be made publicly available upon acceptance.",
        "arxiv_id": "2505.17501",
        "ARXIVID": "2505.17501",
        "COMMENT": "Does not match any specific criterion but is related to multimodal emotion recognition, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.17702": {
        "authors": [
            "Xueyang Li",
            "Jiahao Li",
            "Yu Song",
            "Yunzhong Lou",
            "Xiangdong Zhou"
        ],
        "title": "Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek",
        "abstract": "arXiv:2505.17702v1 Announce Type: new  Abstract: The advent of Computer-Aided Design (CAD) generative modeling will significantly transform the design of industrial products. The recent research endeavor has extended into the realm of Large Language Models (LLMs). In contrast to fine-tuning methods, training-free approaches typically utilize the advanced closed-source LLMs, thereby offering enhanced flexibility and efficiency in the development of AI agents for generating CAD parametric models. However, the substantial cost and limitations of local deployment of the top-tier closed-source LLMs pose challenges in practical applications. The Seek-CAD is the pioneer exploration of locally deployed open-source inference LLM DeepSeek-R1 for CAD parametric model generation with a training-free methodology. This study is the first investigation to incorporate both visual and Chain-of-Thought (CoT) feedback within the self-refinement mechanism for generating CAD models. Specifically, the initial generated parametric CAD model is rendered into a sequence of step-wise perspective images, which are subsequently processed by a Vision Language Model (VLM) alongside the corresponding CoTs derived from DeepSeek-R1 to assess the CAD model generation. Then, the feedback is utilized by DeepSeek-R1 to refine the initial generated model for the next round of generation. Moreover, we present an innovative 3D CAD model dataset structured around the SSR (Sketch, Sketch-based feature, and Refinements) triple design paradigm. This dataset encompasses a wide range of CAD commands, thereby aligning effectively with industrial application requirements and proving suitable for the generation of LLMs. Extensive experiments validate the effectiveness of Seek-CAD under various metrics.",
        "arxiv_id": "2505.17702",
        "ARXIVID": "2505.17702",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling in CAD, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.18024": {
        "authors": [
            "Xiaobao Wei",
            "Jiawei Liu",
            "Dongbo Yang",
            "Junda Cheng",
            "Changyong Shu",
            "Wei Wang"
        ],
        "title": "A Wavelet-based Stereo Matching Framework for Solving Frequency Convergence Inconsistency",
        "abstract": "arXiv:2505.18024v1 Announce Type: new  Abstract: We find that the EPE evaluation metrics of RAFT-stereo converge inconsistently in the low and high frequency regions, resulting high frequency degradation (e.g., edges and thin objects) during the iterative process. The underlying reason for the limited performance of current iterative methods is that it optimizes all frequency components together without distinguishing between high and low frequencies. We propose a wavelet-based stereo matching framework (Wavelet-Stereo) for solving frequency convergence inconsistency. Specifically, we first explicitly decompose an image into high and low frequency components using discrete wavelet transform. Then, the high-frequency and low-frequency components are fed into two different multi-scale frequency feature extractors. Finally, we propose a novel LSTM-based high-frequency preservation update operator containing an iterative frequency adapter to provide adaptive refined high-frequency features at different iteration steps by fine-tuning the initial high-frequency features. By processing high and low frequency components separately, our framework can simultaneously refine high-frequency information in edges and low-frequency information in smooth regions, which is especially suitable for challenging scenes with fine details and textures in the distance. Extensive experiments demonstrate that our Wavelet-Stereo outperforms the state-of-the-art methods and ranks 1st on both the KITTI 2015 and KITTI 2012 leaderboards for almost all metrics. We will provide code and pre-trained models to encourage further exploration, application, and development of our innovative framework (https://github.com/SIA-IDE/Wavelet-Stereo).",
        "arxiv_id": "2505.18024",
        "ARXIVID": "2505.18024",
        "COMMENT": "Does not match any specific criterion but is related to computer vision and machine learning, focusing on stereo matching with wavelet-based methods.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.17442": {
        "authors": [
            "Hao Jing",
            "Anhong Wang",
            "Yifan Zhang",
            "Donghan Bu",
            "Junhui Hou"
        ],
        "title": "Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds",
        "abstract": "arXiv:2505.17442v1 Announce Type: new  Abstract: Regarding intelligent transportation systems for vehicle networking, low-bitrate transmission via lossy point cloud compression is vital for facilitating real-time collaborative perception among vehicles with restricted bandwidth. In existing compression transmission systems, the sender lossily compresses point coordinates and reflectance to generate a transmission code stream, which faces transmission burdens from reflectance encoding and limited detection robustness due to information loss. To address these issues, this paper proposes a 3D object detection framework with reflectance prediction-based knowledge distillation (RPKD). We compress point coordinates while discarding reflectance during low-bitrate transmission, and feed the decoded non-reflectance compressed point clouds into a student detector. The discarded reflectance is then reconstructed by a geometry-based reflectance prediction (RP) module within the student detector for precise detection. A teacher detector with the same structure as student detector is designed for performing reflectance knowledge distillation (RKD) and detection knowledge distillation (DKD) from raw to compressed point clouds. Our RPKD framework jointly trains detectors on both raw and compressed point clouds to improve the student detector's robustness. Experimental results on the KITTI dataset and Waymo Open Dataset demonstrate that our method can boost detection accuracy for compressed point clouds across multiple code rates. Notably, at a low code rate of 2.146 Bpp on the KITTI dataset, our RPKD-PV achieves the highest mAP of 73.6, outperforming existing detection methods with the PV-RCNN baseline.",
        "arxiv_id": "2505.17442",
        "ARXIVID": "2505.17442",
        "COMMENT": "This paper does not match any of the specific criteria. It focuses on 3D object detection in compressed point clouds, which is not directly related to the specified interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.17317": {
        "authors": [
            "Alyson East",
            "Elizabeth G. Campolongo",
            "Luke Meyers",
            "S M Rayeed",
            "Samuel Stevens",
            "Iuliia Zarubiieva",
            "Isadora E. Fluck",
            "Jennifer C. Gir\\'on",
            "Maximiliane Jousse",
            "Scott Lowe",
            "Kayla I Perry",
            "Isabelle Betancourt",
            "Noah Charney",
            "Evan Donoso",
            "Nathan Fox",
            "Kim J. Landsbergen",
            "Ekaterina Nepovinnykh",
            "Michelle Ramirez",
            "Parkash Singh",
            "Khum Thapa-Magar",
            "Matthew Thompson",
            "Evan Waite",
            "Tanya Berger-Wolf",
            "Hilmar Lapp",
            "Paula Mabee",
            "Graham Taylor",
            "Sydne Record"
        ],
        "title": "Optimizing Image Capture for Computer Vision-Powered Taxonomic Identification and Trait Recognition of Biodiversity Specimens",
        "abstract": "arXiv:2505.17317v1 Announce Type: new  Abstract: Biological collections house millions of specimens documenting Earth's biodiversity, with digital images increasingly available through open-access platforms. Most imaging protocols were developed for human visual interpretation without considering computational analysis requirements. This paper aims to bridge the gap between current imaging practices and the potential for automated analysis by presenting key considerations for creating biological specimen images optimized for computer vision applications. We provide conceptual computer vision topics for context, addressing fundamental concerns including model generalization, data leakage, and comprehensive metadata documentation, and outline practical guidance on specimen imagine, and data storage. These recommendations were synthesized through interdisciplinary collaboration between taxonomists, collection managers, ecologists, and computer scientists. Through this synthesis, we have identified ten interconnected considerations that form a framework for successfully integrating biological specimen images into computer vision pipelines. The key elements include: (1) comprehensive metadata documentation, (2) standardized specimen positioning, (3) consistent size and color calibration, (4) protocols for handling multiple specimens in one image, (5) uniform background selection, (6) controlled lighting, (7) appropriate resolution and magnification, (8) optimal file formats, (9) robust data archiving strategies, and (10) accessible data sharing practices. By implementing these recommendations, collection managers, taxonomists, and biodiversity informaticians can generate images that support automated trait extraction, species identification, and novel ecological and evolutionary analyses at unprecedented scales. Successful implementation lies in thorough documentation of methodological choices.",
        "arxiv_id": "2505.17317",
        "ARXIVID": "2505.17317",
        "COMMENT": "Does not match any specific criteria. Focuses on optimizing image capture for biodiversity specimens, which is not directly related to spatial intelligence, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.17249": {
        "authors": [
            "Yuran Sun",
            "Susu Xu",
            "Chenguang Wang",
            "Xilei Zhao"
        ],
        "title": "Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse Reinforcement Learning",
        "abstract": "arXiv:2505.17249v1 Announce Type: new  Abstract: Big trajectory data hold great promise for human mobility analysis, but their utility is often constrained by the absence of critical traveler attributes, particularly sociodemographic information. While prior studies have explored predicting such attributes from mobility patterns, they often overlooked underlying cognitive mechanisms and exhibited low predictive accuracy. This study introduces SILIC, short for Sociodemographic Inference with LLM-guided Inverse Reinforcement Learning (IRL) and Cognitive Chain Reasoning (CCR), a theoretically grounded framework that leverages LLMs to infer sociodemographic attributes from observed mobility patterns by capturing latent behavioral intentions and reasoning through psychological constructs. Particularly, our approach explicitly follows the Theory of Planned Behavior (TPB), a foundational behavioral framework in transportation research, to model individuals' latent cognitive processes underlying travel decision-making. The LLMs further provide heuristic guidance to improve IRL reward function initialization and update by addressing its ill-posedness and optimization challenges arising from the vast and unstructured reward space. Evaluated in the 2017 Puget Sound Regional Council Household Travel Survey, our method substantially outperforms state-of-the-art baselines and shows great promise for enriching big trajectory data to support more behaviorally grounded applications in transportation planning and beyond.",
        "arxiv_id": "2505.17249",
        "ARXIVID": "2505.17249",
        "COMMENT": "Does not match any specific criteria. Focuses on sociodemographic inference using LLMs and behavioral theory, which is not directly related to spatial intelligence, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.17457": {
        "authors": [
            "Jiaxuan Lu",
            "Junyan Shi",
            "Yuhui Lin",
            "Fang Yan",
            "Yue Gao",
            "Shaoting Zhang",
            "Xiaosong Wang"
        ],
        "title": "Graph Mamba for Efficient Whole Slide Image Understanding",
        "abstract": "arXiv:2505.17457v1 Announce Type: new  Abstract: Whole Slide Images (WSIs) in histopathology present a significant challenge for large-scale medical image analysis due to their high resolution, large size, and complex tile relationships. Existing Multiple Instance Learning (MIL) methods, such as Graph Neural Networks (GNNs) and Transformer-based models, face limitations in scalability and computational cost. To bridge this gap, we propose the WSI-GMamba framework, which synergistically combines the relational modeling strengths of GNNs with the efficiency of Mamba, the State Space Model designed for sequence learning. The proposed GMamba block integrates Message Passing, Graph Scanning & Flattening, and feature aggregation via a Bidirectional State Space Model (Bi-SSM), achieving Transformer-level performance with 7* fewer FLOPs. By leveraging the complementary strengths of lightweight GNNs and Mamba, the WSI-GMamba framework delivers a scalable solution for large-scale WSI analysis, offering both high accuracy and computational efficiency for slide-level classification.",
        "arxiv_id": "2505.17457",
        "ARXIVID": "2505.17457",
        "COMMENT": "Does not match any specific criteria. Focuses on efficient whole slide image understanding using GNNs and state space models, which is not directly related to the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.17437": {
        "authors": [
            "Yuanshao Zhu",
            "James Jianqiao Yu",
            "Xiangyu Zhao",
            "Xiao Han",
            "Qidong Liu",
            "Xuetao Wei",
            "Yuxuan Liang"
        ],
        "title": "Learning Generalized and Flexible Trajectory Models from Omni-Semantic Supervision",
        "abstract": "arXiv:2505.17437v1 Announce Type: new  Abstract: The widespread adoption of mobile devices and data collection technologies has led to an exponential increase in trajectory data, presenting significant challenges in spatio-temporal data mining, particularly for efficient and accurate trajectory retrieval. However, existing methods for trajectory retrieval face notable limitations, including inefficiencies in large-scale data, lack of support for condition-based queries, and reliance on trajectory similarity measures. To address the above challenges, we propose OmniTraj, a generalized and flexible omni-semantic trajectory retrieval framework that integrates four complementary modalities or semantics -- raw trajectories, topology, road segments, and regions -- into a unified system. Unlike traditional approaches that are limited to computing and processing trajectories as a single modality, OmniTraj designs dedicated encoders for each modality, which are embedded and fused into a shared representation space. This design enables OmniTraj to support accurate and flexible queries based on any individual modality or combination thereof, overcoming the rigidity of traditional similarity-based methods. Extensive experiments on two real-world datasets demonstrate the effectiveness of OmniTraj in handling large-scale data, providing flexible, multi-modality queries, and supporting downstream tasks and applications.",
        "arxiv_id": "2505.17437",
        "ARXIVID": "2505.17437",
        "COMMENT": "This paper does not match any specific criteria but is related to spatio-temporal data mining and trajectory retrieval.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.17783": {
        "authors": [
            "Dekai Zhu",
            "Stefan Gavranovic",
            "Flavien Boussuge",
            "Benjamin Busam",
            "Slobodan Ilic"
        ],
        "title": "Generative Data Augmentation for Object Point Cloud Segmentation",
        "abstract": "arXiv:2505.17783v1 Announce Type: new  Abstract: Data augmentation is widely used to train deep learning models to address data scarcity. However, traditional data augmentation (TDA) typically relies on simple geometric transformation, such as random rotation and rescaling, resulting in minimal data diversity enrichment and limited model performance improvement. State-of-the-art generative models for 3D shape generation rely on the denoising diffusion probabilistic models and manage to generate realistic novel point clouds for 3D content creation and manipulation. Nevertheless, the generated 3D shapes lack associated point-wise semantic labels, restricting their usage in enlarging the training data for point cloud segmentation tasks. To bridge the gap between data augmentation techniques and the advanced diffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to a part-aware generative model that can generate high-quality point clouds conditioned on given segmentation masks. Leveraging the novel generative model, we introduce a 3-step generative data augmentation (GDA) pipeline for point cloud segmentation training. Our GDA approach requires only a small amount of labeled samples but enriches the training data with generated variants and pseudo-labeled samples, which are validated by a novel diffusion-based pseudo-label filtering method. Extensive experiments on two large-scale synthetic datasets and a real-world medical dataset demonstrate that our GDA method outperforms TDA approach and related semi-supervised and self-supervised methods.",
        "arxiv_id": "2505.17783",
        "ARXIVID": "2505.17783",
        "COMMENT": "This paper does not match any specific criteria but is related to generative modeling for 3D point cloud segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.17225": {
        "authors": [
            "Doohyuk Jang",
            "Yoonjeon Kim",
            "Chanjae Park",
            "Hyun Ryu",
            "Eunho Yang"
        ],
        "title": "Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models",
        "abstract": "arXiv:2505.17225v1 Announce Type: new  Abstract: Large language models have demonstrated remarkable proficiency in long and complex reasoning tasks. However, they frequently exhibit a problematic reliance on familiar reasoning patterns, a phenomenon we term \\textit{reasoning rigidity}. Despite explicit instructions from users, these models often override clearly stated conditions and default to habitual reasoning trajectories, leading to incorrect conclusions. This behavior presents significant challenges, particularly in domains such as mathematics and logic puzzle, where precise adherence to specified constraints is critical. To systematically investigate reasoning rigidity, a behavior largely unexplored in prior work, we introduce a expert-curated diagnostic set, \\dataset{}. Our dataset includes specially modified variants of existing mathematical benchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately redesigned to require deviation from familiar reasoning strategies. Using this dataset, we identify recurring contamination patterns that occur when models default to ingrained reasoning. Specifically, we categorize this contamination into three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust, and (iii) Partial Instruction Attention, each causing models to ignore or distort provided instructions. We publicly release our diagnostic set to facilitate future research on mitigating reasoning rigidity in language models.",
        "arxiv_id": "2505.17225",
        "ARXIVID": "2505.17225",
        "COMMENT": "Does not match any specific criterion but is related to reasoning in large language models, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.17218": {
        "authors": [
            "Lianghuan Huang",
            "Shuo Li",
            "Sagnik Anupam",
            "Insup Lee",
            "Osbert Bastani"
        ],
        "title": "Effective Reinforcement Learning for Reasoning in Language Models",
        "abstract": "arXiv:2505.17218v1 Announce Type: new  Abstract: Reinforcement learning (RL) has emerged as a promising strategy for improving the reasoning capabilities of language models (LMs) in domains such as mathematics and coding. However, most modern RL algorithms were designed to target robotics applications, which differ significantly from LM reasoning. We analyze RL algorithm design decisions for LM reasoning, for both accuracy and computational efficiency, focusing on relatively small models due to computational constraints. Our findings are: (i) on-policy RL significantly outperforms supervised fine-tuning (SFT), (ii) PPO-based off-policy updates increase accuracy instead of reduce variance, and (iii) removing KL divergence can lead to more concise generations and higher accuracy. Furthermore, we find that a key bottleneck to computational efficiency is that the optimal batch sizes for inference and backpropagation are different. We propose a novel algorithm, DASH, that performs preemptive sampling (i.e., sample a large batch and accumulate gradient updates in small increments), and gradient filtering (i.e., drop samples with small advantage estimates). We show that DASH reduces training time by 83% compared to a standard implementation of GRPO without sacrificing accuracy. Our findings provide valuable insights on designing effective RL algorithms for LM reasoning.",
        "arxiv_id": "2505.17218",
        "ARXIVID": "2505.17218",
        "COMMENT": "Does not match any specific criteria but discusses reinforcement learning for reasoning in language models, which is tangentially related to embodied AI and spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.17867": {
        "authors": [
            "Konstantinos Spathis",
            "Nikolaos Kardaris",
            "Petros Maragos"
        ],
        "title": "Multi-task Learning For Joint Action and Gesture Recognition",
        "abstract": "arXiv:2505.17867v1 Announce Type: new  Abstract: In practical applications, computer vision tasks often need to be addressed simultaneously. Multitask learning typically achieves this by jointly training a single deep neural network to learn shared representations, providing efficiency and improving generalization. Although action and gesture recognition are closely related tasks, since they focus on body and hand movements, current state-of-the-art methods handle them separately. In this paper, we show that employing a multi-task learning paradigm for action and gesture recognition results in more efficient, robust and generalizable visual representations, by leveraging the synergies between these tasks. Extensive experiments on multiple action and gesture datasets demonstrate that handling actions and gestures in a single architecture can achieve better performance for both tasks in comparison to their single-task learning variants.",
        "arxiv_id": "2505.17867",
        "ARXIVID": "2505.17867",
        "COMMENT": "This paper does not match any specific criteria but is generally related to computer vision and multitask learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.18135": {
        "authors": [
            "Kazem Faghih",
            "Wenxiao Wang",
            "Yize Cheng",
            "Siddhant Bharti",
            "Gaurang Sriramanan",
            "Sriram Balasubramanian",
            "Parsa Hosseini",
            "Soheil Feizi"
        ],
        "title": "Gaming Tool Preferences in Agentic LLMs",
        "abstract": "arXiv:2505.18135v1 Announce Type: new  Abstract: Large language models (LLMs) can now access a wide range of external tools, thanks to the Model Context Protocol (MCP). This greatly expands their abilities as various agents. However, LLMs rely entirely on the text descriptions of tools to decide which ones to use--a process that is surprisingly fragile. In this work, we expose a vulnerability in prevalent tool/function-calling protocols by investigating a series of edits to tool descriptions, some of which can drastically increase a tool's usage from LLMs when competing with alternatives. Through controlled experiments, we show that tools with properly edited descriptions receive over 10 times more usage from GPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further evaluate how various edits to tool descriptions perform when competing directly with one another and how these trends generalize or differ across a broader set of 10 different models. These phenomenons, while giving developers a powerful way to promote their tools, underscore the need for a more reliable foundation for agentic LLMs to select and utilize tools and resources.",
        "arxiv_id": "2505.18135",
        "ARXIVID": "2505.18135",
        "COMMENT": "Does not match any specific criterion but discusses tool preferences in agentic LLMs, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}