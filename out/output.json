{
    "2509.26644": {
        "authors": [
            "Jessica Bader",
            "Mateusz Pach",
            "Maria A. Bravo",
            "Serge Belongie",
            "Zeynep Akata"
        ],
        "title": "Stitch: Training-Free Position Control in Multimodal Diffusion Transformers",
        "abstract": "arXiv:2509.26644v1 Announce Type: new  Abstract: Text-to-Image (T2I) generation models have advanced rapidly in recent years, but accurately capturing spatial relationships like \"above\" or \"to the right of\" poses a persistent challenge. Earlier methods improved spatial relationship following with external position control. However, as architectures evolved to enhance image quality, these techniques became incompatible with modern models. We propose Stitch, a training-free method for incorporating external position control into Multi-Modal Diffusion Transformers (MMDiT) via automatically-generated bounding boxes. Stitch produces images that are both spatially accurate and visually appealing by generating individual objects within designated bounding boxes and seamlessly stitching them together. We find that targeted attention heads capture the information necessary to isolate and cut out individual objects mid-generation, without needing to fully complete the image. We evaluate Stitch on PosEval, our benchmark for position-based T2I generation. Featuring five new tasks that extend the concept of Position beyond the basic GenEval task, PosEval demonstrates that even top models still have significant room for improvement in position-based generation. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances base models, even improving FLUX by 218% on GenEval's Position task and by 206% on PosEval. Stitch achieves state-of-the-art results with Qwen-Image on PosEval, improving over previous models by 54%, all accomplished while integrating position control into leading models training-free. Code is available at https://github.com/ExplainableML/Stitch.",
        "arxiv_id": "2509.26644",
        "ARXIVID": "2509.26644",
        "COMMENT": "The paper does not match any specific criteria closely. It discusses position control in text-to-image generation using diffusion transformers, which is not directly related to joint generation and segmentation, unified diffusion models for multiple tasks, or matting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2509.25776": {
        "authors": [
            "Mingyu Kang",
            "Yong Suk Choi"
        ],
        "title": "Editable Noise Map Inversion: Encoding Target-image into Noise For High-Fidelity Image Manipulation",
        "abstract": "arXiv:2509.25776v1 Announce Type: new  Abstract: Text-to-image diffusion models have achieved remarkable success in generating high-quality and diverse images. Building on these advancements, diffusion models have also demonstrated exceptional performance in text-guided image editing. A key strategy for effective image editing involves inverting the source image into editable noise maps associated with the target image. However, previous inversion methods face challenges in adhering closely to the target text prompt. The limitation arises because inverted noise maps, while enabling faithful reconstruction of the source image, restrict the flexibility needed for desired edits. To overcome this issue, we propose Editable Noise Map Inversion (ENM Inversion), a novel inversion technique that searches for optimal noise maps to ensure both content preservation and editability. We analyze the properties of noise maps for enhanced editability. Based on this analysis, our method introduces an editable noise refinement that aligns with the desired edits by minimizing the difference between the reconstructed and edited noise maps. Extensive experiments demonstrate that ENM Inversion outperforms existing approaches across a wide range of image editing tasks in both preservation and edit fidelity with target prompts. Our approach can also be easily applied to video editing, enabling temporal consistency and content manipulation across frames.",
        "arxiv_id": "2509.25776",
        "ARXIVID": "2509.25776",
        "COMMENT": "The paper does not match any specific criteria closely. It focuses on image editing using diffusion models, which is not directly related to joint generation and segmentation, unified diffusion models for multiple tasks, or matting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.25502": {
        "authors": [
            "Kaiqing Lin",
            "Zhiyuan Yan",
            "Ruoxin Chen",
            "Junyan Ye",
            "Ke-Yue Zhang",
            "Yue Zhou",
            "Peng Jin",
            "Bin Li",
            "Taiping Yao",
            "Shouhong Ding"
        ],
        "title": "Seeing Before Reasoning: A Unified Framework for Generalizable and Explainable Fake Image Detection",
        "abstract": "arXiv:2509.25502v1 Announce Type: new  Abstract: Detecting AI-generated images with multimodal large language models (MLLMs) has gained increasing attention, due to their rich world knowledge, common-sense reasoning, and potential for explainability. However, naively applying those MLLMs for detection often leads to suboptimal performance. We argue that the root of this failure lies in a fundamental mismatch: MLLMs are asked to reason about fakes before they can truly see them. First, they do not really see: existing MLLMs' vision encoders are primarily optimized for semantic-oriented recognition rather than the perception of low-level signals, leaving them insensitive to subtle forgery traces. Without access to reliable perceptual evidence, the model grounds its judgment on incomplete and limited visual observations. Second, existing finetuning data for detection typically uses narrow, instruction-style formats, which diverge sharply from the diverse, heterogeneous distributions seen in pretraining. In the absence of meaningful visual cues, the model therefore exploits these linguistic shortcuts, resulting in catastrophic forgetting of pretrained knowledge (even the basic dialogue capabilities). In response, we advocate for a new paradigm: seeing before reasoning. We propose that MLLMs should first be trained to perceive artifacts-strengthening their artifact-aware visual perception-so that subsequent reasoning is grounded in actual observations. We therefore propose Forensic-Chat, a generalizable, explainable, and still-conversational (for multi-round dialogue) assistant for fake image detection. We also propose ExplainFake-Bench, a benchmark tailored for the evaluation of the MLLM's explainability for image forensics from five key aspects. Extensive experiments show its superiority of generalization and genuinely reliable explainability.",
        "arxiv_id": "2509.25502",
        "ARXIVID": "2509.25502",
        "COMMENT": "No specific criteria match.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.25940": {
        "authors": [
            "Debottam Dutta",
            "Jianchong Chen",
            "Rajalaxmi Rajagopalan",
            "Yu-Lin Wei",
            "Romit Roy Choudhury"
        ],
        "title": "CO3: Contrasting Concepts Compose Better",
        "abstract": "arXiv:2509.25940v1 Announce Type: new  Abstract: We propose to improve multi-concept prompt fidelity in text-to-image diffusion models. We begin with common failure cases-prompts like \"a cat and a dog\" that sometimes yields images where one concept is missing, faint, or colliding awkwardly with another. We hypothesize that this happens when the diffusion model drifts into mixed modes that over-emphasize a single concept it learned strongly during training. Instead of re-training, we introduce a corrective sampling strategy that steers away from regions where the joint prompt behavior overlaps too strongly with any single concept in the prompt. The goal is to steer towards \"pure\" joint modes where all concepts can coexist with balanced visual presence. We further show that existing multi-concept guidance schemes can operate in unstable weight regimes that amplify imbalance; we characterize favorable regions and adapt sampling to remain within them. Our approach, CO3, is plug-and-play, requires no model tuning, and complements standard classifier-free guidance. Experiments on diverse multi-concept prompts indicate improvements in concept coverage, balance and robustness, with fewer dropped or distorted concepts compared to standard baselines and prior compositional methods. Results suggest that lightweight corrective guidance can substantially mitigate brittle semantic alignment behavior in modern diffusion systems.",
        "arxiv_id": "2509.25940",
        "ARXIVID": "2509.25940",
        "COMMENT": "No specific criteria match.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.26008": {
        "authors": [
            "Zhiwei Zhang",
            "Ruikai Xu",
            "Weijian Zhang",
            "Zhizhong Zhang",
            "Xin Tan",
            "Jingyu Gong",
            "Yuan Xie",
            "Lizhuang Ma"
        ],
        "title": "PFDepth: Heterogeneous Pinhole-Fisheye Joint Depth Estimation via Distortion-aware Gaussian-Splatted Volumetric Fusion",
        "abstract": "arXiv:2509.26008v1 Announce Type: new  Abstract: In this paper, we present the first pinhole-fisheye framework for heterogeneous multi-view depth estimation, PFDepth. Our key insight is to exploit the complementary characteristics of pinhole and fisheye imagery (undistorted vs. distorted, small vs. large FOV, far vs. near field) for joint optimization. PFDepth employs a unified architecture capable of processing arbitrary combinations of pinhole and fisheye cameras with varied intrinsics and extrinsics. Within PFDepth, we first explicitly lift 2D features from each heterogeneous view into a canonical 3D volumetric space. Then, a core module termed Heterogeneous Spatial Fusion is designed to process and fuse distortion-aware volumetric features across overlapping and non-overlapping regions. Additionally, we subtly reformulate the conventional voxel fusion into a novel 3D Gaussian representation, in which learnable latent Gaussian spheres dynamically adapt to local image textures for finer 3D aggregation. Finally, fused volume features are rendered into multi-view depth maps. Through extensive experiments, we demonstrate that PFDepth sets a state-of-the-art performance on KITTI-360 and RealHet datasets over current mainstream depth networks. To the best of our knowledge, this is the first systematic study of heterogeneous pinhole-fisheye depth estimation, offering both technical novelty and valuable empirical insights.",
        "arxiv_id": "2509.26008",
        "ARXIVID": "2509.26008",
        "COMMENT": "No specific criteria match.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.25738": {
        "authors": [
            "Tingmin Li",
            "Yixuan Li",
            "Yang Yang"
        ],
        "title": "The 1st Solution for MOSEv1 Challenge on LSVOS 2025: CGFSeg",
        "abstract": "arXiv:2509.25738v1 Announce Type: new  Abstract: Video Object Segmentation (VOS) aims to track and segment specific objects across entire video sequences, yet it remains highly challenging under complex real-world scenarios. The MOSEv1 and LVOS dataset, adopted in the MOSEv1 challenge on LSVOS 2025, which is specifically designed to enhance the robustness of VOS models in complex real-world scenarios, including long-term object disappearances and reappearances, as well as the presence of small and inconspicuous objects. In this paper, we present our improved method, Confidence-Guided Fusion Segmentation (CGFSeg), for the VOS task in the MOSEv1 Challenge. During training, the feature extractor of SAM2 is frozen, while the remaining components are fine-tuned to preserve strong feature extraction ability and improve segmentation accuracy. In the inference stage, we introduce a pixel-check strategy that progressively refines predictions by exploiting complementary strengths of multiple models, thereby yielding robust final masks. As a result, our method achieves a J&F score of 86.37% on the test set, ranking 1st in the MOSEv1 Challenge at LSVOS 2025. These results highlight the effectiveness of our approach in addressing the challenges of VOS task in complex scenarios.",
        "arxiv_id": "2509.25738",
        "ARXIVID": "2509.25738",
        "COMMENT": "No specific criteria match.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.25866": {
        "authors": [
            "Chi Zhang",
            "Haibo Qiu",
            "Qiming Zhang",
            "Zhixiong Zeng",
            "Lin Ma",
            "Jing Zhang"
        ],
        "title": "DeepSketcher: Internalizing Visual Manipulation for Multimodal Reasoning",
        "abstract": "arXiv:2509.25866v1 Announce Type: new  Abstract: The \"thinking with images\" paradigm represents a pivotal shift in the reasoning of Vision Language Models (VLMs), moving from text-dominant chain-of-thought to image-interactive reasoning. By invoking visual tools or generating intermediate visual representations, VLMs can iteratively attend to fine-grained regions, enabling deeper image understanding and more faithful multimodal reasoning. As an emerging paradigm, however, it still leaves substantial room for exploration in data construction accuracy, structural design, and broader application scenarios, which offer rich opportunities for advancing multimodal reasoning. To further advance this line of work, we present DeepSketcher, a comprehensive suite comprising both an image-text interleaved dataset and a self-contained model. The dataset contains 31k chain-of-thought (CoT) reasoning trajectories with diverse tool calls and resulting edited images, covering a wide range of data types and manipulation instructions with high annotation accuracy. Building on this resource, we design a model that performs interleaved image-text reasoning and natively generates \"visual thoughts\" by operating directly in the visual embedding space, rather than invoking external tools and repeatedly re-encoding generated images. This design enables tool-free and more flexible \"thinking with images\". Extensive experiments on multimodal reasoning benchmarks demonstrate strong performance, validating both the utility of the dataset and the effectiveness of the model design.",
        "arxiv_id": "2509.25866",
        "ARXIVID": "2509.25866",
        "COMMENT": "No specific criteria match.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.26621": {
        "authors": [
            "Xiyi Chen",
            "Shaofei Wang",
            "Marko Mihajlovic",
            "Taewon Kang",
            "Sergey Prokudin",
            "Ming Lin"
        ],
        "title": "HART: Human Aligned Reconstruction Transformer",
        "abstract": "arXiv:2509.26621v1 Announce Type: new  Abstract: We introduce HART, a unified framework for sparse-view human reconstruction. Given a small set of uncalibrated RGB images of a person as input, it outputs a watertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat representation for photorealistic novel-view rendering. Prior methods for clothed human reconstruction either optimize parametric templates, which overlook loose garments and human-object interactions, or train implicit functions under simplified camera assumptions, limiting applicability in real scenes. In contrast, HART predicts per-pixel 3D point maps, normals, and body correspondences, and employs an occlusion-aware Poisson reconstruction to recover complete geometry, even in self-occluded regions. These predictions also align with a parametric SMPL-X body model, ensuring that reconstructed geometry remains consistent with human structure while capturing loose clothing and interactions. These human-aligned meshes initialize Gaussian splats to further enable sparse-view rendering. While trained on only 2.3K synthetic scans, HART achieves state-of-the-art results: Chamfer Distance improves by 18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for SMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on a wide range of datasets. These results suggest that feed-forward transformers can serve as a scalable model for robust human reconstruction in real-world settings. Code and models will be released.",
        "arxiv_id": "2509.26621",
        "ARXIVID": "2509.26621",
        "COMMENT": "No specific criteria match.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.26096": {
        "authors": [
            "Shigui Li",
            "Wei Chen",
            "Delu Zeng"
        ],
        "title": "EVODiff: Entropy-aware Variance Optimized Diffusion Inference",
        "abstract": "arXiv:2509.26096v1 Announce Type: new  Abstract: Diffusion models (DMs) excel in image generation, but suffer from slow inference and the training-inference discrepancies. Although gradient-based solvers like DPM-Solver accelerate the denoising inference, they lack theoretical foundations in information transmission efficiency. In this work, we introduce an information-theoretic perspective on the inference processes of DMs, revealing that successful denoising fundamentally reduces conditional entropy in reverse transitions. This principle leads to our key insights into the inference processes: (1) data prediction parameterization outperforms its noise counterpart, and (2) optimizing conditional variance offers a reference-free way to minimize both transition and reconstruction errors. Based on these insights, we propose an entropy-aware variance optimized method for the generative process of DMs, called EVODiff, which systematically reduces uncertainty by optimizing conditional entropy during denoising. Extensive experiments on DMs validate our insights and demonstrate that our method significantly and consistently outperforms state-of-the-art (SOTA) gradient-based solvers. For example, compared to the DPM-Solver++, EVODiff reduces the reconstruction error by up to 45.5\\% (FID improves from 5.10 to 2.78) at 10 function evaluations (NFE) on CIFAR-10, cuts the NFE cost by 25\\% (from 20 to 15 NFE) for high-quality samples on ImageNet-256, and improves text-to-image generation while reducing artifacts. Code is available at https://github.com/ShiguiLi/EVODiff.",
        "arxiv_id": "2509.26096",
        "ARXIVID": "2509.26096",
        "COMMENT": "No specific criteria match.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.25771": {
        "authors": [
            "Jia Jun Cheng Xian",
            "Muchen Li",
            "Haotian Yang",
            "Xin Tao",
            "Pengfei Wan",
            "Leonid Sigal",
            "Renjie Liao"
        ],
        "title": "Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs",
        "abstract": "arXiv:2509.25771v1 Announce Type: new  Abstract: Recent advances in diffusion-based text-to-image (T2I) models have led to remarkable success in generating high-quality images from textual prompts. However, ensuring accurate alignment between the text and the generated image remains a significant challenge for state-of-the-art diffusion models. To address this, existing studies employ reinforcement learning with human feedback (RLHF) to align T2I outputs with human preferences. These methods, however, either rely directly on paired image preference data or require a learned reward function, both of which depend heavily on costly, high-quality human annotations and thus face scalability limitations. In this work, we introduce Text Preference Optimization (TPO), a framework that enables \"free-lunch\" alignment of T2I models, achieving alignment without the need for paired image preference data. TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using a large language model. Our framework is general and compatible with existing preference-based algorithms. We extend both DPO and KTO to our setting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations across multiple benchmarks show that our methods consistently outperform their original counterparts, delivering better human preference scores and improved text-to-image alignment. Our Open-source code is available at https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment.",
        "arxiv_id": "2509.25771",
        "ARXIVID": "2509.25771",
        "COMMENT": "No specific criteria match.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.25731": {
        "authors": [
            "Zhenghao Zhang",
            "Ziying Zhang",
            "Junchao Liao",
            "Xiangyu Meng",
            "Qiang Hu",
            "Siyu Zhu",
            "Xiaoyun Zhang",
            "Long Qin",
            "Weizhi Wang"
        ],
        "title": "LaTo: Landmark-tokenized Diffusion Transformer for Fine-grained Human Face Editing",
        "abstract": "arXiv:2509.25731v1 Announce Type: new  Abstract: Recent multimodal models for instruction-based face editing enable semantic manipulation but still struggle with precise attribute control and identity preservation. Structural facial representations such as landmarks are effective for intermediate supervision, yet most existing methods treat them as rigid geometric constraints, which can degrade identity when conditional landmarks deviate significantly from the source (e.g., large expression or pose changes, inaccurate landmark estimates). To address these limitations, we propose LaTo, a landmark-tokenized diffusion transformer for fine-grained, identity-preserving face editing. Our key innovations include: (1) a landmark tokenizer that directly quantizes raw landmark coordinates into discrete facial tokens, obviating the need for dense pixel-wise correspondence; (2) a location-mapping positional encoding that integrates facial and image tokens for unified processing, enabling flexible yet decoupled geometry-appearance interactions with high efficiency and strong identity preservation; and (3) a landmark predictor that leverages vision-language models to infer target landmarks from instructions and source images, whose structured chain-of-thought improves estimation accuracy and interactive control. To mitigate data scarcity, we curate HFL-150K, to our knowledge the largest benchmark for this task, containing over 150K real face pairs with fine-grained instructions. Extensive experiments show that LaTo outperforms state-of-the-art methods by 7.8% in identity preservation and 4.6% in semantic consistency. Code and dataset will be made publicly available upon acceptance.",
        "arxiv_id": "2509.25731",
        "ARXIVID": "2509.25731",
        "COMMENT": "No specific criteria match.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2509.26641": {
        "authors": [
            "Yuxin Song",
            "Wenkai Dong",
            "Shizun Wang",
            "Qi Zhang",
            "Song Xue",
            "Tao Yuan",
            "Hu Yang",
            "Haocheng Feng",
            "Hang Zhou",
            "Xinyan Xiao",
            "Jingdong Wang"
        ],
        "title": "Query-Kontext: An Unified Multimodal Model for Image Generation and Editing",
        "abstract": "arXiv:2509.26641v1 Announce Type: new  Abstract: Unified Multimodal Models (UMMs) have demonstrated remarkable performance in text-to-image generation (T2I) and editing (TI2I), whether instantiated as assembled unified frameworks which couple powerful vision-language model (VLM) with diffusion-based generator, or as naive Unified Multimodal Models with an early fusion of understanding and generation modalities. We contend that in current unified frameworks, the crucial capability of multimodal generative reasoning which encompasses instruction understanding, grounding, and image referring for identity preservation and faithful reconstruction, is intrinsically entangled with high-fidelity synthesis. In this work, we introduce Query-Kontext, a novel approach that bridges the VLM and diffusion model via a multimodal ``kontext'' composed of semantic cues and coarse-grained image conditions encoded from multimodal inputs. This design delegates the complex ability of multimodal generative reasoning to powerful VLM while reserving diffusion model's role for high-quality visual synthesis. To achieve this, we propose a three-stage progressive training strategy. First, we connect the VLM to a lightweight diffusion head via multimodal kontext tokens to unleash the VLM's generative reasoning ability. Second, we scale this head to a large, pre-trained diffusion model to enhance visual detail and realism. Finally, we introduce a low-level image encoder to improve image fidelity and perform instruction tuning on downstream tasks. Furthermore, we build a comprehensive data pipeline integrating real, synthetic, and open-source datasets, covering diverse multimodal reference-to-image scenarios, including image generation, instruction-driven editing, customized generation, and multi-subject composition. Experiments show that our approach matches strong unified baselines and even outperforms task-specific state-of-the-art methods in several cases.",
        "arxiv_id": "2509.26641",
        "ARXIVID": "2509.26641",
        "COMMENT": "No specific criteria match.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}