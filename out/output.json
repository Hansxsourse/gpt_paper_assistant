{
    "2602.09609": {
        "authors": [
            "Jialun Liu",
            "Yukuo Ma",
            "Xiao Cao",
            "Tian Li",
            "Gonghu Shang",
            "Haibin Huang",
            "Chi Zhang",
            "Xuelong Li",
            "Cong Liu",
            "Junqi Liu",
            "Jiakui Hu",
            "Robby T. Tan",
            "Shiwen Zhang",
            "Liying Yang",
            "Xiaoyan Yang",
            "Qizhen Weng",
            "Xiangzhen Chang",
            "Yuanzhi Liang",
            "Yifan Xu",
            "Zhiyong Huang",
            "Zuoxin Li",
            "Xuelong Li"
        ],
        "title": "Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing",
        "abstract": "arXiv:2602.09609v1 Announce Type: new  Abstract: Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks.",
        "arxiv_id": "2602.09609",
        "ARXIVID": "2602.09609",
        "COMMENT": "Matches criterion 2 closely as it proposes a unified multimodal framework for video generation and editing using diffusion-based generators, which aligns with the unified diffusion models for multiple vision tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2602.09268": {
        "authors": [
            "Nikita Starodubcev",
            "Daniil Pakhomov",
            "Zongze Wu",
            "Ilya Drobyshevskiy",
            "Yuchen Liu",
            "Zhonghao Wang",
            "Yuqian Zhou",
            "Zhe Lin",
            "Dmitry Baranchuk"
        ],
        "title": "Rethinking Global Text Conditioning in Diffusion Transformers",
        "abstract": "arXiv:2602.09268v1 Announce Type: new  Abstract: Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.",
        "arxiv_id": "2602.09268",
        "ARXIVID": "2602.09268",
        "COMMENT": "Does not match any specific criteria closely. The paper focuses on text conditioning in diffusion transformers, which is not directly related to the unified diffusion models for multiple vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    }
}