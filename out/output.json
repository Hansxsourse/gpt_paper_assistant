{
    "2505.23764": {
        "authors": [
            "Sihan Yang",
            "Runsen Xu",
            "Yiman Xie",
            "Sizhe Yang",
            "Mo Li",
            "Jingli Lin",
            "Chenming Zhu",
            "Xiaochen Chen",
            "Haodong Duan",
            "Xiangyu Yue",
            "Dahua Lin",
            "Tai Wang",
            "Jiangmiao Pang"
        ],
        "title": "MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence",
        "abstract": "arXiv:2505.23764v1 Announce Type: new  Abstract: Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiple-choice questions from over 120,000 images, each paired with carefully designed distractors and a step-by-step reasoning process. We conduct extensive experiments and thoroughly evaluate 34 open-source and proprietary MLLMs, observing a wide gap: the strongest open-source model attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while humans score 97%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering valuable insights for advancing multi-image spatial intelligence. Project page: https://runsenxu.com/projects/MMSI_Bench .",
        "arxiv_id": "2505.23764",
        "ARXIVID": "2505.23764",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for multi-image spatial intelligence with detailed error analysis.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2505.23359": {
        "authors": [
            "Yuanxin Liu",
            "Kun Ouyang",
            "Haoning Wu",
            "Yi Liu",
            "Lin Sui",
            "Xinhao Li",
            "Yan Zhong",
            "Y. Charles",
            "Xinyu Zhou",
            "Xu Sun"
        ],
        "title": "VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning?",
        "abstract": "arXiv:2505.23359v1 Announce Type: new  Abstract: Recent studies have shown that long chain-of-thought (CoT) reasoning can significantly enhance the performance of large language models (LLMs) on complex tasks. However, this benefit is yet to be demonstrated in the domain of video understanding, since most existing benchmarks lack the reasoning depth required to demonstrate the advantages of extended CoT chains. While recent efforts have proposed benchmarks aimed at video reasoning, the tasks are often knowledge-driven and do not rely heavily on visual content. To bridge this gap, we introduce VideoReasonBench, a benchmark designed to evaluate vision-centric, complex video reasoning. To ensure visual richness and high reasoning complexity, each video in VideoReasonBench depicts a sequence of fine-grained operations on a latent state that is only visible in part of the video. The questions evaluate three escalating levels of video reasoning skills: recalling observed visual information, inferring the content of latent states, and predicting information beyond the video. Under such task setting, models have to precisely recall multiple operations in the video, and perform step-by-step reasoning to get correct final answers for these questions. Using VideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal LLMs (MLLMs), finding that most perform poorly on complex video reasoning, e.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced Gemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our investigations on \"test-time scaling\" further reveal that extended thinking budget, while offering none or minimal benefits on existing video benchmarks, is essential for improving the performance on VideoReasonBench.",
        "arxiv_id": "2505.23359",
        "ARXIVID": "2505.23359",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for video reasoning in MLLMs, focusing on complex vision-centric tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2505.22914": {
        "authors": [
            "Maksim Kolodiazhnyi",
            "Denis Tarasov",
            "Dmitrii Zhemchuzhnikov",
            "Alexander Nikulin",
            "Ilya Zisman",
            "Anna Vorontsova",
            "Anton Konushin",
            "Vladislav Kurenkov",
            "Danila Rukhovich"
        ],
        "title": "cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning",
        "abstract": "arXiv:2505.22914v1 Announce Type: new  Abstract: Computer-Aided Design (CAD) plays a central role in engineering and manufacturing, making it possible to create precise and editable 3D models. Using a variety of sensor or user-provided data as inputs for CAD reconstruction can democratize access to design applications. However, existing methods typically focus on a single input modality, such as point clouds, images, or text, which limits their generalizability and robustness. Leveraging recent advances in vision-language models (VLM), we propose a multi-modal CAD reconstruction model that simultaneously processes all three input modalities. Inspired by large language model (LLM) training paradigms, we adopt a two-stage pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated data, followed by reinforcement learning (RL) fine-tuning using online feedback, obtained programatically. Furthermore, we are the first to explore RL fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such as Group Relative Preference Optimization (GRPO) outperform offline alternatives. In the DeepCAD benchmark, our SFT model outperforms existing single-modal approaches in all three input modalities simultaneously. More importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three challenging datasets, including a real-world one.",
        "arxiv_id": "2505.22914",
        "ARXIVID": "2505.22914",
        "COMMENT": "Matches criterion 2 as it proposes a multi-modal CAD reconstruction model leveraging vision-language models and introduces reinforcement learning fine-tuning for CAD tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2505.23757": {
        "authors": [
            "Haohan Chi",
            "Huan-ang Gao",
            "Ziming Liu",
            "Jianing Liu",
            "Chenyu Liu",
            "Jinwei Li",
            "Kaisen Yang",
            "Yangcheng Yu",
            "Zeda Wang",
            "Wenyi Li",
            "Leichen Wang",
            "Xingtao Hu",
            "Hao Sun",
            "Hang Zhao",
            "Hao Zhao"
        ],
        "title": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models",
        "abstract": "arXiv:2505.23757v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models for autonomous driving show promise but falter in unstructured corner case scenarios, largely due to a scarcity of targeted benchmarks. To address this, we introduce Impromptu VLA. Our core contribution is the Impromptu VLA Dataset: over 80,000 meticulously curated video clips, distilled from over 2M source clips sourced from 8 open-source large-scale datasets. This dataset is built upon our novel taxonomy of four challenging unstructured categories and features rich, planning-oriented question-answering annotations and action trajectories. Crucially, experiments demonstrate that VLAs trained with our dataset achieve substantial performance gains on established benchmarks--improving closed-loop NeuroNCAP scores and collision rates, and reaching near state-of-the-art L2 accuracy in open-loop nuScenes trajectory prediction. Furthermore, our Q&A suite serves as an effective diagnostic, revealing clear VLM improvements in perception, prediction, and planning. Our code, data and models are available at https://github.com/ahydchh/Impromptu-VLA.",
        "arxiv_id": "2505.23757",
        "ARXIVID": "2505.23757",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for Vision-Language-Action models in autonomous driving.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2505.23161": {
        "authors": [
            "Antonio D'Orazio",
            "Maria Rosaria Briglia",
            "Donato Crisostomi",
            "Dario Loi",
            "Emanuele Rodol\\`a",
            "Iacopo Masi"
        ],
        "title": "Implicit Inversion turns CLIP into a Decoder",
        "abstract": "arXiv:2505.23161v1 Announce Type: new  Abstract: CLIP is a discriminative model trained to align images and text in a shared embedding space. Due to its multimodal structure, it serves as the backbone of many generative pipelines, where a decoder is trained to map from the shared space back to images. In this work, we show that image synthesis is nevertheless possible using CLIP alone -- without any decoder, training, or fine-tuning. Our approach optimizes a frequency-aware implicit neural representation that encourages coarse-to-fine generation by stratifying frequencies across network layers. To stabilize this inverse mapping, we introduce adversarially robust initialization, a lightweight Orthogonal Procrustes projection to align local text and image embeddings, and a blending loss that anchors outputs to natural image statistics. Without altering CLIP's weights, this framework unlocks capabilities such as text-to-image generation, style transfer, and image reconstruction. These findings suggest that discriminative models may hold untapped generative potential, hidden in plain sight.",
        "arxiv_id": "2505.23161",
        "ARXIVID": "2505.23161",
        "COMMENT": "Matches criterion 4 as it explores the generative potential of CLIP, a vision foundation model, for tasks like text-to-image generation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2505.23558": {
        "authors": [
            "Xu Chu",
            "Xinrong Chen",
            "Guanyu Wang",
            "Zhijie Tan",
            "Kui Huang",
            "Wenyu Lv",
            "Tong Mo",
            "Weiping Li"
        ],
        "title": "Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information",
        "abstract": "arXiv:2505.23558v1 Announce Type: new  Abstract: Inference time scaling drives extended reasoning to enhance the performance of Vision-Language Models (VLMs), thus forming powerful Vision-Language Reasoning Models (VLRMs). However, long reasoning dilutes visual tokens, causing visual information to receive less attention and may trigger hallucinations. Although introducing text-only reflection processes shows promise in language models, we demonstrate that it is insufficient to suppress hallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain (Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a vision-text reflection process that guides the model to re-attention visual information during reasoning. We first propose a reinforcement learning method Balanced Reflective Policy Optimization (BRPO), which guides the model to decide when to generate vision-text reflection on its own and balance the number and length of reflections. Then, we formally prove that VLRMs lose attention to visual tokens as reasoning progresses, and demonstrate that supplementing visual information during reflection enhances visual attention. Therefore, during training and inference, Visual Token COPY and Visual Token ROUTE are introduced to force the model to re-attention visual information at the visual level, addressing the limitations of text-only reflection. Experiments on multiple visual QA datasets and hallucination metrics indicate that Qwen-LA achieves leading accuracy performance while reducing hallucinations. Our code is available at: https://github.com/Liar406/Look_Again.",
        "arxiv_id": "2505.23558",
        "ARXIVID": "2505.23558",
        "COMMENT": "Matches criterion 2 as it introduces a vision-language reasoning model (Qwen-LA) with novel techniques to mitigate hallucinations and improve visual attention.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2505.23596": {
        "authors": [
            "Linqiang Guo (Peter)",
            "Wei Liu (Peter)",
            "Yi Wen Heng (Peter)",
            "Tse-Hsun (Peter)",
            "Chen",
            "Yang Wang"
        ],
        "title": "MAPLE: A Mobile Assistant with Persistent Finite State Machines for Recovery Reasoning",
        "abstract": "arXiv:2505.23596v1 Announce Type: new  Abstract: Mobile GUI agents aim to autonomously complete user-instructed tasks across mobile apps. Recent advances in Multimodal Large Language Models (MLLMs) enable these agents to interpret UI screens, identify actionable elements, and perform interactions such as tapping or typing. However, existing agents remain reactive: they reason only over the current screen and lack a structured model of app navigation flow, limiting their ability to understand context, detect unexpected outcomes, and recover from errors. We present MAPLE, a state-aware multi-agent framework that abstracts app interactions as a Finite State Machine (FSM). We computationally model each UI screen as a discrete state and user actions as transitions, allowing the FSM to provide a structured representation of the app execution. MAPLE consists of specialized agents responsible for four phases of task execution: planning, execution, verification, error recovery, and knowledge retention. These agents collaborate to dynamically construct FSMs in real time based on perception data extracted from the UI screen, allowing the GUI agents to track navigation progress and flow, validate action outcomes through pre- and post-conditions of the states, and recover from errors by rolling back to previously stable states. Our evaluation results on two challenging cross-app benchmarks, Mobile-Eval-E and SPA-Bench, show that MAPLE outperforms the state-of-the-art baseline, improving task success rate by up to 12%, recovery success by 13.8%, and action accuracy by 6.5%. Our results highlight the importance of structured state modeling in guiding mobile GUI agents during task execution. Moreover, our FSM representation can be integrated into future GUI agent architectures as a lightweight, model-agnostic memory layer to support structured planning, execution verification, and error recovery.",
        "arxiv_id": "2505.23596",
        "ARXIVID": "2505.23596",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework (MAPLE) for mobile GUI agents with structured state modeling, which is a new method for embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.23043": {
        "authors": [
            "Jihai Zhang",
            "Tianle Li",
            "Linjie Li",
            "Zhengyuan Yang",
            "Yu Cheng"
        ],
        "title": "Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation",
        "abstract": "arXiv:2505.23043v1 Announce Type: new  Abstract: Recent advancements in unified vision-language models (VLMs), which integrate both visual understanding and generation capabilities, have attracted significant attention. The underlying hypothesis is that a unified architecture with mixed training on both understanding and generation tasks can enable mutual enhancement between understanding and generation. However, this hypothesis remains underexplored in prior works on unified VLMs. To address this gap, this paper systematically investigates the generalization across understanding and generation tasks in unified VLMs. Specifically, we design a dataset closely aligned with real-world scenarios to facilitate extensive experiments and quantitative evaluations. We evaluate multiple unified VLM architectures to validate our findings. Our key findings are as follows. First, unified VLMs trained with mixed data exhibit mutual benefits in understanding and generation tasks across various architectures, and this mutual benefits can scale up with increased data. Second, better alignment between multimodal input and output spaces will lead to better generalization. Third, the knowledge acquired during generation tasks can transfer to understanding tasks, and this cross-task generalization occurs within the base language model, beyond modality adapters. Our findings underscore the critical necessity of unifying understanding and generation in VLMs, offering valuable insights for the design and optimization of unified VLMs.",
        "arxiv_id": "2505.23043",
        "ARXIVID": "2505.23043",
        "COMMENT": "Matches criterion 2 as it discusses unified vision-language models (VLLMs) and their generalization across understanding and generation tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.23012": {
        "authors": [
            "Shanaka Ramesh Gunasekara",
            "Wanqing Li",
            "Philip Ogunbona",
            "Jack Yang"
        ],
        "title": "Spatio-Temporal Joint Density Driven Learning for Skeleton-Based Action Recognition",
        "abstract": "arXiv:2505.23012v1 Announce Type: new  Abstract: Traditional approaches in unsupervised or self supervised learning for skeleton-based action classification have concentrated predominantly on the dynamic aspects of skeletal sequences. Yet, the intricate interaction between the moving and static elements of the skeleton presents a rarely tapped discriminative potential for action classification. This paper introduces a novel measurement, referred to as spatial-temporal joint density (STJD), to quantify such interaction. Tracking the evolution of this density throughout an action can effectively identify a subset of discriminative moving and/or static joints termed \"prime joints\" to steer self-supervised learning. A new contrastive learning strategy named STJD-CL is proposed to align the representation of a skeleton sequence with that of its prime joints while simultaneously contrasting the representations of prime and nonprime joints. In addition, a method called STJD-MP is developed by integrating it with a reconstruction-based framework for more effective learning. Experimental evaluations on the NTU RGB+D 60, NTU RGB+D 120, and PKUMMD datasets in various downstream tasks demonstrate that the proposed STJD-CL and STJD-MP improved performance, particularly by 3.5 and 3.6 percentage points over the state-of-the-art contrastive methods on the NTU RGB+D 120 dataset using X-sub and X-set evaluations, respectively.",
        "arxiv_id": "2505.23012",
        "ARXIVID": "2505.23012",
        "COMMENT": "Matches criterion 1 as it introduces a novel spatial-temporal joint density measurement for skeleton-based action recognition, which relates to spatial intelligence.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.23716": {
        "authors": [
            "Lihan Jiang",
            "Yucheng Mao",
            "Linning Xu",
            "Tao Lu",
            "Kerui Ren",
            "Yichen Jin",
            "Xudong Xu",
            "Mulin Yu",
            "Jiangmiao Pang",
            "Feng Zhao",
            "Dahua Lin",
            "Bo Dai"
        ],
        "title": "AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views",
        "abstract": "arXiv:2505.23716v1 Announce Type: new  Abstract: We introduce AnySplat, a feed forward network for novel view synthesis from uncalibrated image collections. In contrast to traditional neural rendering pipelines that demand known camera poses and per scene optimization, or recent feed forward methods that buckle under the computational weight of dense views, our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi view datasets without any pose annotations. In extensive zero shot evaluations, AnySplat matches the quality of pose aware baselines in both sparse and dense view scenarios while surpassing existing pose free approaches. Moreover, it greatly reduce rendering latency compared to optimization based neural fields, bringing real time novel view synthesis within reach for unconstrained capture settings.Project page: https://city-super.github.io/anysplat/",
        "arxiv_id": "2505.23716",
        "ARXIVID": "2505.23716",
        "COMMENT": "Matches criterion 1 as it introduces a novel feed-forward method for spatial understanding in 3D Gaussian splatting and novel view synthesis.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.23518": {
        "authors": [
            "Hangoo Kang",
            "Jehyeok Yeon",
            "Gagandeep Singh"
        ],
        "title": "TRAP: Targeted Redirecting of Agentic Preferences",
        "abstract": "arXiv:2505.23518v1 Announce Type: new  Abstract: Autonomous agentic AI systems powered by vision-language models (VLMs) are rapidly advancing toward real-world deployment, yet their cross-modal reasoning capabilities introduce new attack surfaces for adversarial manipulation that exploit semantic reasoning across modalities. Existing adversarial attacks typically rely on visible pixel perturbations or require privileged model or environment access, making them impractical for stealthy, real-world exploitation. We introduce TRAP, a generative adversarial framework that manipulates the agent's decision-making using diffusion-based semantic injections. Our method combines negative prompt-based degradation with positive semantic optimization, guided by a Siamese semantic network and layout-aware spatial masking. Without requiring access to model internals, TRAP produces visually natural images yet induces consistent selection biases in agentic AI systems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO) dataset, building multi-candidate decision scenarios. Across these scenarios, TRAP achieves a 100% attack success rate on leading models, including LLaVA-34B, Gemma3, and Mistral-3.1, significantly outperforming baselines such as SPSA, Bandit, and standard diffusion approaches. These results expose a critical vulnerability: Autonomous agents can be consistently misled through human-imperceptible cross-modal manipulations. These findings highlight the need for defense strategies beyond pixel-level robustness to address semantic vulnerabilities in cross-modal decision-making.",
        "arxiv_id": "2505.23518",
        "ARXIVID": "2505.23518",
        "COMMENT": "Matches criterion 1 as it discusses spatial intelligence and cross-modal reasoning vulnerabilities in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.23566": {
        "authors": [
            "Yu Li",
            "Jin Jiang",
            "Jianhua Zhu",
            "Shuai Peng",
            "Baole Wei",
            "Yuxuan Zhou",
            "Liangcai Gao"
        ],
        "title": "Uni-MuMER: Unified Multi-Task Fine-Tuning of Vision-Language Model for Handwritten Mathematical Expression Recognition",
        "abstract": "arXiv:2505.23566v1 Announce Type: new  Abstract: Handwritten Mathematical Expression Recognition (HMER) remains a persistent challenge in Optical Character Recognition (OCR) due to the inherent freedom of symbol layout and variability in handwriting styles. Prior methods have faced performance bottlenecks, proposing isolated architectural modifications that are difficult to integrate coherently into a unified framework. Meanwhile, recent advances in pretrained vision-language models (VLMs) have demonstrated strong cross-task generalization, offering a promising foundation for developing unified solutions. In this paper, we introduce Uni-MuMER, which fully fine-tunes a VLM for the HMER task without modifying its architecture, effectively injecting domain-specific knowledge into a generalist framework. Our method integrates three data-driven tasks: Tree-Aware Chain-of-Thought (Tree-CoT) for structured spatial reasoning, Error-Driven Learning (EDL) for reducing confusion among visually similar characters, and Symbol Counting (SC) for improving recognition consistency in long expressions. Experiments on the CROHME and HME100K datasets show that Uni-MuMER achieves new state-of-the-art performance, surpassing the best lightweight specialized model SSAN by 16.31% and the top-performing VLM Gemini2.5-flash by 24.42% in the zero-shot setting. Our datasets, models, and code are open-sourced at: https://github.com/BFlameSwift/Uni-MuMER",
        "arxiv_id": "2505.23566",
        "ARXIVID": "2505.23566",
        "COMMENT": "Matches criterion 2 as it fine-tunes a vision-language model for handwritten mathematical expression recognition, integrating novel tasks like Tree-CoT and Error-Driven Learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.22976": {
        "authors": [
            "Kewei Lian",
            "Shaofei Cai",
            "Yilun Du",
            "Yitao Liang"
        ],
        "title": "Toward Memory-Aided World Models: Benchmarking via Spatial Consistency",
        "abstract": "arXiv:2505.22976v1 Announce Type: new  Abstract: The ability to simulate the world in a spatially consistent manner is a crucial requirements for effective world models. Such a model enables high-quality visual generation, and also ensures the reliability of world models for downstream tasks such as simulation and planning. Designing a memory module is a crucial component for addressing spatial consistency: such a model must not only retain long-horizon observational information, but also enables the construction of explicit or implicit internal spatial representations. However, there are no dataset designed to promote the development of memory modules by explicitly enforcing spatial consistency constraints. Furthermore, most existing benchmarks primarily emphasize visual coherence or generation quality, neglecting the requirement of long-range spatial consistency. To bridge this gap, we construct a dataset and corresponding benchmark by sampling 150 distinct locations within the open-world environment of Minecraft, collecting about 250 hours (20 million frames) of loop-based navigation videos with actions. Our dataset follows a curriculum design of sequence lengths, allowing models to learn spatial consistency on increasingly complex navigation trajectories. Furthermore, our data collection pipeline is easily extensible to new Minecraft environments and modules. Four representative world model baselines are evaluated on our benchmark. Dataset, benchmark, and code are open-sourced to support future research.",
        "arxiv_id": "2505.22976",
        "ARXIVID": "2505.22976",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for spatial consistency in world models, focusing on memory modules and long-range spatial reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.22810": {
        "authors": [
            "Zhoufaran Yang",
            "Yan Shu",
            "Zhifei Yang",
            "Yan Zhang",
            "Yu Li",
            "Keyang Lu",
            "Gangyan Zeng",
            "Shaohui Liu",
            "Yu Zhou",
            "Nicu Sebe"
        ],
        "title": "VidText: Towards Comprehensive Evaluation for Video Text Understanding",
        "abstract": "arXiv:2505.22810v1 Announce Type: new  Abstract: Visual texts embedded in videos carry rich semantic information, which is crucial for both holistic video understanding and fine-grained reasoning about local human actions. However, existing video understanding benchmarks largely overlook textual information, while OCR-specific benchmarks are constrained to static images, limiting their ability to capture the interaction between text and dynamic visual contexts. To address this gap, we propose VidText, a new benchmark designed for comprehensive and in-depth evaluation of video text understanding. VidText offers the following key features: 1) It covers a wide range of real-world scenarios and supports multilingual content, encompassing diverse settings where video text naturally appears. 2) It introduces a hierarchical evaluation framework with video-level, clip-level, and instance-level tasks, enabling assessment of both global summarization and local retrieval capabilities. 3) The benchmark also introduces a set of paired perception reasoning tasks, ranging from visual text perception to cross-modal reasoning between textual and visual information. Extensive experiments on 18 state-of-the-art Large Multimodal Models (LMMs) reveal that current models struggle across most tasks, with significant room for improvement. Further analysis highlights the impact of both model-intrinsic factors, such as input resolution and OCR capability, and external factors, including the use of auxiliary information and Chain-of-Thought reasoning strategies. We hope VidText will fill the current gap in video understanding benchmarks and serve as a foundation for future research on multimodal reasoning with video text in dynamic environments.",
        "arxiv_id": "2505.22810",
        "ARXIVID": "2505.22810",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for video text understanding, addressing gaps in existing benchmarks and focusing on multimodal reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.23522": {
        "authors": [
            "Fengxiang Wang",
            "Mingshuo Chen",
            "Xuming He",
            "YiFan Zhang",
            "Feng Liu",
            "Zijie Guo",
            "Zhenghao Hu",
            "Jiong Wang",
            "Jingyi Xu",
            "Zhangrui Li",
            "Fenghua Ling",
            "Ben Fei",
            "Weijia Li",
            "Long Lan",
            "Wenjing Yang",
            "Wenlong Zhang",
            "Lei Bai"
        ],
        "title": "OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data",
        "abstract": "arXiv:2505.23522v1 Announce Type: new  Abstract: Existing benchmarks for Earth science multimodal learning exhibit critical limitations in systematic coverage of geosystem components and cross-sphere interactions, often constrained to isolated subsystems (only in Human-activities sphere or atmosphere) with limited evaluation dimensions (less than 16 tasks). To address these gaps, we introduce OmniEarth-Bench, the first comprehensive multimodal benchmark spanning all six Earth science spheres (atmosphere, lithosphere, Oceansphere, cryosphere, biosphere and Human-activities sphere) and cross-spheres with one hundred expert-curated evaluation dimensions. Leveraging observational data from satellite sensors and in-situ measurements, OmniEarth-Bench integrates 29,779 annotations across four tiers: perception, general reasoning, scientific knowledge reasoning and chain-of-thought (CoT) reasoning. This involves the efforts of 2-5 experts per sphere to establish authoritative evaluation dimensions and curate relevant observational datasets, 40 crowd-sourcing annotators to assist experts for annotations, and finally, OmniEarth-Bench is validated via hybrid expert-crowd workflows to reduce label ambiguity. Experiments on 9 state-of-the-art MLLMs reveal that even the most advanced models struggle with our benchmarks, where none of them reach 35\\% accuracy. Especially, in some cross-spheres tasks, the performance of leading models like GPT-4o drops to 0.0\\%. OmniEarth-Bench sets a new standard for geosystem-aware AI, advancing both scientific discovery and practical applications in environmental monitoring and disaster prediction. The dataset, source code, and trained models were released.",
        "arxiv_id": "2505.23522",
        "ARXIVID": "2505.23522",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for multimodal Earth science data, focusing on cross-sphere interactions and tasks that previous work ignored.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.23129": {
        "authors": [
            "Bin Wang",
            "Pingjun Li",
            "Jinkun Liu",
            "Jun Cheng",
            "Hailong Lei",
            "Yinze Rong",
            "Huan-ang Gao",
            "Kangliang Chen",
            "Xing Pan",
            "Weihao Gu"
        ],
        "title": "HMAD: Advancing E2E Driving with Anchored Offset Proposals and Simulation-Supervised Multi-target Scoring",
        "abstract": "arXiv:2505.23129v1 Announce Type: new  Abstract: End-to-end autonomous driving faces persistent challenges in both generating diverse, rule-compliant trajectories and robustly selecting the optimal path from these options via learned, multi-faceted evaluation. To address these challenges, we introduce HMAD, a framework integrating a distinctive Bird's-Eye-View (BEV) based trajectory proposal mechanism with learned multi-criteria scoring. HMAD leverages BEVFormer and employs learnable anchored queries, initialized from a trajectory dictionary and refined via iterative offset decoding (inspired by DiffusionDrive), to produce numerous diverse and stable candidate trajectories. A key innovation, our simulation-supervised scorer module, then evaluates these proposals against critical metrics including no at-fault collisions, drivable area compliance, comfortableness, and overall driving quality (i.e., extended PDM score). Demonstrating its efficacy, HMAD achieves a 44.5% driving score on the CVPR 2025 private test set. This work highlights the benefits of effectively decoupling robust trajectory generation from comprehensive, safety-aware learned scoring for advanced autonomous driving.",
        "arxiv_id": "2505.23129",
        "ARXIVID": "2505.23129",
        "COMMENT": "Matches criterion 3 as it introduces a new method for end-to-end autonomous driving with a novel simulation-supervised scoring mechanism.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2505.23272": {
        "authors": [
            "Yazhou Zhang",
            "Chunwang Zou",
            "Qimeng Liu",
            "Lu Rong",
            "Ben Yao",
            "Zheng Lian",
            "Qiuchi Li",
            "Peng Zhang",
            "Jing Qin"
        ],
        "title": "Are MLMs Trapped in the Visual Room?",
        "abstract": "arXiv:2505.23272v1 Announce Type: new  Abstract: Can multi-modal large models (MLMs) that can ``see'' an image be said to ``understand'' it? Drawing inspiration from Searle's Chinese Room, we propose the \\textbf{Visual Room} argument: a system may process and describe every detail of visual inputs by following algorithmic rules, without genuinely comprehending the underlying intention. This dilemma challenges the prevailing assumption that perceptual mastery implies genuine understanding. In implementation, we introduce a two-tier evaluation framework spanning perception and cognition. The perception component evaluates whether MLMs can accurately capture the surface-level details of visual contents, where the cognitive component examines their ability to infer sarcasm polarity. To support this framework, We further introduce a high-quality multi-modal sarcasm dataset comprising both 924 static images and 100 dynamic videos. All sarcasm labels are annotated by the original authors and verified by independent reviewers to ensure clarity and consistency. We evaluate eight state-of-the-art (SoTA) MLMs. Our results highlight three key findings: (1) MLMs perform well on perception tasks; (2) even with correct perception, models exhibit an average error rate of ~16.1\\% in sarcasm understanding, revealing a significant gap between seeing and understanding; (3) error analysis attributes this gap to deficiencies in emotional reasoning, commonsense inference, and context alignment. This work provides empirical grounding for the proposed Visual Room argument and offers a new evaluation paradigm for MLMs.",
        "arxiv_id": "2505.23272",
        "ARXIVID": "2505.23272",
        "COMMENT": "Matches criterion 2 as it evaluates multi-modal large language models (MLLMs) and introduces a new evaluation framework.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2505.23694": {
        "authors": [
            "Li Ren",
            "Chen Chen",
            "Liqiang Wang",
            "Kien Hua"
        ],
        "title": "DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers",
        "abstract": "arXiv:2505.23694v1 Announce Type: new  Abstract: Visual Prompt Tuning (VPT) has become a promising solution for Parameter-Efficient Fine-Tuning (PEFT) approach for Vision Transformer (ViT) models by partially fine-tuning learnable tokens while keeping most model parameters frozen. Recent research has explored modifying the connection structures of the prompts. However, the fundamental correlation and distribution between the prompts and image tokens remain unexplored. In this paper, we leverage metric learning techniques to investigate how the distribution of prompts affects fine-tuning performance. Specifically, we propose a novel framework, Distribution Aware Visual Prompt Tuning (DA-VPT), to guide the distributions of the prompts by learning the distance metric from their class-related semantic data. Our method demonstrates that the prompts can serve as an effective bridge to share semantic information between image patches and the class token. We extensively evaluated our approach on popular benchmarks in both recognition and segmentation tasks. The results demonstrate that our approach enables more effective and efficient fine-tuning of ViT models by leveraging semantic information to guide the learning of the prompts, leading to improved performance on various downstream vision tasks.",
        "arxiv_id": "2505.23694",
        "ARXIVID": "2505.23694",
        "COMMENT": "Matches criterion 4 as it focuses on improving Vision Transformers (ViTs) using semantic-guided visual prompt tuning, which is related to vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2505.22705": {
        "authors": [
            "Qi Cai",
            "Jingwen Chen",
            "Yang Chen",
            "Yehao Li",
            "Fuchen Long",
            "Yingwei Pan",
            "Zhaofan Qiu",
            "Yiheng Zhang",
            "Fengbin Gao",
            "Peihan Xu",
            "Yimeng Wang",
            "Kai Yu",
            "Wenxuan Chen",
            "Ziwei Feng",
            "Zijian Gong",
            "Jianzhuang Pan",
            "Yi Peng",
            "Rui Tian",
            "Siyu Wang",
            "Bo Zhao",
            "Ting Yao",
            "Tao Mei"
        ],
        "title": "HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer",
        "abstract": "arXiv:2505.22705v1 Announce Type: new  Abstract: Recent advancements in image generative foundation models have prioritized quality improvements but often at the cost of increased computational complexity and inference latency. To address this critical trade-off, we introduce HiDream-I1, a new open-source image generative foundation model with 17B parameters that achieves state-of-the-art image generation quality within seconds. HiDream-I1 is constructed with a new sparse Diffusion Transformer (DiT) structure. Specifically, it starts with a dual-stream decoupled design of sparse DiT with dynamic Mixture-of-Experts (MoE) architecture, in which two separate encoders are first involved to independently process image and text tokens. Then, a single-stream sparse DiT structure with dynamic MoE architecture is adopted to trigger multi-model interaction for image generation in a cost-efficient manner. To support flexiable accessibility with varied model capabilities, we provide HiDream-I1 in three variants: HiDream-I1-Full, HiDream-I1-Dev, and HiDream-I1-Fast.   Furthermore, we go beyond the typical text-to-image generation and remould HiDream-I1 with additional image conditions to perform precise, instruction-based editing on given images, yielding a new instruction-based image editing model namely HiDream-E1. Ultimately, by integrating text-to-image generation and instruction-based image editing, HiDream-I1 evolves to form a comprehensive image agent (HiDream-A1) capable of fully interactive image creation and refinement. To accelerate multi-modal AIGC research, we have open-sourced all the codes and model weights of HiDream-I1-Full, HiDream-I1-Dev, HiDream-I1-Fast, HiDream-E1 through our project websites: https://github.com/HiDream-ai/HiDream-I1 and https://github.com/HiDream-ai/HiDream-E1. All features can be directly experienced via https://vivago.ai/studio.",
        "arxiv_id": "2505.22705",
        "ARXIVID": "2505.22705",
        "COMMENT": "Matches criterion 4 as it introduces a new image generative foundation model with novel sparse Diffusion Transformer architecture.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2505.23438": {
        "authors": [
            "Lingyan Ran",
            "Yali Li",
            "Tao Zhuo",
            "Shizhou Zhang",
            "Yanning Zhang"
        ],
        "title": "Adaptive Spatial Augmentation for Semi-supervised Semantic Segmentation",
        "abstract": "arXiv:2505.23438v1 Announce Type: new  Abstract: In semi-supervised semantic segmentation (SSSS), data augmentation plays a crucial role in the weak-to-strong consistency regularization framework, as it enhances diversity and improves model generalization. Recent strong augmentation methods have primarily focused on intensity-based perturbations, which have minimal impact on the semantic masks. In contrast, spatial augmentations like translation and rotation have long been acknowledged for their effectiveness in supervised semantic segmentation tasks, but they are often ignored in SSSS. In this work, we demonstrate that spatial augmentation can also contribute to model training in SSSS, despite generating inconsistent masks between the weak and strong augmentations. Furthermore, recognizing the variability among images, we propose an adaptive augmentation strategy that dynamically adjusts the augmentation for each instance based on entropy. Extensive experiments show that our proposed Adaptive Spatial Augmentation (\\textbf{ASAug}) can be integrated as a pluggable module, consistently improving the performance of existing methods and achieving state-of-the-art results on benchmark datasets such as PASCAL VOC 2012, Cityscapes, and COCO.",
        "arxiv_id": "2505.23438",
        "ARXIVID": "2505.23438",
        "COMMENT": "Matches criterion 1 as it proposes a novel adaptive spatial augmentation strategy for semi-supervised semantic segmentation, which relates to spatial understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.23474": {
        "authors": [
            "Xiang Li",
            "Haiyang Yu",
            "Xinghua Zhang",
            "Ziyang Huang",
            "Shizhu He",
            "Kang Liu",
            "Jun Zhao",
            "Fei Huang",
            "Yongbin Li"
        ],
        "title": "Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns",
        "abstract": "arXiv:2505.23474v1 Announce Type: new  Abstract: Process Reward Models (PRMs) are crucial in complex reasoning and problem-solving tasks (e.g., LLM agents with long-horizon decision-making) by verifying the correctness of each intermediate reasoning step. In real-world scenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to solve a problem, potentially suffering from errors under various reasoning patterns. Therefore, PRMs are required to identify errors under various reasoning patterns during the reasoning process. However, existing benchmarks mainly focus on evaluating PRMs with stepwise correctness, ignoring a systematic evaluation of PRMs under various reasoning patterns. To mitigate this gap, we introduce Socratic-PRMBench, a new benchmark to evaluate PRMs systematically under six reasoning patterns, including Transformation, Decomposition, Regather, Deduction, Verification, and Integration. Socratic-PRMBench}comprises 2995 reasoning paths with flaws within the aforementioned six reasoning patterns. Through our experiments on both PRMs and LLMs prompted as critic models, we identify notable deficiencies in existing PRMs. These observations underscore the significant weakness of current PRMs in conducting evaluations on reasoning steps under various reasoning patterns. We hope Socratic-PRMBench can serve as a comprehensive testbed for systematic evaluation of PRMs under diverse reasoning patterns and pave the way for future development of PRMs.",
        "arxiv_id": "2505.23474",
        "ARXIVID": "2505.23474",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (Socratic-PRMBench) for evaluating Process Reward Models under diverse reasoning patterns, which is relevant to embodied AI benchmarks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.23130": {
        "authors": [
            "Haoyu Chen",
            "Keda Tao",
            "Yizao Wang",
            "Xinlei Wang",
            "Lei Zhu",
            "Jinjin Gu"
        ],
        "title": "PhotoArtAgent: Intelligent Photo Retouching with Language Model-Based Artist Agents",
        "abstract": "arXiv:2505.23130v1 Announce Type: new  Abstract: Photo retouching is integral to photographic art, extending far beyond simple technical fixes to heighten emotional expression and narrative depth. While artists leverage expertise to create unique visual effects through deliberate adjustments, non-professional users often rely on automated tools that produce visually pleasing results but lack interpretative depth and interactive transparency. In this paper, we introduce PhotoArtAgent, an intelligent system that combines Vision-Language Models (VLMs) with advanced natural language reasoning to emulate the creative process of a professional artist. The agent performs explicit artistic analysis, plans retouching strategies, and outputs precise parameters to Lightroom through an API. It then evaluates the resulting images and iteratively refines them until the desired artistic vision is achieved. Throughout this process, PhotoArtAgent provides transparent, text-based explanations of its creative rationale, fostering meaningful interaction and user control. Experimental results show that PhotoArtAgent not only surpasses existing automated tools in user studies but also achieves results comparable to those of professional human artists.",
        "arxiv_id": "2505.23130",
        "ARXIVID": "2505.23130",
        "COMMENT": "Matches criterion 2 as it combines vision-language models with natural language reasoning for intelligent photo retouching, showcasing a novel application of VLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.23153": {
        "authors": [
            "Fan Wang",
            "Shaoshan Liu"
        ],
        "title": "Conceptual Framework Toward Embodied Collective Adaptive Intelligence",
        "abstract": "arXiv:2505.23153v1 Announce Type: new  Abstract: Collective Adaptive Intelligence (CAI) represent a transformative approach in artificial intelligence, wherein numerous autonomous agents collaborate, adapt, and self-organize to navigate complex, dynamic environments. This paradigm is particularly impactful in embodied AI applications, where adaptability and resilience are paramount. By enabling systems to reconfigure themselves in response to unforeseen challenges, CAI facilitate robust performance in real-world scenarios. This article introduces a conceptual framework for designing and analyzing CAI. It delineates key attributes including task generalization, resilience, scalability, and self-assembly, aiming to bridge theoretical foundations with practical methodologies for engineering adaptive, emergent intelligence. By providing a structured foundation for understanding and implementing CAI, this work seeks to guide researchers and practitioners in developing more resilient, scalable, and adaptable AI systems across various domains.",
        "arxiv_id": "2505.23153",
        "ARXIVID": "2505.23153",
        "COMMENT": "Matches criterion 3 as it introduces a conceptual framework for embodied collective adaptive intelligence, focusing on novel aspects like scalability and self-assembly.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.23590": {
        "authors": [
            "Zifu Wang",
            "Junyi Zhu",
            "Bo Tang",
            "Zhiyu Li",
            "Feiyu Xiong",
            "Jiaqian Yu",
            "Matthew B. Blaschko"
        ],
        "title": "Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles",
        "abstract": "arXiv:2505.23590v1 Announce Type: new  Abstract: The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL using jigsaw puzzles as a structured experimental framework, revealing several key findings. \\textit{Firstly,} we find that MLLMs, initially performing near to random guessing on simple puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. \\textit{Secondly,} training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. \\textit{Thirdly,} MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. \\textit{Fourthly,} we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. \\textit{Finally,} our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: \\href{https://github.com/zifuwanggg/Jigsaw-R1}{https://github.com/zifuwanggg/Jigsaw-R1}.",
        "arxiv_id": "2505.23590",
        "ARXIVID": "2505.23590",
        "COMMENT": "Matches criterion 3 as it studies rule-based visual reinforcement learning with a novel experimental framework.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2505.23292": {
        "authors": [
            "Evangelos Charalampakis",
            "Vasileios Mygdalis",
            "Ioannis Pitas"
        ],
        "title": "Federated Unsupervised Semantic Segmentation",
        "abstract": "arXiv:2505.23292v1 Announce Type: new  Abstract: This work explores the application of Federated Learning (FL) in Unsupervised Semantic image Segmentation (USS). Recent USS methods extract pixel-level features using frozen visual foundation models and refine them through self-supervised objectives that encourage semantic grouping. These features are then grouped to semantic clusters to produce segmentation masks. Extending these ideas to federated settings requires feature representation and cluster centroid alignment across distributed clients -- an inherently difficult task under heterogeneous data distributions in the absence of supervision. To address this, we propose FUSS Federated Unsupervised image Semantic Segmentation) which is, to our knowledge, the first framework to enable fully decentralized, label-free semantic segmentation training. FUSS introduces novel federation strategies that promote global consistency in feature and prototype space, jointly optimizing local segmentation heads and shared semantic centroids. Experiments on both benchmark and real-world datasets, including binary and multi-class segmentation tasks, show that FUSS consistently outperforms local-only client trainings as well as extensions of classical FL algorithms under varying client data distributions. To support reproducibility, full code will be released upon manuscript acceptance.",
        "arxiv_id": "2505.23292",
        "ARXIVID": "2505.23292",
        "COMMENT": "Matches criterion 4 as it explores the use of visual foundation models for unsupervised semantic segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.23253": {
        "authors": [
            "Yixun Liang",
            "Kunming Luo",
            "Xiao Chen",
            "Rui Chen",
            "Hongyu Yan",
            "Weiyu Li",
            "Jiarui Liu",
            "Ping Tan"
        ],
        "title": "UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes",
        "abstract": "arXiv:2505.23253v1 Announce Type: new  Abstract: We present UniTEX, a novel two-stage 3D texture generation framework to create high-quality, consistent textures for 3D assets. Existing approaches predominantly rely on UV-based inpainting to refine textures after reprojecting the generated multi-view images onto the 3D shapes, which introduces challenges related to topological ambiguity. To address this, we propose to bypass the limitations of UV mapping by operating directly in a unified 3D functional space. Specifically, we first propose that lifts texture generation into 3D space via Texture Functions (TFs)--a continuous, volumetric representation that maps any 3D point to a texture value based solely on surface proximity, independent of mesh topology. Then, we propose to predict these TFs directly from images and geometry inputs using a transformer-based Large Texturing Model (LTM). To further enhance texture quality and leverage powerful 2D priors, we develop an advanced LoRA-based strategy for efficiently adapting large-scale Diffusion Transformers (DiTs) for high-quality multi-view texture synthesis as our first stage. Extensive experiments demonstrate that UniTEX achieves superior visual quality and texture integrity compared to existing approaches, offering a generalizable and scalable solution for automated 3D texture generation. Code will available in: https://github.com/YixunLiang/UniTEX.",
        "arxiv_id": "2505.23253",
        "ARXIVID": "2505.23253",
        "COMMENT": "Matches criterion 4 as it focuses on a novel framework for 3D texture generation, which aligns with vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.23434": {
        "authors": [
            "Tianhang Wang",
            "Fan Lu",
            "Sanqing Qu",
            "Guo Yu",
            "Shihang Du",
            "Ya Wu",
            "Yuan Huang",
            "Guang Chen"
        ],
        "title": "UrbanCraft: Urban View Extrapolation via Hierarchical Sem-Geometric Priors",
        "abstract": "arXiv:2505.23434v1 Announce Type: new  Abstract: Existing neural rendering-based urban scene reconstruction methods mainly focus on the Interpolated View Synthesis (IVS) setting that synthesizes from views close to training camera trajectory. However, IVS can not guarantee the on-par performance of the novel view outside the training camera distribution (\\textit{e.g.}, looking left, right, or downwards), which limits the generalizability of the urban reconstruction application. Previous methods have optimized it via image diffusion, but they fail to handle text-ambiguous or large unseen view angles due to coarse-grained control of text-only diffusion. In this paper, we design UrbanCraft, which surmounts the Extrapolated View Synthesis (EVS) problem using hierarchical sem-geometric representations serving as additional priors. Specifically, we leverage the partially observable scene to reconstruct coarse semantic and geometric primitives, establishing a coarse scene-level prior through an occupancy grid as the base representation. Additionally, we incorporate fine instance-level priors from 3D bounding boxes to enhance object-level details and spatial relationships. Building on this, we propose the \\textbf{H}ierarchical \\textbf{S}emantic-Geometric-\\textbf{G}uided Variational Score Distillation (HSG-VSD), which integrates semantic and geometric constraints from pretrained UrbanCraft2D into the score distillation sampling process, forcing the distribution to be consistent with the observable scene. Qualitative and quantitative comparisons demonstrate the effectiveness of our methods on EVS problem.",
        "arxiv_id": "2505.23434",
        "ARXIVID": "2505.23434",
        "COMMENT": "Matches criterion 3 as it addresses extrapolated view synthesis with hierarchical semantic-geometric priors, which could be relevant to embodied AI methods.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.23115": {
        "authors": [
            "Yunshen Wang",
            "Yicheng Liu",
            "Tianyuan Yuan",
            "Yucheng Mao",
            "Yingshi Liang",
            "Xiuyu Yang",
            "Honggang Zhang",
            "Hang Zhao"
        ],
        "title": "Diffusion-Based Generative Models for 3D Occupancy Prediction in Autonomous Driving",
        "abstract": "arXiv:2505.23115v1 Announce Type: new  Abstract: Accurately predicting 3D occupancy grids from visual inputs is critical for autonomous driving, but current discriminative methods struggle with noisy data, incomplete observations, and the complex structures inherent in 3D scenes. In this work, we reframe 3D occupancy prediction as a generative modeling task using diffusion models, which learn the underlying data distribution and incorporate 3D scene priors. This approach enhances prediction consistency, noise robustness, and better handles the intricacies of 3D spatial structures. Our extensive experiments show that diffusion-based generative models outperform state-of-the-art discriminative approaches, delivering more realistic and accurate occupancy predictions, especially in occluded or low-visibility regions. Moreover, the improved predictions significantly benefit downstream planning tasks, highlighting the practical advantages of our method for real-world autonomous driving applications.",
        "arxiv_id": "2505.23115",
        "ARXIVID": "2505.23115",
        "COMMENT": "Matches criterion 3 as it reframes 3D occupancy prediction as a generative modeling task, which could be relevant to embodied AI methods.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.23693": {
        "authors": [
            "Tingyu Song",
            "Tongyan Hu",
            "Guo Gan",
            "Yilun Zhao"
        ],
        "title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos",
        "abstract": "arXiv:2505.23693v1 Announce Type: new  Abstract: MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on interpreting AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VF-Eval, which introduces four tasks-coherence validation, error awareness, error type detection, and reasoning evaluation-to comprehensively evaluate the abilities of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks. This highlights the challenging nature of our benchmark. Additionally, to investigate the practical applications of VF-Eval in improving video generation, we conduct an experiment, RePrompt, demonstrating that aligning MLLMs more closely with human feedback can benefit video generation.",
        "arxiv_id": "2505.23693",
        "ARXIVID": "2505.23693",
        "COMMENT": "Matches criterion 2 as it introduces a benchmark for evaluating MLLMs on AI-generated content videos, which aligns with the interest in VLLMs/MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.23280": {
        "authors": [
            "Chuandong Liu",
            "Huijiao Wang",
            "Lei Yu",
            "Gui-Song Xia"
        ],
        "title": "Holistic Large-Scale Scene Reconstruction via Mixed Gaussian Splatting",
        "abstract": "arXiv:2505.23280v1 Announce Type: new  Abstract: Recent advances in 3D Gaussian Splatting have shown remarkable potential for novel view synthesis. However, most existing large-scale scene reconstruction methods rely on the divide-and-conquer paradigm, which often leads to the loss of global scene information and requires complex parameter tuning due to scene partitioning and local optimization. To address these limitations, we propose MixGS, a novel holistic optimization framework for large-scale 3D scene reconstruction. MixGS models the entire scene holistically by integrating camera pose and Gaussian attributes into a view-aware representation, which is decoded into fine-detailed Gaussians. Furthermore, a novel mixing operation combines decoded and original Gaussians to jointly preserve global coherence and local fidelity. Extensive experiments on large-scale scenes demonstrate that MixGS achieves state-of-the-art rendering quality and competitive speed, while significantly reducing computational requirements, enabling large-scale scene reconstruction training on a single 24GB VRAM GPU. The code will be released at https://github.com/azhuantou/MixGS.",
        "arxiv_id": "2505.23280",
        "ARXIVID": "2505.23280",
        "COMMENT": "Matches criterion 3 as it proposes a novel holistic optimization framework for large-scale 3D scene reconstruction, which could be relevant to embodied AI benchmarks or methods.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.23745": {
        "authors": [
            "Hao Dong",
            "Moru Liu",
            "Jian Liang",
            "Eleni Chatzi",
            "Olga Fink"
        ],
        "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction",
        "abstract": "arXiv:2505.23745v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM.",
        "arxiv_id": "2505.23745",
        "ARXIVID": "2505.23745",
        "COMMENT": "Matches criterion 2 as it focuses on improving trust and reliability in Vision-Language Models (VLMs), which aligns with the interest in VLLMs/MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.22869": {
        "authors": [
            "Junbo Yin",
            "Chao Zha",
            "Wenjia He",
            "Chencheng Xu",
            "Xin Gao"
        ],
        "title": "CFP-Gen: Combinatorial Functional Protein Generation via Diffusion Language Models",
        "abstract": "arXiv:2505.22869v1 Announce Type: new  Abstract: Existing PLMs generate protein sequences based on a single-condition constraint from a specific modality, struggling to simultaneously satisfy multiple constraints across different modalities. In this work, we introduce CFP-Gen, a novel diffusion language model for Combinatorial Functional Protein GENeration. CFP-Gen facilitates the de novo protein design by integrating multimodal conditions with functional, sequence, and structural constraints. Specifically, an Annotation-Guided Feature Modulation (AGFM) module is introduced to dynamically adjust the protein feature distribution based on composable functional annotations, e.g., GO terms, IPR domains and EC numbers. Meanwhile, the Residue-Controlled Functional Encoding (RCFE) module captures residue-wise interaction to ensure more precise control. Additionally, off-the-shelf 3D structure encoders can be seamlessly integrated to impose geometric constraints. We demonstrate that CFP-Gen enables high-throughput generation of novel proteins with functionality comparable to natural proteins, while achieving a high success rate in designing multifunctional proteins. Code and data available at https://github.com/yinjunbo/cfpgen.",
        "arxiv_id": "2505.22869",
        "ARXIVID": "2505.22869",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling in protein design, which is tangentially relevant to your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.23444": {
        "authors": [
            "Runmin Jiang",
            "Genpei Zhang",
            "Yuntian Yang",
            "Siqi Wu",
            "Yuheng Zhang",
            "Wanyue Feng",
            "Yizhou Zhao",
            "Xi Xiao",
            "Xiao Wang",
            "Tianyang Wang",
            "Xingjian Li",
            "Min Xu"
        ],
        "title": "CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis",
        "abstract": "arXiv:2505.23444v1 Announce Type: new  Abstract: Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of macromolecules, but developing robust models for downstream analysis is hindered by the scarcity of high-quality annotated data. While synthetic data generation has emerged as a potential solution, existing methods often fail to capture both the structural diversity of biological specimens and the complex, spatially varying noise inherent in cryo-EM imaging. To overcome these limitations, we propose CryoCCD, a synthesis framework that integrates biophysical modeling with generative techniques. Specifically, CryoCCD produces multi-scale cryo-EM micrographs that reflect realistic biophysical variability through compositional heterogeneity, cellular context, and physics-informed imaging. To generate realistic noise, we employ a conditional diffusion model, enhanced by cycle consistency to preserve structural fidelity and mask-aware contrastive learning to capture spatially adaptive noise patterns. Extensive experiments show that CryoCCD generates structurally accurate micrographs and enhances performance in downstream tasks, outperforming state-of-the-art baselines in both particle picking and reconstruction.",
        "arxiv_id": "2505.23444",
        "ARXIVID": "2505.23444",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling in a specific domain (cryo-EM synthesis), which is tangentially relevant to your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.23209": {
        "authors": [
            "Akash Dhasade",
            "Divyansh Jhunjhunwala",
            "Milos Vujasinovic",
            "Gauri Joshi",
            "Anne-Marie Kermarrec"
        ],
        "title": "Navigating the Accuracy-Size Trade-Off with Flexible Model Merging",
        "abstract": "arXiv:2505.23209v1 Announce Type: new  Abstract: Model merging has emerged as an efficient method to combine multiple single-task fine-tuned models. The merged model can enjoy multi-task capabilities without expensive training. While promising, merging into a single model often suffers from an accuracy gap with respect to individual fine-tuned models. On the other hand, deploying all individual fine-tuned models incurs high costs. We propose FlexMerge, a novel data-free model merging framework to flexibly generate merged models of varying sizes, spanning the spectrum from a single merged model to retaining all individual fine-tuned models. FlexMerge treats fine-tuned models as collections of sequential blocks and progressively merges them using any existing data-free merging method, halting at a desired size. We systematically explore the accuracy-size trade-off exhibited by different merging algorithms in combination with FlexMerge. Extensive experiments on vision and NLP benchmarks, with up to 30 tasks, reveal that even modestly larger merged models can provide substantial accuracy improvements over a single model. By offering fine-grained control over fused model size, FlexMerge provides a flexible, data-free, and high-performance solution for diverse deployment scenarios.",
        "arxiv_id": "2505.23209",
        "ARXIVID": "2505.23209",
        "COMMENT": "Does not match any specific criteria. Focuses on model merging techniques, which are not directly related to spatial intelligence, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.23742": {
        "authors": [
            "Yufan Deng",
            "Xun Guo",
            "Yuanyang Yin",
            "Jacob Zhiyuan Fang",
            "Yiding Yang",
            "Yizhi Wang",
            "Shenghai Yuan",
            "Angtian Wang",
            "Bo Liu",
            "Haibin Huang",
            "Chongyang Ma"
        ],
        "title": "MAGREF: Masked Guidance for Any-Reference Video Generation",
        "abstract": "arXiv:2505.23742v1 Announce Type: new  Abstract: Video generation has made substantial strides with the emergence of deep generative models, especially diffusion-based approaches. However, video generation based on multiple reference subjects still faces significant challenges in maintaining multi-subject consistency and ensuring high generation quality. In this paper, we propose MAGREF, a unified framework for any-reference video generation that introduces masked guidance to enable coherent multi-subject video synthesis conditioned on diverse reference images and a textual prompt. Specifically, we propose (1) a region-aware dynamic masking mechanism that enables a single model to flexibly handle various subject inference, including humans, objects, and backgrounds, without architectural changes, and (2) a pixel-wise channel concatenation mechanism that operates on the channel dimension to better preserve appearance features. Our model delivers state-of-the-art video generation quality, generalizing from single-subject training to complex multi-subject scenarios with coherent synthesis and precise control over individual subjects, outperforming existing open-source and commercial baselines. To facilitate evaluation, we also introduce a comprehensive multi-subject video benchmark. Extensive experiments demonstrate the effectiveness of our approach, paving the way for scalable, controllable, and high-fidelity multi-subject video synthesis. Code and model can be found at: https://github.com/MAGREF-Video/MAGREF",
        "arxiv_id": "2505.23742",
        "ARXIVID": "2505.23742",
        "COMMENT": "Does not match any specific criteria but focuses on video generation with multi-subject consistency, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.23406": {
        "authors": [
            "Binyamin Manela",
            "Sharon Gannot",
            "Ethan Fetyaya"
        ],
        "title": "Video Editing for Audio-Visual Dubbing",
        "abstract": "arXiv:2505.23406v1 Announce Type: new  Abstract: Visual dubbing, the synchronization of facial movements with new speech, is crucial for making content accessible across different languages, enabling broader global reach. However, current methods face significant limitations. Existing approaches often generate talking faces, hindering seamless integration into original scenes, or employ inpainting techniques that discard vital visual information like partial occlusions and lighting variations. This work introduces EdiDub, a novel framework that reformulates visual dubbing as a content-aware editing task. EdiDub preserves the original video context by utilizing a specialized conditioning scheme to ensure faithful and accurate modifications rather than mere copying. On multiple benchmarks, including a challenging occluded-lip dataset, EdiDub significantly improves identity preservation and synchronization. Human evaluations further confirm its superiority, achieving higher synchronization and visual naturalness scores compared to the leading methods. These results demonstrate that our content-aware editing approach outperforms traditional generation or inpainting, particularly in maintaining complex visual elements while ensuring accurate lip synchronization.",
        "arxiv_id": "2505.23406",
        "ARXIVID": "2505.23406",
        "COMMENT": "Does not match any specific criterion but is related to video editing and synchronization, which is tangentially relevant to your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.22855": {
        "authors": [
            "Ruining Deng",
            "Junchao Zhu",
            "Juming Xiong",
            "Can Cui",
            "Tianyuan Yao",
            "Junlin Guo",
            "Siqi Lu",
            "Marilyn Lionts",
            "Mengmeng Yin",
            "Yu Wang",
            "Shilin Zhao",
            "Yucheng Tang",
            "Yihe Yang",
            "Paul Dennis Simonson",
            "Mert R. Sabuncu",
            "Haichun Yang",
            "Yuankai Huo"
        ],
        "title": "IRS: Incremental Relationship-guided Segmentation for Digital Pathology",
        "abstract": "arXiv:2505.22855v1 Announce Type: new  Abstract: Continual learning is rapidly emerging as a key focus in computer vision, aiming to develop AI systems capable of continuous improvement, thereby enhancing their value and practicality in diverse real-world applications. In healthcare, continual learning holds great promise for continuously acquired digital pathology data, which is collected in hospitals on a daily basis. However, panoramic segmentation on digital whole slide images (WSIs) presents significant challenges, as it is often infeasible to obtain comprehensive annotations for all potential objects, spanning from coarse structures (e.g., regions and unit objects) to fine structures (e.g., cells). This results in temporally and partially annotated data, posing a major challenge in developing a holistic segmentation framework. Moreover, an ideal segmentation model should incorporate new phenotypes, unseen diseases, and diverse populations, making this task even more complex. In this paper, we introduce a novel and unified Incremental Relationship-guided Segmentation (IRS) learning scheme to address temporally acquired, partially annotated data while maintaining out-of-distribution (OOD) continual learning capacity in digital pathology. The key innovation of IRS lies in its ability to realize a new spatial-temporal OOD continual learning paradigm by mathematically modeling anatomical relationships between existing and newly introduced classes through a simple incremental universal proposition matrix. Experimental results demonstrate that the IRS method effectively handles the multi-scale nature of pathological segmentation, enabling precise kidney segmentation across various structures (regions, units, and cells) as well as OOD disease lesions at multiple magnifications. This capability significantly enhances domain generalization, making IRS a robust approach for real-world digital pathology applications.",
        "arxiv_id": "2505.22855",
        "ARXIVID": "2505.22855",
        "COMMENT": "Does not match any specific criterion but is related to continual learning and segmentation in digital pathology, which is tangentially relevant to your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.23180": {
        "authors": [
            "Ping Wang",
            "Lishun Wang",
            "Gang Qu",
            "Xiaodong Wang",
            "Yulun Zhang",
            "Xin Yuan"
        ],
        "title": "Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction Networks for Single-Pixel Imaging",
        "abstract": "arXiv:2505.23180v1 Announce Type: new  Abstract: Deep-unrolling and plug-and-play (PnP) approaches have become the de-facto standard solvers for single-pixel imaging (SPI) inverse problem. PnP approaches, a class of iterative algorithms where regularization is implicitly performed by an off-the-shelf deep denoiser, are flexible for varying compression ratios (CRs) but are limited in reconstruction accuracy and speed. Conversely, unrolling approaches, a class of multi-stage neural networks where a truncated iterative optimization process is transformed into an end-to-end trainable network, typically achieve better accuracy with faster inference but require fine-tuning or even retraining when CR changes. In this paper, we address the challenge of integrating the strengths of both classes of solvers. To this end, we design an efficient deep image restorer (DIR) for the unrolling of HQS (half quadratic splitting) and ADMM (alternating direction method of multipliers). More importantly, a general proximal trajectory (PT) loss function is proposed to train HQS/ADMM-unrolling networks such that learned DIR approximates the proximal operator of an ideal explicit restoration regularizer. Extensive experiments demonstrate that, the resulting proximal unrolling networks can not only flexibly handle varying CRs with a single model like PnP algorithms, but also outperform previous CR-specific unrolling networks in both reconstruction accuracy and speed. Source codes and models are available at https://github.com/pwangcs/ProxUnroll.",
        "arxiv_id": "2505.23180",
        "ARXIVID": "2505.23180",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.23365": {
        "authors": [
            "Yang Qiao",
            "Xiaoyu Zhong",
            "Xiaofeng Gu",
            "Zhiguo Yu"
        ],
        "title": "MCFNet: A Multimodal Collaborative Fusion Network for Fine-Grained Semantic Classification",
        "abstract": "arXiv:2505.23365v1 Announce Type: new  Abstract: Multimodal information processing has become increasingly important for enhancing image classification performance. However, the intricate and implicit dependencies across different modalities often hinder conventional methods from effectively capturing fine-grained semantic interactions, thereby limiting their applicability in high-precision classification tasks. To address this issue, we propose a novel Multimodal Collaborative Fusion Network (MCFNet) designed for fine-grained classification. The proposed MCFNet architecture incorporates a regularized integrated fusion module that improves intra-modal feature representation through modality-specific regularization strategies, while facilitating precise semantic alignment via a hybrid attention mechanism. Additionally, we introduce a multimodal decision classification module, which jointly exploits inter-modal correlations and unimodal discriminative features by integrating multiple loss functions within a weighted voting paradigm. Extensive experiments and ablation studies on benchmark datasets demonstrate that the proposed MCFNet framework achieves consistent improvements in classification accuracy, confirming its effectiveness in modeling subtle cross-modal semantics.",
        "arxiv_id": "2505.23365",
        "ARXIVID": "2505.23365",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.23432": {
        "authors": [
            "Elisa Celis",
            "Lingxiao Huang",
            "Nisheeth K. Vishnoi"
        ],
        "title": "A Mathematical Framework for AI-Human Integration in Work",
        "abstract": "arXiv:2505.23432v1 Announce Type: new  Abstract: The rapid rise of Generative AI (GenAI) tools has sparked debate over their role in complementing or replacing human workers across job contexts. We present a mathematical framework that models jobs, workers, and worker-job fit, introducing a novel decomposition of skills into decision-level and action-level subskills to reflect the complementary strengths of humans and GenAI. We analyze how changes in subskill abilities affect job success, identifying conditions for sharp transitions in success probability. We also establish sufficient conditions under which combining workers with complementary subskills significantly outperforms relying on a single worker. This explains phenomena such as productivity compression, where GenAI assistance yields larger gains for lower-skilled workers. We demonstrate the framework' s practicality using data from O*NET and Big-Bench Lite, aligning real-world data with our model via subskill-division methods. Our results highlight when and how GenAI complements human skills, rather than replacing them.",
        "arxiv_id": "2505.23432",
        "ARXIVID": "2505.23432",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.23473": {
        "authors": [
            "Xiaorui Wu",
            "Xiaofeng Mao",
            "Fei Li",
            "Xin Zhang",
            "Xiaolu Zhang",
            "Jun Zhou",
            "Yuxiang Peng",
            "Li Zheng",
            "Chong Teng",
            "Donghong Ji",
            "Zhuang Li"
        ],
        "title": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions",
        "abstract": "arXiv:2505.23473v1 Announce Type: new  Abstract: Large language models (LLMs) frequently refuse to respond to pseudo-malicious instructions: semantically harmless input queries triggering unnecessary LLM refusals due to conservative safety alignment, significantly impairing user experience. Collecting such instructions is crucial for evaluating and mitigating over-refusals, but existing instruction curation methods, like manual creation or instruction rewriting, either lack scalability or fail to produce sufficiently diverse and effective refusal-inducing prompts. To address these limitations, we introduce EVOREFUSE, a prompt optimization approach that generates diverse pseudo-malicious instructions consistently eliciting confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm exploring the instruction space in more diverse directions than existing methods via mutation strategies and recombination, and iteratively evolves seed instructions to maximize evidence lower bound on LLM refusal probability. Using EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582 pseudo-malicious instructions that outperforms the next-best benchmark with 140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater lexical diversity, and 40.03% improved LLM response confidence scores; and EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with responses for supervised and preference-based alignment training. LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to 14.31% fewer over-refusals than models trained on the second-best alignment dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context.",
        "arxiv_id": "2505.23473",
        "ARXIVID": "2505.23473",
        "COMMENT": "Does not match any specific criteria. Focuses on prompt optimization for mitigating over-refusals in LLMs, which is not directly related to spatial intelligence, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.23214": {
        "authors": [
            "Wenhao Xu",
            "Shuchen Zheng",
            "Changwei Wang",
            "Zherui Zhang",
            "Chuan Ren",
            "Rongtao Xu",
            "Shibiao Xu"
        ],
        "title": "SAMamba: Adaptive State Space Modeling with Hierarchical Vision for Infrared Small Target Detection",
        "abstract": "arXiv:2505.23214v1 Announce Type: new  Abstract: Infrared small target detection (ISTD) is vital for long-range surveillance in military, maritime, and early warning applications. ISTD is challenged by targets occupying less than 0.15% of the image and low distinguishability from complex backgrounds. Existing deep learning methods often suffer from information loss during downsampling and inefficient global context modeling. This paper presents SAMamba, a novel framework integrating SAM2's hierarchical feature learning with Mamba's selective sequence modeling. Key innovations include: (1) A Feature Selection Adapter (FS-Adapter) for efficient natural-to-infrared domain adaptation via dual-stage selection (token-level with a learnable task embedding and channel-wise adaptive transformations); (2) A Cross-Channel State-Space Interaction (CSI) module for efficient global context modeling with linear complexity using selective state space modeling; and (3) A Detail-Preserving Contextual Fusion (DPCF) module that adaptively combines multi-scale features with a gating mechanism to balance high-resolution and low-resolution feature contributions. SAMamba addresses core ISTD challenges by bridging the domain gap, maintaining fine-grained details, and efficiently modeling long-range dependencies. Experiments on NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets show SAMamba significantly outperforms state-of-the-art methods, especially in challenging scenarios with heterogeneous backgrounds and varying target scales. Code: https://github.com/zhengshuchen/SAMamba.",
        "arxiv_id": "2505.23214",
        "ARXIVID": "2505.23214",
        "COMMENT": "Does not match any specific criteria but focuses on infrared small target detection, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.23439": {
        "authors": [
            "Ben Li",
            "Minqi Li",
            "Jie Ren",
            "Kaibing Zhang"
        ],
        "title": "VITON-DRR: Details Retention Virtual Try-on via Non-rigid Registration",
        "abstract": "arXiv:2505.23439v1 Announce Type: new  Abstract: Image-based virtual try-on aims to fit a target garment to a specific person image and has attracted extensive research attention because of its huge application potential in the e-commerce and fashion industries. To generate high-quality try-on results, accurately warping the clothing item to fit the human body plays a significant role, as slight misalignment may lead to unrealistic artifacts in the fitting image. Most existing methods warp the clothing by feature matching and thin-plate spline (TPS). However, it often fails to preserve clothing details due to self-occlusion, severe misalignment between poses, etc. To address these challenges, this paper proposes a detail retention virtual try-on method via accurate non-rigid registration (VITON-DRR) for diverse human poses. Specifically, we reconstruct a human semantic segmentation using a dual-pyramid-structured feature extractor. Then, a novel Deformation Module is designed for extracting the cloth key points and warping them through an accurate non-rigid registration algorithm. Finally, the Image Synthesis Module is designed to synthesize the deformed garment image and generate the human pose information adaptively. {Compared with} traditional methods, the proposed VITON-DRR can make the deformation of fitting images more accurate and retain more garment details. The experimental results demonstrate that the proposed method performs better than state-of-the-art methods.",
        "arxiv_id": "2505.23439",
        "ARXIVID": "2505.23439",
        "COMMENT": "Does not match any specific criteria but focuses on virtual try-on with non-rigid registration, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.23265": {
        "authors": [
            "Zheng Sun",
            "Yi Wei",
            "Long Yu"
        ],
        "title": "Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening with MLLMs",
        "abstract": "arXiv:2505.23265v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) are of great application across many domains, such as multimodal understanding and generation. With the development of diffusion models (DM) and unified MLLMs, the performance of image generation has been significantly improved, however, the study of image screening is rare and its performance with MLLMs is unsatisfactory due to the lack of data and the week image aesthetic reasoning ability in MLLMs. In this work, we propose a complete solution to address these problems in terms of data and methodology. For data, we collect a comprehensive medical image screening dataset with 1500+ samples, each sample consists of a medical image, four generated images, and a multiple-choice answer. The dataset evaluates the aesthetic reasoning ability under four aspects: \\textit{(1) Appearance Deformation, (2) Principles of Physical Lighting and Shadow, (3) Placement Layout, (4) Extension Rationality}. For methodology, we utilize long chains of thought (CoT) and Group Relative Policy Optimization with Dynamic Proportional Accuracy reward, called DPA-GRPO, to enhance the image aesthetic reasoning ability of MLLMs. Our experimental results reveal that even state-of-the-art closed-source MLLMs, such as GPT-4o and Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic reasoning. In contrast, by leveraging the reinforcement learning approach, we are able to surpass the score of both large-scale models and leading closed-source models using a much smaller model. We hope our attempt on medical image screening will serve as a regular configuration in image aesthetic reasoning in the future.",
        "arxiv_id": "2505.23265",
        "ARXIVID": "2505.23265",
        "COMMENT": "Does not match any specific criteria but discusses image aesthetic reasoning in MLLMs, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.23119": {
        "authors": [
            "Keren Ye",
            "Ignacio Garcia Dorado",
            "Michalis Raptis",
            "Mauricio Delbracio",
            "Irene Zhu",
            "Peyman Milanfar",
            "Hossein Talebi"
        ],
        "title": "TextSR: Diffusion Super-Resolution with Multilingual OCR Guidance",
        "abstract": "arXiv:2505.23119v1 Announce Type: new  Abstract: While recent advancements in Image Super-Resolution (SR) using diffusion models have shown promise in improving overall image quality, their application to scene text images has revealed limitations. These models often struggle with accurate text region localization and fail to effectively model image and multilingual character-to-shape priors. This leads to inconsistencies, the generation of hallucinated textures, and a decrease in the perceived quality of the super-resolved text.   To address these issues, we introduce TextSR, a multimodal diffusion model specifically designed for Multilingual Scene Text Image Super-Resolution. TextSR leverages a text detector to pinpoint text regions within an image and then employs Optical Character Recognition (OCR) to extract multilingual text from these areas. The extracted text characters are then transformed into visual shapes using a UTF-8 based text encoder and cross-attention. Recognizing that OCR may sometimes produce inaccurate results in real-world scenarios, we have developed two innovative methods to enhance the robustness of our model. By integrating text character priors with the low-resolution text images, our model effectively guides the super-resolution process, enhancing fine details within the text and improving overall legibility. The superior performance of our model on both the TextZoom and TextVQA datasets sets a new benchmark for STISR, underscoring the efficacy of our approach.",
        "arxiv_id": "2505.23119",
        "ARXIVID": "2505.23119",
        "COMMENT": "Does not match any specific criteria but focuses on multilingual OCR-guided super-resolution, which is tangentially related to vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.23058": {
        "authors": [
            "Yutong Xie",
            "Zhuoheng Li",
            "Xiyuan Wang",
            "Yijun Pan",
            "Qijia Liu",
            "Xingzhi Cui",
            "Kuang-Yu Lo",
            "Ruoyi Gao",
            "Xingjian Zhang",
            "Jin Huang",
            "Walter Yuan",
            "Matthew O. Jackson",
            "Qiaozhu Mei"
        ],
        "title": "Be.FM: Open Foundation Models for Human Behavior",
        "abstract": "arXiv:2505.23058v1 Announce Type: new  Abstract: Despite their success in numerous fields, the potential of foundation models for modeling and understanding human behavior remains largely unexplored. We introduce Be.FM, one of the first open foundation models designed for human behavior modeling. Built upon open-source large language models and fine-tuned on a diverse range of behavioral data, Be.FM can be used to understand and predict human decision-making. We construct a comprehensive set of benchmark tasks for testing the capabilities of behavioral foundation models. Our results demonstrate that Be.FM can predict behaviors, infer characteristics of individuals and populations, generate insights about contexts, and apply behavioral science knowledge.",
        "arxiv_id": "2505.23058",
        "ARXIVID": "2505.23058",
        "COMMENT": "Does not match any specific criteria but introduces a foundation model for human behavior, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.23559": {
        "authors": [
            "Kunlun Zhu",
            "Jiaxun Zhang",
            "Ziheng Qi",
            "Nuoxing Shang",
            "Zijia Liu",
            "Peixuan Han",
            "Yue Su",
            "Haofei Yu",
            "Jiaxuan You"
        ],
        "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents",
        "abstract": "arXiv:2505.23559v1 Announce Type: new  Abstract: Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critical ethical and safety concerns. To systematically address these challenges, we introduce \\textbf{SafeScientist}, an innovative AI scientist framework explicitly designed to enhance safety and ethical responsibility in AI-driven scientific exploration. SafeScientist proactively refuses ethically inappropriate or high-risk tasks and rigorously emphasizes safety throughout the research process. To achieve comprehensive safety oversight, we integrate multiple defensive mechanisms, including prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component. Complementing SafeScientist, we propose \\textbf{SciSafetyBench}, a novel benchmark specifically designed to evaluate AI safety in scientific contexts, comprising 240 high-risk scientific tasks across 6 domains, alongside 30 specially designed scientific tools and 120 tool-related risk tasks. Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35\\% compared to traditional AI scientist frameworks, without compromising scientific output quality. Additionally, we rigorously validate the robustness of our safety pipeline against diverse adversarial attack methods, further confirming the effectiveness of our integrated approach. The code and data will be available at https://github.com/ulab-uiuc/SafeScientist. \\textcolor{red}{Warning: this paper contains example data that may be offensive or harmful.}",
        "arxiv_id": "2505.23559",
        "ARXIVID": "2505.23559",
        "COMMENT": "Relevant to the general interest area but does not directly match any specific criterion. Focuses on safety in scientific discovery automation using LLM agents.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.23268": {
        "authors": [
            "Spyros Barbakos",
            "Charalampos Antoniadis",
            "Gerasimos Potamianos",
            "Gianluca Setti"
        ],
        "title": "Unsupervised Transcript-assisted Video Summarization and Highlight Detection",
        "abstract": "arXiv:2505.23268v1 Announce Type: new  Abstract: Video consumption is a key part of daily life, but watching entire videos can be tedious. To address this, researchers have explored video summarization and highlight detection to identify key video segments. While some works combine video frames and transcripts, and others tackle video summarization and highlight detection using Reinforcement Learning (RL), no existing work, to the best of our knowledge, integrates both modalities within an RL framework. In this paper, we propose a multimodal pipeline that leverages video frames and their corresponding transcripts to generate a more condensed version of the video and detect highlights using a modality fusion mechanism. The pipeline is trained within an RL framework, which rewards the model for generating diverse and representative summaries while ensuring the inclusion of video segments with meaningful transcript content. The unsupervised nature of the training allows for learning from large-scale unannotated datasets, overcoming the challenge posed by the limited size of existing annotated datasets. Our experiments show that using the transcript in video summarization and highlight detection achieves superior results compared to relying solely on the visual content of the video.",
        "arxiv_id": "2505.23268",
        "ARXIVID": "2505.23268",
        "COMMENT": "Relevant to the general interest area but does not directly match any specific criterion. Focuses on video summarization and highlight detection using multimodal RL.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.23367": {
        "authors": [
            "Jeonghyeok Do",
            "Sungpyo Kim",
            "Geunhyuk Youk",
            "Jaehyup Lee",
            "Munchurl Kim"
        ],
        "title": "PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening",
        "abstract": "arXiv:2505.23367v1 Announce Type: new  Abstract: PAN-sharpening aims to fuse high-resolution panchromatic (PAN) images with low-resolution multi-spectral (MS) images to generate high-resolution multi-spectral (HRMS) outputs. However, cross-modality misalignment -- caused by sensor placement, acquisition timing, and resolution disparity -- induces a fundamental challenge. Conventional deep learning methods assume perfect pixel-wise alignment and rely on per-pixel reconstruction losses, leading to spectral distortion, double edges, and blurring when misalignment is present. To address this, we propose PAN-Crafter, a modality-consistent alignment framework that explicitly mitigates the misalignment gap between PAN and MS modalities. At its core, Modality-Adaptive Reconstruction (MARs) enables a single network to jointly reconstruct HRMS and PAN images, leveraging PAN's high-frequency details as auxiliary self-supervision. Additionally, we introduce Cross-Modality Alignment-Aware Attention (CM3A), a novel mechanism that bidirectionally aligns MS texture to PAN structure and vice versa, enabling adaptive feature refinement across modalities. Extensive experiments on multiple benchmark datasets demonstrate that our PAN-Crafter outperforms the most recent state-of-the-art method in all metrics, even with 50.11$\\times$ faster inference time and 0.63$\\times$ the memory size. Furthermore, it demonstrates strong generalization performance on unseen satellite datasets, showing its robustness across different conditions.",
        "arxiv_id": "2505.23367",
        "ARXIVID": "2505.23367",
        "COMMENT": "Does not match any specific criteria. Focuses on PAN-sharpening for satellite imagery, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.23120": {
        "authors": [
            "Siyuan Wang",
            "Jiawei Liu",
            "Wei Wang",
            "Yeying Jin",
            "Jinsong Du",
            "Zhi Han"
        ],
        "title": "MMGT: Motion Mask Guided Two-Stage Network for Co-Speech Gesture Video Generation",
        "abstract": "arXiv:2505.23120v1 Announce Type: new  Abstract: Co-Speech Gesture Video Generation aims to generate vivid speech videos from audio-driven still images, which is challenging due to the diversity of different parts of the body in terms of amplitude of motion, audio relevance, and detailed features. Relying solely on audio as the control signal often fails to capture large gesture movements in video, leading to more pronounced artifacts and distortions. Existing approaches typically address this issue by introducing additional a priori information, but this can limit the practical application of the task. Specifically, we propose a Motion Mask-Guided Two-Stage Network (MMGT) that uses audio, as well as motion masks and motion features generated from the audio signal to jointly drive the generation of synchronized speech gesture videos. In the first stage, the Spatial Mask-Guided Audio Pose Generation (SMGA) Network generates high-quality pose videos and motion masks from audio, effectively capturing large movements in key regions such as the face and gestures. In the second stage, we integrate the Motion Masked Hierarchical Audio Attention (MM-HAA) into the Stabilized Diffusion Video Generation model, overcoming limitations in fine-grained motion generation and region-specific detail control found in traditional methods. This guarantees high-quality, detailed upper-body video generation with accurate texture and motion details. Evaluations show improved video quality, lip-sync, and gesture. The model and code are available at https://github.com/SIA-IDE/MMGT.",
        "arxiv_id": "2505.23120",
        "ARXIVID": "2505.23120",
        "COMMENT": "Does not match any specific criteria. Focuses on co-speech gesture video generation, which is not directly related to spatial understanding, VLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.23744": {
        "authors": [
            "Qiang Wang",
            "Xiang Song",
            "Yuhang He",
            "Jizhou Han",
            "Chenhao Ding",
            "Xinyuan Gao",
            "Yihong Gong"
        ],
        "title": "Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need",
        "abstract": "arXiv:2505.23744v1 Announce Type: new  Abstract: Deep neural networks (DNNs) often underperform in real-world, dynamic settings where data distributions change over time. Domain Incremental Learning (DIL) offers a solution by enabling continual model adaptation, with Parameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce knowledge conflicts. However, existing PIDIL methods struggle with parameter selection accuracy, especially as the number of domains and corresponding classes grows. To address this, we propose SOYO, a lightweight framework that improves domain selection in PIDIL. SOYO introduces a Gaussian Mixture Compressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior domain data efficiently, while a Multi-level Domain Feature Fusion Network (MDFN) enhances domain feature extraction. Our framework supports multiple Parameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks such as image classification, object detection, and speech enhancement. Experimental results on six benchmarks demonstrate SOYO's consistent superiority over existing baselines, showcasing its robustness and adaptability in complex, evolving environments. The codes will be released in https://github.com/qwangcv/SOYO.",
        "arxiv_id": "2505.23744",
        "ARXIVID": "2505.23744",
        "COMMENT": "Does not match any specific criteria. Focuses on domain incremental learning, which is not directly related to spatial understanding, VLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.23525": {
        "authors": [
            "Jiahao Cui",
            "Yan Chen",
            "Mingwang Xu",
            "Hanlin Shang",
            "Yuxuan Chen",
            "Yun Zhan",
            "Zilong Dong",
            "Yao Yao",
            "Jingdong Wang",
            "Siyu Zhu"
        ],
        "title": "Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization and Temporal Motion Modulation",
        "abstract": "arXiv:2505.23525v1 Announce Type: new  Abstract: Generating highly dynamic and photorealistic portrait animations driven by audio and skeletal motion remains challenging due to the need for precise lip synchronization, natural facial expressions, and high-fidelity body motion dynamics. We propose a human-preference-aligned diffusion framework that addresses these challenges through two key innovations. First, we introduce direct preference optimization tailored for human-centric animation, leveraging a curated dataset of human preferences to align generated outputs with perceptual metrics for portrait motion-video alignment and naturalness of expression. Second, the proposed temporal motion modulation resolves spatiotemporal resolution mismatches by reshaping motion conditions into dimensionally aligned latent features through temporal channel redistribution and proportional feature expansion, preserving the fidelity of high-frequency motion details in diffusion-based synthesis. The proposed mechanism is complementary to existing UNet and DiT-based portrait diffusion approaches, and experiments demonstrate obvious improvements in lip-audio synchronization, expression vividness, body motion coherence over baseline methods, alongside notable gains in human preference metrics. Our model and source code can be found at: https://github.com/xyz123xyz456/hallo4.",
        "arxiv_id": "2505.23525",
        "ARXIVID": "2505.23525",
        "COMMENT": "Does not match any specific criteria. Focuses on portrait animation and diffusion-based synthesis, which is outside the specified topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.23397": {
        "authors": [
            "Ahmad Mohsin",
            "Helge Janicke",
            "Ahmed Ibrahim",
            "Iqbal H. Sarker",
            "Seyit Camtepe"
        ],
        "title": "A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy",
        "abstract": "arXiv:2505.23397v1 Announce Type: new  Abstract: This article presents a structured framework for Human-AI collaboration in Security Operations Centers (SOCs), integrating AI autonomy, trust calibration, and Human-in-the-loop decision making. Existing frameworks in SOCs often focus narrowly on automation, lacking systematic structures to manage human oversight, trust calibration, and scalable autonomy with AI. Many assume static or binary autonomy settings, failing to account for the varied complexity, criticality, and risk across SOC tasks considering Humans and AI collaboration. To address these limitations, we propose a novel autonomy tiered framework grounded in five levels of AI autonomy from manual to fully autonomous, mapped to Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This enables adaptive and explainable AI integration across core SOC functions, including monitoring, protection, threat detection, alert triage, and incident response. The proposed framework differentiates itself from previous research by creating formal connections between autonomy, trust, and HITL across various SOC levels, which allows for adaptive task distribution according to operational complexity and associated risks. The framework is exemplified through a simulated cyber range that features the cybersecurity AI-Avatar, a fine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates human-AI collaboration for SOC tasks, reducing alert fatigue, enhancing response coordination, and strategically calibrating trust. This research systematically presents both the theoretical and practical aspects and feasibility of designing next-generation cognitive SOCs that leverage AI not to replace but to enhance human decision-making.",
        "arxiv_id": "2505.23397",
        "ARXIVID": "2505.23397",
        "COMMENT": "Does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.23201": {
        "authors": [
            "Hao Wu",
            "Junzhou Chen",
            "Ronghui Zhang",
            "Nengchao Lyu",
            "Hongyu Hu",
            "Yanyong Guo",
            "Tony Z. Qiu"
        ],
        "title": "WTEFNet: Real-Time Low-Light Object Detection for Advanced Driver-Assistance Systems",
        "abstract": "arXiv:2505.23201v1 Announce Type: new  Abstract: Object detection is a cornerstone of environmental perception in advanced driver assistance systems(ADAS). However, most existing methods rely on RGB cameras, which suffer from significant performance degradation under low-light conditions due to poor image quality. To address this challenge, we proposes WTEFNet, a real-time object detection framework specifically designed for low-light scenarios, with strong adaptability to mainstream detectors. WTEFNet comprises three core modules: a Low-Light Enhancement (LLE) module, a Wavelet-based Feature Extraction (WFE) module, and an Adaptive Fusion Detection (AFFD) module. The LLE enhances dark regions while suppressing overexposed areas; the WFE applies multi-level discrete wavelet transforms to isolate high- and low-frequency components, enabling effective denoising and structural feature retention; the AFFD fuses semantic and illumination features for robust detection. To support training and evaluation, we introduce GSN, a manually annotated dataset covering both clear and rainy night-time scenes. Extensive experiments on BDD100K, SHIFT, nuScenes, and GSN demonstrate that WTEFNet achieves state-of-the-art accuracy under low-light conditions. Furthermore, deployment on a embedded platform (NVIDIA Jetson AGX Orin) confirms the framework's suitability for real-time ADAS applications.",
        "arxiv_id": "2505.23201",
        "ARXIVID": "2505.23201",
        "COMMENT": "Does not match any specific criteria. Focuses on low-light object detection for ADAS, which is not directly related to spatial intelligence, VLLMs, MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.22793": {
        "authors": [
            "Srishti Yadav",
            "Lauren Tilton",
            "Maria Antoniak",
            "Taylor Arnold",
            "Jiaang Li",
            "Siddhesh Milind Pawar",
            "Antonia Karamolegkou",
            "Stella Frank",
            "Zhaochong An",
            "Negar Rostamzadeh",
            "Daniel Hershcovich",
            "Serge Belongie",
            "Ekaterina Shutova"
        ],
        "title": "Cultural Evaluations of Vision-Language Models Have a Lot to Learn from Cultural Theory",
        "abstract": "arXiv:2505.22793v1 Announce Type: new  Abstract: Modern vision-language models (VLMs) often fail at cultural competency evaluations and benchmarks. Given the diversity of applications built upon VLMs, there is renewed interest in understanding how they encode cultural nuances. While individual aspects of this problem have been studied, we still lack a comprehensive framework for systematically identifying and annotating the nuanced cultural dimensions present in images for VLMs. This position paper argues that foundational methodologies from visual culture studies (cultural studies, semiotics, and visual studies) are necessary for cultural analysis of images. Building upon this review, we propose a set of five frameworks, corresponding to cultural dimensions, that must be considered for a more complete analysis of the cultural competencies of VLMs.",
        "arxiv_id": "2505.22793",
        "ARXIVID": "2505.22793",
        "COMMENT": "Does not match any specific criteria but discusses cultural evaluations of VLMs, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}