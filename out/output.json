{
    "2505.10604": {
        "authors": [
            "Chonghan Liu",
            "Haoran Wang",
            "Felix Henry",
            "Pu Miao",
            "Yajie Zhang",
            "Yu Zhao",
            "Peiran Wu"
        ],
        "title": "MIRAGE: A Multi-modal Benchmark for Spatial Perception, Reasoning, and Intelligence",
        "abstract": "arXiv:2505.10604v1 Announce Type: new  Abstract: Spatial perception and reasoning are core components of human cognition, encompassing object recognition, spatial relational understanding, and dynamic reasoning. Despite progress in computer vision, existing benchmarks reveal significant gaps in models' abilities to accurately recognize object attributes and reason about spatial relationships, both essential for dynamic reasoning. To address these limitations, we propose MIRAGE, a multi-modal benchmark designed to evaluate models' capabilities in Counting (object attribute recognition), Relation (spatial relational reasoning), and Counting with Relation. Through diverse and complex scenarios requiring fine-grained recognition and reasoning, MIRAGE highlights critical limitations in state-of-the-art models, underscoring the need for improved representations and reasoning frameworks. By targeting these foundational abilities, MIRAGE provides a pathway toward spatiotemporal reasoning in future research.",
        "arxiv_id": "2505.10604",
        "ARXIVID": "2505.10604",
        "COMMENT": "Matches criterion 1 and 3. Proposes a new multi-modal benchmark (MIRAGE) for spatial perception, reasoning, and intelligence, addressing gaps in spatial relational understanding and reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2505.10685": {
        "authors": [
            "Lingjun Zhao",
            "Sizhe Wei",
            "James Hays",
            "Lu Gan"
        ],
        "title": "GaussianFormer3D: Multi-Modal Gaussian-based Semantic Occupancy Prediction with 3D Deformable Attention",
        "abstract": "arXiv:2505.10685v1 Announce Type: new  Abstract: 3D semantic occupancy prediction is critical for achieving safe and reliable autonomous driving. Compared to camera-only perception systems, multi-modal pipelines, especially LiDAR-camera fusion methods, can produce more accurate and detailed predictions. Although most existing works utilize a dense grid-based representation, in which the entire 3D space is uniformly divided into discrete voxels, the emergence of 3D Gaussians provides a compact and continuous object-centric representation. In this work, we propose a multi-modal Gaussian-based semantic occupancy prediction framework utilizing 3D deformable attention, named as GaussianFormer3D. We introduce a voxel-to-Gaussian initialization strategy to provide 3D Gaussians with geometry priors from LiDAR data, and design a LiDAR-guided 3D deformable attention mechanism for refining 3D Gaussians with LiDAR-camera fusion features in a lifted 3D space. We conducted extensive experiments on both on-road and off-road datasets, demonstrating that our GaussianFormer3D achieves high prediction accuracy that is comparable to state-of-the-art multi-modal fusion-based methods with reduced memory consumption and improved efficiency.",
        "arxiv_id": "2505.10685",
        "ARXIVID": "2505.10685",
        "COMMENT": "Matches criterion 1 as it proposes a novel method for 3D semantic occupancy prediction using multi-modal data and deformable attention.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2505.11404": {
        "authors": [
            "Wenchuan Zhang",
            "Penghao Zhang",
            "Jingru Guo",
            "Tao Cheng",
            "Jie Chen",
            "Shuwan Zhang",
            "Zhang Zhang",
            "Yuhao Yi",
            "Hong Bu"
        ],
        "title": "Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner",
        "abstract": "arXiv:2505.11404v1 Announce Type: new  Abstract: Recent advances in vision language models (VLMs) have enabled broad progress in the general medical field. However, pathology still remains a more challenging subdomain, with current pathology specific VLMs exhibiting limitations in both diagnostic accuracy and reasoning plausibility. Such shortcomings are largely attributable to the nature of current pathology datasets, which are primarily composed of image description pairs that lack the depth and structured diagnostic paradigms employed by real world pathologists. In this study, we leverage pathology textbooks and real world pathology experts to construct high-quality, reasoning-oriented datasets. Building on this, we introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs for knowledge infusion; (2) supervised fine-tuning on 500k high-quality Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement learning using Group Relative Policy Optimization and Decoupled Clip and Dynamic sAmpling Policy Optimization strategies for multimodal reasoning quality refinement. To further assess the alignment quality of our dataset, we propose PathoCLIP, trained on the same figure-caption corpus used for continued pretraining. Comprehensive experimental results demonstrate that both PathoCLIP and Patho-R1 achieve robust performance across a wide range of pathology-related tasks, including zero-shot classification, cross-modal retrieval, Visual Question Answering, and Multiple Choice Question. Our project is available at the Patho-R1 repository: https://github.com/Wenchuan-Zhang/Patho-R1.",
        "arxiv_id": "2505.11404",
        "ARXIVID": "2505.11404",
        "COMMENT": "Matches criterion 2. Introduces Patho-R1, a multimodal reinforcement learning-based pathology reasoner, leveraging vision-language models (VLLMs) with a novel training pipeline.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.11191": {
        "authors": [
            "Kasra Borazjani",
            "Payam Abdisarabshali",
            "Fardis Nadimi",
            "Naji Khosravan",
            "Minghui Liwang",
            "Xianbin Wang",
            "Yiguang Hong",
            "Seyyedali Hosseinalipour"
        ],
        "title": "Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration",
        "abstract": "arXiv:2505.11191v1 Announce Type: new  Abstract: As embodied AI systems become increasingly multi-modal, personalized, and interactive, they must learn effectively from diverse sensory inputs, adapt continually to user preferences, and operate safely under resource and privacy constraints. These challenges expose a pressing need for machine learning models capable of swift, context-aware adaptation while balancing model generalization and personalization. Here, two methods emerge as suitable candidates, each offering parts of these capabilities: Foundation Models (FMs) provide a pathway toward generalization across tasks and modalities, whereas Federated Learning (FL) offers the infrastructure for distributed, privacy-preserving model updates and user-level model personalization. However, when used in isolation, each of these approaches falls short of meeting the complex and diverse capability requirements of real-world embodied environments. In this vision paper, we introduce Federated Foundation Models (FFMs) for embodied AI, a new paradigm that unifies the strengths of multi-modal multi-task (M3T) FMs with the privacy-preserving distributed nature of FL, enabling intelligent systems at the wireless edge. We collect critical deployment dimensions of FFMs in embodied AI ecosystems under a unified framework, which we name \"EMBODY\": Embodiment heterogeneity, Modality richness and imbalance, Bandwidth and compute constraints, On-device continual learning, Distributed control and autonomy, and Yielding safety, privacy, and personalization. For each, we identify concrete challenges and envision actionable research directions. We also present an evaluation framework for deploying FFMs in embodied AI systems, along with the associated trade-offs.",
        "arxiv_id": "2505.11191",
        "ARXIVID": "2505.11191",
        "COMMENT": "This paper matches criterion 3 as it introduces a vision paper on Federated Foundation Models (FFMs) for embodied AI, focusing on multi-modal, multi-task learning and privacy-preserving distributed systems.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.10875": {
        "authors": [
            "Alexey Magay",
            "Dhurba Tripathi",
            "Yu Hao",
            "Yi Fang"
        ],
        "title": "A Light and Smart Wearable Platform with Multimodal Foundation Model for Enhanced Spatial Reasoning in People with Blindness and Low Vision",
        "abstract": "arXiv:2505.10875v1 Announce Type: new  Abstract: People with blindness and low vision (pBLV) face significant challenges, struggling to navigate environments and locate objects due to limited visual cues. Spatial reasoning is crucial for these individuals, as it enables them to understand and interpret the spatial relationships in their surroundings, enhancing their ability to navigate and interact more safely and independently. Current multi-modal large language (MLLM) models for low vision people lack the spatial reasoning capabilities needed to effectively assist in these tasks. Moreover, there is a notable absence of lightweight, easy-to-use systems that allow pBLV to effectively perceive and interact with their surrounding environment. In this paper, we propose a novel spatial enhanced multi-modal large language model based approach for visually impaired individuals. By fine-tuning the MLLM to incorporate spatial reasoning capabilities, our method significantly improves the understanding of environmental context, which is critical for navigation and object recognition. The innovation extends to a hardware component, designed as an attachment for glasses, ensuring increased accessibility and ease of use. This integration leverages advanced VLMs to interpret visual data and provide real-time, spatially aware feedback to the user. Our approach aims to bridge the gap between advanced machine learning models and practical, user-friendly assistive devices, offering a robust solution for visually impaired users to navigate their surroundings more effectively and independently. The paper includes an in-depth evaluation using the VizWiz dataset, demonstrating substantial improvements in accuracy and user experience. Additionally, we design a comprehensive dataset to evaluate our method's effectiveness in realworld situations, demonstrating substantial improvements in accuracy and user experience.",
        "arxiv_id": "2505.10875",
        "ARXIVID": "2505.10875",
        "COMMENT": "This paper matches criterion 1 and 2 as it proposes a spatially enhanced multi-modal large language model for assisting visually impaired individuals, focusing on spatial reasoning and MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.11247": {
        "authors": [
            "Mingxing Peng",
            "Yuting Xie",
            "Xusen Guo",
            "Ruoyu Yao",
            "Hai Yang",
            "Jun Ma"
        ],
        "title": "LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios",
        "abstract": "arXiv:2505.11247v1 Announce Type: new  Abstract: Ensuring the safety and robustness of autonomous driving systems necessitates a comprehensive evaluation in safety-critical scenarios. However, these safety-critical scenarios are rare and difficult to collect from real-world driving data, posing significant challenges to effectively assessing the performance of autonomous vehicles. Typical existing methods often suffer from limited controllability and lack user-friendliness, as extensive expert knowledge is essentially required. To address these challenges, we propose LD-Scene, a novel framework that integrates Large Language Models (LLMs) with Latent Diffusion Models (LDMs) for user-controllable adversarial scenario generation through natural language. Our approach comprises an LDM that captures realistic driving trajectory distributions and an LLM-based guidance module that translates user queries into adversarial loss functions, facilitating the generation of scenarios aligned with user queries. The guidance module integrates an LLM-based Chain-of-Thought (CoT) code generator and an LLM-based code debugger, enhancing the controllability and robustness in generating guidance functions. Extensive experiments conducted on the nuScenes dataset demonstrate that LD-Scene achieves state-of-the-art performance in generating realistic, diverse, and effective adversarial scenarios. Furthermore, our framework provides fine-grained control over adversarial behaviors, thereby facilitating more effective testing tailored to specific driving scenarios.",
        "arxiv_id": "2505.11247",
        "ARXIVID": "2505.11247",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework (LD-Scene) for generating adversarial safety-critical driving scenarios, which is a new method in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.10610": {
        "authors": [
            "Zhaowei Wang",
            "Wenhao Yu",
            "Xiyu Ren",
            "Jipeng Zhang",
            "Yu Zhao",
            "Rohit Saxena",
            "Liang Cheng",
            "Ginny Wong",
            "Simon See",
            "Pasquale Minervini",
            "Yangqiu Song",
            "Mark Steedman"
        ],
        "title": "MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly",
        "abstract": "arXiv:2505.10610v1 Announce Type: new  Abstract: The rapid extension of context windows in large vision-language models has given rise to long-context vision-language models (LCVLMs), which are capable of handling hundreds of images with interleaved text tokens in a single forward pass. In this work, we introduce MMLongBench, the first benchmark covering a diverse set of long-context vision-language tasks, to evaluate LCVLMs effectively and thoroughly. MMLongBench is composed of 13,331 examples spanning five different categories of downstream tasks, such as Visual RAG and Many-Shot ICL. It also provides broad coverage of image types, including various natural and synthetic images. To assess the robustness of the models to different input lengths, all examples are delivered at five standardized input lengths (8K-128K tokens) via a cross-modal tokenization scheme that combines vision patches and text tokens. Through a thorough benchmarking of 46 closed-source and open-source LCVLMs, we provide a comprehensive analysis of the current models' vision-language long-context ability. Our results show that: i) performance on a single task is a weak proxy for overall long-context capability; ii) both closed-source and open-source models face challenges in long-context vision-language tasks, indicating substantial room for future improvement; iii) models with stronger reasoning ability tend to exhibit better long-context performance. By offering wide task coverage, various image types, and rigorous length control, MMLongBench provides the missing foundation for diagnosing and advancing the next generation of LCVLMs.",
        "arxiv_id": "2505.10610",
        "ARXIVID": "2505.10610",
        "COMMENT": "Matches criterion 3 as it introduces MMLongBench, a benchmark for long-context vision-language models, addressing a novel angle in evaluating these models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.10827": {
        "authors": [
            "Nail Ibrahimli",
            "Julian F. P. Kooij",
            "Liangliang Nan"
        ],
        "title": "NeuSEditor: From Multi-View Images to Text-Guided Neural Surface Edits",
        "abstract": "arXiv:2505.10827v1 Announce Type: new  Abstract: Implicit surface representations are valued for their compactness and continuity, but they pose significant challenges for editing. Despite recent advancements, existing methods often fail to preserve identity and maintain geometric consistency during editing. To address these challenges, we present NeuSEditor, a novel method for text-guided editing of neural implicit surfaces derived from multi-view images. NeuSEditor introduces an identity-preserving architecture that efficiently separates scenes into foreground and background, enabling precise modifications without altering the scene-specific elements. Our geometry-aware distillation loss significantly enhances rendering and geometric quality. Our method simplifies the editing workflow by eliminating the need for continuous dataset updates and source prompting. NeuSEditor outperforms recent state-of-the-art methods like PDS and InstructNeRF2NeRF, delivering superior quantitative and qualitative results. For more visual results, visit: neuseditor.github.io.",
        "arxiv_id": "2505.10827",
        "ARXIVID": "2505.10827",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for text-guided editing of neural implicit surfaces, which relates to spatial understanding and intelligence.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.10671": {
        "authors": [
            "Yuki Kawana",
            "Shintaro Shiba",
            "Quan Kong",
            "Norimasa Kobori"
        ],
        "title": "GA3CE: Unconstrained 3D Gaze Estimation with Gaze-Aware 3D Context Encoding",
        "abstract": "arXiv:2505.10671v1 Announce Type: new  Abstract: We propose a novel 3D gaze estimation approach that learns spatial relationships between the subject and objects in the scene, and outputs 3D gaze direction. Our method targets unconstrained settings, including cases where close-up views of the subject's eyes are unavailable, such as when the subject is distant or facing away. Previous approaches typically rely on either 2D appearance alone or incorporate limited spatial cues using depth maps in the non-learnable post-processing step. Estimating 3D gaze direction from 2D observations in these scenarios is challenging; variations in subject pose, scene layout, and gaze direction, combined with differing camera poses, yield diverse 2D appearances and 3D gaze directions even when targeting the same 3D scene. To address this issue, we propose GA3CE: Gaze-Aware 3D Context Encoding. Our method represents subject and scene using 3D poses and object positions, treating them as 3D context to learn spatial relationships in 3D space. Inspired by human vision, we align this context in an egocentric space, significantly reducing spatial complexity. Furthermore, we propose D$^3$ (direction-distance-decomposed) positional encoding to better capture the spatial relationship between 3D context and gaze direction in direction and distance space. Experiments demonstrate substantial improvements, reducing mean angle error by 13%-37% compared to leading baselines on benchmark datasets in single-frame settings.",
        "arxiv_id": "2505.10671",
        "ARXIVID": "2505.10671",
        "COMMENT": "Matches criterion 1. Proposes a novel 3D gaze estimation method (GA3CE) that learns spatial relationships in 3D space, improving spatial understanding in unconstrained settings.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2505.10996": {
        "authors": [
            "Yunkang Cao",
            "Yuqi Cheng",
            "Xiaohao Xu",
            "Yiheng Zhang",
            "Yihan Sun",
            "Yuxiang Tan",
            "Yuxin Zhang",
            "Xiaonan Huang",
            "Weiming Shen"
        ],
        "title": "Visual Anomaly Detection under Complex View-Illumination Interplay: A Large-Scale Benchmark",
        "abstract": "arXiv:2505.10996v1 Announce Type: new  Abstract: The practical deployment of Visual Anomaly Detection (VAD) systems is hindered by their sensitivity to real-world imaging variations, particularly the complex interplay between viewpoint and illumination which drastically alters defect visibility. Current benchmarks largely overlook this critical challenge. We introduce Multi-View Multi-Illumination Anomaly Detection (M2AD), a new large-scale benchmark comprising 119,880 high-resolution images designed explicitly to probe VAD robustness under such interacting conditions. By systematically capturing 999 specimens across 10 categories using 12 synchronized views and 10 illumination settings (120 configurations total), M2AD enables rigorous evaluation. We establish two evaluation protocols: M2AD-Synergy tests the ability to fuse information across diverse configurations, and M2AD-Invariant measures single-image robustness against realistic view-illumination effects. Our extensive benchmarking shows that state-of-the-art VAD methods struggle significantly on M2AD, demonstrating the profound challenge posed by view-illumination interplay. This benchmark serves as an essential tool for developing and validating VAD methods capable of overcoming real-world complexities. Our full dataset and test suite will be released at https://hustcyq.github.io/M2AD to facilitate the field.",
        "arxiv_id": "2505.10996",
        "ARXIVID": "2505.10996",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for visual anomaly detection under complex conditions, focusing on robustness and real-world challenges.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2505.11493": {
        "authors": [
            "Yusu Qian",
            "Jiasen Lu",
            "Tsu-Jui Fu",
            "Xinze Wang",
            "Chen Chen",
            "Yinfei Yang",
            "Wenze Hu",
            "Zhe Gan"
        ],
        "title": "GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing",
        "abstract": "arXiv:2505.11493v1 Announce Type: new  Abstract: Editing images using natural language instructions has become a natural and expressive way to modify visual content; yet, evaluating the performance of such models remains challenging. Existing evaluation approaches often rely on image-text similarity metrics like CLIP, which lack precision. In this work, we introduce a new benchmark designed to evaluate text-guided image editing models in a more grounded manner, along two critical dimensions: (i) functional correctness, assessed via automatically generated multiple-choice questions that verify whether the intended change was successfully applied; and (ii) image content preservation, which ensures that non-targeted regions of the image remain visually consistent using an object-aware masking technique and preservation scoring. The benchmark includes over 1000 high-quality editing examples across 20 diverse content categories, each annotated with detailed editing instructions, evaluation questions, and spatial object masks. We conduct a large-scale study comparing GPT-Image-1, the latest flagship in the text-guided image editing space, against several state-of-the-art editing models, and validate our automatic metrics against human ratings. Results show that GPT-Image-1 leads in instruction-following accuracy, but often over-modifies irrelevant image regions, highlighting a key trade-off in the current model behavior. GIE-Bench provides a scalable, reproducible framework for advancing more accurate evaluation of text-guided image editing.",
        "arxiv_id": "2505.11493",
        "ARXIVID": "2505.11493",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for evaluating text-guided image editing models with novel evaluation metrics.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2505.11129": {
        "authors": [
            "Makoto Yamada",
            "Kian Ming A. Chai",
            "Ayoub Rhim",
            "Satoki Ishikawa",
            "Mohammad Sabokrou",
            "Yao-Hung Hubert Tsai"
        ],
        "title": "PhiNet v2: A Mask-Free Brain-Inspired Vision Foundation Model from Video",
        "abstract": "arXiv:2505.11129v1 Announce Type: new  Abstract: Recent advances in self-supervised learning (SSL) have revolutionized computer vision through innovative architectures and learning objectives, yet they have not fully leveraged insights from biological visual processing systems. Recently, a brain-inspired SSL model named PhiNet was proposed; it is based on a ResNet backbone and operates on static image inputs with strong augmentation. In this paper, we introduce PhiNet v2, a novel Transformer-based architecture that processes temporal visual input (that is, sequences of images) without relying on strong augmentation. Our model leverages variational inference to learn robust visual representations from continuous input streams, similar to human visual processing. Through extensive experimentation, we demonstrate that PhiNet v2 achieves competitive performance compared to state-of-the-art vision foundation models, while maintaining the ability to learn from sequential input without strong data augmentation. This work represents a significant step toward more biologically plausible computer vision systems that process visual information in a manner more closely aligned with human cognitive processes.",
        "arxiv_id": "2505.11129",
        "ARXIVID": "2505.11129",
        "COMMENT": "Matches criterion 4 as it introduces a vision foundation model inspired by biological visual processing, with a novel Transformer-based architecture for sequential input.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2505.11289": {
        "authors": [
            "Reginald McLean",
            "Evangelos Chatzaroulas",
            "Luc McCutcheon",
            "Frank R\\\"oder",
            "Tianhe Yu",
            "Zhanpeng He",
            "K. R. Zentner",
            "Ryan Julian",
            "J K Terry",
            "Isaac Woungang",
            "Nariman Farsad",
            "Pablo Samuel Castro"
        ],
        "title": "Meta-World+: An Improved, Standardized, RL Benchmark",
        "abstract": "arXiv:2505.11289v1 Announce Type: new  Abstract: Meta-World is widely used for evaluating multi-task and meta-reinforcement learning agents, which are challenged to master diverse skills simultaneously. Since its introduction however, there have been numerous undocumented changes which inhibit a fair comparison of algorithms. This work strives to disambiguate these results from the literature, while also leveraging the past versions of Meta-World to provide insights into multi-task and meta-reinforcement learning benchmark design. Through this process we release a new open-source version of Meta-World (https://github.com/Farama-Foundation/Metaworld/) that has full reproducibility of past results, is more technically ergonomic, and gives users more control over the tasks that are included in a task set.",
        "arxiv_id": "2505.11289",
        "ARXIVID": "2505.11289",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for multi-task and meta-reinforcement learning with a focus on reproducibility and ergonomic improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2505.10888": {
        "authors": [
            "Saad Manzur",
            "Bryan Vela",
            "Brandon Vela",
            "Aditya Agrawal",
            "Lan-Anh Dang-Vu",
            "David Li",
            "Wayne Hayes"
        ],
        "title": "PoseBench3D: A Cross-Dataset Analysis Framework for 3D Human Pose Estimation",
        "abstract": "arXiv:2505.10888v1 Announce Type: new  Abstract: Reliable three-dimensional human pose estimation is becoming increasingly important for real-world applications, yet much of prior work has focused solely on the performance within a single dataset. In practice, however, systems must adapt to diverse viewpoints, environments, and camera setups -- conditions that differ significantly from those encountered during training, which is often the case in real-world scenarios. To address these challenges, we present a standardized testing environment in which each method is evaluated on a variety of datasets, ensuring consistent and fair cross-dataset comparisons -- allowing for the analysis of methods on previously unseen data. Therefore, we propose PoseBench3D, a unified framework designed to systematically re-evaluate prior and future models across four of the most widely used datasets for human pose estimation -- with the framework able to support novel and future datasets as the field progresses. Through a unified interface, our framework provides datasets in a pre-configured yet easily modifiable format, ensuring compatibility with diverse model architectures. We re-evaluated the work of 18 methods, either trained or gathered from existing literature, and reported results using both Mean Per Joint Position Error (MPJPE) and Procrustes Aligned Mean Per Joint Position Error (PA-MPJPE) metrics, yielding more than 100 novel cross-dataset evaluation results. Additionally, we analyze performance differences resulting from various pre-processing techniques and dataset preparation parameters -- offering further insight into model generalization capabilities.",
        "arxiv_id": "2505.10888",
        "ARXIVID": "2505.10888",
        "COMMENT": "This paper matches criterion 3 as it introduces a new benchmark framework (PoseBench3D) for cross-dataset analysis in 3D human pose estimation, focusing on generalization and unseen data.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.10823": {
        "authors": [
            "Xue Li",
            "Jameson Merkow",
            "Noel C. F. Codella",
            "Alberto Santamaria-Pang",
            "Naiteek Sangani",
            "Alexander Ersoy",
            "Christopher Burt",
            "John W. Garrett",
            "Richard J. Bruce",
            "Joshua D. Warner",
            "Tyler Bradshaw",
            "Ivan Tarapov",
            "Matthew P. Lungren",
            "Alan B. McMillan"
        ],
        "title": "From Embeddings to Accuracy: Comparing Foundation Models for Radiographic Classification",
        "abstract": "arXiv:2505.10823v1 Announce Type: new  Abstract: Foundation models, pretrained on extensive datasets, have significantly advanced machine learning by providing robust and transferable embeddings applicable to various domains, including medical imaging diagnostics. This study evaluates the utility of embeddings derived from both general-purpose and medical domain-specific foundation models for training lightweight adapter models in multi-class radiography classification, focusing specifically on tube placement assessment. A dataset comprising 8842 radiographs classified into seven distinct categories was employed to extract embeddings using six foundation models: DenseNet121, BiomedCLIP, Med-Flamingo, MedImageInsight, Rad-DINO, and CXR-Foundation. Adapter models were subsequently trained using classical machine learning algorithms. Among these combinations, MedImageInsight embeddings paired with an support vector machine adapter yielded the highest mean area under the curve (mAUC) at 93.8%, followed closely by Rad-DINO (91.1%) and CXR-Foundation (89.0%). In comparison, BiomedCLIP and DenseNet121 exhibited moderate performance with mAUC scores of 83.0% and 81.8%, respectively, whereas Med-Flamingo delivered the lowest performance at 75.1%. Notably, most adapter models demonstrated computational efficiency, achieving training within one minute and inference within seconds on CPU, underscoring their practicality for clinical applications. Furthermore, fairness analyses on adapters trained on MedImageInsight-derived embeddings indicated minimal disparities, with gender differences in performance within 2% and standard deviations across age groups not exceeding 3%. These findings confirm that foundation model embeddings-especially those from MedImageInsight-facilitate accurate, computationally efficient, and equitable diagnostic classification using lightweight adapters for radiographic image analysis.",
        "arxiv_id": "2505.10823",
        "ARXIVID": "2505.10823",
        "COMMENT": "Matches criterion 4 as it evaluates foundation models for radiographic classification, which is an application of vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.10601": {
        "authors": [
            "Chuang Chen",
            "Wenyi Ge"
        ],
        "title": "SRMamba: Mamba for Super-Resolution of LiDAR Point Clouds",
        "abstract": "arXiv:2505.10601v1 Announce Type: new  Abstract: In recent years, range-view-based LiDAR point cloud super-resolution techniques attract significant attention as a low-cost method for generating higher-resolution point cloud data. However, due to the sparsity and irregular structure of LiDAR point clouds, the point cloud super-resolution problem remains a challenging topic, especially for point cloud upsampling under novel views. In this paper, we propose SRMamba, a novel method for super-resolution of LiDAR point clouds in sparse scenes, addressing the key challenge of recovering the 3D spatial structure of point clouds from novel views. Specifically, we implement projection technique based on Hough Voting and Hole Compensation strategy to eliminate horizontally linear holes in range image. To improve the establishment of long-distance dependencies and to focus on potential geometric features in vertical 3D space, we employ Visual State Space model and Multi-Directional Scanning mechanism to mitigate the loss of 3D spatial structural information due to the range image. Additionally, an asymmetric U-Net network adapts to the input characteristics of LiDARs with different beam counts, enabling super-resolution reconstruction for multi-beam point clouds. We conduct a series of experiments on multiple challenging public LiDAR datasets (SemanticKITTI and nuScenes), and SRMamba demonstrates significant superiority over other algorithms in both qualitative and quantitative evaluations.",
        "arxiv_id": "2505.10601",
        "ARXIVID": "2505.10601",
        "COMMENT": "Matches criterion 1 as it focuses on super-resolution of LiDAR point clouds, which involves spatial understanding and intelligence.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.11454": {
        "authors": [
            "Shaina Raza",
            "Aravind Narayanan",
            "Vahid Reza Khazaie",
            "Ashmal Vayani",
            "Mukund S. Chettiar",
            "Amandeep Singh",
            "Mubarak Shah",
            "Deval Pandya"
        ],
        "title": "HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation",
        "abstract": "arXiv:2505.11454v1 Announce Type: new  Abstract: Large multimodal models (LMMs) now excel on many vision language benchmarks, however, they still struggle with human centered criteria such as fairness, ethics, empathy, and inclusivity, key to aligning with human values. We introduce HumaniBench, a holistic benchmark of 32K real-world image question pairs, annotated via a scalable GPT4o assisted pipeline and exhaustively verified by domain experts. HumaniBench evaluates seven Human Centered AI (HCAI) principles: fairness, ethics, understanding, reasoning, language inclusivity, empathy, and robustness, across seven diverse tasks, including open and closed ended visual question answering (VQA), multilingual QA, visual grounding, empathetic captioning, and robustness tests. Benchmarking 15 state of the art LMMs (open and closed source) reveals that proprietary models generally lead, though robustness and visual grounding remain weak points. Some open-source models also struggle to balance accuracy with adherence to human-aligned principles. HumaniBench is the first benchmark purpose built around HCAI principles. It provides a rigorous testbed for diagnosing alignment gaps and guiding LMMs toward behavior that is both accurate and socially responsible. Dataset, annotation prompts, and evaluation code are available at: https://vectorinstitute.github.io/HumaniBench",
        "arxiv_id": "2505.11454",
        "ARXIVID": "2505.11454",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (HumaniBench) for evaluating large multimodal models with a focus on human-centered AI principles.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.10784": {
        "authors": [
            "Qiushi Guo",
            "Jason Rambach"
        ],
        "title": "SynRailObs: A Synthetic Dataset for Obstacle Detection in Railway Scenarios",
        "abstract": "arXiv:2505.10784v1 Announce Type: new  Abstract: Detecting potential obstacles in railway environments is critical for preventing serious accidents. Identifying a broad range of obstacle categories under complex conditions requires large-scale datasets with precisely annotated, high-quality images. However, existing publicly available datasets fail to meet these requirements, thereby hindering progress in railway safety research. To address this gap, we introduce SynRailObs, a high-fidelity synthetic dataset designed to represent a diverse range of weather conditions and geographical features. Furthermore, diffusion models are employed to generate rare and difficult-to-capture obstacles that are typically challenging to obtain in real-world scenarios. To evaluate the effectiveness of SynRailObs, we perform experiments in real-world railway environments, testing on both ballasted and ballastless tracks across various weather conditions. The results demonstrate that SynRailObs holds substantial potential for advancing obstacle detection in railway safety applications. Models trained on this dataset show consistent performance across different distances and environmental conditions. Moreover, the model trained on SynRailObs exhibits zero-shot capabilities, which are essential for applications in security-sensitive domains. The data is available in https://www.kaggle.com/datasets/qiushi910/synrailobs.",
        "arxiv_id": "2505.10784",
        "ARXIVID": "2505.10784",
        "COMMENT": "Matches criterion 3 as it introduces a synthetic dataset for obstacle detection in railway scenarios, focusing on diverse conditions and rare obstacles.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.11049": {
        "authors": [
            "Yue Liu",
            "Shengfang Zhai",
            "Mingzhe Du",
            "Yulin Chen",
            "Tri Cao",
            "Hongcheng Gao",
            "Cheng Wang",
            "Xinfeng Li",
            "Kun Wang",
            "Junfeng Fang",
            "Jiaheng Zhang",
            "Bryan Hooi"
        ],
        "title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning",
        "abstract": "arXiv:2505.11049v1 Announce Type: new  Abstract: To enhance the safety of VLMs, this paper introduces a novel reasoning-based VLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the guard model to deliberatively reason before making moderation decisions via online RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with 123K samples and 631K reasoning steps, spanning text, image, and text-image inputs. Then, based on it, we cold-start our model's reasoning ability via SFT. In addition, we further enhance reasoning regarding moderation through online RL. Concretely, to enhance diversity and difficulty of samples, we conduct rejection sampling followed by data augmentation via the proposed safety-aware data concatenation. Besides, we use a dynamic clipping parameter to encourage exploration in early stages and exploitation in later stages. To balance performance and token efficiency, we design a length-aware safety reward that integrates accuracy, format, and token cost. Extensive experiments demonstrate the superiority of our model. Remarkably, it surpasses the runner-up by 19.27% F1 score on average. We release data, code, and models (3B/7B) of GuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/",
        "arxiv_id": "2505.11049",
        "ARXIVID": "2505.11049",
        "COMMENT": "Does not directly match any specific criterion but is related to improving reasoning in vision-language models, which aligns with your friend's general interest.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2505.10989": {
        "authors": [
            "Haiyang Shen",
            "Hang Yan",
            "Zhongshi Xing",
            "Mugeng Liu",
            "Yue Li",
            "Zhiyang Chen",
            "Yuxiang Wang",
            "Jiuzheng Wang",
            "Yun Ma"
        ],
        "title": "RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization",
        "abstract": "arXiv:2505.10989v1 Announce Type: new  Abstract: RAG can enhance the performance of LLMs on knowledge-intensive tasks. Various RAG paradigms, including vanilla, planning-based, and iterative RAG, are built upon 2 cores: the retriever, which should robustly select relevant documents across complex queries, and the generator, which should faithfully synthesize responses. However, existing retrievers rely heavily on public knowledge and struggle with queries of varying logical complexity and clue completeness, while generators frequently face fidelity problems. In this work, we introduce RAGSynth, a framework that includes a data construction modeling and a corresponding synthetic data generation implementation, designed to optimize retriever robustness and generator fidelity. Additionally, we present SynthBench, a benchmark encompassing 8 domain-specific documents across 4 domains, featuring diverse query complexities, clue completeness, and fine-grained citation granularity. Leveraging RAGSynth, we generate a large-scale synthetic dataset, including single and multi-hop. Extensive experiments demonstrate that the synthetic data significantly improves the robustness of the retrievers and the fidelity of the generators. Additional evaluations confirm that RAGSynth can also generalize well across different domains. By integrating the optimized retrievers into various RAG paradigms, we consistently observe enhanced RAG system performance. We have open-sourced the implementation on https://github.com/EachSheep/RAGSynth.",
        "arxiv_id": "2505.10989",
        "ARXIVID": "2505.10989",
        "COMMENT": "Does not directly match any specific criterion but is related to optimizing retrievers and generators in retrieval-augmented generation, which is tangentially relevant to vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2505.11293": {
        "authors": [
            "Raghuveer Thirukovalluru",
            "Rui Meng",
            "Ye Liu",
            "Karthikeyan K",
            "Mingyi Su",
            "Ping Nie",
            "Semih Yavuz",
            "Yingbo Zhou",
            "Wenhu Chen",
            "Bhuwan Dhingra"
        ],
        "title": "Breaking the Batch Barrier (B3) of Contrastive Learning via Smart Batch Mining",
        "abstract": "arXiv:2505.11293v1 Announce Type: new  Abstract: Contrastive learning (CL) is a prevalent technique for training embedding models, which pulls semantically similar examples (positives) closer in the representation space while pushing dissimilar ones (negatives) further apart. A key source of negatives are 'in-batch' examples, i.e., positives from other examples in the batch. Effectiveness of such models is hence strongly influenced by the size and quality of training batches. In this work, we propose 'Breaking the Batch Barrier' (B3), a novel batch construction strategy designed to curate high-quality batches for CL. Our approach begins by using a pretrained teacher embedding model to rank all examples in the dataset, from which a sparse similarity graph is constructed. A community detection algorithm is then applied to this graph to identify clusters of examples that serve as strong negatives for one another. The clusters are then used to construct batches that are rich in in-batch negatives. Empirical results on the MMEB multimodal embedding benchmark (36 tasks) demonstrate that our method sets a new state of the art, outperforming previous best methods by +1.3 and +2.9 points at the 7B and 2B model scales, respectively. Notably, models trained with B3 surpass existing state-of-the-art results even with a batch size as small as 64, which is 4-16x smaller than that required by other methods.",
        "arxiv_id": "2505.11293",
        "ARXIVID": "2505.11293",
        "COMMENT": "Does not directly match any specific criterion but introduces a novel batch construction strategy for contrastive learning, which is tangentially relevant to your friend's interest in clever statistical tricks.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2505.11131": {
        "authors": [
            "Feiran Li",
            "Qianqian Xu",
            "Shilong Bao",
            "Zhiyong Yang",
            "Xiaochun Cao",
            "Qingming Huang"
        ],
        "title": "One Image is Worth a Thousand Words: A Usability Preservable Text-Image Collaborative Erasing Framework",
        "abstract": "arXiv:2505.11131v1 Announce Type: new  Abstract: Concept erasing has recently emerged as an effective paradigm to prevent text-to-image diffusion models from generating visually undesirable or even harmful content. However, current removal methods heavily rely on manually crafted text prompts, making it challenging to achieve a high erasure (efficacy) while minimizing the impact on other benign concepts (usability). In this paper, we attribute the limitations to the inherent gap between the text and image modalities, which makes it hard to transfer the intricately entangled concept knowledge from text prompts to the image generation process. To address this, we propose a novel solution by directly integrating visual supervision into the erasure process, introducing the first text-image Collaborative Concept Erasing (Co-Erasing) framework. Specifically, Co-Erasing describes the concept jointly by text prompts and the corresponding undesirable images induced by the prompts, and then reduces the generating probability of the target concept through negative guidance. This approach effectively bypasses the knowledge gap between text and image, significantly enhancing erasure efficacy. Additionally, we design a text-guided image concept refinement strategy that directs the model to focus on visual features most relevant to the specified text concept, minimizing disruption to other benign concepts. Finally, comprehensive experiments suggest that Co-Erasing outperforms state-of-the-art erasure approaches significantly with a better trade-off between efficacy and usability. Codes are available at https://github.com/Ferry-Li/Co-Erasing.",
        "arxiv_id": "2505.11131",
        "ARXIVID": "2505.11131",
        "COMMENT": "Does not directly match any specific criterion but is related to text-image collaborative erasing, which is tangentially relevant to vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2505.11344": {
        "authors": [
            "Chenyu Huang",
            "Peng Ye",
            "Shenghe Zheng",
            "Xiaohui Wang",
            "Lei Bai",
            "Tao Chen",
            "Wanli Ouyang"
        ],
        "title": "Dynamic Base model Shift for Delta Compression",
        "abstract": "arXiv:2505.11344v1 Announce Type: new  Abstract: Transformer-based models with the pretrain-finetune paradigm bring about significant progress, along with the heavy storage and deployment costs of finetuned models on multiple tasks. Delta compression attempts to lower the costs by reducing the redundancy of delta parameters (i.e., the difference between the finetuned and pre-trained model weights) through pruning or quantization. However, existing methods by default employ the pretrained model as the base model and compress the delta parameters for every task, which may causes significant performance degradation, especially when the compression rate is extremely high. To tackle this issue, we investigate the impact of different base models on the performance of delta compression and find that the pre-trained base model can hardly be optimal. To this end, we propose Dynamic Base Model Shift (DBMS), which dynamically adapts the base model to the target task before performing delta compression. Specifically, we adjust two parameters, which respectively determine the magnitude of the base model shift and the overall scale of delta compression, to boost the compression performance on each task. Through low-cost learning of these two parameters, our DBMS can maintain most of the finetuned model's performance even under an extremely high compression ratio setting, significantly surpassing existing methods. Moreover, our DBMS is orthogonal and can be integrated with a variety of other methods, and it has been evaluated across different types of models including language, vision transformer, and multi-modal models.",
        "arxiv_id": "2505.11344",
        "ARXIVID": "2505.11344",
        "COMMENT": "This paper does not directly match any criteria but discusses a novel approach to delta compression in transformer-based models, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.10841": {
        "authors": [
            "Jaeguk Kim",
            "Jaewoo Park",
            "Keuntek Lee",
            "Nam Ik Cho"
        ],
        "title": "RefPose: Leveraging Reference Geometric Correspondences for Accurate 6D Pose Estimation of Unseen Objects",
        "abstract": "arXiv:2505.10841v1 Announce Type: new  Abstract: Estimating the 6D pose of unseen objects from monocular RGB images remains a challenging problem, especially due to the lack of prior object-specific knowledge. To tackle this issue, we propose RefPose, an innovative approach to object pose estimation that leverages a reference image and geometric correspondence as guidance. RefPose first predicts an initial pose by using object templates to render the reference image and establish the geometric correspondence needed for the refinement stage. During the refinement stage, RefPose estimates the geometric correspondence of the query based on the generated references and iteratively refines the pose through a render-and-compare approach. To enhance this estimation, we introduce a correlation volume-guided attention mechanism that effectively captures correlations between the query and reference images. Unlike traditional methods that depend on pre-defined object models, RefPose dynamically adapts to new object shapes by leveraging a reference image and geometric correspondence. This results in robust performance across previously unseen objects. Extensive evaluation on the BOP benchmark datasets shows that RefPose achieves state-of-the-art results while maintaining a competitive runtime.",
        "arxiv_id": "2505.10841",
        "ARXIVID": "2505.10841",
        "COMMENT": "This paper does not directly match any criteria but discusses a novel approach to 6D pose estimation using reference geometric correspondences, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.11245": {
        "authors": [
            "Fu-Yun Wang",
            "Yunhao Shui",
            "Jingtan Piao",
            "Keqiang Sun",
            "Hongsheng Li"
        ],
        "title": "Diffusion-NPO: Negative Preference Optimization for Better Preference Aligned Generation of Diffusion Models",
        "abstract": "arXiv:2505.11245v1 Announce Type: new  Abstract: Diffusion models have made substantial advances in image generation, yet models trained on large, unfiltered datasets often yield outputs misaligned with human preferences. Numerous methods have been proposed to fine-tune pre-trained diffusion models, achieving notable improvements in aligning generated outputs with human preferences. However, we argue that existing preference alignment methods neglect the critical role of handling unconditional/negative-conditional outputs, leading to a diminished capacity to avoid generating undesirable outcomes. This oversight limits the efficacy of classifier-free guidance~(CFG), which relies on the contrast between conditional generation and unconditional/negative-conditional generation to optimize output quality. In response, we propose a straightforward but versatile effective approach that involves training a model specifically attuned to negative preferences. This method does not require new training strategies or datasets but rather involves minor modifications to existing techniques. Our approach integrates seamlessly with models such as SD1.5, SDXL, video diffusion models and models that have undergone preference optimization, consistently enhancing their alignment with human preferences.",
        "arxiv_id": "2505.11245",
        "ARXIVID": "2505.11245",
        "COMMENT": "This paper does not directly match any criteria but discusses a novel approach to preference alignment in diffusion models, which might be tangentially interesting for generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.10834": {
        "authors": [
            "Achintha Wijesinghe",
            "Weiwei Wang",
            "Suchinthaka Wanninayaka",
            "Songyang Zhang",
            "Zhi Ding"
        ],
        "title": "TACO: Rethinking Semantic Communications with Task Adaptation and Context Embedding",
        "abstract": "arXiv:2505.10834v1 Announce Type: new  Abstract: Recent advancements in generative artificial intelligence have introduced groundbreaking approaches to innovating next-generation semantic communication, which prioritizes conveying the meaning of a message rather than merely transmitting raw data. A fundamental challenge in semantic communication lies in accurately identifying and extracting the most critical semantic information while adapting to downstream tasks without degrading performance, particularly when the objective at the receiver may evolve over time. To enable flexible adaptation to multiple tasks at the receiver, this work introduces a novel semantic communication framework, which is capable of jointly capturing task-specific information to enhance downstream task performance and contextual information. Through rigorous experiments on popular image datasets and computer vision tasks, our framework shows promising improvement compared to existing work, including superior performance in downstream tasks, better generalizability, ultra-high bandwidth efficiency, and low reconstruction latency.",
        "arxiv_id": "2505.10834",
        "ARXIVID": "2505.10834",
        "COMMENT": "This paper does not directly match any of the criteria but discusses a novel semantic communication framework with task adaptation and context embedding, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.11075": {
        "authors": [
            "Jianghang Lin",
            "Yilin Lu",
            "Yunhang Shen",
            "Chaoyang Zhu",
            "Shengchuan Zhang",
            "Liujuan Cao",
            "Rongrong Ji"
        ],
        "title": "Pseudo-Label Quality Decoupling and Correction for Semi-Supervised Instance Segmentation",
        "abstract": "arXiv:2505.11075v1 Announce Type: new  Abstract: Semi-Supervised Instance Segmentation (SSIS) involves classifying and grouping image pixels into distinct object instances using limited labeled data. This learning paradigm usually faces a significant challenge of unstable performance caused by noisy pseudo-labels of instance categories and pixel masks. We find that the prevalent practice of filtering instance pseudo-labels assessing both class and mask quality with a single score threshold, frequently leads to compromises in the trade-off between the qualities of class and mask labels. In this paper, we introduce a novel Pseudo-Label Quality Decoupling and Correction (PL-DC) framework for SSIS to tackle the above challenges. Firstly, at the instance level, a decoupled dual-threshold filtering mechanism is designed to decouple class and mask quality estimations for instance-level pseudo-labels, thereby independently controlling pixel classifying and grouping qualities. Secondly, at the category level, we introduce a dynamic instance category correction module to dynamically correct the pseudo-labels of instance categories, effectively alleviating category confusion. Lastly, we introduce a pixel-level mask uncertainty-aware mechanism at the pixel level to re-weight the mask loss for different pixels, thereby reducing the impact of noise introduced by pixel-level mask pseudo-labels. Extensive experiments on the COCO and Cityscapes datasets demonstrate that the proposed PL-DC achieves significant performance improvements, setting new state-of-the-art results for SSIS. Notably, our PL-DC shows substantial gains even with minimal labeled data, achieving an improvement of +11.6 mAP with just 1% COCO labeled data and +15.5 mAP with 5% Cityscapes labeled data. The code will be public.",
        "arxiv_id": "2505.11075",
        "ARXIVID": "2505.11075",
        "COMMENT": "Does not match any specific criteria. Focuses on semi-supervised instance segmentation with pseudo-label quality correction, which is not directly related to spatial understanding, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.11034": {
        "authors": [
            "Fabian Gr\\\"oger",
            "Simone Lionetti",
            "Philippe Gottfrois",
            "Alvaro Gonzalez-Jimenez",
            "Ludovic Amruthalingam",
            "Elisabeth Victoria Goessinger",
            "Hanna Lindemann",
            "Marie Bargiela",
            "Marie Hofbauer",
            "Omar Badri",
            "Philipp Tschandl",
            "Arash Koochek",
            "Matthew Groh",
            "Alexander A. Navarini",
            "Marc Pouly"
        ],
        "title": "CleanPatrick: A Benchmark for Image Data Cleaning",
        "abstract": "arXiv:2505.11034v1 Announce Type: new  Abstract: Robust machine learning depends on clean data, yet current image data cleaning benchmarks rely on synthetic noise or narrow human studies, limiting comparison and real-world relevance. We introduce CleanPatrick, the first large-scale benchmark for data cleaning in the image domain, built upon the publicly available Fitzpatrick17k dermatology dataset. We collect 496,377 binary annotations from 933 medical crowd workers, identify off-topic samples (4%), near-duplicates (21%), and label errors (22%), and employ an aggregation model inspired by item-response theory followed by expert review to derive high-quality ground truth. CleanPatrick formalizes issue detection as a ranking task and adopts typical ranking metrics mirroring real audit workflows. Benchmarking classical anomaly detectors, perceptual hashing, SSIM, Confident Learning, NoiseRank, and SelfClean, we find that, on CleanPatrick, self-supervised representations excel at near-duplicate detection, classical methods achieve competitive off-topic detection under constrained review budgets, and label-error detection remains an open challenge for fine-grained medical classification. By releasing both the dataset and the evaluation framework, CleanPatrick enables a systematic comparison of image-cleaning strategies and paves the way for more reliable data-centric artificial intelligence.",
        "arxiv_id": "2505.11034",
        "ARXIVID": "2505.11034",
        "COMMENT": "Does not match any specific criteria. Focuses on data cleaning for image datasets, which is not directly related to spatial understanding, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.11439": {
        "authors": [
            "Utsav Rai",
            "Haozheng Xu",
            "Stamatia Giannarou"
        ],
        "title": "SurgPose: Generalisable Surgical Instrument Pose Estimation using Zero-Shot Learning and Stereo Vision",
        "abstract": "arXiv:2505.11439v1 Announce Type: new  Abstract: Accurate pose estimation of surgical tools in Robot-assisted Minimally Invasive Surgery (RMIS) is essential for surgical navigation and robot control. While traditional marker-based methods offer accuracy, they face challenges with occlusions, reflections, and tool-specific designs. Similarly, supervised learning methods require extensive training on annotated datasets, limiting their adaptability to new tools. Despite their success in other domains, zero-shot pose estimation models remain unexplored in RMIS for pose estimation of surgical instruments, creating a gap in generalising to unseen surgical tools. This paper presents a novel 6 Degrees of Freedom (DoF) pose estimation pipeline for surgical instruments, leveraging state-of-the-art zero-shot RGB-D models like the FoundationPose and SAM-6D. We advanced these models by incorporating vision-based depth estimation using the RAFT-Stereo method, for robust depth estimation in reflective and textureless environments. Additionally, we enhanced SAM-6D by replacing its instance segmentation module, Segment Anything Model (SAM), with a fine-tuned Mask R-CNN, significantly boosting segmentation accuracy in occluded and complex conditions. Extensive validation reveals that our enhanced SAM-6D surpasses FoundationPose in zero-shot pose estimation of unseen surgical instruments, setting a new benchmark for zero-shot RGB-D pose estimation in RMIS. This work enhances the generalisability of pose estimation for unseen objects and pioneers the application of RGB-D zero-shot methods in RMIS.",
        "arxiv_id": "2505.11439",
        "ARXIVID": "2505.11439",
        "COMMENT": "This paper does not directly match any criteria but discusses zero-shot learning for surgical instrument pose estimation, which might be tangentially interesting for embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.10579": {
        "authors": [
            "Germani Elodie",
            "Selin T\\\"urk Ilayda",
            "Zeineddine Fatima",
            "Mourad Charbel",
            "Albarqouni Shadi"
        ],
        "title": "Bias and Generalizability of Foundation Models across Datasets in Breast Mammography",
        "abstract": "arXiv:2505.10579v1 Announce Type: new  Abstract: Over the past decades, computer-aided diagnosis tools for breast cancer have been developed to enhance screening procedures, yet their clinical adoption remains challenged by data variability and inherent biases. Although foundation models (FMs) have recently demonstrated impressive generalizability and transfer learning capabilities by leveraging vast and diverse datasets, their performance can be undermined by spurious correlations that arise from variations in image quality, labeling uncertainty, and sensitive patient attributes. In this work, we explore the fairness and bias of FMs for breast mammography classification by leveraging a large pool of datasets from diverse sources-including data from underrepresented regions and an in-house dataset. Our extensive experiments show that while modality-specific pre-training of FMs enhances performance, classifiers trained on features from individual datasets fail to generalize across domains. Aggregating datasets improves overall performance, yet does not fully mitigate biases, leading to significant disparities across under-represented subgroups such as extreme breast densities and age groups. Furthermore, while domain-adaptation strategies can reduce these disparities, they often incur a performance trade-off. In contrast, fairness-aware techniques yield more stable and equitable performance across subgroups. These findings underscore the necessity of incorporating rigorous fairness evaluations and mitigation strategies into FM-based models to foster inclusive and generalizable AI.",
        "arxiv_id": "2505.10579",
        "ARXIVID": "2505.10579",
        "COMMENT": "This paper explores fairness and bias in foundation models for breast mammography, which is related to vision foundation models but not directly aligned with the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.11182": {
        "authors": [
            "Yuzhuo Dai",
            "Jiaqi Jin",
            "Zhibin Dong",
            "Siwei Wang",
            "Xinwang Liu",
            "En Zhu",
            "Xihong Yang",
            "Xinbiao Gan",
            "Yu Feng"
        ],
        "title": "Imputation-free and Alignment-free: Incomplete Multi-view Clustering Driven by Consensus Semantic Learning",
        "abstract": "arXiv:2505.11182v1 Announce Type: new  Abstract: In incomplete multi-view clustering (IMVC), missing data induce prototype shifts within views and semantic inconsistencies across views. A feasible solution is to explore cross-view consistency in paired complete observations, further imputing and aligning the similarity relationships inherently shared across views. Nevertheless, existing methods are constrained by two-tiered limitations: (1) Neither instance- nor cluster-level consistency learning construct a semantic space shared across views to learn consensus semantics. The former enforces cross-view instances alignment, and wrongly regards unpaired observations with semantic consistency as negative pairs; the latter focuses on cross-view cluster counterparts while coarsely handling fine-grained intra-cluster relationships within views. (2) Excessive reliance on consistency results in unreliable imputation and alignment without incorporating view-specific cluster information. Thus, we propose an IMVC framework, imputation- and alignment-free for consensus semantics learning (FreeCSL). To bridge semantic gaps across all observations, we learn consensus prototypes from available data to discover a shared space, where semantically similar observations are pulled closer for consensus semantics learning. To capture semantic relationships within specific views, we design a heuristic graph clustering based on modularity to recover cluster structure with intra-cluster compactness and inter-cluster separation for cluster semantics enhancement. Extensive experiments demonstrate, compared to state-of-the-art competitors, FreeCSL achieves more confident and robust assignments on IMVC task.",
        "arxiv_id": "2505.11182",
        "ARXIVID": "2505.11182",
        "COMMENT": "Does not match any specific criterion but discusses clustering in multi-view data, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.11497": {
        "authors": [
            "Yushi Huang",
            "Ruihao Gong",
            "Jing Liu",
            "Yifu Ding",
            "Chengtao Lv",
            "Haotong Qin",
            "Jun Zhang"
        ],
        "title": "QVGen: Pushing the Limit of Quantized Video Generative Models",
        "abstract": "arXiv:2505.11497v1 Announce Type: new  Abstract: Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (e.g., 4-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules ($\\Phi$) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of $\\Phi$, we propose a rank-decay strategy that progressively eliminates $\\Phi$. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization $\\mathbf{\\gamma}$ to identify and decay low-contributing components. This strategy retains performance while zeroing out inference overhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs, with parameter sizes ranging from $1.3$B $\\sim14$B, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and $+8.43$ in Scene Consistency on VBench.",
        "arxiv_id": "2505.11497",
        "ARXIVID": "2505.11497",
        "COMMENT": "Does not match any specific criterion but discusses quantization-aware training for video generative models, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.11227": {
        "authors": [
            "Zhangying Feng",
            "Qianglong Chen",
            "Ning Lu",
            "Yongqian Li",
            "Siqi Cheng",
            "Shuangmu Peng",
            "Duyu Tang",
            "Shengcai Liu",
            "Zhirui Zhang"
        ],
        "title": "Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs",
        "abstract": "arXiv:2505.11227v1 Announce Type: new  Abstract: The development of reasoning capabilities represents a critical frontier in large language models (LLMs) research, where reinforcement learning (RL) and process reward models (PRMs) have emerged as predominant methodological frameworks. Contrary to conventional wisdom, empirical evidence from DeepSeek-R1 demonstrates that pure RL training focused on mathematical problem-solving can progressively enhance reasoning abilities without PRM integration, challenging the perceived necessity of process supervision. In this study, we conduct a systematic investigation of the relationship between RL training and PRM capabilities. Our findings demonstrate that problem-solving proficiency and process supervision capabilities represent complementary dimensions of reasoning that co-evolve synergistically during pure RL training. In particular, current PRMs underperform simple baselines like majority voting when applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To address this limitation, we propose Self-PRM, an introspective framework in which models autonomously evaluate and rerank their generated solutions through self-reward mechanisms. Although Self-PRM consistently improves the accuracy of the benchmark (particularly with larger sample sizes), analysis exposes persistent challenges: The approach exhibits low precision (<10\\%) on difficult problems, frequently misclassifying flawed solutions as valid. These analyses underscore the need for continued RL scaling to improve reward alignment and introspective accuracy. Overall, our findings suggest that PRM may not be essential for enhancing complex reasoning, as pure RL not only improves problem-solving skills but also inherently fosters robust PRM capabilities. We hope these findings provide actionable insights for building more reliable and self-aware complex reasoning models.",
        "arxiv_id": "2505.11227",
        "ARXIVID": "2505.11227",
        "COMMENT": "Does not match any specific criterion but discusses reinforcement learning and reasoning capabilities in LLMs, which is tangentially related to your friend's interest in multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}