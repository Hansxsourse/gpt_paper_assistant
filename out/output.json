{
    "2502.03266": {
        "authors": [
            "Ying Zhang",
            "Maoliang Yin",
            "Wenfu Bi",
            "Haibao Yan",
            "Shaohan Bian",
            "Cui-Hua Zhang",
            "Changchun Hua"
        ],
        "title": "ZISVFM: Zero-Shot Object Instance Segmentation in Indoor Robotic Environments with Vision Foundation Models",
        "abstract": "arXiv:2502.03266v1 Announce Type: new  Abstract: Service robots operating in unstructured environments must effectively recognize and segment unknown objects to enhance their functionality. Traditional supervised learningbased segmentation techniques require extensive annotated datasets, which are impractical for the diversity of objects encountered in real-world scenarios. Unseen Object Instance Segmentation (UOIS) methods aim to address this by training models on synthetic data to generalize to novel objects, but they often suffer from the simulation-to-reality gap. This paper proposes a novel approach (ZISVFM) for solving UOIS by leveraging the powerful zero-shot capability of the segment anything model (SAM) and explicit visual representations from a selfsupervised vision transformer (ViT). The proposed framework operates in three stages: (1) generating object-agnostic mask proposals from colorized depth images using SAM, (2) refining these proposals using attention-based features from the selfsupervised ViT to filter non-object masks, and (3) applying K-Medoids clustering to generate point prompts that guide SAM towards precise object segmentation. Experimental validation on two benchmark datasets and a self-collected dataset demonstrates the superior performance of ZISVFM in complex environments, including hierarchical settings such as cabinets, drawers, and handheld objects. Our source code is available at https://github.com/Yinmlmaoliang/zisvfm.",
        "arxiv_id": "2502.03266",
        "ARXIVID": "2502.03266",
        "COMMENT": "This paper matches criterion 4 as it leverages vision foundation models (SAM and ViT) for zero-shot object instance segmentation in robotic environments.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2502.02741": {
        "authors": [
            "Bin Xie",
            "Hao Tang",
            "Yan Yan",
            "Gady Agam"
        ],
        "title": "RFMedSAM 2: Automatic Prompt Refinement for Enhanced Volumetric Medical Image Segmentation with SAM 2",
        "abstract": "arXiv:2502.02741v1 Announce Type: new  Abstract: Segment Anything Model 2 (SAM 2), a prompt-driven foundation model extending SAM to both image and video domains, has shown superior zero-shot performance compared to its predecessor. Building on SAM's success in medical image segmentation, SAM 2 presents significant potential for further advancement. However, similar to SAM, SAM 2 is limited by its output of binary masks, inability to infer semantic labels, and dependence on precise prompts for the target object area. Additionally, direct application of SAM and SAM 2 to medical image segmentation tasks yields suboptimal results. In this paper, we explore the upper performance limit of SAM 2 using custom fine-tuning adapters, achieving a Dice Similarity Coefficient (DSC) of 92.30% on the BTCV dataset, surpassing the state-of-the-art nnUNet by 12%. Following this, we address the prompt dependency by investigating various prompt generators. We introduce a UNet to autonomously generate predicted masks and bounding boxes, which serve as input to SAM 2. Subsequent dual-stage refinements by SAM 2 further enhance performance. Extensive experiments show that our method achieves state-of-the-art results on the AMOS2022 dataset, with a Dice improvement of 2.9% compared to nnUNet, and outperforms nnUNet by 6.4% on the BTCV dataset.",
        "arxiv_id": "2502.02741",
        "ARXIVID": "2502.02741",
        "COMMENT": "This paper matches criterion 4 as it explores the use of SAM 2, a vision foundation model, for medical image segmentation and introduces novel prompt refinement techniques.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.03465": {
        "authors": [
            "Qiuhong Shen",
            "Xuanyu Yi",
            "Mingbao Lin",
            "Hanwang Zhang",
            "Shuicheng Yan",
            "Xinchao Wang"
        ],
        "title": "Seeing World Dynamics in a Nutshell",
        "abstract": "arXiv:2502.03465v1 Announce Type: new  Abstract: We consider the problem of efficiently representing casually captured monocular videos in a spatially- and temporally-coherent manner. While existing approaches predominantly rely on 2D/2.5D techniques treating videos as collections of spatiotemporal pixels, they struggle with complex motions, occlusions, and geometric consistency due to absence of temporal coherence and explicit 3D structure. Drawing inspiration from monocular video as a projection of the dynamic 3D world, we explore representing videos in their intrinsic 3D form through continuous flows of Gaussian primitives in space-time. In this paper, we propose NutWorld, a novel framework that efficiently transforms monocular videos into dynamic 3D Gaussian representations in a single forward pass. At its core, NutWorld introduces a structured spatial-temporal aligned Gaussian (STAG) representation, enabling optimization-free scene modeling with effective depth and flow regularization. Through comprehensive experiments, we demonstrate that NutWorld achieves high-fidelity video reconstruction quality while enabling various downstream applications in real-time. Demos and code will be available at https://github.com/Nut-World/NutWorld.",
        "arxiv_id": "2502.03465",
        "ARXIVID": "2502.03465",
        "COMMENT": "This paper aligns with criterion 1 as it introduces a novel framework for spatially- and temporally-coherent 3D video representation, which is a methodological improvement in spatial understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.03118": {
        "authors": [
            "Wen Yan",
            "Qianye Yang",
            "Shiqi Huang",
            "Yipei Wang",
            "Shonit Punwani",
            "Mark Emberton",
            "Vasilis Stavrinides",
            "Yipeng Hu",
            "Dean Barratt"
        ],
        "title": "Tell2Reg: Establishing spatial correspondence between images by the same language prompts",
        "abstract": "arXiv:2502.03118v1 Announce Type: new  Abstract: Spatial correspondence can be represented by pairs of segmented regions, such that the image registration networks aim to segment corresponding regions rather than predicting displacement fields or transformation parameters. In this work, we show that such a corresponding region pair can be predicted by the same language prompt on two different images using the pre-trained large multimodal models based on GroundingDINO and SAM. This enables a fully automated and training-free registration algorithm, potentially generalisable to a wide range of image registration tasks. In this paper, we present experimental results using one of the challenging tasks, registering inter-subject prostate MR images, which involves both highly variable intensity and morphology between patients. Tell2Reg is training-free, eliminating the need for costly and time-consuming data curation and labelling that was previously required for this registration task. This approach outperforms unsupervised learning-based registration methods tested, and has a performance comparable to weakly-supervised methods. Additional qualitative results are also presented to suggest that, for the first time, there is a potential correlation between language semantics and spatial correspondence, including the spatial invariance in language-prompted regions and the difference in language prompts between the obtained local and global correspondences. Code is available at https://github.com/yanwenCi/Tell2Reg.git.",
        "arxiv_id": "2502.03118",
        "ARXIVID": "2502.03118",
        "COMMENT": "This paper introduces a training-free registration method using large multimodal models, which aligns with criterion 2 (VLLMs/MLLMs) and criterion 3 (novel methods in embodied AI).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.03207": {
        "authors": [
            "Xinyao Liao",
            "Xianfang Zeng",
            "Liao Wang",
            "Gang Yu",
            "Guosheng Lin",
            "Chi Zhang"
        ],
        "title": "MotionAgent: Fine-grained Controllable Video Generation via Motion Field Agent",
        "abstract": "arXiv:2502.03207v1 Announce Type: new  Abstract: We propose MotionAgent, enabling fine-grained motion control for text-guided image-to-video generation. The key technique is the motion field agent that converts motion information in text prompts into explicit motion fields, providing flexible and precise motion guidance. Specifically, the agent extracts the object movement and camera motion described in the text and converts them into object trajectories and camera extrinsics, respectively. An analytical optical flow composition module integrates these motion representations in 3D space and projects them into a unified optical flow. An optical flow adapter takes the flow to control the base image-to-video diffusion model for generating fine-grained controlled videos. The significant improvement in the Video-Text Camera Motion metrics on VBench indicates that our method achieves precise control over camera motion. We construct a subset of VBench to evaluate the alignment of motion information in the text and the generated video, outperforming other advanced models on motion generation accuracy.",
        "arxiv_id": "2502.03207",
        "ARXIVID": "2502.03207",
        "COMMENT": "Matches criterion 1 as it introduces a new method for fine-grained motion control in video generation, which relates to spatial understanding in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.03333": {
        "authors": [
            "Nicolas Deperrois",
            "Hidetoshi Matsuo",
            "Samuel Ruip\\'erez-Campillo",
            "Moritz Vandenhirtz",
            "Sonia Laguna",
            "Alain Ryser",
            "Koji Fujimoto",
            "Mizuho Nishio",
            "Thomas M. Sutter",
            "Julia E. Vogt",
            "Jonas Kluckert",
            "Thomas Frauenfelder",
            "Christian Bl\\\"uthgen",
            "Farhad Nooralahzadeh",
            "Michael Krauthammer"
        ],
        "title": "RadVLM: A Multitask Conversational Vision-Language Model for Radiology",
        "abstract": "arXiv:2502.03333v1 Announce Type: new  Abstract: The widespread use of chest X-rays (CXRs), coupled with a shortage of radiologists, has driven growing interest in automated CXR analysis and AI-assisted reporting. While existing vision-language models (VLMs) show promise in specific tasks such as report generation or abnormality detection, they often lack support for interactive diagnostic capabilities. In this work we present RadVLM, a compact, multitask conversational foundation model designed for CXR interpretation. To this end, we curate a large-scale instruction dataset comprising over 1 million image-instruction pairs containing both single-turn tasks -- such as report generation, abnormality classification, and visual grounding -- and multi-turn, multi-task conversational interactions. After fine-tuning RadVLM on this instruction dataset, we evaluate it across different tasks along with re-implemented baseline VLMs. Our results show that RadVLM achieves state-of-the-art performance in conversational capabilities and visual grounding while remaining competitive in other radiology tasks. Ablation studies further highlight the benefit of joint training across multiple tasks, particularly for scenarios with limited annotated data. Together, these findings highlight the potential of RadVLM as a clinically relevant AI assistant, providing structured CXR interpretation and conversational capabilities to support more effective and accessible diagnostic workflows.",
        "arxiv_id": "2502.03333",
        "ARXIVID": "2502.03333",
        "COMMENT": "This paper matches criterion 2 as it introduces a new multi-task conversational vision-language model (RadVLM) for radiology applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.03183": {
        "authors": [
            "Pengyi Li",
            "Irina Abdullaeva",
            "Alexander Gambashidze",
            "Andrey Kuznetsov",
            "Ivan Oseledets"
        ],
        "title": "MaxInfo: A Training-Free Key-Frame Selection Method Using Maximum Volume for Enhanced Video Understanding",
        "abstract": "arXiv:2502.03183v1 Announce Type: new  Abstract: Modern Video Large Language Models (VLLMs) often rely on uniform frame sampling for video understanding, but this approach frequently fails to capture critical information due to frame redundancy and variations in video content. We propose MaxInfo, a training-free method based on the maximum volume principle, which selects and retains the most representative frames from the input video. By maximizing the geometric volume formed by selected embeddings, MaxInfo ensures that the chosen frames cover the most informative regions of the embedding space, effectively reducing redundancy while preserving diversity. This method enhances the quality of input representations and improves long video comprehension performance across benchmarks. For instance, MaxInfo achieves a 3.28% improvement on LongVideoBench and a 6.4% improvement on EgoSchema for LLaVA-Video-7B. It also achieves a 3.47% improvement for LLaVA-Video-72B. The approach is simple to implement and works with existing VLLMs without the need for additional training, making it a practical and effective alternative to traditional uniform sampling methods.",
        "arxiv_id": "2502.03183",
        "ARXIVID": "2502.03183",
        "COMMENT": "This paper proposes a training-free key-frame selection method for video understanding, which aligns with criterion 2 (VLLMs/MLLMs).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.02977": {
        "authors": [
            "Samyak Rawelekar",
            "Yujun Cai",
            "Yiwei Wang",
            "Ming-Hsuan Yang",
            "Narendra Ahuja"
        ],
        "title": "Disentangling CLIP Features for Enhanced Localized Understanding",
        "abstract": "arXiv:2502.02977v1 Announce Type: new  Abstract: Vision-language models (VLMs) demonstrate impressive capabilities in coarse-grained tasks like image classification and retrieval. However, they struggle with fine-grained tasks that require localized understanding. To investigate this weakness, we comprehensively analyze CLIP features and identify an important issue: semantic features are highly correlated. Specifically, the features of a class encode information about other classes, which we call mutual feature information (MFI). This mutual information becomes evident when we query a specific class and unrelated objects are activated along with the target class. To address this issue, we propose Unmix-CLIP, a novel framework designed to reduce MFI and improve feature disentanglement. We introduce MFI loss, which explicitly separates text features by projecting them into a space where inter-class similarity is minimized. To ensure a corresponding separation in image features, we use multi-label recognition (MLR) to align the image features with the separated text features. This ensures that both image and text features are disentangled and aligned across modalities, improving feature separation for downstream tasks. For the COCO- 14 dataset, Unmix-CLIP reduces feature similarity by 24.9%. We demonstrate its effectiveness through extensive evaluations of MLR and zeroshot semantic segmentation (ZS3). In MLR, our method performs competitively on the VOC2007 and surpasses SOTA approaches on the COCO-14 dataset, using fewer training parameters. Additionally, Unmix-CLIP consistently outperforms existing ZS3 methods on COCO and VOC",
        "arxiv_id": "2502.02977",
        "ARXIVID": "2502.02977",
        "COMMENT": "This paper addresses disentangling CLIP features for localized understanding, which aligns with criterion 2 (VLLMs/MLLMs).",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.02867": {
        "authors": [
            "Minung Kim",
            "Kawon Lee",
            "Jungmo Kim",
            "Sungho Choi",
            "Seungyul Han"
        ],
        "title": "Domain-Invariant Per-Frame Feature Extraction for Cross-Domain Imitation Learning with Visual Observations",
        "abstract": "arXiv:2502.02867v1 Announce Type: new  Abstract: Imitation learning (IL) enables agents to mimic expert behavior without reward signals but faces challenges in cross-domain scenarios with high-dimensional, noisy, and incomplete visual observations. To address this, we propose Domain-Invariant Per-Frame Feature Extraction for Imitation Learning (DIFF-IL), a novel IL method that extracts domain-invariant features from individual frames and adapts them into sequences to isolate and replicate expert behaviors. We also introduce a frame-wise time labeling technique to segment expert behaviors by timesteps and assign rewards aligned with temporal contexts, enhancing task performance. Experiments across diverse visual environments demonstrate the effectiveness of DIFF-IL in addressing complex visual tasks.",
        "arxiv_id": "2502.02867",
        "ARXIVID": "2502.02867",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for cross-domain imitation learning with visual observations, which is relevant to embodied AI methods.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.02982": {
        "authors": [
            "Wenhao Wang",
            "Zijie Yu",
            "William Liu",
            "Rui Ye",
            "Tian Jin",
            "Siheng Chen",
            "Yanfeng Wang"
        ],
        "title": "FedMobileAgent: Training Mobile Agents Using Decentralized Self-Sourced Data from Diverse Users",
        "abstract": "arXiv:2502.02982v1 Announce Type: new  Abstract: The advancement of mobile agents has opened new opportunities for automating tasks on mobile devices. Training these agents requires large-scale high-quality data, which is costly using human labor. Given the vast number of mobile phone users worldwide, if automated data collection from them is feasible, the resulting data volume and the subsequently trained mobile agents could reach unprecedented levels. Nevertheless, two major challenges arise: (1) extracting high-level and low-level user instructions without involving human and (2) utilizing distributed data from diverse users while preserving privacy.   To tackle these challenges, we propose FedMobileAgent, a collaborative framework that trains mobile agents using self-sourced data from diverse users. Specifically, it includes two techniques. First, we propose Auto-Annotation, which enables the automatic collection of high-quality datasets during users' routine phone usage with minimal cost. Second, we introduce adapted aggregation to improve federated training of mobile agents on non-IID user data, by incorporating both episode- and step-level distributions. In distributed settings, FedMobileAgent achieves performance comparable to centralized human-annotated models at less than 0.02\\% of the cost, highlighting its potential for real-world applications.",
        "arxiv_id": "2502.02982",
        "ARXIVID": "2502.02982",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework for training mobile agents using decentralized data, which is relevant to embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.02883": {
        "authors": [
            "Xiaofan Yu",
            "Lanxiang Hu",
            "Benjamin Reichman",
            "Dylan Chu",
            "Rushil Chandrupatla",
            "Xiyuan Zhang",
            "Larry Heck",
            "Tajana Rosing"
        ],
        "title": "SensorChat: Answering Qualitative and Quantitative Questions during Long-Term Multimodal Sensor Interactions",
        "abstract": "arXiv:2502.02883v1 Announce Type: new  Abstract: Natural language interaction with sensing systems is crucial for enabling all users to comprehend sensor data and its impact on their everyday lives. However, existing systems, which typically operate in a Question Answering (QA) manner, are significantly limited in terms of the duration and complexity of sensor data they can handle. In this work, we introduce SensorChat, the first end-to-end QA system designed for long-term sensor monitoring with multimodal and high-dimensional data including time series. SensorChat effectively answers both qualitative (requiring high-level reasoning) and quantitative (requiring accurate responses derived from sensor data) questions in real-world scenarios. To achieve this, SensorChat uses an innovative three-stage pipeline that includes question decomposition, sensor data query, and answer assembly. The first and third stages leverage Large Language Models (LLMs) for intuitive human interactions and to guide the sensor data query process. Unlike existing multimodal LLMs, SensorChat incorporates an explicit query stage to precisely extract factual information from long-duration sensor data. We implement SensorChat and demonstrate its capability for real-time interactions on a cloud server while also being able to run entirely on edge platforms after quantization. Comprehensive QA evaluations show that SensorChat achieves up to 26% higher answer accuracy than state-of-the-art systems on quantitative questions. Additionally, a user study with eight volunteers highlights SensorChat's effectiveness in handling qualitative and open-ended questions.",
        "arxiv_id": "2502.02883",
        "ARXIVID": "2502.02883",
        "COMMENT": "Matches criterion 2 as it introduces a multimodal system leveraging LLMs for sensor data QA, which aligns with VLLMs/MLLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.02936": {
        "authors": [
            "Junkun Jiang",
            "Jie Chen",
            "Ho Yin Au",
            "Mingyuan Chen",
            "Wei Xue",
            "Yike Guo"
        ],
        "title": "Every Angle Is Worth A Second Glance: Mining Kinematic Skeletal Structures from Multi-view Joint Cloud",
        "abstract": "arXiv:2502.02936v1 Announce Type: new  Abstract: Multi-person motion capture over sparse angular observations is a challenging problem under interference from both self- and mutual-occlusions. Existing works produce accurate 2D joint detection, however, when these are triangulated and lifted into 3D, available solutions all struggle in selecting the most accurate candidates and associating them to the correct joint type and target identity. As such, in order to fully utilize all accurate 2D joint location information, we propose to independently triangulate between all same-typed 2D joints from all camera views regardless of their target ID, forming the Joint Cloud. Joint Cloud consist of both valid joints lifted from the same joint type and target ID, as well as falsely constructed ones that are from different 2D sources. These redundant and inaccurate candidates are processed over the proposed Joint Cloud Selection and Aggregation Transformer (JCSAT) involving three cascaded encoders which deeply explore the trajectile, skeletal structural, and view-dependent correlations among all 3D point candidates in the cross-embedding space. An Optimal Token Attention Path (OTAP) module is proposed which subsequently selects and aggregates informative features from these redundant observations for the final prediction of human motion. To demonstrate the effectiveness of JCSAT, we build and publish a new multi-person motion capture dataset BUMocap-X with complex interactions and severe occlusions. Comprehensive experiments over the newly presented as well as benchmark datasets validate the effectiveness of the proposed framework, which outperforms all existing state-of-the-art methods, especially under challenging occlusion scenarios.",
        "arxiv_id": "2502.02936",
        "ARXIVID": "2502.02936",
        "COMMENT": "This paper proposes a novel method for multi-person motion capture, which aligns with criterion 3 (novel methods in embodied AI).",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2502.03430": {
        "authors": [
            "Carlo Biffi",
            "Giorgio Roffo",
            "Pietro Salvagnini",
            "Andrea Cherubini"
        ],
        "title": "A Temporal Convolutional Network-Based Approach and a Benchmark Dataset for Colonoscopy Video Temporal Segmentation",
        "abstract": "arXiv:2502.03430v1 Announce Type: new  Abstract: Following recent advancements in computer-aided detection and diagnosis systems for colonoscopy, the automated reporting of colonoscopy procedures is set to further revolutionize clinical practice. A crucial yet underexplored aspect in the development of these systems is the creation of computer vision models capable of autonomously segmenting full-procedure colonoscopy videos into anatomical sections and procedural phases. In this work, we aim to create the first open-access dataset for this task and propose a state-of-the-art approach, benchmarked against competitive models. We annotated the publicly available REAL-Colon dataset, consisting of 2.7 million frames from 60 complete colonoscopy videos, with frame-level labels for anatomical locations and colonoscopy phases across nine categories. We then present ColonTCN, a learning-based architecture that employs custom temporal convolutional blocks designed to efficiently capture long temporal dependencies for the temporal segmentation of colonoscopy videos. We also propose a dual k-fold cross-validation evaluation protocol for this benchmark, which includes model assessment on unseen, multi-center data.ColonTCN achieves state-of-the-art performance in classification accuracy while maintaining a low parameter count when evaluated using the two proposed k-fold cross-validation settings, outperforming competitive models. We report ablation studies to provide insights into the challenges of this task and highlight the benefits of the custom temporal convolutional blocks, which enhance learning and improve model efficiency. We believe that the proposed open-access benchmark and the ColonTCN approach represent a significant advancement in the temporal segmentation of colonoscopy procedures, fostering further open-access research to address this clinical need.",
        "arxiv_id": "2502.03430",
        "ARXIVID": "2502.03430",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset and method for colonoscopy video segmentation, which is relevant to embodied AI benchmarks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2502.03444": {
        "authors": [
            "Hao Chen",
            "Yujin Han",
            "Fangyi Chen",
            "Xiang Li",
            "Yidong Wang",
            "Jindong Wang",
            "Ze Wang",
            "Zicheng Liu",
            "Difan Zou",
            "Bhiksha Raj"
        ],
        "title": "Masked Autoencoders Are Effective Tokenizers for Diffusion Models",
        "abstract": "arXiv:2502.03444v1 Announce Type: new  Abstract: Recent advances in latent diffusion models have demonstrated their effectiveness for high-resolution image synthesis. However, the properties of the latent space from tokenizer for better learning and generation of diffusion models remain under-explored. Theoretically and empirically, we find that improved generation quality is closely tied to the latent distributions with better structure, such as the ones with fewer Gaussian Mixture modes and more discriminative features. Motivated by these insights, we propose MAETok, an autoencoder (AE) leveraging mask modeling to learn semantically rich latent space while maintaining reconstruction fidelity. Extensive experiments validate our analysis, demonstrating that the variational form of autoencoders is not necessary, and a discriminative latent space from AE alone enables state-of-the-art performance on ImageNet generation using only 128 tokens. MAETok achieves significant practical improvements, enabling a gFID of 1.69 with 76x faster training and 31x higher inference throughput for 512x512 generation. Our findings show that the structure of the latent space, rather than variational constraints, is crucial for effective diffusion models. Code and trained models are released.",
        "arxiv_id": "2502.03444",
        "ARXIVID": "2502.03444",
        "COMMENT": "Matches criterion 4 as it discusses improvements to latent diffusion models and their applications in image synthesis.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.02763": {
        "authors": [
            "Manuel Traub",
            "Martin V. Butz"
        ],
        "title": "Rethinking Vision Transformer for Object Centric Foundation Models",
        "abstract": "arXiv:2502.02763v1 Announce Type: new  Abstract: Recent state-of-the-art object segmentation mechanisms, such as the Segment Anything Model (SAM) and FastSAM, first encode the full image over several layers and then focus on generating the mask for one particular object or area. We present an off-grid Fovea-Like Input Patching (FLIP) approach, which selects image input and encodes it from the beginning in an object-focused manner. While doing so, it separates locational encoding from an object-centric perceptual code. FLIP is more data-efficient and yields improved segmentation performance when masking relatively small objects in high-resolution visual scenes. On standard benchmarks such as Hypersim, KITTI-360, and OpenImages, FLIP achieves Intersection over Union (IoU) scores that approach the performance of SAM with much less compute effort. It surpasses FastSAM in all IoU measurements. We also introduce an additional semi-natural but highly intuitive dataset where FLIP outperforms SAM and FastSAM overall and particularly on relatively small objects. Seeing that FLIP is an end-to-end object-centric segmentation approach, it has high potential particularly for applications that benefit from computationally efficient, spatially highly selective object tracking.",
        "arxiv_id": "2502.02763",
        "ARXIVID": "2502.02763",
        "COMMENT": "Matches criterion 4 as it discusses improvements to vision foundation models (Vision Transformers) and their applications in object segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.03449": {
        "authors": [
            "Xuan Li",
            "Chang Yu",
            "Wenxin Du",
            "Ying Jiang",
            "Tianyi Xie",
            "Yunuo Chen",
            "Yin Yang",
            "Chenfanfu Jiang"
        ],
        "title": "Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion Prior and Differentiable Physics",
        "abstract": "arXiv:2502.03449v1 Announce Type: new  Abstract: Recent advances in large models have significantly advanced image-to-3D reconstruction. However, the generated models are often fused into a single piece, limiting their applicability in downstream tasks. This paper focuses on 3D garment generation, a key area for applications like virtual try-on with dynamic garment animations, which require garments to be separable and simulation-ready. We introduce Dress-1-to-3, a novel pipeline that reconstructs physics-plausible, simulation-ready separated garments with sewing patterns and humans from an in-the-wild image. Starting with the image, our approach combines a pre-trained image-to-sewing pattern generation model for creating coarse sewing patterns with a pre-trained multi-view diffusion model to produce multi-view images. The sewing pattern is further refined using a differentiable garment simulator based on the generated multi-view images. Versatile experiments demonstrate that our optimization approach substantially enhances the geometric alignment of the reconstructed 3D garments and humans with the input image. Furthermore, by integrating a texture generation module and a human motion generation module, we produce customized physics-plausible and realistic dynamic garment demonstrations. Project page: https://dress-1-to-3.github.io/",
        "arxiv_id": "2502.03449",
        "ARXIVID": "2502.03449",
        "COMMENT": "Matches criterion 1 as it involves spatial understanding and reconstruction of 3D garments from images, which could be relevant to embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.02919": {
        "authors": [
            "Wonjun Lee",
            "Bumsub Ham",
            "Suhyun Kim"
        ],
        "title": "Maximizing the Position Embedding for Vision Transformers with Global Average Pooling",
        "abstract": "arXiv:2502.02919v1 Announce Type: new  Abstract: In vision transformers, position embedding (PE) plays a crucial role in capturing the order of tokens. However, in vision transformer structures, there is a limitation in the expressiveness of PE due to the structure where position embedding is simply added to the token embedding. A layer-wise method that delivers PE to each layer and applies independent Layer Normalizations for token embedding and PE has been adopted to overcome this limitation. In this paper, we identify the conflicting result that occurs in a layer-wise structure when using the global average pooling (GAP) method instead of the class token. To overcome this problem, we propose MPVG, which maximizes the effectiveness of PE in a layer-wise structure with GAP. Specifically, we identify that PE counterbalances token embedding values at each layer in a layer-wise structure. Furthermore, we recognize that the counterbalancing role of PE is insufficient in the layer-wise structure, and we address this by maximizing the effectiveness of PE through MPVG. Through experiments, we demonstrate that PE performs a counterbalancing role and that maintaining this counterbalancing directionality significantly impacts vision transformers. As a result, the experimental results show that MPVG outperforms existing methods across vision transformers on various tasks.",
        "arxiv_id": "2502.02919",
        "ARXIVID": "2502.02919",
        "COMMENT": "This paper focuses on improving position embeddings in vision transformers, which is related to vision foundation models (criterion 4).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.03230": {
        "authors": [
            "Jiayi He",
            "Shengeng Tang",
            "Ao Liu",
            "Lechao Cheng",
            "Jingjing Wu",
            "Yanyan Wei"
        ],
        "title": "Efficient Vision Language Model Fine-tuning for Text-based Person Anomaly Search",
        "abstract": "arXiv:2502.03230v1 Announce Type: new  Abstract: This paper presents the HFUT-LMC team's solution to the WWW 2025 challenge on Text-based Person Anomaly Search (TPAS). The primary objective of this challenge is to accurately identify pedestrians exhibiting either normal or abnormal behavior within a large library of pedestrian images. Unlike traditional video analysis tasks, TPAS significantly emphasizes understanding and interpreting the subtle relationships between text descriptions and visual data. The complexity of this task lies in the model's need to not only match individuals to text descriptions in massive image datasets but also accurately differentiate between search results when faced with similar descriptions. To overcome these challenges, we introduce the Similarity Coverage Analysis (SCA) strategy to address the recognition difficulty caused by similar text descriptions. This strategy effectively enhances the model's capacity to manage subtle differences, thus improving both the accuracy and reliability of the search. Our proposed solution demonstrated excellent performance in this challenge.",
        "arxiv_id": "2502.03230",
        "ARXIVID": "2502.03230",
        "COMMENT": "Matches criterion 2 as it focuses on fine-tuning vision-language models for text-based person anomaly search.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2502.02690": {
        "authors": [
            "Yifan Shen",
            "Peiyuan Zhu",
            "Zijian Li",
            "Shaoan Xie",
            "Zeyu Tang",
            "Namrata Deka",
            "Zongfang Liu",
            "Guangyi Chen",
            "Kun Zhang"
        ],
        "title": "Controllable Video Generation with Provable Disentanglement",
        "abstract": "arXiv:2502.02690v1 Announce Type: new  Abstract: Controllable video generation remains a significant challenge, despite recent advances in generating high-quality and consistent videos. Most existing methods for controlling video generation treat the video as a whole, neglecting intricate fine-grained spatiotemporal relationships, which limits both control precision and efficiency. In this paper, we propose Controllable Video Generative Adversarial Networks (CoVoGAN) to disentangle the video concepts, thus facilitating efficient and independent control over individual concepts. Specifically, following the minimal change principle, we first disentangle static and dynamic latent variables. We then leverage the sufficient change property to achieve component-wise identifiability of dynamic latent variables, enabling independent control over motion and identity. To establish the theoretical foundation, we provide a rigorous analysis demonstrating the identifiability of our approach. Building on these theoretical insights, we design a Temporal Transition Module to disentangle latent dynamics. To enforce the minimal change principle and sufficient change property, we minimize the dimensionality of latent dynamic variables and impose temporal conditional independence. To validate our approach, we integrate this module as a plug-in for GANs. Extensive qualitative and quantitative experiments on various video generation benchmarks demonstrate that our method significantly improves generation quality and controllability across diverse real-world scenarios.",
        "arxiv_id": "2502.02690",
        "ARXIVID": "2502.02690",
        "COMMENT": "This paper does not match any specific criteria but is related to generative modeling and video generation, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.02610": {
        "authors": [
            "Mehul Agarwal",
            "Gauri Agarwal",
            "Santiago Benoit",
            "Andrew Lippman",
            "Jean Oh"
        ],
        "title": "Secure & Personalized Music-to-Video Generation via CHARCHA",
        "abstract": "arXiv:2502.02610v1 Announce Type: new  Abstract: Music is a deeply personal experience and our aim is to enhance this with a fully-automated pipeline for personalized music video generation. Our work allows listeners to not just be consumers but co-creators in the music video generation process by creating personalized, consistent and context-driven visuals based on lyrics, rhythm and emotion in the music. The pipeline combines multimodal translation and generation techniques and utilizes low-rank adaptation on listeners' images to create immersive music videos that reflect both the music and the individual. To ensure the ethical use of users' identity, we also introduce CHARCHA (patent pending), a facial identity verification protocol that protects people against unauthorized use of their face while at the same time collecting authorized images from users for personalizing their videos. This paper thus provides a secure and innovative framework for creating deeply personalized music videos.",
        "arxiv_id": "2502.02610",
        "ARXIVID": "2502.02610",
        "COMMENT": "This paper does not match any of the specific criteria but is related to generative modeling in multi-modal learning, which is a general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.03359": {
        "authors": [
            "Ryan Rabinowitz",
            "Steve Cruz",
            "Manuel G\\\"unther",
            "Terrance E. Boult"
        ],
        "title": "GHOST: Gaussian Hypothesis Open-Set Technique",
        "abstract": "arXiv:2502.03359v1 Announce Type: new  Abstract: Evaluations of large-scale recognition methods typically focus on overall performance. While this approach is common, it often fails to provide insights into performance across individual classes, which can lead to fairness issues and misrepresentation. Addressing these gaps is crucial for accurately assessing how well methods handle novel or unseen classes and ensuring a fair evaluation. To address fairness in Open-Set Recognition (OSR), we demonstrate that per-class performance can vary dramatically. We introduce Gaussian Hypothesis Open Set Technique (GHOST), a novel hyperparameter-free algorithm that models deep features using class-wise multivariate Gaussian distributions with diagonal covariance matrices. We apply Z-score normalization to logits to mitigate the impact of feature magnitudes that deviate from the model's expectations, thereby reducing the likelihood of the network assigning a high score to an unknown sample. We evaluate GHOST across multiple ImageNet-1K pre-trained deep networks and test it with four different unknown datasets. Using standard metrics such as AUOSCR, AUROC and FPR95, we achieve statistically significant improvements, advancing the state-of-the-art in large-scale OSR. Source code is provided online.",
        "arxiv_id": "2502.03359",
        "ARXIVID": "2502.03359",
        "COMMENT": "Does not match any specific criteria. Focuses on fairness in Open-Set Recognition and introduces a new algorithm for class-wise performance evaluation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.03081": {
        "authors": [
            "Nona Rajabi",
            "Ant\\^onio H. Ribeiro",
            "Miguel Vasco",
            "Farzaneh Taleb",
            "M\\r{a}rten Bj\\\"orkman",
            "Danica Kragic"
        ],
        "title": "Human-Aligned Image Models Improve Visual Decoding from the Brain",
        "abstract": "arXiv:2502.03081v1 Announce Type: new  Abstract: Decoding visual images from brain activity has significant potential for advancing brain-computer interaction and enhancing the understanding of human perception. Recent approaches align the representation spaces of images and brain activity to enable visual decoding. In this paper, we introduce the use of human-aligned image encoders to map brain signals to images. We hypothesize that these models more effectively capture perceptual attributes associated with the rapid visual stimuli presentations commonly used in visual brain data recording experiments. Our empirical results support this hypothesis, demonstrating that this simple modification improves image retrieval accuracy by up to 21% compared to state-of-the-art methods. Comprehensive experiments confirm consistent performance improvements across diverse EEG architectures, image encoders, alignment methods, participants, and brain imaging modalities.",
        "arxiv_id": "2502.03081",
        "ARXIVID": "2502.03081",
        "COMMENT": "Does not match any specific criteria. Focuses on decoding visual images from brain activity using human-aligned image models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.02779": {
        "authors": [
            "Weicheng Zhu",
            "Haoxu Huang",
            "Huanze Tang",
            "Rushabh Musthyala",
            "Boyang Yu",
            "Long Chen",
            "Emilio Vega",
            "Thomas O'Donnell",
            "Seena Dehkharghani",
            "Jennifer A. Frontera",
            "Arjun V. Masurkar",
            "Kara Melmed",
            "Narges Razavian"
        ],
        "title": "3D Foundation AI Model for Generalizable Disease Detection in Head Computed Tomography",
        "abstract": "arXiv:2502.02779v1 Announce Type: new  Abstract: Head computed tomography (CT) imaging is a widely-used imaging modality with multitudes of medical indications, particularly in assessing pathology of the brain, skull, and cerebrovascular system. It is commonly the first-line imaging in neurologic emergencies given its rapidity of image acquisition, safety, cost, and ubiquity. Deep learning models may facilitate detection of a wide range of diseases. However, the scarcity of high-quality labels and annotations, particularly among less common conditions, significantly hinders the development of powerful models. To address this challenge, we introduce FM-CT: a Foundation Model for Head CT for generalizable disease detection, trained using self-supervised learning. Our approach pre-trains a deep learning model on a large, diverse dataset of 361,663 non-contrast 3D head CT scans without the need for manual annotations, enabling the model to learn robust, generalizable features. To investigate the potential of self-supervised learning in head CT, we employed both discrimination with self-distillation and masked image modeling, and we construct our model in 3D rather than at the slice level (2D) to exploit the structure of head CT scans more comprehensively and efficiently. The model's downstream classification performance is evaluated using internal and three external datasets, encompassing both in-distribution (ID) and out-of-distribution (OOD) data. Our results demonstrate that the self-supervised foundation model significantly improves performance on downstream diagnostic tasks compared to models trained from scratch and previous 3D CT foundation models on scarce annotated datasets. This work highlights the effectiveness of self-supervised learning in medical imaging and sets a new benchmark for head CT image analysis in 3D, enabling broader use of artificial intelligence for head CT-based diagnosis.",
        "arxiv_id": "2502.02779",
        "ARXIVID": "2502.02779",
        "COMMENT": "Does not match any specific criteria. Focuses on a 3D foundation model for medical imaging, which is outside the specified interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.03426": {
        "authors": [
            "Zhihong Xu",
            "Dongxia Wang",
            "Peng Du",
            "Yang Cao",
            "Qing Guo"
        ],
        "title": "TruePose: Human-Parsing-guided Attention Diffusion for Full-ID Preserving Pose Transfer",
        "abstract": "arXiv:2502.03426v1 Announce Type: new  Abstract: Pose-Guided Person Image Synthesis (PGPIS) generates images that maintain a subject's identity from a source image while adopting a specified target pose (e.g., skeleton). While diffusion-based PGPIS methods effectively preserve facial features during pose transformation, they often struggle to accurately maintain clothing details from the source image throughout the diffusion process. This limitation becomes particularly problematic when there is a substantial difference between the source and target poses, significantly impacting PGPIS applications in the fashion industry where clothing style preservation is crucial for copyright protection. Our analysis reveals that this limitation primarily stems from the conditional diffusion model's attention modules failing to adequately capture and preserve clothing patterns. To address this limitation, we propose human-parsing-guided attention diffusion, a novel approach that effectively preserves both facial and clothing appearance while generating high-quality results. We propose a human-parsing-aware Siamese network that consists of three key components: dual identical UNets (TargetNet for diffusion denoising and SourceNet for source image embedding extraction), a human-parsing-guided fusion attention (HPFA), and a CLIP-guided attention alignment (CAA). The HPFA and CAA modules can embed the face and clothes patterns into the target image generation adaptively and effectively. Extensive experiments on both the in-shop clothes retrieval benchmark and the latest in-the-wild human editing dataset demonstrate our method's significant advantages over 13 baseline approaches for preserving both facial and clothes appearance in the source image.",
        "arxiv_id": "2502.03426",
        "ARXIVID": "2502.03426",
        "COMMENT": "Does not match any specific criteria. Focuses on pose-guided person image synthesis and attention diffusion for preserving clothing and facial details.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.02707": {
        "authors": [
            "Shuyang Wu",
            "Yifu Qiu",
            "Ines P. Nearchou",
            "Sandrine Prost",
            "Jonathan A. Fallowfield",
            "Hakan Bilen",
            "Timothy J. Kendall"
        ],
        "title": "Multiple Instance Learning with Coarse-to-Fine Self-Distillation",
        "abstract": "arXiv:2502.02707v1 Announce Type: new  Abstract: Multiple Instance Learning (MIL) for whole slide image (WSI) analysis in computational pathology often neglects instance-level learning as supervision is typically provided only at the bag level. In this work, we present PathMIL, a framework designed to improve MIL through two perspectives: (1) employing instance-level supervision and (2) learning inter-instance contextual information on bag level. Firstly, we propose a novel Coarse-to-Fine Self-Distillation (CFSD) paradigm, to probe and distil a classifier trained with bag-level information to obtain instance-level labels which could effectively provide the supervision for the same classifier in a finer way. Secondly, to capture inter-instance contextual information in WSI, we propose Two-Dimensional Positional Encoding (2DPE), which encodes the spatial appearance of instances within a bag. We also theoretically and empirically prove the instance-level learnability of CFSD. PathMIL is evaluated on multiple benchmarking tasks, including subtype classification (TCGA-NSCLC), tumour classification (CAMELYON16), and an internal benchmark for breast cancer receptor status classification. Our method achieves state-of-the-art performance, with AUC scores of 0.9152 and 0.8524 for estrogen and progesterone receptor status classification, respectively, an AUC of 0.9618 for subtype classification, and 0.8634 for tumour classification, surpassing existing methods.",
        "arxiv_id": "2502.02707",
        "ARXIVID": "2502.02707",
        "COMMENT": "This paper introduces a novel MIL framework for pathology, which does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.02907": {
        "authors": [
            "Jacopo Villa",
            "Jay W. McMahon",
            "Issa A. D. Nesnas"
        ],
        "title": "PoleStack: Robust Pole Estimation of Irregular Objects from Silhouette Stacking",
        "abstract": "arXiv:2502.02907v1 Announce Type: new  Abstract: We present an algorithm to estimate the rotation pole of a principal-axis rotator using silhouette images collected from multiple camera poses. First, a set of images is stacked to form a single silhouette-stack image, where the object's rotation introduces reflective symmetry about the imaged pole direction. We estimate this projected-pole direction by identifying maximum symmetry in the silhouette stack. To handle unknown center-of-mass image location, we apply the Discrete Fourier Transform to produce the silhouette-stack amplitude spectrum, achieving translation invariance and increased robustness to noise. Second, the 3D pole orientation is estimated by combining two or more projected-pole measurements collected from different camera orientations. We demonstrate degree-level pole estimation accuracy using low-resolution imagery, showing robustness to severe surface shadowing and centroid-based image-registration errors. The proposed approach could be suitable for pole estimation during both the approach phase toward a target object and while hovering.",
        "arxiv_id": "2502.02907",
        "ARXIVID": "2502.02907",
        "COMMENT": "This paper introduces a method for pole estimation using silhouette stacking, which does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.03274": {
        "authors": [
            "Vasileios Manginas",
            "Nikolaos Manginas",
            "Edward Stevinson",
            "Sherwin Varghese",
            "Nikos Katzouris",
            "Georgios Paliouras",
            "Alessio Lomuscio"
        ],
        "title": "A Scalable Approach to Probabilistic Neuro-Symbolic Verification",
        "abstract": "arXiv:2502.03274v1 Announce Type: new  Abstract: Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising direction for integrating neural learning with symbolic reasoning. In the probabilistic variant of such systems, a neural network first extracts a set of symbols from sub-symbolic input, which are then used by a symbolic component to reason in a probabilistic manner towards answering a query. In this work, we address the problem of formally verifying the robustness of such NeSy probabilistic reasoning systems, therefore paving the way for their safe deployment in critical domains. We analyze the complexity of solving this problem exactly, and show that it is $\\mathrm{NP}^{\\# \\mathrm{P}}$-hard. To overcome this issue, we propose the first approach for approximate, relaxation-based verification of probabilistic NeSy systems. We demonstrate experimentally that the proposed method scales exponentially better than solver-based solutions and apply our technique to a real-world autonomous driving dataset, where we verify a safety property under large input dimensionalities and network sizes.",
        "arxiv_id": "2502.03274",
        "ARXIVID": "2502.03274",
        "COMMENT": "This paper focuses on neuro-symbolic AI and verification, which does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.03200": {
        "authors": [
            "Marija Kopanja",
            "Milo\\v{s} Savi\\'c",
            "Luca Longo"
        ],
        "title": "CORTEX: A Cost-Sensitive Rule and Tree Extraction Method",
        "abstract": "arXiv:2502.03200v1 Announce Type: new  Abstract: Tree-based and rule-based machine learning models play pivotal roles in explainable artificial intelligence (XAI) due to their unique ability to provide explanations in the form of tree or rule sets that are easily understandable and interpretable, making them essential for applications in which trust in model decisions is necessary. These transparent models are typically used in surrogate modeling, a post-hoc XAI approach for explaining the logic of black-box models, enabling users to comprehend and trust complex predictive systems while maintaining competitive performance. This study proposes the Cost-Sensitive Rule and Tree Extraction (CORTEX) method, a novel rule-based XAI algorithm grounded in the multi-class cost-sensitive decision tree (CSDT) method. The original version of the CSDT is extended to classification problems with more than two classes by inducing the concept of an n-dimensional class-dependent cost matrix. The performance of CORTEX as a rule-extractor XAI method is compared to other post-hoc tree and rule extraction methods across several datasets with different numbers of classes. Several quantitative evaluation metrics are employed to assess the explainability of generated rule sets. Our findings demonstrate that CORTEX is competitive with other tree-based methods and can be superior to other rule-based methods across different datasets. The extracted rule sets suggest the advantages of using the CORTEX method over other methods by producing smaller rule sets with shorter rules on average across datasets with a diverse number of classes. Overall, the results underscore the potential of CORTEX as a powerful XAI tool for scenarios that require the generation of clear, human-understandable rules while maintaining good predictive performance.",
        "arxiv_id": "2502.03200",
        "ARXIVID": "2502.03200",
        "COMMENT": "This paper introduces a novel XAI method but does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.02850": {
        "authors": [
            "Lei Yang",
            "Guowu Yuan",
            "Hao Zhou",
            "Hongyu Liu",
            "Jian Chen",
            "Hao Wu"
        ],
        "title": "RS-YOLOX: A High Precision Detector for Object Detection in Satellite Remote Sensing Images",
        "abstract": "arXiv:2502.02850v1 Announce Type: new  Abstract: Automatic object detection by satellite remote sensing images is of great significance for resource exploration and natural disaster assessment. To solve existing problems in remote sensing image detection, this article proposes an improved YOLOX model for satellite remote sensing image automatic detection. This model is named RS-YOLOX. To strengthen the feature learning ability of the network, we used Efficient Channel Attention (ECA) in the backbone network of YOLOX and combined the Adaptively Spatial Feature Fusion (ASFF) with the neck network of YOLOX. To balance the numbers of positive and negative samples in training, we used the Varifocal Loss function. Finally, to obtain a high-performance remote sensing object detector, we combined the trained model with an open-source framework called Slicing Aided Hyper Inference (SAHI). This work evaluated models on three aerial remote sensing datasets (DOTA-v1.5, TGRS-HRRSD, and RSOD). Our comparative experiments demonstrate that our model has the highest accuracy in detecting objects in remote sensing image datasets.",
        "arxiv_id": "2502.02850",
        "ARXIVID": "2502.02850",
        "COMMENT": "Does not match any specific criteria. Focuses on object detection in satellite remote sensing images.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}