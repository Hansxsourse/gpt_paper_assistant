{
    "2504.11218": {
        "authors": [
            "Zeming wei",
            "Junyi Lin",
            "Yang Liu",
            "Weixing Chen",
            "Jingzhou Luo",
            "Guanbin Li",
            "Liang Lin"
        ],
        "title": "3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians",
        "abstract": "arXiv:2504.11218v1 Announce Type: new  Abstract: 3D affordance reasoning is essential in associating human instructions with the functional regions of 3D objects, facilitating precise, task-oriented manipulations in embodied AI. However, current methods, which predominantly depend on sparse 3D point clouds, exhibit limited generalizability and robustness due to their sensitivity to coordinate variations and the inherent sparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers high-fidelity, real-time rendering with minimal computational overhead by representing scenes as dense, continuous distributions. This positions 3DGS as a highly effective approach for capturing fine-grained affordance details and improving recognition accuracy. Nevertheless, its full potential remains largely untapped due to the absence of large-scale, 3DGS-specific affordance datasets. To overcome these limitations, we present 3DAffordSplat, the first large-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning. This dataset includes 23,677 Gaussian instances, 8,354 point cloud instances, and 6,631 manually annotated affordance labels, encompassing 21 object categories and 18 affordance types. Building upon this dataset, we introduce AffordSplatNet, a novel model specifically designed for affordance reasoning using 3DGS representations. AffordSplatNet features an innovative cross-modal structure alignment module that exploits structural consistency priors to align 3D point cloud and 3DGS representations, resulting in enhanced affordance recognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat dataset significantly advances affordance learning within the 3DGS domain, while AffordSplatNet consistently outperforms existing methods across both seen and unseen settings, highlighting its robust generalization capabilities.",
        "arxiv_id": "2504.11218",
        "ARXIVID": "2504.11218",
        "COMMENT": "Matches criteria 3 as it introduces a new dataset and method for 3D affordance reasoning using 3D Gaussian Splatting, focusing on novel angles in embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2504.11346": {
        "authors": [
            "Yu Gao",
            "Lixue Gong",
            "Qiushan Guo",
            "Xiaoxia Hou",
            "Zhichao Lai",
            "Fanshi Li",
            "Liang Li",
            "Xiaochen Lian",
            "Chao Liao",
            "Liyang Liu",
            "Wei Liu",
            "Yichun Shi",
            "Shiqi Sun",
            "Yu Tian",
            "Zhi Tian",
            "Peng Wang",
            "Rui Wang",
            "Xuanda Wang",
            "Xun Wang",
            "Ye Wang",
            "Guofeng Wu",
            "Jie Wu",
            "Xin Xia",
            "Xuefeng Xiao",
            "Zhonghua Zhai",
            "Xinyu Zhang",
            "Qi Zhang",
            "Yuwei Zhang",
            "Shijia Zhao",
            "Jianchao Yang",
            "Weilin Huang"
        ],
        "title": "Seedream 3.0 Technical Report",
        "abstract": "arXiv:2504.11346v1 Announce Type: new  Abstract: We present Seedream 3.0, a high-performance Chinese-English bilingual image generation foundation model. We develop several technical improvements to address existing challenges in Seedream 2.0, including alignment with complicated prompts, fine-grained typography generation, suboptimal visual aesthetics and fidelity, and limited image resolutions. Specifically, the advancements of Seedream 3.0 stem from improvements across the entire pipeline, from data construction to model deployment. At the data stratum, we double the dataset using a defect-aware training paradigm and a dual-axis collaborative data-sampling framework. Furthermore, we adopt several effective techniques such as mixed-resolution training, cross-modality RoPE, representation alignment loss, and resolution-aware timestep sampling in the pre-training phase. During the post-training stage, we utilize diversified aesthetic captions in SFT, and a VLM-based reward model with scaling, thereby achieving outputs that well align with human preferences. Furthermore, Seedream 3.0 pioneers a novel acceleration paradigm. By employing consistent noise expectation and importance-aware timestep sampling, we achieve a 4 to 8 times speedup while maintaining image quality. Seedream 3.0 demonstrates significant improvements over Seedream 2.0: it enhances overall capabilities, in particular for text-rendering in complicated Chinese characters which is important to professional typography generation. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality.",
        "arxiv_id": "2504.11346",
        "ARXIVID": "2504.11346",
        "COMMENT": "This paper matches criterion 4 as it discusses advancements in a vision foundation model (Seedream 3.0) with applications in image generation.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2504.10885": {
        "authors": [
            "Zeyu Zhang",
            "Zijian Chen",
            "Zicheng Zhang",
            "Yuze Sun",
            "Yuan Tian",
            "Ziheng Jia",
            "Chunyi Li",
            "Xiaohong Liu",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ],
        "title": "PuzzleBench: A Fully Dynamic Evaluation Framework for Large Multimodal Models on Puzzle Solving",
        "abstract": "arXiv:2504.10885v1 Announce Type: new  Abstract: Large Multimodal Models (LMMs) have demonstrated impressive capabilities across a wide range of multimodal tasks, achieving ever-increasing performance on various evaluation benchmarks. However, existing benchmarks are typically static and often overlap with pre-training datasets, leading to fixed complexity constraints and substantial data contamination issues. Meanwhile, manually annotated datasets are labor-intensive, time-consuming, and subject to human bias and inconsistency, leading to reliability and reproducibility issues. To address these problems, we propose a fully dynamic multimodal evaluation framework, named Open-ended Visual Puzzle Generation (OVPG), which aims to generate fresh, diverse, and verifiable evaluation data automatically in puzzle-solving tasks. Specifically, the OVPG pipeline consists of a raw material sampling module, a visual content generation module, and a puzzle rule design module, which ensures that each evaluation instance is primitive, highly randomized, and uniquely solvable, enabling continual adaptation to the evolving capabilities of LMMs. Built upon OVPG, we construct PuzzleBench, a dynamic and scalable benchmark comprising 11,840 VQA samples. It features six carefully designed puzzle tasks targeting three core LMM competencies, visual recognition, logical reasoning, and context understanding. PuzzleBench differs from static benchmarks that quickly become outdated. It enables ongoing dataset refreshing through OVPG and a rich set of open-ended puzzle designs, allowing seamless adaptation to the evolving capabilities of LMMs.",
        "arxiv_id": "2504.10885",
        "ARXIVID": "2504.10885",
        "COMMENT": "This paper matches criterion 3 as it introduces a new benchmark (PuzzleBench) for evaluating large multimodal models with a novel dynamic evaluation framework.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.11075": {
        "authors": [
            "Dongmin Kim",
            "Hoshinori Kanazawa",
            "Naoto Yoshida",
            "Yasuo Kuniyoshi"
        ],
        "title": "Emergence of Goal-Directed Behaviors via Active Inference with Self-Prior",
        "abstract": "arXiv:2504.11075v1 Announce Type: new  Abstract: Infants often exhibit goal-directed behaviors, such as reaching for a sensory stimulus, even when no external reward criterion is provided. These intrinsically motivated behaviors facilitate spontaneous exploration and learning of the body and environment during early developmental stages. Although computational modeling can offer insight into the mechanisms underlying such behaviors, many existing studies on intrinsic motivation focus primarily on how exploration contributes to acquiring external rewards. In this paper, we propose a novel density model for an agent's own multimodal sensory experiences, called the \"self-prior,\" and investigate whether it can autonomously induce goal-directed behavior. Integrated within an active inference framework based on the free energy principle, the self-prior generates behavioral references purely from an intrinsic process that minimizes mismatches between average past sensory experiences and current observations. This mechanism is also analogous to the acquisition and utilization of a body schema through continuous interaction with the environment. We examine this approach in a simulated environment and confirm that the agent spontaneously reaches toward a tactile stimulus. Our study implements intrinsically motivated behavior shaped by the agent's own sensory experiences, demonstrating the spontaneous emergence of intentional behavior during early development.",
        "arxiv_id": "2504.11075",
        "ARXIVID": "2504.11075",
        "COMMENT": "Matches criteria 3 as it explores goal-directed behaviors in embodied AI using active inference, focusing on intrinsic motivation and novel methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.11171": {
        "authors": [
            "Johannes Jakubik",
            "Felix Yang",
            "Benedikt Blumenstiel",
            "Erik Scheurer",
            "Rocco Sedona",
            "Stefano Maurogiovanni",
            "Jente Bosmans",
            "Nikolaos Dionelis",
            "Valerio Marsocci",
            "Niklas Kopp",
            "Rahul Ramachandran",
            "Paolo Fraccaro",
            "Thomas Brunschwiler",
            "Gabriele Cavallaro",
            "Juan Bernabe-Moreno",
            "Nicolas Long\\'ep\\'e"
        ],
        "title": "TerraMind: Large-Scale Generative Multimodality for Earth Observation",
        "abstract": "arXiv:2504.11171v1 Announce Type: new  Abstract: We present TerraMind, the first any-to-any generative, multimodal foundation model for Earth observation (EO). Unlike other multimodal models, TerraMind is pretrained on dual-scale representations combining both token-level and pixel-level data across modalities. On a token level, TerraMind encodes high-level contextual information to learn cross-modal relationships, while on a pixel level, TerraMind leverages fine-grained representations to capture critical spatial nuances. We pretrained TerraMind on nine geospatial modalities of a global, large-scale dataset. In this paper, we demonstrate that (i) TerraMind's dual-scale early fusion approach unlocks a range of zero-shot and few-shot applications for Earth observation, (ii) TerraMind introduces \"Thinking-in-Modalities\" (TiM) -- the capability of generating additional artificial data during finetuning and inference to improve the model output -- and (iii) TerraMind achieves beyond state-of-the-art performance in community-standard benchmarks for EO like PANGAEA. The pretraining dataset, the model weights, and our code is open-sourced under a permissive license.",
        "arxiv_id": "2504.11171",
        "ARXIVID": "2504.11171",
        "COMMENT": "Matches criteria 4 as it introduces TerraMind, a generative multi-modal foundation model for Earth observation, showcasing applications of vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.10825": {
        "authors": [
            "Dianbing Xi",
            "Jiepeng Wang",
            "Yuanzhi Liang",
            "Xi Qiu",
            "Yuchi Huo",
            "Rui Wang",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "title": "OmniVDiff: Omni Controllable Video Diffusion for Generation and Understanding",
        "abstract": "arXiv:2504.10825v1 Announce Type: new  Abstract: In this paper, we propose a novel framework for controllable video diffusion, OmniVDiff, aiming to synthesize and comprehend multiple video visual content in a single diffusion model. To achieve this, OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, while employing an adaptive control strategy that dynamically adjusts the role of each visual modality during the diffusion process, either as a generation modality or a conditioning modality. This allows flexible manipulation of each modality's role, enabling support for a wide range of tasks. Consequently, our model supports three key functionalities: (1) Text-conditioned video generation: multi-modal visual video sequences (i.e., rgb, depth, canny, segmentaion) are generated based on the text conditions in one diffusion process; (2) Video understanding: OmniVDiff can estimate the depth, canny map, and semantic segmentation across the input rgb frames while ensuring coherence with the rgb input; and (3) X-conditioned video generation: OmniVDiff generates videos conditioned on fine-grained attributes (e.g., depth maps or segmentation maps). By integrating these diverse tasks into a unified video diffusion framework, OmniVDiff enhances the flexibility and scalability for controllable video diffusion, making it an effective tool for a variety of downstream applications, such as video-to-video translation. Extensive experiments demonstrate the effectiveness of our approach, highlighting its potential for various video-related applications.",
        "arxiv_id": "2504.10825",
        "ARXIVID": "2504.10825",
        "COMMENT": "Matches criteria 2 and 4 as it introduces a novel framework for controllable video diffusion with multi-modal capabilities and applications in video understanding and generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.11455": {
        "authors": [
            "Junke Wang",
            "Zhi Tian",
            "Xun Wang",
            "Xinyu Zhang",
            "Weilin Huang",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "title": "SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL",
        "abstract": "arXiv:2504.11455v1 Announce Type: new  Abstract: This work presents SimpleAR, a vanilla autoregressive visual generation framework without complex architecure modifications. Through careful exploration of training and inference optimization, we demonstrate that: 1) with only 0.5B parameters, our model can generate 1024x1024 resolution images with high fidelity, and achieve competitive results on challenging text-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) training could lead to significant improvements on generation aesthectics and prompt alignment; and 3) when optimized with inference acceleraton techniques like vLLM, the time for SimpleAR to generate an 1024x1024 image could be reduced to around 14 seconds. By sharing these findings and open-sourcing the code, we hope to reveal the potential of autoregressive visual generation and encourage more participation in this research field. Code is available at https://github.com/wdrink/SimpleAR.",
        "arxiv_id": "2504.11455",
        "ARXIVID": "2504.11455",
        "COMMENT": "Matches criteria 2 as it explores autoregressive visual generation with optimization techniques, relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.10567": {
        "authors": [
            "Yushu Wu",
            "Yanyu Li",
            "Ivan Skorokhodov",
            "Anil Kag",
            "Willi Menapace",
            "Sharath Girish",
            "Aliaksandr Siarohin",
            "Yanzhi Wang",
            "Sergey Tulyakov"
        ],
        "title": "H3AE: High Compression, High Speed, and High Quality AutoEncoder for Video Diffusion Models",
        "abstract": "arXiv:2504.10567v1 Announce Type: new  Abstract: Autoencoder (AE) is the key to the success of latent diffusion models for image and video generation, reducing the denoising resolution and improving efficiency. However, the power of AE has long been underexplored in terms of network design, compression ratio, and training strategy. In this work, we systematically examine the architecture design choices and optimize the computation distribution to obtain a series of efficient and high-compression video AEs that can decode in real time on mobile devices. We also unify the design of plain Autoencoder and image-conditioned I2V VAE, achieving multifunctionality in a single network. In addition, we find that the widely adopted discriminative losses, i.e., GAN, LPIPS, and DWT losses, provide no significant improvements when training AEs at scale. We propose a novel latent consistency loss that does not require complicated discriminator design or hyperparameter tuning, but provides stable improvements in reconstruction quality. Our AE achieves an ultra-high compression ratio and real-time decoding speed on mobile while outperforming prior art in terms of reconstruction metrics by a large margin. We finally validate our AE by training a DiT on its latent space and demonstrate fast, high-quality text-to-video generation capability.",
        "arxiv_id": "2504.10567",
        "ARXIVID": "2504.10567",
        "COMMENT": "Matches criteria 2 as it proposes a novel autoencoder for video diffusion models, improving efficiency and quality, which is relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.10746": {
        "authors": [
            "Xiulong Liu",
            "Anurag Kumar",
            "Paul Calamia",
            "Sebastia V. Amengual",
            "Calvin Murdock",
            "Ishwarya Ananthabhotla",
            "Philip Robinson",
            "Eli Shlizerman",
            "Vamsi Krishna Ithapu",
            "Ruohan Gao"
        ],
        "title": "Hearing Anywhere in Any Environment",
        "abstract": "arXiv:2504.10746v1 Announce Type: new  Abstract: In mixed reality applications, a realistic acoustic experience in spatial environments is as crucial as the visual experience for achieving true immersion. Despite recent advances in neural approaches for Room Impulse Response (RIR) estimation, most existing methods are limited to the single environment on which they are trained, lacking the ability to generalize to new rooms with different geometries and surface materials. We aim to develop a unified model capable of reconstructing the spatial acoustic experience of any environment with minimum additional measurements. To this end, we present xRIR, a framework for cross-room RIR prediction. The core of our generalizable approach lies in combining a geometric feature extractor, which captures spatial context from panorama depth images, with a RIR encoder that extracts detailed acoustic features from only a few reference RIR samples. To evaluate our method, we introduce ACOUSTICROOMS, a new dataset featuring high-fidelity simulation of over 300,000 RIRs from 260 rooms. Experiments show that our method strongly outperforms a series of baselines. Furthermore, we successfully perform sim-to-real transfer by evaluating our model on four real-world environments, demonstrating the generalizability of our approach and the realism of our dataset.",
        "arxiv_id": "2504.10746",
        "ARXIVID": "2504.10746",
        "COMMENT": "This paper matches criterion 3 as it introduces a new benchmark (ACOUSTICROOMS) and a novel method (xRIR) for spatial acoustic modeling, which is relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2504.10905": {
        "authors": [
            "Yukang Lin",
            "Yan Hong",
            "Zunnan Xu",
            "Xindi Li",
            "Chao Xu",
            "Chuanbiao Song",
            "Ronghui Li",
            "Haoxing Chen",
            "Jun Lan",
            "Huijia Zhu",
            "Weiqiang Wang",
            "Jianfu Zhang",
            "Xiu Li"
        ],
        "title": "InterAnimate: Taming Region-aware Diffusion Model for Realistic Human Interaction Animation",
        "abstract": "arXiv:2504.10905v1 Announce Type: new  Abstract: Recent video generation research has focused heavily on isolated actions, leaving interactive motions-such as hand-face interactions-largely unexamined. These interactions are essential for emerging biometric authentication systems, which rely on interactive motion-based anti-spoofing approaches. From a security perspective, there is a growing need for large-scale, high-quality interactive videos to train and strengthen authentication models. In this work, we introduce a novel paradigm for animating realistic hand-face interactions. Our approach simultaneously learns spatio-temporal contact dynamics and biomechanically plausible deformation effects, enabling natural interactions where hand movements induce anatomically accurate facial deformations while maintaining collision-free contact. To facilitate this research, we present InterHF, a large-scale hand-face interaction dataset featuring 18 interaction patterns and 90,000 annotated videos. Additionally, we propose InterAnimate, a region-aware diffusion model designed specifically for interaction animation. InterAnimate leverages learnable spatial and temporal latents to effectively capture dynamic interaction priors and integrates a region-aware interaction mechanism that injects these priors into the denoising process. To the best of our knowledge, this work represents the first large-scale effort to systematically study human hand-face interactions. Qualitative and quantitative results show InterAnimate produces highly realistic animations, setting a new benchmark. Code and data will be made public to advance research.",
        "arxiv_id": "2504.10905",
        "ARXIVID": "2504.10905",
        "COMMENT": "Matches criterion 3 as it introduces a novel dataset and method for animating human interactions, focusing on biomechanically plausible deformations, which is a novel angle in embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2504.11172": {
        "authors": [
            "Benedikt Blumenstiel",
            "Paolo Fraccaro",
            "Valerio Marsocci",
            "Johannes Jakubik",
            "Stefano Maurogiovanni",
            "Mikolaj Czerkawski",
            "Rocco Sedona",
            "Gabriele Cavallaro",
            "Thomas Brunschwiler",
            "Juan Bernabe-Moreno",
            "Nicolas Long\\'ep\\'e"
        ],
        "title": "TerraMesh: A Planetary Mosaic of Multimodal Earth Observation Data",
        "abstract": "arXiv:2504.11172v1 Announce Type: new  Abstract: Large-scale foundation models in Earth Observation can learn versatile, label-efficient representations by leveraging massive amounts of unlabeled data. However, existing public datasets are often limited in scale, geographic coverage, or sensor variety. We introduce TerraMesh, a new globally diverse, multimodal dataset combining optical, synthetic aperture radar, elevation, and land-cover modalities in an Analysis-Ready Data format. TerraMesh includes over 9 million samples with eight spatiotemporal aligned modalities, enabling large-scale pre-training and fostering robust cross-modal correlation learning. We provide detailed data processing steps, comprehensive statistics, and empirical evidence demonstrating improved model performance when pre-trained on TerraMesh. The dataset will be made publicly available with a permissive license.",
        "arxiv_id": "2504.11172",
        "ARXIVID": "2504.11172",
        "COMMENT": "Matches criterion 4 as it introduces a new multimodal dataset for Earth Observation, which can be used for vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2504.10685": {
        "authors": [
            "Yuqian Fu",
            "Xingyu Qiu",
            "Bin Ren",
            "Yanwei Fu",
            "Radu Timofte",
            "Nicu Sebe",
            "Ming-Hsuan Yang",
            "Luc Van Gool",
            "Kaijin Zhang",
            "Qingpeng Nong",
            "Xiugang Dong",
            "Hong Gao",
            "Xiangsheng Zhou",
            "Jiancheng Pan",
            "Yanxing Liu",
            "Xiao He",
            "Jiahao Li",
            "Yuze Sun",
            "Xiaomeng Huang",
            "Zhenyu Zhang",
            "Ran Ma",
            "Yuhan Liu",
            "Zijian Zhuang",
            "Shuai Yi",
            "Yixiong Zou",
            "Lingyi Hong",
            "Mingxi Chen",
            "Runze Li",
            "Xingdong Sheng",
            "Wenqiang Zhang",
            "Weisen Chen",
            "Yongxin Yan",
            "Xinguo Chen",
            "Yuanjie Shao",
            "Zhengrong Zuo",
            "Nong Sang",
            "Hao Wu",
            "Haoran Sun",
            "Shuming Hu",
            "Yan Zhang",
            "Zhiguang Shi",
            "Yu Zhang",
            "Chao Chen",
            "Tao Wang",
            "Da Feng",
            "Linhai Zhuo",
            "Ziming Lin",
            "Yali Huang",
            "Jie Me",
            "Yiming Yang",
            "Mi Guo",
            "Mingyuan Jiu",
            "Mingliang Xu",
            "Maomao Xiong",
            "Qunshu Zhang",
            "Xinyu Cao",
            "Yuqing Yang",
            "Dianmo Sheng",
            "Xuanpu Zhao",
            "Zhiyu Li",
            "Xuyang Ding",
            "Wenqian Li"
        ],
        "title": "NTIRE 2025 Challenge on Cross-Domain Few-Shot Object Detection: Methods and Results",
        "abstract": "arXiv:2504.10685v1 Announce Type: new  Abstract: Cross-Domain Few-Shot Object Detection (CD-FSOD) poses significant challenges to existing object detection and few-shot detection models when applied across domains. In conjunction with NTIRE 2025, we organized the 1st CD-FSOD Challenge, aiming to advance the performance of current object detectors on entirely novel target domains with only limited labeled data. The challenge attracted 152 registered participants, received submissions from 42 teams, and concluded with 13 teams making valid final submissions. Participants approached the task from diverse perspectives, proposing novel models that achieved new state-of-the-art (SOTA) results under both open-source and closed-source settings. In this report, we present an overview of the 1st NTIRE 2025 CD-FSOD Challenge, highlighting the proposed solutions and summarizing the results submitted by the participants.",
        "arxiv_id": "2504.10685",
        "ARXIVID": "2504.10685",
        "COMMENT": "This paper matches criterion 3 as it discusses a challenge on cross-domain few-shot object detection, which involves novel methods and benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.11101": {
        "authors": [
            "Yulong Zhang",
            "Tianyi Liang",
            "Xinyue Huang",
            "Erfei Cui",
            "Xu Guo",
            "Pei Chu",
            "Chenhui Li",
            "Ru Zhang",
            "Wenhai Wang",
            "Gongshen Liu"
        ],
        "title": "Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and Self-Improving OCR",
        "abstract": "arXiv:2504.11101v1 Announce Type: new  Abstract: The Optical Character Recognition (OCR) task is important for evaluating Vision-Language Models (VLMs) and providing high-quality data sources for LLM training data. While state-of-the-art VLMs show improved average OCR accuracy, they still struggle with sample-level quality degradation and lack reliable automatic detection of low-quality outputs. We introduce Consensus Entropy (CE), a training-free post-inference method that quantifies OCR uncertainty by aggregating outputs from multiple VLMs. Our approach exploits a key insight: correct VLM OCR predictions converge in output space while errors diverge. We develop a lightweight multi-model framework that effectively identifies problematic samples, selects the best outputs and combines model strengths. Experiments across multiple OCR benchmarks and VLMs demonstrate that CE outperforms VLM-as-judge approaches and single-model baselines at the same cost and achieves state-of-the-art results across multiple metrics. For instance, our solution demonstrates: achieving 15.2\\% higher F1 scores than VLM-as-judge methods in quality verification, delivering 6.0\\% accuracy gains on mathematical calculation tasks, and requiring rephrasing only 7.3\\% of inputs while maintaining overall performance. Notably, the entire process requires neither training nor supervision while maintaining plug-and-play functionality throughout.",
        "arxiv_id": "2504.11101",
        "ARXIVID": "2504.11101",
        "COMMENT": "This paper matches criterion 2 as it focuses on Vision-Language Models (VLMs) and introduces a novel post-inference method for OCR tasks, leveraging multi-VLM agreement.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.11024": {
        "authors": [
            "Andrea Simonelli",
            "Norman M\\\"uller",
            "Peter Kontschieder"
        ],
        "title": "Easy3D: A Simple Yet Effective Method for 3D Interactive Segmentation",
        "abstract": "arXiv:2504.11024v1 Announce Type: new  Abstract: The increasing availability of digital 3D environments, whether through image-based 3D reconstruction, generation, or scans obtained by robots, is driving innovation across various applications. These come with a significant demand for 3D interaction, such as 3D Interactive Segmentation, which is useful for tasks like object selection and manipulation. Additionally, there is a persistent need for solutions that are efficient, precise, and performing well across diverse settings, particularly in unseen environments and with unfamiliar objects. In this work, we introduce a 3D interactive segmentation method that consistently surpasses previous state-of-the-art techniques on both in-domain and out-of-domain datasets. Our simple approach integrates a voxel-based sparse encoder with a lightweight transformer-based decoder that implements implicit click fusion, achieving superior performance and maximizing efficiency. Our method demonstrates substantial improvements on benchmark datasets, including ScanNet, ScanNet++, S3DIS, and KITTI-360, and also on unseen geometric distributions such as the ones obtained by Gaussian Splatting. The project web-page is available at https://simonelli-andrea.github.io/easy3d.",
        "arxiv_id": "2504.11024",
        "ARXIVID": "2504.11024",
        "COMMENT": "This paper matches criterion 1 as it proposes a new method for 3D interactive segmentation, which is a methodological improvement in spatial understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.10852": {
        "authors": [
            "Pengxiao Han",
            "Changkun Ye",
            "Jinguang Tong",
            "Cuicui Jiang",
            "Jie Hong",
            "Li Fang",
            "Xuesong Li"
        ],
        "title": "Enhancing Features in Long-tailed Data Using Large Vision Mode",
        "abstract": "arXiv:2504.10852v1 Announce Type: new  Abstract: Language-based foundation models, such as large language models (LLMs) or large vision-language models (LVLMs), have been widely studied in long-tailed recognition. However, the need for linguistic data is not applicable to all practical tasks. In this study, we aim to explore using large vision models (LVMs) or visual foundation models (VFMs) to enhance long-tailed data features without any language information. Specifically, we extract features from the LVM and fuse them with features in the baseline network's map and latent space to obtain the augmented features. Moreover, we design several prototype-based losses in the latent space to further exploit the potential of the augmented features. In the experimental section, we validate our approach on two benchmark datasets: ImageNet-LT and iNaturalist2018.",
        "arxiv_id": "2504.10852",
        "ARXIVID": "2504.10852",
        "COMMENT": "Matches criterion 4 as it explores the use of large vision models (LVMs) for enhancing long-tailed data features, which is relevant to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.11150": {
        "authors": [
            "Mahir Gulzar",
            "Yar Muhammad",
            "Naveed Muhammad"
        ],
        "title": "GC-GAT: Multimodal Vehicular Trajectory Prediction using Graph Goal Conditioning and Cross-context Attention",
        "abstract": "arXiv:2504.11150v1 Announce Type: new  Abstract: Predicting future trajectories of surrounding vehicles heavily relies on what contextual information is given to a motion prediction model. The context itself can be static (lanes, regulatory elements, etc) or dynamic (traffic participants). This paper presents a lane graph-based motion prediction model that first predicts graph-based goal proposals and later fuses them with cross attention over multiple contextual elements. We follow the famous encoder-interactor-decoder architecture where the encoder encodes scene context using lightweight Gated Recurrent Units, the interactor applies cross-context attention over encoded scene features and graph goal proposals, and the decoder regresses multimodal trajectories via Laplacian Mixture Density Network from the aggregated encodings. Using cross-attention over graph-based goal proposals gives robust trajectory estimates since the model learns to attend to future goal-relevant scene elements for the intended agent. We evaluate our work on nuScenes motion prediction dataset, achieving state-of-the-art results.",
        "arxiv_id": "2504.11150",
        "ARXIVID": "2504.11150",
        "COMMENT": "Matches criterion 3 as it focuses on a novel method for motion prediction using graph-based goal proposals and cross-context attention, which is relevant to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.10829": {
        "authors": [
            "Hengyu Shi",
            "Junhao Su",
            "Huansheng Ning",
            "Xiaoming Wei",
            "Jialin Gao"
        ],
        "title": "LayoutCoT: Unleashing the Deep Reasoning Potential of Large Language Models for Layout Generation",
        "abstract": "arXiv:2504.10829v1 Announce Type: new  Abstract: Conditional layout generation aims to automatically generate visually appealing and semantically coherent layouts from user-defined constraints. While recent methods based on generative models have shown promising results, they typically require substantial amounts of training data or extensive fine-tuning, limiting their versatility and practical applicability. Alternatively, some training-free approaches leveraging in-context learning with Large Language Models (LLMs) have emerged, but they often suffer from limited reasoning capabilities and overly simplistic ranking mechanisms, which restrict their ability to generate consistently high-quality layouts. To this end, we propose LayoutCoT, a novel approach that leverages the reasoning capabilities of LLMs through a combination of Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT) techniques. Specifically, LayoutCoT transforms layout representations into a standardized serialized format suitable for processing by LLMs. A Layout-aware RAG is used to facilitate effective retrieval and generate a coarse layout by LLMs. This preliminary layout, together with the selected exemplars, is then fed into a specially designed CoT reasoning module for iterative refinement, significantly enhancing both semantic coherence and visual quality. We conduct extensive experiments on five public datasets spanning three conditional layout generation tasks. Experimental results demonstrate that LayoutCoT achieves state-of-the-art performance without requiring training or fine-tuning. Notably, our CoT reasoning module enables standard LLMs, even those without explicit deep reasoning abilities, to outperform specialized deep-reasoning models such as deepseek-R1, highlighting the potential of our approach in unleashing the deep reasoning capabilities of LLMs for layout generation tasks.",
        "arxiv_id": "2504.10829",
        "ARXIVID": "2504.10829",
        "COMMENT": "Matches criterion 2 as it leverages LLMs with novel reasoning techniques for layout generation, showcasing advancements in multi-modal reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.11230": {
        "authors": [
            "Jingshun Huang",
            "Haitao Lin",
            "Tianyu Wang",
            "Yanwei Fu",
            "Xiangyang Xue",
            "Yi Zhu"
        ],
        "title": "CAP-Net: A Unified Network for 6D Pose and Size Estimation of Categorical Articulated Parts from a Single RGB-D Image",
        "abstract": "arXiv:2504.11230v1 Announce Type: new  Abstract: This paper tackles category-level pose estimation of articulated objects in robotic manipulation tasks and introduces a new benchmark dataset. While recent methods estimate part poses and sizes at the category level, they often rely on geometric cues and complex multi-stage pipelines that first segment parts from the point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation for 6D poses. These approaches overlook dense semantic cues from RGB images, leading to suboptimal accuracy, particularly for objects with small parts. To address these limitations, we propose a single-stage Network, CAP-Net, for estimating the 6D poses and sizes of Categorical Articulated Parts. This method combines RGB-D features to generate instance segmentation and NPCS representations for each part in an end-to-end manner. CAP-Net uses a unified network to simultaneously predict point-wise class labels, centroid offsets, and NPCS maps. A clustering algorithm then groups points of the same predicted class based on their estimated centroid distances to isolate each part. Finally, the NPCS region of each part is aligned with the point cloud to recover its final pose and size. To bridge the sim-to-real domain gap, we introduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date, featuring photorealistic RGB images and depth noise simulated from real sensors. Experimental evaluations on the RGBD-Art dataset demonstrate that our method significantly outperforms the state-of-the-art approach. Real-world deployments of our model in robotic tasks underscore its robustness and exceptional sim-to-real transfer capabilities, confirming its substantial practical utility. Our dataset, code and pre-trained models are available on the project page.",
        "arxiv_id": "2504.11230",
        "ARXIVID": "2504.11230",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset and a novel method for articulated object pose estimation in robotic tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.10831": {
        "authors": [
            "Hyojun Ahn",
            "Seungcheol Oh",
            "Gyu Seon Kim",
            "Soyi Jung",
            "Soohyun Park",
            "Joongheon Kim"
        ],
        "title": "Hallucination-Aware Generative Pretrained Transformer for Cooperative Aerial Mobility Control",
        "abstract": "arXiv:2504.10831v1 Announce Type: new  Abstract: This paper proposes SafeGPT, a two-tiered framework that integrates generative pretrained transformers (GPTs) with reinforcement learning (RL) for efficient and reliable unmanned aerial vehicle (UAV) last-mile deliveries. In the proposed design, a Global GPT module assigns high-level tasks such as sector allocation, while an On-Device GPT manages real-time local route planning. An RL-based safety filter monitors each GPT decision and overrides unsafe actions that could lead to battery depletion or duplicate visits, effectively mitigating hallucinations. Furthermore, a dual replay buffer mechanism helps both the GPT modules and the RL agent refine their strategies over time. Simulation results demonstrate that SafeGPT achieves higher delivery success rates compared to a GPT-only baseline, while substantially reducing battery consumption and travel distance. These findings validate the efficacy of combining GPT-based semantic reasoning with formal safety guarantees, contributing a viable solution for robust and energy-efficient UAV logistics.",
        "arxiv_id": "2504.10831",
        "ARXIVID": "2504.10831",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework for UAV logistics with GPT and RL integration, focusing on safety and efficiency.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.11301": {
        "authors": [
            "Yangyang Zhuang",
            "Wenjia Jiang",
            "Jiayu Zhang",
            "Ze Yang",
            "Joey Tianyi Zhou",
            "Chi Zhang"
        ],
        "title": "Learning to Be A Doctor: Searching for Effective Medical Agent Architectures",
        "abstract": "arXiv:2504.11301v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents have demonstrated strong capabilities across a wide range of tasks, and their application in the medical domain holds particular promise due to the demand for high generalizability and reliance on interdisciplinary knowledge. However, existing medical agent systems often rely on static, manually crafted workflows that lack the flexibility to accommodate diverse diagnostic requirements and adapt to emerging clinical scenarios. Motivated by the success of automated machine learning (AutoML), this paper introduces a novel framework for the automated design of medical agent architectures. Specifically, we define a hierarchical and expressive agent search space that enables dynamic workflow adaptation through structured modifications at the node, structural, and framework levels. Our framework conceptualizes medical agents as graph-based architectures composed of diverse, functional node types and supports iterative self-improvement guided by diagnostic feedback. Experimental results on skin disease diagnosis tasks demonstrate that the proposed method effectively evolves workflow structures and significantly enhances diagnostic accuracy over time. This work represents the first fully automated framework for medical agent architecture design and offers a scalable, adaptable foundation for deploying intelligent agents in real-world clinical environments.",
        "arxiv_id": "2504.11301",
        "ARXIVID": "2504.11301",
        "COMMENT": "This paper does not match any of the criteria but introduces a novel automated framework for medical agent architecture design, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.11239": {
        "authors": [
            "Chang Yang",
            "Ruiyu Wang",
            "Junzhe Jiang",
            "Qi Jiang",
            "Qinggang Zhang",
            "Yanchen Deng",
            "Shuxin Li",
            "Shuyue Hu",
            "Bo Li",
            "Florian T. Pokorny",
            "Xiao Huang",
            "Xinrun Wang"
        ],
        "title": "Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning Benchmark for LLMs",
        "abstract": "arXiv:2504.11239v1 Announce Type: new  Abstract: Reasoning is the fundamental capability of large language models (LLMs). Due to the rapid progress of LLMs, there are two main issues of current benchmarks: i) these benchmarks can be crushed in a short time (less than 1 year), and ii) these benchmarks may be easily hacked. To handle these issues, we propose the ever-scalingness for building the benchmarks which are uncrushable, unhackable, auto-verifiable and general. This paper presents Nondeterministic Polynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark for LLMs. Specifically, the NPPC has three main modules: i) npgym, which provides a unified interface of 25 well-known NP-complete problems and can generate any number of instances with any levels of complexities, ii) npsolver: which provides a unified interface to evaluate the problem instances with both online and offline models via APIs and local deployments, respectively, and iii) npeval: which provides the comprehensive and ready-to-use tools to analyze the performances of LLMs over different problems, the number of tokens, the aha moments, the reasoning errors and the solution errors. Extensive experiments over widely-used LLMs demonstrate: i) NPPC can successfully decrease the performances of advanced LLMs' performances to below 10%, demonstrating that NPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the most powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and o1/o3-mini in most NP-complete problems considered, and iii) the numbers of tokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and DeepSeek-R1, are observed first to increase and then decrease when the problem instances become more and more difficult. We believe that NPPC is the first ever-scaling reasoning benchmark, serving as the uncrushable and unhackable testbed for LLMs toward artificial general intelligence (AGI).",
        "arxiv_id": "2504.11239",
        "ARXIVID": "2504.11239",
        "COMMENT": "Does not match any specific criteria but introduces a reasoning benchmark for LLMs, which is tangentially relevant to your friend's interest in reasoning and multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.11019": {
        "authors": [
            "Hyejin Lee",
            "Seokjun Hong",
            "Jeonghoon Song",
            "Haechan Cho",
            "Zhixiong Jin",
            "Byeonghun Kim",
            "Joobin Jin",
            "Jaegyun Im",
            "Byeongjoon Noh",
            "Hwasoo Yeo"
        ],
        "title": "DRIFT open dataset: A drone-derived intelligence for traffic analysis in urban environmen",
        "abstract": "arXiv:2504.11019v1 Announce Type: new  Abstract: Reliable traffic data are essential for understanding urban mobility and developing effective traffic management strategies. This study introduces the DRone-derived Intelligence For Traffic analysis (DRIFT) dataset, a large-scale urban traffic dataset collected systematically from synchronized drone videos at approximately 250 meters altitude, covering nine interconnected intersections in Daejeon, South Korea. DRIFT provides high-resolution vehicle trajectories that include directional information, processed through video synchronization and orthomap alignment, resulting in a comprehensive dataset of 81,699 vehicle trajectories. Through our DRIFT dataset, researchers can simultaneously analyze traffic at multiple scales - from individual vehicle maneuvers like lane-changes and safety metrics such as time-to-collision to aggregate network flow dynamics across interconnected urban intersections. The DRIFT dataset is structured to enable immediate use without additional preprocessing, complemented by open-source models for object detection and trajectory extraction, as well as associated analytical tools. DRIFT is expected to significantly contribute to academic research and practical applications, such as traffic flow analysis and simulation studies. The dataset and related resources are publicly accessible at https://github.com/AIxMobility/The-DRIFT.",
        "arxiv_id": "2504.11019",
        "ARXIVID": "2504.11019",
        "COMMENT": "This paper does not match any of the criteria but introduces a new dataset for traffic analysis, which might be tangentially interesting for embodied AI applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.10563": {
        "authors": [
            "Qikai Yang",
            "Cheng Ji",
            "Huaiying Luo",
            "Panfeng Li",
            "Zhicheng Ding"
        ],
        "title": "Data Augmentation Through Random Style Replacement",
        "abstract": "arXiv:2504.10563v1 Announce Type: new  Abstract: In this paper, we introduce a novel data augmentation technique that combines the advantages of style augmentation and random erasing by selectively replacing image subregions with style-transferred patches. Our approach first applies a random style transfer to training images, then randomly substitutes selected areas of these images with patches derived from the style-transferred versions. This method is able to seamlessly accommodate a wide range of existing style transfer algorithms and can be readily integrated into diverse data augmentation pipelines. By incorporating our strategy, the training process becomes more robust and less prone to overfitting. Comparative experiments demonstrate that, relative to previous style augmentation methods, our technique achieves superior performance and faster convergence.",
        "arxiv_id": "2504.10563",
        "ARXIVID": "2504.10563",
        "COMMENT": "This paper does not match any of the criteria but introduces a novel data augmentation technique, which might be tangentially interesting for generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11080": {
        "authors": [
            "Elman Ghazaei",
            "Erchan Aptoula"
        ],
        "title": "Change State Space Models for Remote Sensing Change Detection",
        "abstract": "arXiv:2504.11080v1 Announce Type: new  Abstract: Despite their frequent use for change detection, both ConvNets and Vision transformers (ViT) exhibit well-known limitations, namely the former struggle to model long-range dependencies while the latter are computationally inefficient, rendering them challenging to train on large-scale datasets. Vision Mamba, an architecture based on State Space Models has emerged as an alternative addressing the aforementioned deficiencies and has been already applied to remote sensing change detection, though mostly as a feature extracting backbone. In this article the Change State Space Model is introduced, that has been specifically designed for change detection by focusing on the relevant changes between bi-temporal images, effectively filtering out irrelevant information. By concentrating solely on the changed features, the number of network parameters is reduced, enhancing significantly computational efficiency while maintaining high detection performance and robustness against input degradation. The proposed model has been evaluated via three benchmark datasets, where it outperformed ConvNets, ViTs, and Mamba-based counterparts at a fraction of their computational complexity. The implementation will be made available at https://github.com/Elman295/CSSM upon acceptance.",
        "arxiv_id": "2504.11080",
        "ARXIVID": "2504.11080",
        "COMMENT": "This paper introduces a new model for change detection in remote sensing, which is not directly related to the criteria but involves spatial intelligence improvements.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11055": {
        "authors": [
            "Alireza Salehi",
            "Mohammadreza Salehi",
            "Reshad Hosseini",
            "Cees G. M. Snoek",
            "Makoto Yamada",
            "Mohammad Sabokrou"
        ],
        "title": "Crane: Context-Guided Prompt Learning and Attention Refinement for Zero-Shot Anomaly Detections",
        "abstract": "arXiv:2504.11055v1 Announce Type: new  Abstract: Anomaly Detection (AD) involves identifying deviations from normal data distributions and is critical in fields such as medical diagnostics and industrial defect detection. Traditional AD methods typically require the availability of normal training samples; however, this assumption is not always feasible, as collecting such data can be impractical. Additionally, these methods often struggle to generalize across different domains. Recent advancements, such as AnomalyCLIP and AdaCLIP, utilize the zero-shot generalization capabilities of CLIP but still face a performance gap between image-level and pixel-level anomaly detection. To address this gap, we propose a novel approach that conditions the prompts of the text encoder based on image context extracted from the vision encoder. Also, to capture fine-grained variations more effectively, we have modified the CLIP vision encoder and altered the extraction of dense features. These changes ensure that the features retain richer spatial and structural information for both normal and anomalous prompts. Our method achieves state-of-the-art performance, improving performance by 2% to 29% across different metrics on 14 datasets. This demonstrates its effectiveness in both image-level and pixel-level anomaly detection.",
        "arxiv_id": "2504.11055",
        "ARXIVID": "2504.11055",
        "COMMENT": "This paper does not directly match any of the criteria but involves spatial understanding improvements in anomaly detection, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.10929": {
        "authors": [
            "Chang Yu",
            "Yisi Luo",
            "Kai Ye",
            "Xile Zhao",
            "Deyu Meng"
        ],
        "title": "Cross-Frequency Implicit Neural Representation with Self-Evolving Parameters",
        "abstract": "arXiv:2504.10929v1 Announce Type: new  Abstract: Implicit neural representation (INR) has emerged as a powerful paradigm for visual data representation. However, classical INR methods represent data in the original space mixed with different frequency components, and several feature encoding parameters (e.g., the frequency parameter $\\omega$ or the rank $R$) need manual configurations. In this work, we propose a self-evolving cross-frequency INR using the Haar wavelet transform (termed CF-INR), which decouples data into four frequency components and employs INRs in the wavelet space. CF-INR allows the characterization of different frequency components separately, thus enabling higher accuracy for data representation. To more precisely characterize cross-frequency components, we propose a cross-frequency tensor decomposition paradigm for CF-INR with self-evolving parameters, which automatically updates the rank parameter $R$ and the frequency parameter $\\omega$ for each frequency component through self-evolving optimization. This self-evolution paradigm eliminates the laborious manual tuning of these parameters, and learns a customized cross-frequency feature encoding configuration for each dataset. We evaluate CF-INR on a variety of visual data representation and recovery tasks, including image regression, inpainting, denoising, and cloud removal. Extensive experiments demonstrate that CF-INR outperforms state-of-the-art methods in each case.",
        "arxiv_id": "2504.10929",
        "ARXIVID": "2504.10929",
        "COMMENT": "This paper does not match any of the specific criteria. It focuses on implicit neural representation and tensor decomposition, which is not directly related to the listed interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11202": {
        "authors": [
            "Junjie Luo",
            "John Mamish",
            "Alan Fu",
            "Thomas Concannon",
            "Josiah Hester",
            "Emma Alexander",
            "Qi Guo"
        ],
        "title": "Focal Split: Untethered Snapshot Depth from Differential Defocus",
        "abstract": "arXiv:2504.11202v1 Announce Type: new  Abstract: We introduce Focal Split, a handheld, snapshot depth camera with fully onboard power and computing based on depth-from-differential-defocus (DfDD). Focal Split is passive, avoiding power consumption of light sources. Its achromatic optical system simultaneously forms two differentially defocused images of the scene, which can be independently captured using two photosensors in a snapshot. The data processing is based on the DfDD theory, which efficiently computes a depth and a confidence value for each pixel with only 500 floating point operations (FLOPs) per pixel from the camera measurements. We demonstrate a Focal Split prototype, which comprises a handheld custom camera system connected to a Raspberry Pi 5 for real-time data processing. The system consumes 4.9 W and is powered on a 5 V, 10,000 mAh battery. The prototype can measure objects with distances from 0.4 m to 1.2 m, outputting 480$\\times$360 sparse depth maps at 2.1 frames per second (FPS) using unoptimized Python scripts. Focal Split is DIY friendly. A comprehensive guide to building your own Focal Split depth camera, code, and additional data can be found at https://focal-split.qiguo.org.",
        "arxiv_id": "2504.11202",
        "ARXIVID": "2504.11202",
        "COMMENT": "This paper does not match any of the specific criteria. It introduces a depth camera system, which is not directly related to the listed interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11434": {
        "authors": [
            "Yifan Ding",
            "Xixi Liu",
            "Jonas Unger",
            "Gabriel Eilertsen"
        ],
        "title": "Enhancing Out-of-Distribution Detection with Extended Logit Normalization",
        "abstract": "arXiv:2504.11434v1 Announce Type: new  Abstract: Out-of-distribution (OOD) detection is essential for the safe deployment of machine learning models. Recent advances have explored improved classification losses and representation learning strategies to enhance OOD detection. However, these methods are often tailored to specific post-hoc detection techniques, limiting their generalizability. In this work, we identify a critical issue in Logit Normalization (LogitNorm), which inhibits its effectiveness in improving certain post-hoc OOD detection methods. To address this, we propose Extended Logit Normalization ($\\textbf{ELogitNorm}$), a novel hyperparameter-free formulation that significantly benefits a wide range of post-hoc detection methods. By incorporating feature distance-awareness to LogitNorm, $\\textbf{ELogitNorm}$ shows more robust OOD separability and in-distribution (ID) confidence calibration than its predecessor. Extensive experiments across standard benchmarks demonstrate that our approach outperforms state-of-the-art training-time methods in OOD detection while maintaining strong ID classification accuracy.",
        "arxiv_id": "2504.11434",
        "ARXIVID": "2504.11434",
        "COMMENT": "This paper does not match any of the specific criteria. It focuses on out-of-distribution detection, which is not directly related to the listed interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11160": {
        "authors": [
            "Haohan Chen",
            "Hongjia Liu",
            "Shiyong Lan",
            "Wenwu Wang",
            "Yixin Qiao",
            "Yao Li",
            "Guonan Deng"
        ],
        "title": "DMAGaze: Gaze Estimation Based on Feature Disentanglement and Multi-Scale Attention",
        "abstract": "arXiv:2504.11160v1 Announce Type: new  Abstract: Gaze estimation, which predicts gaze direction, commonly faces the challenge of interference from complex gaze-irrelevant information in face images. In this work, we propose DMAGaze, a novel gaze estimation framework that exploits information from facial images in three aspects: gaze-relevant global features (disentangled from facial image), local eye features (extracted from cropped eye patch), and head pose estimation features, to improve overall performance. Firstly, we design a new continuous mask-based Disentangler to accurately disentangle gaze-relevant and gaze-irrelevant information in facial images by achieving the dual-branch disentanglement goal through separately reconstructing the eye and non-eye regions. Furthermore, we introduce a new cascaded attention module named Multi-Scale Global Local Attention Module (MS-GLAM). Through a customized cascaded attention structure, it effectively focuses on global and local information at multiple scales, further enhancing the information from the Disentangler. Finally, the global gaze-relevant features disentangled by the upper face branch, combined with head pose and local eye features, are passed through the detection head for high-precision gaze estimation. Our proposed DMAGaze has been extensively validated on two mainstream public datasets, achieving state-of-the-art performance.",
        "arxiv_id": "2504.11160",
        "ARXIVID": "2504.11160",
        "COMMENT": "This paper does not match any of the specific criteria. It focuses on gaze estimation using disentanglement and attention mechanisms, which is not directly related to the listed interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11347": {
        "authors": [
            "Soyoung Yoo",
            "Namwoo Kang"
        ],
        "title": "DeepWheel: Generating a 3D Synthetic Wheel Dataset for Design and Performance Evaluation",
        "abstract": "arXiv:2504.11347v1 Announce Type: new  Abstract: Data-driven design is emerging as a powerful strategy to accelerate engineering innovation. However, its application to vehicle wheel design remains limited due to the lack of large-scale, high-quality datasets that include 3D geometry and physical performance metrics. To address this gap, this study proposes a synthetic design-performance dataset generation framework using generative AI. The proposed framework first generates 2D rendered images using Stable Diffusion, and then reconstructs the 3D geometry through 2.5D depth estimation. Structural simulations are subsequently performed to extract engineering performance data. To further expand the design and performance space, topology optimization is applied, enabling the generation of a more diverse set of wheel designs. The final dataset, named DeepWheel, consists of over 6,000 photo-realistic images and 900 structurally analyzed 3D models. This multi-modal dataset serves as a valuable resource for surrogate model training, data-driven inverse design, and design space exploration. The proposed methodology is also applicable to other complex design domains. The dataset is released under the Creative Commons Attribution-NonCommercial 4.0 International(CC BY-NC 4.0) and is available on the https://www.smartdesignlab.org/datasets",
        "arxiv_id": "2504.11347",
        "ARXIVID": "2504.11347",
        "COMMENT": "This paper does not match any of the specific criteria. It focuses on a synthetic dataset for wheel design using generative AI, which is outside the scope of the listed interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.10519": {
        "authors": [
            "Yuhang Yao",
            "Haixin Wang",
            "Yibo Chen",
            "Jiawen Wang",
            "Min Chang Jordan Ren",
            "Bosheng Ding",
            "Salman Avestimehr",
            "Chaoyang He"
        ],
        "title": "Toward Super Agent System with Hybrid AI Routers",
        "abstract": "arXiv:2504.10519v1 Announce Type: new  Abstract: AI Agents powered by Large Language Models are transforming the world through enormous applications. A super agent has the potential to fulfill diverse user needs, such as summarization, coding, and research, by accurately understanding user intent and leveraging the appropriate tools to solve tasks. However, to make such an agent viable for real-world deployment and accessible at scale, significant optimizations are required to ensure high efficiency and low cost. This paper presents a design of the Super Agent System. Upon receiving a user prompt, the system first detects the intent of the user, then routes the request to specialized task agents with the necessary tools or automatically generates agentic workflows. In practice, most applications directly serve as AI assistants on edge devices such as phones and robots. As different language models vary in capability and cloud-based models often entail high computational costs, latency, and privacy concerns, we then explore the hybrid mode where the router dynamically selects between local and cloud models based on task complexity. Finally, we introduce the blueprint of an on-device super agent enhanced with cloud. With advances in multi-modality models and edge hardware, we envision that most computations can be handled locally, with cloud collaboration only as needed. Such architecture paves the way for super agents to be seamlessly integrated into everyday life in the near future.",
        "arxiv_id": "2504.10519",
        "ARXIVID": "2504.10519",
        "COMMENT": "Does not match any specific criteria but discusses hybrid AI routers for super agent systems, which is tangentially related to multi-modal AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11441": {
        "authors": [
            "Elizabeth Fons",
            "Rachneet Kaur",
            "Zhen Zeng",
            "Soham Palande",
            "Tucker Balch",
            "Svitlana Vyetrenko",
            "Manuela Veloso"
        ],
        "title": "TADACap: Time-series Adaptive Domain-Aware Captioning",
        "abstract": "arXiv:2504.11441v1 Announce Type: new  Abstract: While image captioning has gained significant attention, the potential of captioning time-series images, prevalent in areas like finance and healthcare, remains largely untapped. Existing time-series captioning methods typically offer generic, domain-agnostic descriptions of time-series shapes and struggle to adapt to new domains without substantial retraining. To address these limitations, we introduce TADACap, a retrieval-based framework to generate domain-aware captions for time-series images, capable of adapting to new domains without retraining. Building on TADACap, we propose a novel retrieval strategy that retrieves diverse image-caption pairs from a target domain database, namely TADACap-diverse. We benchmarked TADACap-diverse against state-of-the-art methods and ablation variants. TADACap-diverse demonstrates comparable semantic accuracy while requiring significantly less annotation effort.",
        "arxiv_id": "2504.11441",
        "ARXIVID": "2504.11441",
        "COMMENT": "Does not match any specific criteria but introduces a novel framework for time-series captioning, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11200": {
        "authors": [
            "Irene Celino",
            "Mario Scrocca",
            "Agnese Chiatti"
        ],
        "title": "Mutual Understanding between People and Systems via Neurosymbolic AI and Knowledge Graphs",
        "abstract": "arXiv:2504.11200v1 Announce Type: new  Abstract: This chapter investigates the concept of mutual understanding between humans and systems, positing that Neuro-symbolic Artificial Intelligence (NeSy AI) methods can significantly enhance this mutual understanding by leveraging explicit symbolic knowledge representations with data-driven learning models. We start by introducing three critical dimensions to characterize mutual understanding: sharing knowledge, exchanging knowledge, and governing knowledge. Sharing knowledge involves aligning the conceptual models of different agents to enable a shared understanding of the domain of interest. Exchanging knowledge relates to ensuring the effective and accurate communication between agents. Governing knowledge concerns establishing rules and processes to regulate the interaction between agents. Then, we present several different use case scenarios that demonstrate the application of NeSy AI and Knowledge Graphs to aid meaningful exchanges between human, artificial, and robotic agents. These scenarios highlight both the potential and the challenges of combining top-down symbolic reasoning with bottom-up neural learning, guiding the discussion of the coverage provided by current solutions along the dimensions of sharing, exchanging, and governing knowledge. Concurrently, this analysis facilitates the identification of gaps and less developed aspects in mutual understanding to address in future research.",
        "arxiv_id": "2504.11200",
        "ARXIVID": "2504.11200",
        "COMMENT": "Does not match any specific criteria but discusses neuro-symbolic AI and knowledge graphs, which are tangentially related to your friend's general interest in AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11423": {
        "authors": [
            "Dazhong Shen",
            "Guanglu Song",
            "Yi Zhang",
            "Bingqi Ma",
            "Lujundong Li",
            "Dongzhi Jiang",
            "Zhuofan Zong",
            "Yu Liu"
        ],
        "title": "ADT: Tuning Diffusion Models with Adversarial Supervision",
        "abstract": "arXiv:2504.11423v1 Announce Type: new  Abstract: Diffusion models have achieved outstanding image generation by reversing a forward noising process to approximate true data distributions. During training, these models predict diffusion scores from noised versions of true samples in a single forward pass, while inference requires iterative denoising starting from white noise. This training-inference divergences hinder the alignment between inference and training data distributions, due to potential prediction biases and cumulative error accumulation. To address this problem, we propose an intuitive but effective fine-tuning framework, called Adversarial Diffusion Tuning (ADT), by stimulating the inference process during optimization and aligning the final outputs with training data by adversarial supervision. Specifically, to achieve robust adversarial training, ADT features a siamese-network discriminator with a fixed pre-trained backbone and lightweight trainable parameters, incorporates an image-to-image sampling strategy to smooth discriminative difficulties, and preserves the original diffusion loss to prevent discriminator hacking. In addition, we carefully constrain the backward-flowing path for back-propagating gradients along the inference path without incurring memory overload or gradient explosion. Finally, extensive experiments on Stable Diffusion models (v1.5, XL, and v3), demonstrate that ADT significantly improves both distribution alignment and image quality.",
        "arxiv_id": "2504.11423",
        "ARXIVID": "2504.11423",
        "COMMENT": "Does not match any specific criteria but proposes a fine-tuning framework for diffusion models, which is tangentially relevant to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11354": {
        "authors": [
            "Haiming Wang",
            "Mert Unsal",
            "Xiaohan Lin",
            "Mantas Baksys",
            "Junqi Liu",
            "Marco Dos Santos",
            "Flood Sung",
            "Marina Vinyes",
            "Zhenzhe Ying",
            "Zekai Zhu",
            "Jianqiao Lu",
            "Hugues de Saxc\\'e",
            "Bolton Bailey",
            "Chendong Song",
            "Chenjun Xiao",
            "Dehao Zhang",
            "Ebony Zhang",
            "Frederick Pu",
            "Han Zhu",
            "Jiawei Liu",
            "Jonas Bayer",
            "Julien Michel",
            "Longhui Yu",
            "L\\'eo Dreyfus-Schmidt",
            "Lewis Tunstall",
            "Luigi Pagani",
            "Moreira Machado",
            "Pauline Bourigault",
            "Ran Wang",
            "Stanislas Polu",
            "Thibaut Barroyer",
            "Wen-Ding Li",
            "Yazhe Niu",
            "Yann Fleureau",
            "Yangyang Hu",
            "Zhouliang Yu",
            "Zihan Wang",
            "Zhilin Yang",
            "Zhengying Liu",
            "Jia Li"
        ],
        "title": "Kimina-Prover Preview: Towards Large Formal Reasoning Models with Reinforcement Learning",
        "abstract": "arXiv:2504.11354v1 Announce Type: new  Abstract: We introduce Kimina-Prover Preview, a large language model that pioneers a novel reasoning-driven exploration paradigm for formal theorem proving, as showcased in this preview release. Trained with a large-scale reinforcement learning pipeline from Qwen2.5-72B, Kimina-Prover demonstrates strong performance in Lean 4 proof generation by employing a structured reasoning pattern we term \\textit{formal reasoning pattern}. This approach allows the model to emulate human problem-solving strategies in Lean, iteratively generating and refining proof steps. Kimina-Prover sets a new state-of-the-art on the miniF2F benchmark, reaching 80.7% with pass@8192. Beyond improved benchmark performance, our work yields several key insights: (1) Kimina-Prover exhibits high sample efficiency, delivering strong results even with minimal sampling (pass@1) and scaling effectively with computational budget, stemming from its unique reasoning pattern and RL training; (2) we demonstrate clear performance scaling with model size, a trend previously unobserved for neural theorem provers in formal mathematics; (3) the learned reasoning style, distinct from traditional search algorithms, shows potential to bridge the gap between formal verification and informal mathematical intuition. We open source distilled versions with 1.5B and 7B parameters of Kimina-Prover",
        "arxiv_id": "2504.11354",
        "ARXIVID": "2504.11354",
        "COMMENT": "Does not match any specific criteria but focuses on formal reasoning models, which is tangentially relevant to your friend's interest in reasoning and AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.10893": {
        "authors": [
            "Yize Zhang",
            "Tianshu Wang",
            "Sirui Chen",
            "Kun Wang",
            "Xingyu Zeng",
            "Hongyu Lin",
            "Xianpei Han",
            "Le Sun",
            "Chaochao Lu"
        ],
        "title": "ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search",
        "abstract": "arXiv:2504.10893v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated impressive capabilities and are receiving increasing attention to enhance their reasoning through scaling test--time compute. However, their application in open--ended, knowledge--intensive, complex reasoning scenarios is still limited. Reasoning--oriented methods struggle to generalize to open--ended scenarios due to implicit assumptions of complete world knowledge. Meanwhile, knowledge--augmented reasoning (KAR) methods fail to address two core challenges: 1) error propagation, where errors in early steps cascade through the chain, and 2) verification bottleneck, where the explore--exploit tradeoff arises in multi--branch decision processes. To overcome these limitations, we introduce ARise, a novel framework that integrates risk assessment of intermediate reasoning states with dynamic retrieval--augmented generation (RAG) within a Monte Carlo tree search paradigm. This approach enables effective construction and optimization of reasoning plans across multiple maintained hypothesis branches. Experimental results show that ARise significantly outperforms the state--of--the--art KAR methods by up to 23.10%, and the latest RAG-equipped large reasoning models by up to 25.37%.",
        "arxiv_id": "2504.10893",
        "ARXIVID": "2504.10893",
        "COMMENT": "Does not match any specific criteria but is related to reasoning in large language models, which is tangentially relevant to your friend's interest in multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.10920": {
        "authors": [
            "Peipei Song",
            "Long Zhang",
            "Long Lan",
            "Weidong Chen",
            "Dan Guo",
            "Xun Yang",
            "Meng Wang"
        ],
        "title": "Towards Efficient Partially Relevant Video Retrieval with Active Moment Discovering",
        "abstract": "arXiv:2504.10920v1 Announce Type: new  Abstract: Partially relevant video retrieval (PRVR) is a practical yet challenging task in text-to-video retrieval, where videos are untrimmed and contain much background content. The pursuit here is of both effective and efficient solutions to capture the partial correspondence between text queries and untrimmed videos. Existing PRVR methods, which typically focus on modeling multi-scale clip representations, however, suffer from content independence and information redundancy, impairing retrieval performance. To overcome these limitations, we propose a simple yet effective approach with active moment discovering (AMDNet). We are committed to discovering video moments that are semantically consistent with their queries. By using learnable span anchors to capture distinct moments and applying masked multi-moment attention to emphasize salient moments while suppressing redundant backgrounds, we achieve more compact and informative video representations. To further enhance moment modeling, we introduce a moment diversity loss to encourage different moments of distinct regions and a moment relevance loss to promote semantically query-relevant moments, which cooperate with a partially relevant retrieval loss for end-to-end optimization. Extensive experiments on two large-scale video datasets (\\ie, TVR and ActivityNet Captions) demonstrate the superiority and efficiency of our AMDNet. In particular, AMDNet is about 15.5 times smaller (\\#parameters) while 6.0 points higher (SumR) than the up-to-date method GMMFormer on TVR.",
        "arxiv_id": "2504.10920",
        "ARXIVID": "2504.10920",
        "COMMENT": "Does not match any specific criteria but involves video retrieval with active moment discovery, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.10976": {
        "authors": [
            "Linhao Li",
            "Yongzhang Tan",
            "Siyuan Yang",
            "Hao Cheng",
            "Yongfeng Dong",
            "Liang Yang"
        ],
        "title": "Adaptive Decision Boundary for Few-Shot Class-Incremental Learning",
        "abstract": "arXiv:2504.10976v1 Announce Type: new  Abstract: Few-Shot Class-Incremental Learning (FSCIL) aims to continuously learn new classes from a limited set of training samples without forgetting knowledge of previously learned classes. Conventional FSCIL methods typically build a robust feature extractor during the base training session with abundant training samples and subsequently freeze this extractor, only fine-tuning the classifier in subsequent incremental phases. However, current strategies primarily focus on preventing catastrophic forgetting, considering only the relationship between novel and base classes, without paying attention to the specific decision spaces of each class. To address this challenge, we propose a plug-and-play Adaptive Decision Boundary Strategy (ADBS), which is compatible with most FSCIL methods. Specifically, we assign a specific decision boundary to each class and adaptively adjust these boundaries during training to optimally refine the decision spaces for the classes in each session. Furthermore, to amplify the distinctiveness between classes, we employ a novel inter-class constraint loss that optimizes the decision boundaries and prototypes for each class. Extensive experiments on three benchmarks, namely CIFAR100, miniImageNet, and CUB200, demonstrate that incorporating our ADBS method with existing FSCIL techniques significantly improves performance, achieving overall state-of-the-art results.",
        "arxiv_id": "2504.10976",
        "ARXIVID": "2504.10976",
        "COMMENT": "Does not match any specific criteria but is related to few-shot learning, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.10888": {
        "authors": [
            "Jiahuan Long",
            "Wen Yao",
            "Tingsong Jiang",
            "Chao Ma"
        ],
        "title": "CDUPatch: Color-Driven Universal Adversarial Patch Attack for Dual-Modal Visible-Infrared Detectors",
        "abstract": "arXiv:2504.10888v1 Announce Type: new  Abstract: Adversarial patches are widely used to evaluate the robustness of object detection systems in real-world scenarios. These patches were initially designed to deceive single-modal detectors (e.g., visible or infrared) and have recently been extended to target visible-infrared dual-modal detectors. However, existing dual-modal adversarial patch attacks have limited attack effectiveness across diverse physical scenarios. To address this, we propose CDUPatch, a universal cross-modal patch attack against visible-infrared object detectors across scales, views, and scenarios. Specifically, we observe that color variations lead to different levels of thermal absorption, resulting in temperature differences in infrared imaging. Leveraging this property, we propose an RGB-to-infrared adapter that maps RGB patches to infrared patches, enabling unified optimization of cross-modal patches. By learning an optimal color distribution on the adversarial patch, we can manipulate its thermal response and generate an adversarial infrared texture. Additionally, we introduce a multi-scale clipping strategy and construct a new visible-infrared dataset, MSDrone, which contains aerial vehicle images in varying scales and perspectives. These data augmentation strategies enhance the robustness of our patch in real-world conditions. Experiments on four benchmark datasets (e.g., DroneVehicle, LLVIP, VisDrone, MSDrone) show that our method outperforms existing patch attacks in the digital domain. Extensive physical tests further confirm strong transferability across scales, views, and scenarios.",
        "arxiv_id": "2504.10888",
        "ARXIVID": "2504.10888",
        "COMMENT": "Does not match any specific criteria but involves adversarial attacks in dual-modal detectors, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.11015": {
        "authors": [
            "Chenyang Zhu",
            "Xing Zhang",
            "Yuyang Sun",
            "Ching-Chun Chang",
            "Isao Echizen"
        ],
        "title": "AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and Localization in Diffusion Era",
        "abstract": "arXiv:2504.11015v1 Announce Type: new  Abstract: Recent advances in image generation, particularly diffusion models, have significantly lowered the barrier for creating sophisticated forgeries, making image manipulation detection and localization (IMDL) increasingly challenging. While prior work in IMDL has focused largely on natural images, the anime domain remains underexplored-despite its growing vulnerability to AI-generated forgeries. Misrepresentations of AI-generated images as hand-drawn artwork, copyright violations, and inappropriate content modifications pose serious threats to the anime community and industry. To address this gap, we propose AnimeDL-2M, the first large-scale benchmark for anime IMDL with comprehensive annotations. It comprises over two million images including real, partially manipulated, and fully AI-generated samples. Experiments indicate that models trained on existing IMDL datasets of natural images perform poorly when applied to anime images, highlighting a clear domain gap between anime and natural images. To better handle IMDL tasks in anime domain, we further propose AniXplore, a novel model tailored to the visual characteristics of anime imagery. Extensive evaluations demonstrate that AniXplore achieves superior performance compared to existing methods. Dataset and code can be found in https://flytweety.github.io/AnimeDL2M/.",
        "arxiv_id": "2504.11015",
        "ARXIVID": "2504.11015",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and domain-specific detection tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.10986": {
        "authors": [
            "Bo-Cheng Hu",
            "Ge-Peng Ji",
            "Dian Shao",
            "Deng-Ping Fan"
        ],
        "title": "PraNet-V2: Dual-Supervised Reverse Attention for Medical Image Segmentation",
        "abstract": "arXiv:2504.10986v1 Announce Type: new  Abstract: Accurate medical image segmentation is essential for effective diagnosis and treatment. Previously, PraNet-V1 was proposed to enhance polyp segmentation by introducing a reverse attention (RA) module that utilizes background information. However, PraNet-V1 struggles with multi-class segmentation tasks. To address this limitation, we propose PraNet-V2, which, compared to PraNet-V1, effectively performs a broader range of tasks including multi-class segmentation. At the core of PraNet-V2 is the Dual-Supervised Reverse Attention (DSRA) module, which incorporates explicit background supervision, independent background modeling, and semantically enriched attention fusion. Our PraNet-V2 framework demonstrates strong performance on four polyp segmentation datasets. Additionally, by integrating DSRA to iteratively enhance foreground segmentation results in three state-of-the-art semantic segmentation models, we achieve up to a 1.36% improvement in mean Dice score. Code is available at: https://github.com/ai4colonoscopy/PraNet-V2/tree/main/binary_seg/jittor.",
        "arxiv_id": "2504.10986",
        "ARXIVID": "2504.10986",
        "COMMENT": "This paper does not match any of the specific criteria. It focuses on medical image segmentation, which is outside the scope of the listed interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.10979": {
        "authors": [
            "Pancheng Zhao",
            "Deng-Ping Fan",
            "Shupeng Cheng",
            "Salman Khan",
            "Fahad Shahbaz Khan",
            "David Clifton",
            "Peng Xu",
            "Jufeng Yang"
        ],
        "title": "Deep Learning in Concealed Dense Prediction",
        "abstract": "arXiv:2504.10979v1 Announce Type: new  Abstract: Deep learning is developing rapidly and handling common computer vision tasks well. It is time to pay attention to more complex vision tasks, as model size, knowledge, and reasoning capabilities continue to improve. In this paper, we introduce and review a family of complex tasks, termed Concealed Dense Prediction (CDP), which has great value in agriculture, industry, etc. CDP's intrinsic trait is that the targets are concealed in their surroundings, thus fully perceiving them requires fine-grained representations, prior knowledge, auxiliary reasoning, etc. The contributions of this review are three-fold: (i) We introduce the scope, characteristics, and challenges specific to CDP tasks and emphasize their essential differences from generic vision tasks. (ii) We develop a taxonomy based on concealment counteracting to summarize deep learning efforts in CDP through experiments on three tasks. We compare 25 state-of-the-art methods across 12 widely used concealed datasets. (iii) We discuss the potential applications of CDP in the large model era and summarize 6 potential research directions. We offer perspectives for the future development of CDP by constructing a large-scale multimodal instruction fine-tuning dataset, CvpINST, and a concealed visual perception agent, CvpAgent.",
        "arxiv_id": "2504.10979",
        "ARXIVID": "2504.10979",
        "COMMENT": "Does not match any specific criteria but reviews concealed dense prediction tasks, which is tangentially relevant to your friend's interest in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.10808": {
        "authors": [
            "Md Rakibul Hasan",
            "Shafin Rahman",
            "Md Zakir Hossain",
            "Aneesh Krishna",
            "Tom Gedeon"
        ],
        "title": "Tabular foundation model to detect empathy from visual cues",
        "abstract": "arXiv:2504.10808v1 Announce Type: new  Abstract: Detecting empathy from video interactions is an emerging area of research. Video datasets, however, are often released as extracted features (i.e., tabular data) rather than raw footage due to privacy and ethical concerns. Prior research on such tabular datasets established tree-based classical machine learning approaches as the best-performing models. Motivated by the recent success of textual foundation models (i.e., large language models), we explore the use of tabular foundation models in empathy detection from tabular visual features. We experiment with two recent tabular foundation models $-$ TabPFN v2 and TabICL $-$ through in-context learning and fine-tuning setups. Our experiments on a public human-robot interaction benchmark demonstrate a significant boost in cross-subject empathy detection accuracy over several strong baselines (accuracy: $0.590 \\rightarrow 0.730$; AUC: $0.564 \\rightarrow 0.669$). In addition to performance improvement, we contribute novel insights and an evaluation setup to ensure generalisation on unseen subjects in this public benchmark. As the practice of releasing video features as tabular datasets is likely to persist due to privacy constraints, our findings will be widely applicable to future empathy detection video datasets as well.",
        "arxiv_id": "2504.10808",
        "ARXIVID": "2504.10808",
        "COMMENT": "Does not match any specific criteria but explores tabular foundation models, which is tangentially relevant to your friend's interest in machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.11326": {
        "authors": [
            "Henghui Ding",
            "Chang Liu",
            "Nikhila Ravi",
            "Shuting He",
            "Yunchao Wei",
            "Song Bai",
            "Philip Torr",
            "Kehuan Song",
            "Xinglin Xie",
            "Kexin Zhang",
            "Licheng Jiao",
            "Lingling Li",
            "Shuyuan Yang",
            "Xuqiang Cao",
            "Linnan Zhao",
            "Jiaxuan Zhao",
            "Fang Liu",
            "Mengjiao Wang",
            "Junpei Zhang",
            "Xu Liu",
            "Yuting Yang",
            "Mengru Ma",
            "Hao Fang",
            "Runmin Cong",
            "Xiankai Lu",
            "Zhiyang Che",
            "Wei Zhan",
            "Tianming Liang",
            "Haichao Jiang",
            "Wei-Shi Zheng",
            "Jian-Fang Hu",
            "Haobo Yuan",
            "Xiangtai Li",
            "Tao Zhang",
            "Lu Qi",
            "Ming-Hsuan Yang"
        ],
        "title": "PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild",
        "abstract": "arXiv:2504.11326v1 Announce Type: new  Abstract: This report provides a comprehensive overview of the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025. It summarizes the challenge outcomes, participating methodologies, and future research directions. The challenge features two tracks: MOSE, which focuses on complex scene video object segmentation, and MeViS, which targets motion-guided, language-based video segmentation. Both tracks introduce new, more challenging datasets designed to better reflect real-world scenarios. Through detailed evaluation and analysis, the challenge offers valuable insights into the current state-of-the-art and emerging trends in complex video segmentation. More information can be found on the workshop website: https://pvuw.github.io/.",
        "arxiv_id": "2504.11326",
        "ARXIVID": "2504.11326",
        "COMMENT": "Does not match any specific criteria but involves video segmentation challenges, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.11310": {
        "authors": [
            "Dayong Liu",
            "Qingrui Zhang",
            "Zeyang Meng"
        ],
        "title": "Intelligent driving vehicle front multi-target tracking and detection based on YOLOv5 and point cloud 3D projection",
        "abstract": "arXiv:2504.11310v1 Announce Type: new  Abstract: In multi-target tracking and detection tasks, it is necessary to continuously track multiple targets, such as vehicles, pedestrians, etc. To achieve this goal, the system must be able to continuously acquire and process image frames containing these targets. These consecutive frame images enable the algorithm to update the position and state of the target in real-time in each frame of the image. How to accurately associate the detected target with the target in the previous or next frame to form a stable trajectory is a complex problem. Therefore, a multi object tracking and detection method for intelligent driving vehicles based on YOLOv5 and point cloud 3D projection is proposed. Using Retinex algorithm to enhance the image of the environment in front of the vehicle, remove light interference in the image, and build an intelligent detection model based on YOLOv5 network structure. The enhanced image is input into the model, and multiple targets in front of the vehicle are identified through feature extraction and target localization. By combining point cloud 3D projection technology, the correlation between the position changes of adjacent frame images in the projection coordinate system can be inferred. By sequentially projecting the multi-target recognition results of multiple consecutive frame images into the 3D laser point cloud environment, effective tracking of the motion trajectories of all targets in front of the vehicle can be achieved. The experimental results show that the application of this method for intelligent driving vehicle front multi-target tracking and detection yields a MOTA (Tracking Accuracy) value greater than 30, demonstrating its superior tracking and detection performance.",
        "arxiv_id": "2504.11310",
        "ARXIVID": "2504.11310",
        "COMMENT": "Does not match any specific criteria but involves multi-target tracking and detection, which is tangentially related to spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.11128": {
        "authors": [
            "P. Tomkiewicz",
            "J. Jaworski",
            "P. Zielonka",
            "A. Wilinski"
        ],
        "title": "K-means Enhanced Density Gradient Analysis for Urban and Transport Metrics Using Multi-Modal Satellite Imagery",
        "abstract": "arXiv:2504.11128v1 Announce Type: new  Abstract: This paper presents a novel computational approach for evaluating urban metrics through density gradient analysis using multi-modal satellite imagery, with applications including public transport and other urban systems. By combining optical and Synthetic Aperture Radar (SAR) data, we develop a method to segment urban areas, identify urban centers, and quantify density gradients. Our approach calculates two key metrics: the density gradient coefficient ($\\alpha$) and the minimum effective distance (LD) at which density reaches a target threshold. We further employ machine learning techniques, specifically K-means clustering, to objectively identify uniform and high-variability regions within density gradient plots. We demonstrate that these metrics provide an effective screening tool for public transport analyses by revealing the underlying urban structure. Through comparative analysis of two representative cities with contrasting urban morphologies (monocentric vs polycentric), we establish relationships between density gradient characteristics and public transport network topologies. Cities with clear density peaks in their gradient plots indicate distinct urban centers requiring different transport strategies than those with more uniform density distributions. This methodology offers urban planners a cost-effective, globally applicable approach to preliminary public transport assessment using freely available satellite data. The complete implementation, with additional examples and documentation, is available in an open-source repository under the MIT license at https://github.com/nexri/Satellite-Imagery-Urban-Analysis.",
        "arxiv_id": "2504.11128",
        "ARXIVID": "2504.11128",
        "COMMENT": "Does not match any specific criteria but involves multi-modal satellite imagery and urban analysis, which is tangentially related to spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}