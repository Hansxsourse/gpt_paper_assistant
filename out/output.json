{
    "2503.00329": {
        "authors": [
            "Benjamin Schneider",
            "Florian Kerschbaum",
            "Wenhu Chen"
        ],
        "title": "ABC: Achieving Better Control of Multimodal Embeddings using VLMs",
        "abstract": "arXiv:2503.00329v1 Announce Type: new  Abstract: Visual embedding models excel at zero-shot tasks like visual retrieval and classification. However, these models cannot be used for tasks that contain ambiguity or require user instruction. These tasks necessitate a multimodal embedding model, which outputs embeddings that combine visual and natural language input. Existing CLIP-based approaches embed images and text independently, and fuse the result. We find that this results in weak interactions between modalities, and poor user control over the representation. We introduce ABC, an open-source multimodal embedding model that uses a vision-language model backbone to deeply integrate image features with natural language instructions. ABC achieves bestfor-size performance on MSCOCO image-to-text retrieval and is the top performing model on classification and VQA tasks in the Massive Multimodal Embedding Benchmark. With a strongly unified vision-language representation, ABC can use natural language to solve subtle and potentially ambiguous visual retrieval problems. To evaluate this capability, we design CtrlBench, a benchmark that requires interleaving textual instructions with image content for correct retrieval. ABC advances the state of multimodal embeddings by offering high-quality representations and flexible natural language control. Our model and datasets are available at our project page.",
        "arxiv_id": "2503.00329",
        "ARXIVID": "2503.00329",
        "COMMENT": "Matches criterion 2 as it introduces a new multimodal embedding model (ABC) that deeply integrates vision and language features, achieving state-of-the-art performance on multiple benchmarks. It also introduces a new benchmark (CtrlBench) for evaluating multimodal embeddings.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.00359": {
        "authors": [
            "Qianqian Shen",
            "Yunhan Zhao",
            "Nahyun Kwon",
            "Jeeeun Kim",
            "Yanan Li",
            "Shu Kong"
        ],
        "title": "Solving Instance Detection from an Open-World Perspective",
        "abstract": "arXiv:2503.00359v1 Announce Type: new  Abstract: Instance detection (InsDet) aims to localize specific object instances within a novel scene imagery based on given visual references. Technically, it requires proposal detection to identify all possible object instances, followed by instance-level matching to pinpoint the ones of interest. Its open-world nature supports its wide-ranging applications from robotics to AR/VR, but also presents significant challenges: methods must generalize to unknown testing data distributions because (1) the testing scene imagery is unseen during training, and (2) there are domain gaps between visual references and detected proposals. Existing methods attempt to tackle these challenges by synthesizing diverse training examples or utilizing off-the-shelf foundation models (FMs). However, they only partially capitalize the available open-world information. In this paper, we approach InsDet from an Open-World perspective, introducing our method IDOW. We find that, while pretrained FMs yield high recall in instance detection, they are not specifically optimized for instance-level feature matching. To address this, we adapt pretrained FMs for improved instance-level matching using open-world data. Our approach incorporates metric learning along with novel data augmentations, which sample distractors as negative examples and synthesize novel-view instances to enrich the visual references. Extensive experiments demonstrate that our method significantly outperforms prior works, achieving >10 AP over previous results on two recently released challenging benchmark datasets in both conventional and novel instance detection settings.",
        "arxiv_id": "2503.00359",
        "ARXIVID": "2503.00359",
        "COMMENT": "Matches criterion 3 as it focuses on a novel method for instance detection in open-world settings, addressing challenges with domain gaps and unseen data distributions. It also introduces new data augmentations and metric learning for improved instance-level matching.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.01342": {
        "authors": [
            "Hao Tang",
            "Chenwei Xie",
            "Haiyang Wang",
            "Xiaoyi Bao",
            "Tingyu Weng",
            "Pandeng Li",
            "Yun Zheng",
            "Liwei Wang"
        ],
        "title": "UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface",
        "abstract": "arXiv:2503.01342v2 Announce Type: new  Abstract: Generalist models have achieved remarkable success in both language and vision-language tasks, showcasing the potential of unified modeling. However, effectively integrating fine-grained perception tasks like detection and segmentation into these models remains a significant challenge. This is primarily because these tasks often rely heavily on task-specific designs and architectures that can complicate the modeling process. To address this challenge, we present \\ours, a framework that \\textbf{U}nifies \\textbf{F}ine-grained visual perception tasks through an \\textbf{O}pen-ended language interface. By transforming all perception targets into the language space, \\ours unifies object-level detection, pixel-level segmentation, and image-level vision-language tasks into a single model. Additionally, we introduce a novel embedding retrieval approach that relies solely on the language interface to support segmentation tasks. Our framework bridges the gap between fine-grained perception and vision-language tasks, significantly simplifying architectural design and training strategies while achieving comparable or superior performance to methods with intricate task-specific designs. After multi-task training on five standard visual perception datasets, \\ours outperforms the previous state-of-the-art generalist models by 12.3 mAP on COCO instance segmentation and 3.3 mIoU on ADE20K semantic segmentation. Furthermore, our method seamlessly integrates with existing MLLMs, effectively combining fine-grained perception capabilities with their advanced language abilities, thereby enabling more challenging tasks such as reasoning segmentation. Code and models are available at https://github.com/nnnth/UFO.",
        "arxiv_id": "2503.01342",
        "ARXIVID": "2503.01342",
        "COMMENT": "Matches criterion 2 as it introduces a unified framework for fine-grained visual perception tasks and integrates with MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.00495": {
        "authors": [
            "Xuanchen Li",
            "Jianyu Wang",
            "Yuhao Cheng",
            "Yikun Zeng",
            "Xingyu Ren",
            "Wenhan Zhu",
            "Weiming Zhao",
            "Yichao Yan"
        ],
        "title": "Towards High-fidelity 3D Talking Avatar with Personalized Dynamic Texture",
        "abstract": "arXiv:2503.00495v1 Announce Type: new  Abstract: Significant progress has been made for speech-driven 3D face animation, but most works focus on learning the motion of mesh/geometry, ignoring the impact of dynamic texture. In this work, we reveal that dynamic texture plays a key role in rendering high-fidelity talking avatars, and introduce a high-resolution 4D dataset \\textbf{TexTalk4D}, consisting of 100 minutes of audio-synced scan-level meshes with detailed 8K dynamic textures from 100 subjects. Based on the dataset, we explore the inherent correlation between motion and texture, and propose a diffusion-based framework \\textbf{TexTalker} to simultaneously generate facial motions and dynamic textures from speech. Furthermore, we propose a novel pivot-based style injection strategy to capture the complicity of different texture and motion styles, which allows disentangled control. TexTalker, as the first method to generate audio-synced facial motion with dynamic texture, not only outperforms the prior arts in synthesising facial motions, but also produces realistic textures that are consistent with the underlying facial movements. Project page: https://xuanchenli.github.io/TexTalk/.",
        "arxiv_id": "2503.00495",
        "ARXIVID": "2503.00495",
        "COMMENT": "This paper matches criterion 4 as it introduces a high-fidelity 3D talking avatar with dynamic texture, leveraging vision foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.00413": {
        "authors": [
            "Tianyu Huai",
            "Jie Zhou",
            "Xingjiao Wu",
            "Qin Chen",
            "Qingchun Bai",
            "Ze Zhou",
            "Liang He"
        ],
        "title": "CL-MoE: Enhancing Multimodal Large Language Model with Dual Momentum Mixture-of-Experts for Continual Visual Question Answering",
        "abstract": "arXiv:2503.00413v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have garnered widespread attention from researchers due to their remarkable understanding and generation capabilities in visual language tasks (e.g., visual question answering). However, the rapid pace of knowledge updates in the real world makes offline training of MLLMs costly, and when faced with non-stationary data streams, MLLMs suffer from catastrophic forgetting during learning. In this paper, we propose an MLLMs-based dual momentum Mixture-of-Experts (CL-MoE) framework for continual visual question answering (VQA). We integrate MLLMs with continual learning to utilize the rich commonsense knowledge in LLMs. We introduce a Dual-Router MoE (RMoE) strategy to select the global and local experts using task-level and instance-level routers, to robustly assign weights to the experts most appropriate for the task. Then, we design a dynamic Momentum MoE (MMoE) to update the parameters of experts dynamically based on the relationships between the experts and tasks/instances, so that the model can absorb new knowledge while maintaining existing knowledge. The extensive experimental results indicate that our method achieves state-of-the-art performance on 10 VQA tasks, proving the effectiveness of our approach.",
        "arxiv_id": "2503.00413",
        "ARXIVID": "2503.00413",
        "COMMENT": "Matches criterion 2 as it proposes a new MLLM framework (CL-MoE) for continual visual question answering.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.00371": {
        "authors": [
            "Xuehao Gao",
            "Yang Yang",
            "Shaoyi Du",
            "Guo-Jun Qi",
            "Junwei Han"
        ],
        "title": "Jointly Understand Your Command and Intention:Reciprocal Co-Evolution between Scene-Aware 3D Human Motion Synthesis and Analysis",
        "abstract": "arXiv:2503.00371v1 Announce Type: new  Abstract: As two intimate reciprocal tasks, scene-aware human motion synthesis and analysis require a joint understanding between multiple modalities, including 3D body motions, 3D scenes, and textual descriptions. In this paper, we integrate these two paired processes into a Co-Evolving Synthesis-Analysis (CESA) pipeline and mutually benefit their learning. Specifically, scene-aware text-to-human synthesis generates diverse indoor motion samples from the same textual description to enrich human-scene interaction intra-class diversity, thus significantly benefiting training a robust human motion analysis system. Reciprocally, human motion analysis would enforce semantic scrutiny on each synthesized motion sample to ensure its semantic consistency with the given textual description, thus improving realistic motion synthesis. Considering that real-world indoor human motions are goal-oriented and path-guided, we propose a cascaded generation strategy that factorizes text-driven scene-specific human motion generation into three stages: goal inferring, path planning, and pose synthesizing. Coupling CESA with this powerful cascaded motion synthesis model, we jointly improve realistic human motion synthesis and robust human motion analysis in 3D scenes.",
        "arxiv_id": "2503.00371",
        "ARXIVID": "2503.00371",
        "COMMENT": "Matches criterion 3 as it proposes a novel pipeline for scene-aware 3D human motion synthesis and analysis, focusing on joint understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.01547": {
        "authors": [
            "Arash Nasr Esfahani",
            "Hamed Hosseini",
            "Mehdi Tale Masouleh",
            "Ahmad Kalhor",
            "Hedieh Sajedi"
        ],
        "title": "AI-Driven Relocation Tracking in Dynamic Kitchen Environments",
        "abstract": "arXiv:2503.01547v1 Announce Type: new  Abstract: As smart homes become more prevalent in daily life, the ability to understand dynamic environments is essential which is increasingly dependent on AI systems. This study focuses on developing an intelligent algorithm which can navigate a robot through a kitchen, recognizing objects, and tracking their relocation. The kitchen was chosen as the testing ground due to its dynamic nature as objects are frequently moved, rearranged and replaced. Various techniques, such as SLAM feature-based tracking and deep learning-based object detection (e.g., Faster R-CNN), are commonly used for object tracking. Additionally, methods such as optical flow analysis and 3D reconstruction have also been used to track the relocation of objects. These approaches often face challenges when it comes to problems such as lighting variations and partial occlusions, where parts of the object are hidden in some frames but visible in others. The proposed method in this study leverages the YOLOv5 architecture, initialized with pre-trained weights and subsequently fine-tuned on a custom dataset. A novel method was developed, introducing a frame-scoring algorithm which calculates a score for each object based on its location and features within all frames. This scoring approach helps to identify changes by determining the best-associated frame for each object and comparing the results in each scene, overcoming limitations seen in other methods while maintaining simplicity in design. The experimental results demonstrate an accuracy of 97.72%, a precision of 95.83% and a recall of 96.84% for this algorithm, which highlights the efficacy of the model in detecting spatial changes.",
        "arxiv_id": "2503.01547",
        "ARXIVID": "2503.01547",
        "COMMENT": "Matches criterion 1 as it focuses on spatial understanding and object tracking in dynamic environments.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.00068": {
        "authors": [
            "Ziyu Wu",
            "Yufan Xiong",
            "Mengting Niu",
            "Fangting Xie",
            "Quan Wan",
            "Qijun Ying",
            "Boyan Liu",
            "Xiaohui Cai"
        ],
        "title": "PI-HMR: Towards Robust In-bed Temporal Human Shape Reconstruction with Contact Pressure Sensing",
        "abstract": "arXiv:2503.00068v1 Announce Type: new  Abstract: Long-term in-bed monitoring benefits automatic and real-time health management within healthcare, and the advancement of human shape reconstruction technologies further enhances the representation and visualization of users' activity patterns. However, existing technologies are primarily based on visual cues, facing serious challenges in non-light-of-sight and privacy-sensitive in-bed scenes. Pressure-sensing bedsheets offer a promising solution for real-time motion reconstruction. Yet, limited exploration in model designs and data have hindered its further development. To tackle these issues, we propose a general framework that bridges gaps in data annotation and model design. Firstly, we introduce SMPLify-IB, an optimization method that overcomes the depth ambiguity issue in top-view scenarios through gravity constraints, enabling generating high-quality 3D human shape annotations for in-bed datasets. Then we present PI-HMR, a temporal-based human shape estimator to regress meshes from pressure sequences. By integrating multi-scale feature fusion with high-pressure distribution and spatial position priors, PI-HMR outperforms SOTA methods with 17.01mm Mean-Per-Joint-Error decrease. This work provides a whole",
        "arxiv_id": "2503.00068",
        "ARXIVID": "2503.00068",
        "COMMENT": "This paper matches criterion 3 as it proposes a novel framework for in-bed human shape reconstruction using pressure-sensing bedsheets, which is a new angle in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.01122": {
        "authors": [
            "Shizhan Liu",
            "Hao Zheng",
            "Hang Yu",
            "Jianguo Li"
        ],
        "title": "ACCORD: Alleviating Concept Coupling through Dependence Regularization for Text-to-Image Diffusion Personalization",
        "abstract": "arXiv:2503.01122v1 Announce Type: new  Abstract: Image personalization has garnered attention for its ability to customize Text-to-Image generation using only a few reference images. However, a key challenge in image personalization is the issue of conceptual coupling, where the limited number of reference images leads the model to form unwanted associations between the personalization target and other concepts. Current methods attempt to tackle this issue indirectly, leading to a suboptimal balance between text control and personalization fidelity. In this paper, we take a direct approach to the concept coupling problem through statistical analysis, revealing that it stems from two distinct sources of dependence discrepancies. We therefore propose two complementary plug-and-play loss functions: Denoising Decouple Loss and Prior Decouple loss, each designed to minimize one type of dependence discrepancy. Extensive experiments demonstrate that our approach achieves a superior trade-off between text control and personalization fidelity.",
        "arxiv_id": "2503.01122",
        "ARXIVID": "2503.01122",
        "COMMENT": "Matches criterion 4 as it addresses text-to-image diffusion personalization with a novel statistical approach.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2503.01531": {
        "authors": [
            "Songlin Dong",
            "Zhengdong Zhou",
            "Chenhao Ding",
            "Xinyuan Gao",
            "Alex Kot",
            "Yihong Gong"
        ],
        "title": "Diversity Covariance-Aware Prompt Learning for Vision-Language Models",
        "abstract": "arXiv:2503.01531v1 Announce Type: new  Abstract: Prompt tuning can further enhance the performance of visual-language models across various downstream tasks (e.g., few-shot learning), enabling them to better adapt to specific applications and needs. In this paper, we present a Diversity Covariance-Aware framework that learns distributional information from the data to enhance the few-shot ability of the prompt model. First, we propose a covariance-aware method that models the covariance relationships between visual features and uses anisotropic Mahalanobis distance, instead of the suboptimal cosine distance, to measure the similarity between two modalities. We rigorously derive and prove the validity of this modeling process. Then, we propose the diversity-aware method, which learns multiple diverse soft prompts to capture different attributes of categories and aligns them independently with visual modalities. This method achieves multi-centered covariance modeling, leading to more diverse decision boundaries. Extensive experiments on 11 datasets in various tasks demonstrate the effectiveness of our method.",
        "arxiv_id": "2503.01531",
        "ARXIVID": "2503.01531",
        "COMMENT": "Matches criterion 4 as it proposes a novel prompt learning framework for vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.01416": {
        "authors": [
            "Ramanathan Rajendiran",
            "Debaditya Roy",
            "Basura Fernando"
        ],
        "title": "Learning to Generate Long-term Future Narrations Describing Activities of Daily Living",
        "abstract": "arXiv:2503.01416v1 Announce Type: new  Abstract: Anticipating future events is crucial for various application domains such as healthcare, smart home technology, and surveillance. Narrative event descriptions provide context-rich information, enhancing a system's future planning and decision-making capabilities. We propose a novel task: $\\textit{long-term future narration generation}$, which extends beyond traditional action anticipation by generating detailed narrations of future daily activities. We introduce a visual-language model, ViNa, specifically designed to address this challenging task. ViNa integrates long-term videos and corresponding narrations to generate a sequence of future narrations that predict subsequent events and actions over extended time horizons. ViNa extends existing multimodal models that perform only short-term predictions or describe observed videos by generating long-term future narrations for a broader range of daily activities. We also present a novel downstream application that leverages the generated narrations called future video retrieval to help users improve planning for a task by visualizing the future. We evaluate future narration generation on the largest egocentric dataset Ego4D.",
        "arxiv_id": "2503.01416",
        "ARXIVID": "2503.01416",
        "COMMENT": "Matches criterion 4 as it introduces a visual-language model (ViNa) for long-term future narration generation, which is a novel application.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2503.01754": {
        "authors": [
            "Guande Wu",
            "Huan Song",
            "Yawei Wang",
            "Qiaojing Yan",
            "Yijun Tian",
            "Lin Lee Cheong",
            "Panpan Xu"
        ],
        "title": "Enhancing Multi-hop Reasoning in Vision-Language Models via Self-Distillation with Multi-Prompt Ensembling",
        "abstract": "arXiv:2503.01754v1 Announce Type: new  Abstract: Multi-modal large language models have seen rapid advancement alongside large language models. However, while language models can effectively leverage chain-of-thought prompting for zero or few-shot learning, similar prompting strategies are less effective for multi-modal LLMs due to modality gaps and task complexity. To address this challenge, we explore two prompting approaches: a dual-query method that separates multi-modal input analysis and answer generation into two prompting steps, and an ensemble prompting method that combines multiple prompt variations to arrive at the final answer. Although these approaches enhance the model's reasoning capabilities without fine-tuning, they introduce significant inference overhead. Therefore, building on top of these two prompting techniques, we propose a self-distillation framework such that the model can improve itself without any annotated data. Our self-distillation framework learns representation intervention modules from the reasoning traces collected from ensembled dual-query prompts, in the form of hidden representations. The lightweight intervention modules operate in parallel with the frozen original model, which makes it possible to maintain computational efficiency while significantly improving model capability. We evaluate our method on five widely-used VQA benchmarks, demonstrating its effectiveness in performing multi-hop reasoning for complex tasks.",
        "arxiv_id": "2503.01754",
        "ARXIVID": "2503.01754",
        "COMMENT": "Matches criterion 2 as it enhances reasoning in multi-modal large language models using self-distillation and multi-prompt ensembling.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.01208": {
        "authors": [
            "Tianjie Ju",
            "Yi Hua",
            "Hao Fei",
            "Zhenyu Shao",
            "Yubin Zheng",
            "Haodong Zhao",
            "Mong-Li Lee",
            "Wynne Hsu",
            "Zhuosheng Zhang",
            "Gongshen Liu"
        ],
        "title": "Watch Out Your Album! On the Inadvertent Privacy Memorization in Multi-Modal Large Language Models",
        "abstract": "arXiv:2503.01208v1 Announce Type: new  Abstract: Multi-Modal Large Language Models (MLLMs) have exhibited remarkable performance on various vision-language tasks such as Visual Question Answering (VQA). Despite accumulating evidence of privacy concerns associated with task-relevant content, it remains unclear whether MLLMs inadvertently memorize private content that is entirely irrelevant to the training tasks. In this paper, we investigate how randomly generated task-irrelevant private content can become spuriously correlated with downstream objectives due to partial mini-batch training dynamics, thus causing inadvertent memorization. Concretely, we randomly generate task-irrelevant watermarks into VQA fine-tuning images at varying probabilities and propose a novel probing framework to determine whether MLLMs have inadvertently encoded such content. Our experiments reveal that MLLMs exhibit notably different training behaviors in partial mini-batch settings with task-irrelevant watermarks embedded. Furthermore, through layer-wise probing, we demonstrate that MLLMs trigger distinct representational patterns when encountering previously seen task-irrelevant knowledge, even if this knowledge does not influence their output during prompting. Our code is available at https://github.com/illusionhi/ProbingPrivacy.",
        "arxiv_id": "2503.01208",
        "ARXIVID": "2503.01208",
        "COMMENT": "Matches criterion 2 as it investigates privacy memorization in MLLMs, which is a novel angle in vision-language models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.00747": {
        "authors": [
            "Fei Teng",
            "Buyin Deng",
            "Boyuan Zheng",
            "Kai Luo",
            "Kunyu Peng",
            "Jiaming Zhang",
            "Kailun Yang"
        ],
        "title": "Unifying Light Field Perception with Field of Parallax",
        "abstract": "arXiv:2503.00747v1 Announce Type: new  Abstract: Field of Parallax (FoP)}, a spatial field that distills the common features from different LF representations to provide flexible and consistent support for multi-task learning. FoP is built upon three core features--projection difference, adjacency divergence, and contextual consistency--which are essential for cross-task adaptability. To implement FoP, we design a two-step angular adapter: the first step captures angular-specific differences, while the second step consolidates contextual consistency to ensure robust representation. Leveraging the FoP-based representation, we introduce the LFX framework, the first to handle arbitrary LF representations seamlessly, unifying LF multi-task vision. We evaluated LFX across three different tasks, achieving new state-of-the-art results, compared with previous task-specific architectures: 84.74% in mIoU for semantic segmentation on UrbanLF, 0.84% in AP for object detection on PKU, and 0.030 in MAE and 0.026 in MAE for salient object detection on Duftv2 and PKU, respectively. The source code will be made publicly available at https://github.com/warriordby/LFX.",
        "arxiv_id": "2503.00747",
        "ARXIVID": "2503.00747",
        "COMMENT": "This paper aligns with criterion 1 as it introduces a novel spatial representation (Field of Parallax) for multi-task learning in light field perception.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.01144": {
        "authors": [
            "Zhenqi Dai",
            "Ting Liu",
            "Xingxing Zhang",
            "Yunchao Wei",
            "Yanning Zhang"
        ],
        "title": "One-shot In-context Part Segmentation",
        "abstract": "arXiv:2503.01144v1 Announce Type: new  Abstract: In this paper, we present the One-shot In-context Part Segmentation (OIParts) framework, designed to tackle the challenges of part segmentation by leveraging visual foundation models (VFMs). Existing training-based one-shot part segmentation methods that utilize VFMs encounter difficulties when faced with scenarios where the one-shot image and test image exhibit significant variance in appearance and perspective, or when the object in the test image is partially visible. We argue that training on the one-shot example often leads to overfitting, thereby compromising the model's generalization capability. Our framework offers a novel approach to part segmentation that is training-free, flexible, and data-efficient, requiring only a single in-context example for precise segmentation with superior generalization ability. By thoroughly exploring the complementary strengths of VFMs, specifically DINOv2 and Stable Diffusion, we introduce an adaptive channel selection approach by minimizing the intra-class distance for better exploiting these two features, thereby enhancing the discriminatory power of the extracted features for the fine-grained parts. We have achieved remarkable segmentation performance across diverse object categories. The OIParts framework not only eliminates the need for extensive labeled data but also demonstrates superior generalization ability. Through comprehensive experimentation on three benchmark datasets, we have demonstrated the superiority of our proposed method over existing part segmentation approaches in one-shot settings.",
        "arxiv_id": "2503.01144",
        "ARXIVID": "2503.01144",
        "COMMENT": "Matches criterion 4 as it leverages visual foundation models (VFMs) for one-shot part segmentation with a novel training-free approach.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.01582": {
        "authors": [
            "Saad Ejaz",
            "Hriday Bavle",
            "Laura Ribeiro",
            "Holger Voos",
            "Jose Luis Sanchez-Lopez"
        ],
        "title": "Category-level Meta-learned NeRF Priors for Efficient Object Mapping",
        "abstract": "arXiv:2503.01582v1 Announce Type: new  Abstract: In 3D object mapping, category-level priors enable efficient object reconstruction and canonical pose estimation, requiring only a single prior per semantic category (e.g., chair, book, laptop). Recently, DeepSDF has predominantly been used as a category-level shape prior, but it struggles to reconstruct sharp geometry and is computationally expensive. In contrast, NeRFs capture fine details but have yet to be effectively integrated with category-level priors in a real-time multi-object mapping framework. To bridge this gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapper that integrates category-level priors with object-level NeRFs to enhance reconstruction efficiency while enabling canonical object pose estimation. PRENOM gets to know objects on a first-name basis by meta-learning on synthetic reconstruction tasks generated from open-source shape datasets. To account for object category variations, it employs a multi-objective genetic algorithm to optimize the NeRF architecture for each category, balancing reconstruction quality and training time. Additionally, prior-based probabilistic ray sampling directs sampling toward expected object regions, accelerating convergence and improving reconstruction quality under constrained resources. Experimental results on a low-end GPU highlight the ability of PRENOM to achieve high-quality reconstructions while maintaining computational feasibility. Specifically, comparisons with prior-free NeRF-based approaches on a synthetic dataset show a 21% lower Chamfer distance, demonstrating better reconstruction quality. Furthermore, evaluations against other approaches using shape priors on a noisy real-world dataset indicate a 13% improvement averaged across all reconstruction metrics, a boost in rotation estimation accuracy, and comparable translation and size estimation performance, while being trained for 5x less time.",
        "arxiv_id": "2503.01582",
        "ARXIVID": "2503.01582",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for object mapping using NeRFs and category-level priors, focusing on efficiency and reconstruction quality.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.01175": {
        "authors": [
            "Hongye Cheng",
            "Tianyu Wang",
            "Guangsi Shi",
            "Zexing Zhao",
            "Yanwei Fu"
        ],
        "title": "HOP: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech Gesture Generation",
        "abstract": "arXiv:2503.01175v1 Announce Type: new  Abstract: Co-speech gestures are crucial non-verbal cues that enhance speech clarity and expressiveness in human communication, which have attracted increasing attention in multimodal research. While the existing methods have made strides in gesture accuracy, challenges remain in generating diverse and coherent gestures, as most approaches assume independence among multimodal inputs and lack explicit modeling of their interactions. In this work, we propose a novel multimodal learning method named HOP for co-speech gesture generation that captures the heterogeneous entanglement between gesture motion, audio rhythm, and text semantics, enabling the generation of coordinated gestures. By leveraging spatiotemporal graph modeling, we achieve the alignment of audio and action. Moreover, to enhance modality coherence, we build the audio-text semantic representation based on a reprogramming module, which is beneficial for cross-modality adaptation. Our approach enables the trimodal system to learn each other's features and represent them in the form of topological entanglement. Extensive experiments demonstrate that HOP achieves state-of-the-art performance, offering more natural and expressive co-speech gesture generation. More information, codes, and demos are available here: https://star-uu-wang.github.io/HOP/",
        "arxiv_id": "2503.01175",
        "ARXIVID": "2503.01175",
        "COMMENT": "This paper matches criterion 2 as it focuses on co-speech gesture generation using multi-modal learning, which involves vision-language models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.01222": {
        "authors": [
            "Wenbin Wang",
            "Yongcheng Jing",
            "Liang Ding",
            "Yingjie Wang",
            "Li Shen",
            "Yong Luo",
            "Bo Du",
            "Dacheng Tao"
        ],
        "title": "Retrieval-Augmented Perception: High-Resolution Image Perception Meets Visual RAG",
        "abstract": "arXiv:2503.01222v1 Announce Type: new  Abstract: High-resolution (HR) image perception remains a key challenge in multimodal large language models (MLLMs). To overcome the limitations of existing methods, this paper shifts away from prior dedicated heuristic approaches and revisits the most fundamental idea to HR perception by enhancing the long-context capability of MLLMs, driven by recent advances in long-context techniques like retrieval-augmented generation (RAG) for general LLMs. Towards this end, this paper presents the first study exploring the use of RAG to address HR perception challenges. Specifically, we propose Retrieval-Augmented Perception (RAP), a training-free framework that retrieves and fuses relevant image crops while preserving spatial context using the proposed Spatial-Awareness Layout. To accommodate different tasks, the proposed Retrieved-Exploration Search (RE-Search) dynamically selects the optimal number of crops based on model confidence and retrieval scores. Experimental results on HR benchmarks demonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving a 43% improvement on $V^*$ Bench and 19% on HR-Bench.",
        "arxiv_id": "2503.01222",
        "ARXIVID": "2503.01222",
        "COMMENT": "Matches criterion 2 as it introduces a novel retrieval-augmented framework for high-resolution image perception in MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.01785": {
        "authors": [
            "Ziyu Liu",
            "Zeyi Sun",
            "Yuhang Zang",
            "Xiaoyi Dong",
            "Yuhang Cao",
            "Haodong Duan",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "title": "Visual-RFT: Visual Reinforcement Fine-Tuning",
        "abstract": "arXiv:2503.01785v1 Announce Type: new  Abstract: Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1 learns from feedback on its answers, which is especially useful in applications when fine-tuning data is scarce. Recent open-source work like DeepSeek-R1 demonstrates that reinforcement learning with verifiable reward is one key direction in reproducing o1. While the R1-style model has demonstrated success in language models, its application in multi-modal domains remains under-explored. This work introduces Visual Reinforcement Fine-Tuning (Visual-RFT), which further extends the application areas of RFT on visual tasks. Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs) to generate multiple responses containing reasoning tokens and final answers for each input, and then uses our proposed visual perception verifiable reward functions to update the model via the policy optimization algorithm such as Group Relative Policy Optimization (GRPO). We design different verifiable reward functions for different perception tasks, such as the Intersection over Union (IoU) reward for object detection. Experimental results on fine-grained image classification, few-shot object detection, reasoning grounding, as well as open-vocabulary object detection benchmarks show the competitive performance and advanced generalization ability of Visual-RFT compared with Supervised Fine-tuning (SFT). For example, Visual-RFT improves accuracy by $24.3\\%$ over the baseline in one-shot fine-grained image classification with around 100 samples. In few-shot object detection, Visual-RFT also exceeds the baseline by $21.9$ on COCO's two-shot setting and $15.4$ on LVIS. Our Visual-RFT represents a paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven approach that enhances reasoning and adaptability for domain-specific tasks.",
        "arxiv_id": "2503.01785",
        "ARXIVID": "2503.01785",
        "COMMENT": "Matches criterion 2 as it introduces a novel reinforcement fine-tuning method for visual tasks using LVLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.02012": {
        "authors": [
            "Parv Kapoor",
            "Abigail Hammer",
            "Ashish Kapoor",
            "Karen Leung",
            "Eunsuk Kang"
        ],
        "title": "Pretrained Embeddings as a Behavior Specification Mechanism",
        "abstract": "arXiv:2503.02012v1 Announce Type: new  Abstract: We propose an approach to formally specifying the behavioral properties of systems that rely on a perception model for interactions with the physical world. The key idea is to introduce embeddings -- mathematical representations of a real-world concept -- as a first-class construct in a specification language, where properties are expressed in terms of distances between a pair of ideal and observed embeddings. To realize this approach, we propose a new type of temporal logic called Embedding Temporal Logic (ETL), and describe how it can be used to express a wider range of properties about AI-enabled systems than previously possible. We demonstrate the applicability of ETL through a preliminary evaluation involving planning tasks in robots that are driven by foundation models; the results are promising, showing that embedding-based specifications can be used to steer a system towards desirable behaviors.",
        "arxiv_id": "2503.02012",
        "ARXIVID": "2503.02012",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for specifying behaviors in embodied AI systems using embeddings.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.01298": {
        "authors": [
            "Yi Wang",
            "Mushui Liu",
            "Wanggui He",
            "Longxiang Zhang",
            "Ziwei Huang",
            "Guanghao Zhang",
            "Fangxun Shu",
            "Zhong Tao",
            "Dong She",
            "Zhelun Yu",
            "Haoyuan Li",
            "Weilong Dai",
            "Mingli Song",
            "Jie Song",
            "Hao Jiang"
        ],
        "title": "MINT: Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation",
        "abstract": "arXiv:2503.01298v1 Announce Type: new  Abstract: Unified generative models have demonstrated extraordinary performance in both text and image generation. However, they tend to underperform when generating intricate images with various interwoven conditions, which is hard to solely rely on straightforward text-to-image generation. In response to this challenge, we introduce MINT, an innovative unified generative model, empowered with native multimodal chain of thought (MCoT) for enhanced image generation for the first time. Firstly, we design Mixture of Transformer Experts (MTXpert), an expert-parallel structure that effectively supports both natural language generation (NLG) and visual capabilities, while avoiding potential modality conflicts that could hinder the full potential of each modality. Building on this, we propose an innovative MCoT training paradigm, a step-by-step approach to multimodal thinking, reasoning, and reflection specifically designed to enhance image generation. This paradigm equips MINT with nuanced, element-wise decoupled alignment and a comprehensive understanding of textual and visual components. Furthermore, it fosters advanced multimodal reasoning and self-reflection, enabling the construction of images that are firmly grounded in the logical relationships between these elements. Notably, MINT has been validated to exhibit superior performance across multiple benchmarks for text-to-image (T2I) and image-to-text (I2T) tasks.",
        "arxiv_id": "2503.01298",
        "ARXIVID": "2503.01298",
        "COMMENT": "Matches criterion 2 as it introduces a unified generative model with multimodal chain of thought for enhanced image generation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.00382": {
        "authors": [
            "Xuehao Gao",
            "Yang Yang",
            "Shaoyi Du",
            "Yang Wu",
            "Yebin Liu",
            "Guo-Jun Qi"
        ],
        "title": "EigenActor: Variant Body-Object Interaction Generation Evolved from Invariant Action Basis Reasoning",
        "abstract": "arXiv:2503.00382v2 Announce Type: new  Abstract: This paper explores a cross-modality synthesis task that infers 3D human-object interactions (HOIs) from a given text-based instruction. Existing text-to-HOI synthesis methods mainly deploy a direct mapping from texts to object-specific 3D body motions, which may encounter a performance bottleneck since the huge cross-modality gap. In this paper, we observe that those HOI samples with the same interaction intention toward different targets, e.g., \"lift a chair\" and \"lift a cup\", always encapsulate similar action-specific body motion patterns while characterizing different object-specific interaction styles. Thus, learning effective action-specific motion priors and object-specific interaction priors is crucial for a text-to-HOI model and dominates its performances on text-HOI semantic consistency and body-object interaction realism. In light of this, we propose a novel body pose generation strategy for the text-to-HOI task: infer object-agnostic canonical body action first and then enrich object-specific interaction styles. Specifically, the first canonical body action inference stage focuses on learning intra-class shareable body motion priors and mapping given text-based semantics to action-specific canonical 3D body motions. Then, in the object-specific interaction inference stage, we focus on object affordance learning and enrich object-specific interaction styles on an inferred action-specific body motion basis. Extensive experiments verify that our proposed text-to-HOI synthesis system significantly outperforms other SOTA methods on three large-scale datasets with better semantic consistency and interaction realism performances.",
        "arxiv_id": "2503.00382",
        "ARXIVID": "2503.00382",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for text-to-3D human-object interaction synthesis, focusing on a new angle of canonical body action inference.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.01115": {
        "authors": [
            "Zhipeng Huang",
            "Shaobin Zhuang",
            "Canmiao Fu",
            "Binxin Yang",
            "Ying Zhang",
            "Chong Sun",
            "Zhizheng Zhang",
            "Yali Wang",
            "Chen Li",
            "Zheng-Jun Zha"
        ],
        "title": "WeGen: A Unified Model for Interactive Multimodal Generation as We Chat",
        "abstract": "arXiv:2503.01115v1 Announce Type: new  Abstract: Existing multimodal generative models fall short as qualified design copilots, as they often struggle to generate imaginative outputs once instructions are less detailed or lack the ability to maintain consistency with the provided references. In this work, we introduce WeGen, a model that unifies multimodal generation and understanding, and promotes their interplay in iterative generation. It can generate diverse results with high creativity for less detailed instructions. And it can progressively refine prior generation results or integrating specific contents from references following the instructions in its chat with users. During this process, it is capable of preserving consistency in the parts that the user is already satisfied with. To this end, we curate a large-scale dataset, extracted from Internet videos, containing rich object dynamics and auto-labeled dynamics descriptions by advanced foundation models to date. These two information are interleaved into a single sequence to enable WeGen to learn consistency-aware generation where the specified dynamics are generated while the consistency of unspecified content is preserved aligned with instructions. Besides, we introduce a prompt self-rewriting mechanism to enhance generation diversity. Extensive experiments demonstrate the effectiveness of unifying multimodal understanding and generation in WeGen and show it achieves state-of-the-art performance across various visual generation benchmarks. These also demonstrate the potential of WeGen as a user-friendly design copilot as desired. The code and models will be available at https://github.com/hzphzp/WeGen.",
        "arxiv_id": "2503.01115",
        "ARXIVID": "2503.01115",
        "COMMENT": "This paper introduces WeGen, a unified model for multimodal generation and understanding, which aligns with criterion 2 on new VLLMs or MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.02239": {
        "authors": [
            "Keshu Wu",
            "Pei Li",
            "Yang Zhou",
            "Rui Gan",
            "Junwei You",
            "Yang Cheng",
            "Jingwen Zhu",
            "Steven T. Parker",
            "Bin Ran",
            "David A. Noyce",
            "Zhengzhong Tu"
        ],
        "title": "V2X-LLM: Enhancing V2X Integration and Understanding in Connected Vehicle Corridors",
        "abstract": "arXiv:2503.02239v1 Announce Type: new  Abstract: The advancement of Connected and Automated Vehicles (CAVs) and Vehicle-to-Everything (V2X) offers significant potential for enhancing transportation safety, mobility, and sustainability. However, the integration and analysis of the diverse and voluminous V2X data, including Basic Safety Messages (BSMs) and Signal Phase and Timing (SPaT) data, present substantial challenges, especially on Connected Vehicle Corridors. These challenges include managing large data volumes, ensuring real-time data integration, and understanding complex traffic scenarios. Although these projects have developed an advanced CAV data pipeline that enables real-time communication between vehicles, infrastructure, and other road users for managing connected vehicle and roadside unit (RSU) data, significant hurdles in data comprehension and real-time scenario analysis and reasoning persist. To address these issues, we introduce the V2X-LLM framework, a novel enhancement to the existing CV data pipeline. V2X-LLM leverages Large Language Models (LLMs) to improve the understanding and real-time analysis of V2X data. The framework includes four key tasks: Scenario Explanation, offering detailed narratives of traffic conditions; V2X Data Description, detailing vehicle and infrastructure statuses; State Prediction, forecasting future traffic states; and Navigation Advisory, providing optimized routing instructions. By integrating LLM-driven reasoning with V2X data within the data pipeline, the V2X-LLM framework offers real-time feedback and decision support for traffic management. This integration enhances the accuracy of traffic analysis, safety, and traffic optimization. Demonstrations in a real-world urban corridor highlight the framework's potential to advance intelligent transportation systems.",
        "arxiv_id": "2503.02239",
        "ARXIVID": "2503.02239",
        "COMMENT": "This paper aligns with criterion 2 as it introduces a framework leveraging large language models (LLMs) for V2X data analysis, which is relevant to multi-modal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00049": {
        "authors": [
            "Jiamin Luo",
            "Jingjing Wang",
            "Junxiao Ma",
            "Yujie Jin",
            "Shoushan Li",
            "Guodong Zhou"
        ],
        "title": "Omni-SILA: Towards Omni-scene Driven Visual Sentiment Identifying, Locating and Attributing in Videos",
        "abstract": "arXiv:2503.00049v1 Announce Type: new  Abstract: Prior studies on Visual Sentiment Understanding (VSU) primarily rely on the explicit scene information (e.g., facial expression) to judge visual sentiments, which largely ignore implicit scene information (e.g., human action, objection relation and visual background), while such information is critical for precisely discovering visual sentiments. Motivated by this, this paper proposes a new Omni-scene driven visual Sentiment Identifying, Locating and Attributing in videos (Omni-SILA) task, aiming to interactively and precisely identify, locate and attribute visual sentiments through both explicit and implicit scene information. Furthermore, this paper believes that this Omni-SILA task faces two key challenges: modeling scene and highlighting implicit scene beyond explicit. To this end, this paper proposes an Implicit-enhanced Causal MoE (ICM) approach for addressing the Omni-SILA task. Specifically, a Scene-Balanced MoE (SBM) and an Implicit-Enhanced Causal (IEC) blocks are tailored to model scene information and highlight the implicit scene information beyond explicit, respectively. Extensive experimental results on our constructed explicit and implicit Omni-SILA datasets demonstrate the great advantage of the proposed ICM approach over advanced Video-LLMs.",
        "arxiv_id": "2503.00049",
        "ARXIVID": "2503.00049",
        "COMMENT": "This paper aligns with criterion 4 as it discusses visual sentiment understanding in videos, which involves vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00743": {
        "authors": [
            "Dilxat Muhtar",
            "Enzhuo Zhang",
            "Zhenshi Li",
            "Feng Gu",
            "Yanglangxing He",
            "Pengfeng Xiao",
            "Xueliang Zhang"
        ],
        "title": "Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models",
        "abstract": "arXiv:2503.00743v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have demonstrated great potential in interpreting remote sensing (RS) images through language-guided semantic understanding. However, the effectiveness of these VLMs critically depends on high-quality image-text training data that captures rich semantic relationships between visual content and language descriptions. Unlike natural images, RS lacks large-scale interleaved image-text pairs from web data, making data collection challenging. While current approaches rely primarily on rule-based methods or flagship VLMs for data synthesis, a systematic framework for automated quality assessment of such synthetically generated RS visionlanguage data is notably absent. To fill this gap, we propose a novel score model trained on large-scale RS visionlanguage preference data for automated quality assessment. Our empirical results demonstrate that fine-tuning CLIP or advanced VLMs (e.g., Qwen2-VL) with the top 30% of data ranked by our score model achieves superior interpretation accuracy compared to both full-data fine-tuning and CLIP-score-based ranking approaches. Furthermore, we demonstrate applications of our scoring model for reinforcement learning (RL) training and best-of-N (BoN) testtime scaling, enabling significant improvements in VLM performance for RS tasks.",
        "arxiv_id": "2503.00743",
        "ARXIVID": "2503.00743",
        "COMMENT": "This paper aligns with criterion 2 as it focuses on improving vision-language models for remote sensing through quality-driven data curation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00429": {
        "authors": [
            "Jingyi Yang",
            "Xun Lin",
            "Zitong Yu",
            "Liepiao Zhang",
            "Xin Liu",
            "Hui Li",
            "Xiaochen Yuan",
            "Xiaochun Cao"
        ],
        "title": "DADM: Dual Alignment of Domain and Modality for Face Anti-spoofing",
        "abstract": "arXiv:2503.00429v1 Announce Type: new  Abstract: With the availability of diverse sensor modalities (i.e., RGB, Depth, Infrared) and the success of multi-modal learning, multi-modal face anti-spoofing (FAS) has emerged as a prominent research focus. The intuition behind it is that leveraging multiple modalities can uncover more intrinsic spoofing traces. However, this approach presents more risk of misalignment. We identify two main types of misalignment: (1) \\textbf{Intra-domain modality misalignment}, where the importance of each modality varies across different attacks. For instance, certain modalities (e.g., Depth) may be non-defensive against specific attacks (e.g., 3D mask), indicating that each modality has unique strengths and weaknesses in countering particular attacks. Consequently, simple fusion strategies may fall short. (2) \\textbf{Inter-domain modality misalignment}, where the introduction of additional modalities exacerbates domain shifts, potentially overshadowing the benefits of complementary fusion. To tackle (1), we propose a alignment module between modalities based on mutual information, which adaptively enhances favorable modalities while suppressing unfavorable ones. To address (2), we employ a dual alignment optimization method that aligns both sub-domain hyperplanes and modality angle margins, thereby mitigating domain gaps. Our method, dubbed \\textbf{D}ual \\textbf{A}lignment of \\textbf{D}omain and \\textbf{M}odality (DADM), achieves state-of-the-art performance in extensive experiments across four challenging protocols demonstrating its robustness in multi-modal domain generalization scenarios. The codes will be released soon.",
        "arxiv_id": "2503.00429",
        "ARXIVID": "2503.00429",
        "COMMENT": "This paper aligns with criterion 2 as it discusses multi-modal learning and domain alignment for face anti-spoofing, which is relevant to VLLMs/MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00051": {
        "authors": [
            "Quan Quan",
            "Dun Dai"
        ],
        "title": "Correspondence-Free Pose Estimation with Patterns: A Unified Approach for Multi-Dimensional Vision",
        "abstract": "arXiv:2503.00051v1 Announce Type: new  Abstract: 6D pose estimation is a central problem in robot vision. Compared with pose estimation based on point correspondences or its robust versions, correspondence-free methods are often more flexible. However, existing correspondence-free methods often rely on feature representation alignment or end-to-end regression. For such a purpose, a new correspondence-free pose estimation method and its practical algorithms are proposed, whose key idea is the elimination of unknowns by process of addition to separate the pose estimation from correspondence. By taking the considered point sets as patterns, feature functions used to describe these patterns are introduced to establish a sufficient number of equations for optimization. The proposed method is applicable to nonlinear transformations such as perspective projection and can cover various pose estimations from 3D-to-3D points, 3D-to-2D points, and 2D-to-2D points. Experimental results on both simulation and actual data are presented to demonstrate the effectiveness of the proposed method.",
        "arxiv_id": "2503.00051",
        "ARXIVID": "2503.00051",
        "COMMENT": "Matches criterion 3 as it proposes a novel correspondence-free pose estimation method, which is relevant to embodied AI and spatial understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00591": {
        "authors": [
            "Sohan Patnaik",
            "Rishabh Jain",
            "Balaji Krishnamurthy",
            "Mausoom Sarkar"
        ],
        "title": "AesthetiQ: Enhancing Graphic Layout Design via Aesthetic-Aware Preference Alignment of Multi-modal Large Language Models",
        "abstract": "arXiv:2503.00591v1 Announce Type: new  Abstract: Visual layouts are essential in graphic design fields such as advertising, posters, and web interfaces. The application of generative models for content-aware layout generation has recently gained traction. However, these models fail to understand the contextual aesthetic requirements of layout design and do not align with human-like preferences, primarily treating it as a prediction task without considering the final rendered output. To overcome these problems, we offer Aesthetic-Aware Preference Alignment(AAPA), a novel technique to train a Multi-modal Large Language Model (MLLM) for layout prediction that uses MLLM's aesthetic preferences for Direct Preference Optimization over graphic layouts. We propose a data filtering protocol utilizing our layout-quality heuristics for AAPA to ensure training happens on high-quality layouts. Additionally, we introduce a novel evaluation metric that uses another MLLM to compute the win rate of the generated layout against the ground-truth layout based on aesthetics criteria. We also demonstrate the applicability of AAPA for MLLMs of varying scales (1B to 8B parameters) and LLM families (Qwen, Phi, InternLM). By conducting thorough qualitative and quantitative analyses, we verify the efficacy of our approach on two challenging benchmarks - Crello and Webui, showcasing 17%, and 16 improvement over current State-of-The-Art methods, thereby highlighting the potential of MLLMs in aesthetic-aware layout generation.",
        "arxiv_id": "2503.00591",
        "ARXIVID": "2503.00591",
        "COMMENT": "Matches criterion 2 as it proposes a novel training technique for multi-modal large language models (MLLMs) in layout design.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00399": {
        "authors": [
            "Juan Song",
            "Lijie Yang",
            "Mingtao Feng"
        ],
        "title": "Taming Large Multimodal Agents for Ultra-low Bitrate Semantically Disentangled Image Compression",
        "abstract": "arXiv:2503.00399v1 Announce Type: new  Abstract: It remains a significant challenge to compress images at ultra-low bitrate while achieving both semantic consistency and high perceptual quality. We propose a novel image compression framework, Semantically Disentangled Image Compression (SEDIC) in this paper. Our proposed SEDIC leverages large multimodal models (LMMs) to disentangle the image into several essential semantic information, including an extremely compressed reference image, overall and object-level text descriptions, and the semantic masks. A multi-stage semantic decoder is designed to progressively restore the transmitted reference image object-by-object, ultimately producing high-quality and perceptually consistent reconstructions. In each decoding stage, a pre-trained controllable diffusion model is utilized to restore the object details on the reference image conditioned by the text descriptions and semantic masks. Experimental results demonstrate that SEDIC significantly outperforms state-of-the-art approaches, achieving superior perceptual quality and semantic consistency at ultra-low bitrates ($\\le$ 0.05 bpp). Our code is available at https://github.com/yang-xidian/SEDIC.",
        "arxiv_id": "2503.00399",
        "ARXIVID": "2503.00399",
        "COMMENT": "Matches criterion 2 as it leverages large multimodal models (LMMs) for image compression.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00132": {
        "authors": [
            "Anzhe Chen",
            "Hongxiang Yu",
            "Shuxin Li",
            "Yuxi Chen",
            "Zhongxiang Zhou",
            "Wentao Sun",
            "Rong Xiong",
            "Yue Wang"
        ],
        "title": "CNSv2: Probabilistic Correspondence Encoded Neural Image Servo",
        "abstract": "arXiv:2503.00132v1 Announce Type: new  Abstract: Visual servo based on traditional image matching methods often requires accurate keypoint correspondence for high precision control. However, keypoint detection or matching tends to fail in challenging scenarios with inconsistent illuminations or textureless objects, resulting significant performance degradation. Previous approaches, including our proposed Correspondence encoded Neural image Servo policy (CNS), attempted to alleviate these issues by integrating neural control strategies. While CNS shows certain improvement against error correspondence over conventional image-based controllers, it could not fully resolve the limitations arising from poor keypoint detection and matching. In this paper, we continue to address this problem and propose a new solution: Probabilistic Correspondence Encoded Neural Image Servo (CNSv2). CNSv2 leverages probabilistic feature matching to improve robustness in challenging scenarios. By redesigning the architecture to condition on multimodal feature matching, CNSv2 achieves high precision, improved robustness across diverse scenes and runs in real-time. We validate CNSv2 with simulations and real-world experiments, demonstrating its effectiveness in overcoming the limitations of detector-based methods in visual servo tasks.",
        "arxiv_id": "2503.00132",
        "ARXIVID": "2503.00132",
        "COMMENT": "Matches criterion 3 as it proposes a new method for visual servo tasks in embodied AI with probabilistic feature matching.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.01628": {
        "authors": [
            "William Michael Laprade",
            "Jesper Cairo Westergaard",
            "Svend Christensen",
            "Mads Nielsen",
            "Anders Bjorholm Dahl"
        ],
        "title": "A General Purpose Spectral Foundational Model for Both Proximal and Remote Sensing Spectral Imaging",
        "abstract": "arXiv:2503.01628v1 Announce Type: new  Abstract: Spectral imaging data acquired via multispectral and hyperspectral cameras can have hundreds of channels, where each channel records the reflectance at a specific wavelength and bandwidth. Time and resource constraints limit our ability to collect large spectral datasets, making it difficult to build and train predictive models from scratch. In the RGB domain, we can often alleviate some of the limitations of smaller datasets by using pretrained foundational models as a starting point. However, most existing foundation models are pretrained on large datasets of 3-channel RGB images, severely limiting their effectiveness when used with spectral imaging data. The few spectral foundation models that do exist usually have one of two limitations: (1) they are built and trained only on remote sensing data limiting their application in proximal spectral imaging, (2) they utilize the more widely available multispectral imaging datasets with less than 15 channels restricting their use with hundred-channel hyperspectral images. To alleviate these issues, we propose a large-scale foundational model and dataset built upon the masked autoencoder architecture that takes advantage of spectral channel encoding, spatial-spectral masking and ImageNet pretraining for an adaptable and robust model for downstream spectral imaging tasks.",
        "arxiv_id": "2503.01628",
        "ARXIVID": "2503.01628",
        "COMMENT": "Matches criterion 4 as it introduces a spectral foundational model for spectral imaging tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00540": {
        "authors": [
            "Shangzhe Di",
            "Zhelun Yu",
            "Guanghao Zhang",
            "Haoyuan Li",
            "Tao Zhong",
            "Hao Cheng",
            "Bolin Li",
            "Wanggui He",
            "Fangxun Shu",
            "Hao Jiang"
        ],
        "title": "Streaming Video Question-Answering with In-context Video KV-Cache Retrieval",
        "abstract": "arXiv:2503.00540v1 Announce Type: new  Abstract: We propose ReKV, a novel training-free approach that enables efficient streaming video question-answering (StreamingVQA), by seamlessly integrating with existing Video Large Language Models (Video-LLMs). Traditional VideoQA systems struggle with long videos, as they must process entire videos before responding to queries, and repeat this process for each new question. In contrast, our approach analyzes long videos in a streaming manner, allowing for prompt responses as soon as user queries are received. Building on a common Video-LLM, we first incorporate a sliding-window attention mechanism, ensuring that input frames attend to a limited number of preceding frames, thereby reducing computational overhead. To prevent information loss, we store processed video key-value caches (KV-Caches) in RAM and disk, reloading them into GPU memory as needed. Additionally, we introduce a retrieval method that leverages an external retriever or the parameters within Video-LLMs to retrieve only query-relevant KV-Caches, ensuring both efficiency and accuracy in question answering. ReKV enables the separation of video encoding and question-answering across different processes and GPUs, significantly enhancing the efficiency of StreamingVQA. Through comprehensive experimentation, we validate the efficacy and practicality of our approach, which significantly boosts efficiency and enhances applicability over existing VideoQA models.",
        "arxiv_id": "2503.00540",
        "ARXIVID": "2503.00540",
        "COMMENT": "Matches criterion 2 as it integrates Video Large Language Models (Video-LLMs) for streaming video question-answering.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00162": {
        "authors": [
            "Kangda Wei",
            "Zhengyu Zhou",
            "Bingqing Wang",
            "Jun Araki",
            "Lukas Lange",
            "Ruihong Huang",
            "Zhe Feng"
        ],
        "title": "PreMind: Multi-Agent Video Understanding for Advanced Indexing of Presentation-style Videos",
        "abstract": "arXiv:2503.00162v1 Announce Type: new  Abstract: In recent years, online lecture videos have become an increasingly popular resource for acquiring new knowledge. Systems capable of effectively understanding/indexing lecture videos are thus highly desirable, enabling downstream tasks like question answering to help users efficiently locate specific information within videos. This work proposes PreMind, a novel multi-agent multimodal framework that leverages various large models for advanced understanding/indexing of presentation-style videos. PreMind first segments videos into slide-presentation segments using a Vision-Language Model (VLM) to enhance modern shot-detection techniques. Each segment is then analyzed to generate multimodal indexes through three key steps: (1) extracting slide visual content, (2) transcribing speech narratives, and (3) consolidating these visual and speech contents into an integrated understanding. Three innovative mechanisms are also proposed to improve performance: leveraging prior lecture knowledge to refine visual understanding, detecting/correcting speech transcription errors using a VLM, and utilizing a critic agent for dynamic iterative self-reflection in vision analysis. Compared to traditional video indexing methods, PreMind captures rich, reliable multimodal information, allowing users to search for details like abbreviations shown only on slides. Systematic evaluations on the public LPM dataset and an internal enterprise dataset are conducted to validate PreMind's effectiveness, supported by detailed analyses.",
        "arxiv_id": "2503.00162",
        "ARXIVID": "2503.00162",
        "COMMENT": "Matches criterion 2 as it leverages Vision-Language Models (VLMs) for advanced video understanding and indexing.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00513": {
        "authors": [
            "Hanxun Yu",
            "Wentong Li",
            "Song Wang",
            "Junbo Chen",
            "Jianke Zhu"
        ],
        "title": "Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning",
        "abstract": "arXiv:2503.00513v1 Announce Type: new  Abstract: Despite encouraging progress in 3D scene understanding, it remains challenging to develop an effective Large Multi-modal Model (LMM) that is capable of understanding and reasoning in complex 3D environments. Most previous methods typically encode 3D point and 2D image features separately, neglecting interactions between 2D semantics and 3D object properties, as well as the spatial relationships within the 3D environment. This limitation not only hinders comprehensive representations of 3D scene, but also compromises training and inference efficiency. To address these challenges, we propose a unified Instance-aware 3D Large Multi-modal Model (Inst3D-LMM) to deal with multiple 3D scene understanding tasks simultaneously. To obtain the fine-grained instance-level visual tokens, we first introduce a novel Multi-view Cross-Modal Fusion (MCMF) module to inject the multi-view 2D semantics into their corresponding 3D geometric features. For scene-level relation-aware tokens, we further present a 3D Instance Spatial Relation (3D-ISR) module to capture the intricate pairwise spatial relationships among objects. Additionally, we perform end-to-end multi-task instruction tuning simultaneously without the subsequent task-specific fine-tuning. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods across 3D scene understanding, reasoning and grounding tasks. Source code is available at https://github.com/hanxunyu/Inst3D-LMM",
        "arxiv_id": "2503.00513",
        "ARXIVID": "2503.00513",
        "COMMENT": "Matches criterion 4 as it introduces a vision foundation model (Inst3D-LMM) for 3D scene understanding with multi-modal instruction tuning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.01092": {
        "authors": [
            "Wanjun Jia",
            "Fan Yang",
            "Mengfei Duan",
            "Xianchi Chen",
            "Yinxi Wang",
            "Yiming Jiang",
            "Wenrui Chen",
            "Kailun Yang",
            "Zhiyong Li"
        ],
        "title": "One-Shot Affordance Grounding of Deformable Objects in Egocentric Organizing Scenes",
        "abstract": "arXiv:2503.01092v1 Announce Type: new  Abstract: Deformable object manipulation in robotics presents significant challenges due to uncertainties in component properties, diverse configurations, visual interference, and ambiguous prompts. These factors complicate both perception and control tasks. To address these challenges, we propose a novel method for One-Shot Affordance Grounding of Deformable Objects (OS-AGDO) in egocentric organizing scenes, enabling robots to recognize previously unseen deformable objects with varying colors and shapes using minimal samples. Specifically, we first introduce the Deformable Object Semantic Enhancement Module (DefoSEM), which enhances hierarchical understanding of the internal structure and improves the ability to accurately identify local features, even under conditions of weak component information. Next, we propose the ORB-Enhanced Keypoint Fusion Module (OEKFM), which optimizes feature extraction of key components by leveraging geometric constraints and improves adaptability to diversity and visual interference. Additionally, we propose an instance-conditional prompt based on image data and task context, effectively mitigates the issue of region ambiguity caused by prompt words. To validate these methods, we construct a diverse real-world dataset, AGDDO15, which includes 15 common types of deformable objects and their associated organizational actions. Experimental results demonstrate that our approach significantly outperforms state-of-the-art methods, achieving improvements of 6.2%, 3.2%, and 2.9% in KLD, SIM, and NSS metrics, respectively, while exhibiting high generalization performance. Source code and benchmark dataset will be publicly available at https://github.com/Dikay1/OS-AGDO.",
        "arxiv_id": "2503.01092",
        "ARXIVID": "2503.01092",
        "COMMENT": "Matches criterion 1 as it proposes a novel method for affordance grounding in deformable object manipulation, enhancing spatial understanding in embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00803": {
        "authors": [
            "Qingwen Zhang",
            "Ajinkya Khoche",
            "Yi Yang",
            "Li Ling",
            "Sina Sharif Mansouri",
            "Olov Andersson",
            "Patric Jensfelt"
        ],
        "title": "HiMo: High-Speed Objects Motion Compensation in Point Clouds",
        "abstract": "arXiv:2503.00803v1 Announce Type: new  Abstract: LiDAR point clouds often contain motion-induced distortions, degrading the accuracy of object appearances in the captured data. In this paper, we first characterize the underlying reasons for the point cloud distortion and show that this is present in public datasets. We find that this distortion is more pronounced in high-speed environments such as highways, as well as in multi-LiDAR configurations, a common setup for heavy vehicles. Previous work has dealt with point cloud distortion from the ego-motion but fails to consider distortion from the motion of other objects. We therefore introduce a novel undistortion pipeline, HiMo, that leverages scene flow estimation for object motion compensation, correcting the depiction of dynamic objects. We further propose an extension of a state-of-the-art self-supervised scene flow method. Due to the lack of well-established motion distortion metrics in the literature, we also propose two metrics for compensation performance evaluation: compensation accuracy at a point level and shape similarity on objects. To demonstrate the efficacy of our method, we conduct extensive experiments on the Argoverse 2 dataset and a new real-world dataset. Our new dataset is collected from heavy vehicles equipped with multi-LiDARs and on highways as opposed to mostly urban settings in the existing datasets. The source code, including all methods and the evaluation data, will be provided upon publication. See https://kin-zhang.github.io/HiMo for more details.",
        "arxiv_id": "2503.00803",
        "ARXIVID": "2503.00803",
        "COMMENT": "Matches criterion 3 as it introduces a novel method (HiMo) for motion compensation in LiDAR point clouds, addressing a previously overlooked issue.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.02221": {
        "authors": [
            "Yusheng Zhao",
            "Junyu Luo",
            "Xiao Luo",
            "Jinsheng Huang",
            "Jingyang Yuan",
            "Zhiping Xiao",
            "Ming Zhang"
        ],
        "title": "Attention Bootstrapping for Multi-Modal Test-Time Adaptation",
        "abstract": "arXiv:2503.02221v1 Announce Type: new  Abstract: Test-time adaptation aims to adapt a well-trained model to potential distribution shifts at test time using only unlabeled test data, without access to the original training data. While previous efforts mainly focus on a single modality, test-time distribution shift in the multi-modal setting is more complex and calls for new solutions. This paper tackles the problem of multi-modal test-time adaptation by proposing a novel method named Attention Bootstrapping with Principal Entropy Minimization (ABPEM). We observe that test-time distribution shift causes misalignment across modalities, leading to a large gap between intra-modality discrepancies (measured by self-attention) and inter-modality discrepancies (measured by cross-attention). We name this the attention gap. This attention gap widens with more severe distribution shifts, hindering effective modality fusion. To mitigate this attention gap and encourage better modality fusion, we propose attention bootstrapping that promotes cross-attention with the guidance of self-attention. Moreover, to reduce the gradient noise in the commonly-used entropy minimization, we adopt principal entropy minimization, a refinement of entropy minimization that reduces gradient noise by focusing on the principal parts of entropy, excluding less reliable gradient information. Extensive experiments on the benchmarks validate the effectiveness of the proposed ABPEM in comparison with competing baselines.",
        "arxiv_id": "2503.02221",
        "ARXIVID": "2503.02221",
        "COMMENT": "Matches criterion 2 as it proposes a novel method for multi-modal test-time adaptation, which involves cross-modality attention mechanisms.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00853": {
        "authors": [
            "Rui Yi Yong",
            "Samuel Picosson",
            "Arnold Wiliem"
        ],
        "title": "MTReD: 3D Reconstruction Dataset for Fly-over Videos of Maritime Domain",
        "abstract": "arXiv:2503.00853v1 Announce Type: new  Abstract: This work tackles 3D scene reconstruction for a video fly-over perspective problem in the maritime domain, with a specific emphasis on geometrically and visually sound reconstructions. This will allow for downstream tasks such as segmentation, navigation, and localization. To our knowledge, there is no dataset available in this domain. As such, we propose a novel maritime 3D scene reconstruction benchmarking dataset, named as MTReD (Maritime Three-Dimensional Reconstruction Dataset). The MTReD comprises 19 fly-over videos curated from the Internet containing ships, islands, and coastlines. As the task is aimed towards geometrical consistency and visual completeness, the dataset uses two metrics: (1) Reprojection error; and (2) Perception based metrics. We find that existing perception-based metrics, such as Learned Perceptual Image Patch Similarity (LPIPS), do not appropriately measure the completeness of a reconstructed image. Thus, we propose a novel semantic similarity metric utilizing DINOv2 features coined DiFPS (DinoV2 Features Perception Similarity). We perform initial evaluation on two baselines: (1) Structured from Motion (SfM) through Colmap; and (2) the recent state-of-the-art MASt3R model. We find that the reconstructed scenes by MASt3R have higher reprojection errors, but superior perception based metric scores. To this end, some pre-processing methods are explored, and we find a pre-processing method which improves both the reprojection error and perception-based score. We envisage our proposed MTReD to stimulate further research in these directions. The dataset and all the code will be made available in https://github.com/RuiYiYong/MTReD.",
        "arxiv_id": "2503.00853",
        "ARXIVID": "2503.00853",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset (MTReD) for 3D reconstruction in the maritime domain.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00811": {
        "authors": [
            "Lu Ma",
            "Kaibo Cao",
            "Hao Liang",
            "Jiaxin Lin",
            "Zhuang Li",
            "Yuhong Liu",
            "Jihong Zhang",
            "Wentao Zhang",
            "Bin Cui"
        ],
        "title": "Evaluating and Predicting Distorted Human Body Parts for Generated Images",
        "abstract": "arXiv:2503.00811v1 Announce Type: new  Abstract: Recent advancements in text-to-image (T2I) models enable high-quality image synthesis, yet generating anatomically accurate human figures remains challenging. AI-generated images frequently exhibit distortions such as proliferated limbs, missing fingers, deformed extremities, or fused body parts. Existing evaluation metrics like Inception Score (IS) and Fr\\'echet Inception Distance (FID) lack the granularity to detect these distortions, while human preference-based metrics focus on abstract quality assessments rather than anatomical fidelity. To address this gap, we establish the first standards for identifying human body distortions in AI-generated images and introduce Distortion-5K, a comprehensive dataset comprising 4,700 annotated images of normal and malformed human figures across diverse styles and distortion types. Based on this dataset, we propose ViT-HD, a Vision Transformer-based model tailored for detecting human body distortions in AI-generated images, which outperforms state-of-the-art segmentation models and visual language models, achieving an F1 score of 0.899 and IoU of 0.831 on distortion localization. Additionally, we construct the Human Distortion Benchmark with 500 human-centric prompts to evaluate four popular T2I models using trained ViT-HD, revealing that nearly 50\\% of generated images contain distortions. This work pioneers a systematic approach to evaluating anatomical accuracy in AI-generated humans, offering tools to advance the fidelity of T2I models and their real-world applicability. The Distortion-5K dataset, trained ViT-HD will soon be released in our GitHub repository: \\href{https://github.com/TheRoadQaQ/Predicting-Distortion}{https://github.com/TheRoadQaQ/Predicting-Distortion}.",
        "arxiv_id": "2503.00811",
        "ARXIVID": "2503.00811",
        "COMMENT": "This paper introduces a new benchmark (Distortion-5K) for evaluating human body distortions in AI-generated images, which aligns with criterion 3 on building new benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.01103": {
        "authors": [
            "Kaiwen Zheng",
            "Yongxin Chen",
            "Huayu Chen",
            "Guande He",
            "Ming-Yu Liu",
            "Jun Zhu",
            "Qinsheng Zhang"
        ],
        "title": "Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator",
        "abstract": "arXiv:2503.01103v1 Announce Type: new  Abstract: While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that bridges likelihood-based generative training and the GAN objective to bypass this fundamental constraint. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58 to new records of 1.30/0.97 on CIFAR-10/ImageNet-64 datasets, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256$\\times$256.",
        "arxiv_id": "2503.01103",
        "ARXIVID": "2503.01103",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling and introduces a novel optimization framework.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2503.00881": {
        "authors": [
            "You Shen",
            "Zhipeng Zhang",
            "Xinyang Li",
            "Yansong Qu",
            "Yu Lin",
            "Shengchuan Zhang",
            "Liujuan Cao"
        ],
        "title": "Evolving High-Quality Rendering and Reconstruction in a Unified Framework with Contribution-Adaptive Regularization",
        "abstract": "arXiv:2503.00881v1 Announce Type: new  Abstract: Representing 3D scenes from multiview images is a core challenge in computer vision and graphics, which requires both precise rendering and accurate reconstruction. Recently, 3D Gaussian Splatting (3DGS) has garnered significant attention for its high-quality rendering and fast inference speed. Yet, due to the unstructured and irregular nature of Gaussian point clouds, ensuring accurate geometry reconstruction remains difficult. Existing methods primarily focus on geometry regularization, with common approaches including primitive-based and dual-model frameworks. However, the former suffers from inherent conflicts between rendering and reconstruction, while the latter is computationally and storage-intensive. To address these challenges, we propose CarGS, a unified model leveraging Contribution-adaptive regularization to achieve simultaneous, high-quality rendering and surface reconstruction. The essence of our framework is learning adaptive contribution for Gaussian primitives by squeezing the knowledge from geometry regularization into a compact MLP. Additionally, we introduce a geometry-guided densification strategy with clues from both normals and Signed Distance Fields (SDF) to improve the capability of capturing high-frequency details. Our design improves the mutual learning of the two tasks, meanwhile its unified structure does not require separate models as in dual-model based approaches, guaranteeing efficiency. Extensive experiments demonstrate the ability to achieve state-of-the-art (SOTA) results in both rendering fidelity and reconstruction accuracy while maintaining real-time speed and minimal storage size.",
        "arxiv_id": "2503.00881",
        "ARXIVID": "2503.00881",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.01463": {
        "authors": [
            "Zhixiong Nan",
            "Xianghong Li",
            "Jifeng Dai",
            "Tao Xiang"
        ],
        "title": "MI-DETR: An Object Detection Model with Multi-time Inquiries Mechanism",
        "abstract": "arXiv:2503.01463v1 Announce Type: new  Abstract: Based on analyzing the character of cascaded decoder architecture commonly adopted in existing DETR-like models, this paper proposes a new decoder architecture. The cascaded decoder architecture constrains object queries to update in the cascaded direction, only enabling object queries to learn relatively-limited information from image features. However, the challenges for object detection in natural scenes (e.g., extremely-small, heavily-occluded, and confusingly mixed with the background) require an object detection model to fully utilize image features, which motivates us to propose a new decoder architecture with the parallel Multi-time Inquiries (MI) mechanism. MI enables object queries to learn more comprehensive information, and our MI based model, MI-DETR, outperforms all existing DETR-like models on COCO benchmark under different backbones and training epochs, achieving +2.3 AP and +0.6 AP improvements compared to the most representative model DINO and SOTA model Relation-DETR under ResNet-50 backbone. In addition, a series of diagnostic and visualization experiments demonstrate the effectiveness, rationality, and interpretability of MI.",
        "arxiv_id": "2503.01463",
        "ARXIVID": "2503.01463",
        "COMMENT": "Does not match any specific criteria but proposes a novel object detection model with a new decoder architecture.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.01220": {
        "authors": [
            "Jiqing Wu",
            "Ingrid Berg",
            "Yawei Li",
            "Ender Konukoglu",
            "Viktor H. Koelzer"
        ],
        "title": "Tera-MIND: Tera-scale mouse brain simulation via spatial mRNA-guided diffusion",
        "abstract": "arXiv:2503.01220v2 Announce Type: new  Abstract: Holistic 3D modeling of molecularly defined brain structures is crucial for understanding complex brain functions. Emerging tissue profiling technologies enable the construction of a comprehensive atlas of the mammalian brain with sub-cellular resolution and spatially resolved gene expression data. However, such tera-scale volumetric datasets present significant computational challenges in understanding complex brain functions within their native 3D spatial context. Here, we propose the novel generative approach $\\textbf{Tera-MIND}$, which can simulate $\\textbf{Tera}$-scale $\\textbf{M}$ouse bra$\\textbf{IN}$s in 3D using a patch-based and boundary-aware $\\textbf{D}$iffusion model. Taking spatial transcriptomic data as the conditional input, we generate virtual mouse brains with comprehensive cellular morphological detail at teravoxel scale. Through the lens of 3D $gene$-$gene$ self-attention, we identify spatial molecular interactions for key transcriptomic pathways in the murine brain, exemplified by glutamatergic and dopaminergic neuronal systems. Importantly, these $in$-$silico$ biological findings are consistent and reproducible across three tera-scale virtual mouse brains. Therefore, Tera-MIND showcases a promising path toward efficient and generative simulations of whole organ systems for biomedical research. Project website: https://musikisomorphie.github.io/Tera-MIND.html",
        "arxiv_id": "2503.01220",
        "ARXIVID": "2503.01220",
        "COMMENT": "Does not match any specific criteria but involves generative modeling in a biomedical context.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.01774": {
        "authors": [
            "Jay Zhangjie Wu",
            "Yuxuan Zhang",
            "Haithem Turki",
            "Xuanchi Ren",
            "Jun Gao",
            "Mike Zheng Shou",
            "Sanja Fidler",
            "Zan Gojcic",
            "Huan Ling"
        ],
        "title": "Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models",
        "abstract": "arXiv:2503.01774v1 Announce Type: new  Abstract: Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a novel pipeline designed to enhance 3D reconstruction and novel-view synthesis through single-step diffusion models. At the core of our approach is Difix, a single-step image diffusion model trained to enhance and remove artifacts in rendered novel views caused by underconstrained regions of the 3D representation. Difix serves two critical roles in our pipeline. First, it is used during the reconstruction phase to clean up pseudo-training views that are rendered from the reconstruction and then distilled back into 3D. This greatly enhances underconstrained regions and improves the overall 3D representation quality. More importantly, Difix also acts as a neural enhancer during inference, effectively removing residual artifacts arising from imperfect 3D supervision and the limited capacity of current reconstruction models. Difix3D+ is a general solution, a single model compatible with both NeRF and 3DGS representations, and it achieves an average 2$\\times$ improvement in FID score over baselines while maintaining 3D consistency.",
        "arxiv_id": "2503.01774",
        "ARXIVID": "2503.01774",
        "COMMENT": "Does not match any specific criterion but is related to 3D reconstruction and novel-view synthesis.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.00801": {
        "authors": [
            "Zikuan Li",
            "Honghua Chen",
            "Yuecheng Wang",
            "Sibo Wu",
            "Mingqiang Wei",
            "Jun Wang"
        ],
        "title": "STAR-Edge: Structure-aware Local Spherical Curve Representation for Thin-walled Edge Extraction from Unstructured Point Clouds",
        "abstract": "arXiv:2503.00801v1 Announce Type: new  Abstract: Extracting geometric edges from unstructured point clouds remains a significant challenge, particularly in thin-walled structures that are commonly found in everyday objects. Traditional geometric methods and recent learning-based approaches frequently struggle with these structures, as both rely heavily on sufficient contextual information from local point neighborhoods. However, 3D measurement data of thin-walled structures often lack the accurate, dense, and regular neighborhood sampling required for reliable edge extraction, resulting in degraded performance.   In this work, we introduce STAR-Edge, a novel approach designed for detecting and refining edge points in thin-walled structures. Our method leverages a unique representation-the local spherical curve-to create structure-aware neighborhoods that emphasize co-planar points while reducing interference from close-by, non-co-planar surfaces. This representation is transformed into a rotation-invariant descriptor, which, combined with a lightweight multi-layer perceptron, enables robust edge point classification even in the presence of noise and sparse or irregular sampling. Besides, we also use the local spherical curve representation to estimate more precise normals and introduce an optimization function to project initially identified edge points exactly on the true edges. Experiments conducted on the ABC dataset and thin-walled structure-specific datasets demonstrate that STAR-Edge outperforms existing edge detection methods, showcasing better robustness under various challenging conditions.",
        "arxiv_id": "2503.00801",
        "ARXIVID": "2503.00801",
        "COMMENT": "Does not match any specific criterion but is related to computer vision and spatial understanding.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.00948": {
        "authors": [
            "Jie Tian",
            "Xiaoye Qu",
            "Zhenyi Lu",
            "Wei Wei",
            "Sichen Liu",
            "Yu Cheng"
        ],
        "title": "Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think",
        "abstract": "arXiv:2503.00948v1 Announce Type: new  Abstract: Image-to-Video (I2V) generation aims to synthesize a video clip according to a given image and condition (e.g., text). The key challenge of this task lies in simultaneously generating natural motions while preserving the original appearance of the images. However, current I2V diffusion models (I2V-DMs) often produce videos with limited motion degrees or exhibit uncontrollable motion that conflicts with the textual condition. To address these limitations, we propose a novel Extrapolating and Decoupling framework, which introduces model merging techniques to the I2V domain for the first time. Specifically, our framework consists of three separate stages: (1) Starting with a base I2V-DM, we explicitly inject the textual condition into the temporal module using a lightweight, learnable adapter and fine-tune the integrated model to improve motion controllability. (2) We introduce a training-free extrapolation strategy to amplify the dynamic range of the motion, effectively reversing the fine-tuning process to enhance the motion degree significantly. (3) With the above two-stage models excelling in motion controllability and degree, we decouple the relevant parameters associated with each type of motion ability and inject them into the base I2V-DM. Since the I2V-DM handles different levels of motion controllability and dynamics at various denoising time steps, we adjust the motion-aware parameters accordingly over time. Extensive qualitative and quantitative experiments have been conducted to demonstrate the superiority of our framework over existing methods.",
        "arxiv_id": "2503.00948",
        "ARXIVID": "2503.00948",
        "COMMENT": "This paper proposes a novel framework for image-to-video generation, which is tangentially related to multi-modal generative modeling but does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.00823": {
        "authors": [
            "Bowen Zheng",
            "Da-Wei Zhou",
            "Han-Jia Ye",
            "De-Chuan Zhan"
        ],
        "title": "Task-Agnostic Guided Feature Expansion for Class-Incremental Learning",
        "abstract": "arXiv:2503.00823v1 Announce Type: new  Abstract: The ability to learn new concepts while preserve the learned knowledge is desirable for learning systems in Class-Incremental Learning (CIL). Recently, feature expansion of the model become a prevalent solution for CIL, where the old features are fixed during the training of the new task while new features are expanded for the new tasks. However, such task-specific features learned from the new task may collide with the old features, leading to misclassification between tasks. Therefore, the expanded model is often encouraged to capture diverse features from the new task, aiming to avoid such collision. However, the existing solution is largely restricted to the samples from the current task, because of the poor accessibility to previous samples. To promote the learning and transferring of diverse features across tasks, we propose a framework called Task-Agnostic Guided Feature Expansion (TagFex). Firstly, it captures task-agnostic features continually with a separate model, providing extra task-agnostic features for subsequent tasks. Secondly, to obtain useful features from the task-agnostic model for the current task, it aggregates the task-agnostic features with the task-specific feature using a merge attention. Then the aggregated feature is transferred back into the task-specific feature for inference, helping the task-specific model capture diverse features. Extensive experiments show the effectiveness and superiority of TagFex on various CIL settings. Code is available at https://github.com/bwnzheng/TagFex_CVPR2025.",
        "arxiv_id": "2503.00823",
        "ARXIVID": "2503.00823",
        "COMMENT": "Does not match any specific criterion but is relevant to incremental learning and feature expansion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00972": {
        "authors": [
            "Wanwen Chen",
            "Carson Studders",
            "Jamie J. Y. Kwon",
            "Emily H. T. Pang",
            "Eitan Prisman",
            "Septimiu E. Salcudean"
        ],
        "title": "Semantic-ICP: Iterative Closest Point for Non-rigid Multi-Organ Point Cloud Registration",
        "abstract": "arXiv:2503.00972v1 Announce Type: new  Abstract: Point cloud registration is important in computer-aided interventions (CAI). While learning-based point cloud registration methods have been developed, their clinical application is hampered by issues of generalizability and explainability. Therefore, classical point cloud registration methods, such as Iterative Closest Point (ICP), are still widely applied in CAI. ICP methods fail to consider that: (1) the points have well-defined semantic meaning, in that each point can be related to a specific anatomical label; (2) the deformation needs to follow biomechanical energy constraints. In this paper, we present a novel semantic ICP (sem-ICP) method that handles multiple point labels and uses linear elastic energy regularization. We use semantic labels to improve the robustness of the closest point matching and propose a new point cloud deformation representation to apply explicit biomechanical energy regularization. Our experiments on the Learn2reg abdominal MR-CT registration dataset and a trans-oral robotic surgery ultrasound-CT registration dataset show that our method improves the Hausdorff distance compared with other state-of-the-art ICP-based registration methods. We also perform a sensitivity study to show that our rigid initialization achieves better convergence with different initializations and visible ratios.",
        "arxiv_id": "2503.00972",
        "ARXIVID": "2503.00972",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and point cloud registration.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00740": {
        "authors": [
            "Junyao Gao",
            "Yanan Sun",
            "Fei Shen",
            "Xin Jiang",
            "Zhening Xing",
            "Kai Chen",
            "Cairong Zhao"
        ],
        "title": "FaceShot: Bring Any Character into Life",
        "abstract": "arXiv:2503.00740v1 Announce Type: new  Abstract: In this paper, we present FaceShot, a novel training-free portrait animation framework designed to bring any character into life from any driven video without fine-tuning or retraining. We achieve this by offering precise and robust reposed landmark sequences from an appearance-guided landmark matching module and a coordinate-based landmark retargeting module. Together, these components harness the robust semantic correspondences of latent diffusion models to produce facial motion sequence across a wide range of character types. After that, we input the landmark sequences into a pre-trained landmark-driven animation model to generate animated video. With this powerful generalization capability, FaceShot can significantly extend the application of portrait animation by breaking the limitation of realistic portrait landmark detection for any stylized character and driven video. Also, FaceShot is compatible with any landmark-driven animation model, significantly improving overall performance. Extensive experiments on our newly constructed character benchmark CharacBench confirm that FaceShot consistently surpasses state-of-the-art (SOTA) approaches across any character domain. More results are available at our project website https://faceshot2024.github.io/faceshot/.",
        "arxiv_id": "2503.00740",
        "ARXIVID": "2503.00740",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and animation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00266": {
        "authors": [
            "Milad Yazdani",
            "Yasamin Medghalchi",
            "Pooria Ashrafian",
            "Ilker Hacihaliloglu",
            "Dena Shahriari"
        ],
        "title": "Flow Matching for Medical Image Synthesis: Bridging the Gap Between Speed and Quality",
        "abstract": "arXiv:2503.00266v1 Announce Type: new  Abstract: Deep learning models have emerged as a powerful tool for various medical applications. However, their success depends on large, high-quality datasets that are challenging to obtain due to privacy concerns and costly annotation. Generative models, such as diffusion models, offer a potential solution by synthesizing medical images, but their practical adoption is hindered by long inference times. In this paper, we propose the use of an optimal transport flow matching approach to accelerate image generation. By introducing a straighter mapping between the source and target distribution, our method significantly reduces inference time while preserving and further enhancing the quality of the outputs. Furthermore, this approach is highly adaptable, supporting various medical imaging modalities, conditioning mechanisms (such as class labels and masks), and different spatial dimensions, including 2D and 3D. Beyond image generation, it can also be applied to related tasks such as image enhancement. Our results demonstrate the efficiency and versatility of this framework, making it a promising advancement for medical imaging applications. Code with checkpoints and a synthetic dataset (beneficial for classification and segmentation) is now available on: https://github.com/milad1378yz/MOTFM.",
        "arxiv_id": "2503.00266",
        "ARXIVID": "2503.00266",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01691": {
        "authors": [
            "Yuyan Chen",
            "Nico Lang",
            "B. Christian Schmidt",
            "Aditya Jain",
            "Yves Basset",
            "Sara Beery",
            "Maxim Larriv\\'ee",
            "David Rolnick"
        ],
        "title": "Open-Set Recognition of Novel Species in Biodiversity Monitoring",
        "abstract": "arXiv:2503.01691v1 Announce Type: new  Abstract: Machine learning is increasingly being applied to facilitate long-term, large-scale biodiversity monitoring. With most species on Earth still undiscovered or poorly documented, species-recognition models are expected to encounter new species during deployment. We introduce Open-Insects, a fine-grained image recognition benchmark dataset for open-set recognition and out-of-distribution detection in biodiversity monitoring. Open-Insects makes it possible to evaluate algorithms for new species detection on several geographical open-set splits with varying difficulty. Furthermore, we present a test set recently collected in the wild with 59 species that are likely new to science. We evaluate a variety of open-set recognition algorithms, including post-hoc methods, training-time regularization, and training with auxiliary data, finding that the simple post-hoc approach of utilizing softmax scores remains a strong baseline. We also demonstrate how to leverage auxiliary data to improve the detection performance when the training dataset is limited. Our results provide timely insights to guide the development of computer vision methods for biodiversity monitoring and species discovery.",
        "arxiv_id": "2503.01691",
        "ARXIVID": "2503.01691",
        "COMMENT": "Does not match any specific criterion but is related to biodiversity monitoring using machine learning, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00301": {
        "authors": [
            "Zihan Huang",
            "Wei Fang",
            "Tong Bu",
            "Peng Xue",
            "Zecheng Hao",
            "Wenxuan Liu",
            "Yuanhong Tang",
            "Zhaofei Yu",
            "Tiejun Huang"
        ],
        "title": "Differential Coding for Training-Free ANN-to-SNN Conversion",
        "abstract": "arXiv:2503.00301v1 Announce Type: new  Abstract: Spiking Neural Networks (SNNs) exhibit significant potential due to their low energy consumption. Converting Artificial Neural Networks (ANNs) to SNNs is an efficient way to achieve high-performance SNNs. However, many conversion methods are based on rate coding, which requires numerous spikes and longer time-steps compared to directly trained SNNs, leading to increased energy consumption and latency. This article introduces differential coding for ANN-to-SNN conversion, a novel coding scheme that reduces spike counts and energy consumption by transmitting changes in rate information rather than rates directly, and explores its application across various layers. Additionally, the threshold iteration method is proposed to optimize thresholds based on activation distribution when converting Rectified Linear Units (ReLUs) to spiking neurons. Experimental results on various Convolutional Neural Networks (CNNs) and Transformers demonstrate that the proposed differential coding significantly improves accuracy while reducing energy consumption, particularly when combined with the threshold iteration method, achieving state-of-the-art performance.",
        "arxiv_id": "2503.00301",
        "ARXIVID": "2503.00301",
        "COMMENT": "Does not match any specific criterion but is related to neural network optimization, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00793": {
        "authors": [
            "Ukcheol Shin",
            "Kyunghyun Lee",
            "Jean Oh"
        ],
        "title": "Bridging Spectral-wise and Multi-spectral Depth Estimation via Geometry-guided Contrastive Learning",
        "abstract": "arXiv:2503.00793v1 Announce Type: new  Abstract: Deploying depth estimation networks in the real world requires high-level robustness against various adverse conditions to ensure safe and reliable autonomy. For this purpose, many autonomous vehicles employ multi-modal sensor systems, including an RGB camera, NIR camera, thermal camera, LiDAR, or Radar. They mainly adopt two strategies to use multiple sensors: modality-wise and multi-modal fused inference. The former method is flexible but memory-inefficient, unreliable, and vulnerable. Multi-modal fusion can provide high-level reliability, yet it needs a specialized architecture. In this paper, we propose an effective solution, named align-and-fuse strategy, for the depth estimation from multi-spectral images. In the align stage, we align embedding spaces between multiple spectrum bands to learn shareable representation across multi-spectral images by minimizing contrastive loss of global and spatially aligned local features with geometry cue. After that, in the fuse stage, we train an attachable feature fusion module that can selectively aggregate the multi-spectral features for reliable and robust prediction results. Based on the proposed method, a single-depth network can achieve both spectral-invariant and multi-spectral fused depth estimation while preserving reliability, memory efficiency, and flexibility.",
        "arxiv_id": "2503.00793",
        "ARXIVID": "2503.00793",
        "COMMENT": "This paper does not match any specific criteria but is related to multi-spectral depth estimation, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.02701": {
        "authors": [
            "Shuaike Li",
            "Kai Zhang",
            "Qi Liu",
            "Enhong Chen"
        ],
        "title": "MindBridge: Scalable and Cross-Model Knowledge Editing via Memory-Augmented Modality",
        "abstract": "arXiv:2503.02701v1 Announce Type: new  Abstract: Knowledge editing is a technique for efficiently and accurately updating the knowledge of large language models (LLMs) to alleviate obsolescence and correct errors. However, most existing methods overfit to specific models, causing edited knowledge to be discarded during each LLM update and requiring frequent re-editing, which is particularly burdensome in today's rapidly evolving open-source community. To address this issue, we propose the problem of cross-model knowledge editing and introduce MindBridge, a scalable solution inspired by the low coupling between modality processing and LLMs in multi-modal models. MindBridge introduces the novel concept of memory modality, which encodes edited knowledge as an independent modality. It first performs LLM-agnostic pre-training of the memory modality and then integrates it with various LLMs. Extensive experiments on multiple LLMs and popular knowledge editing datasets demonstrate that MindBridge achieves superior performance even in editing tens of thousands of knowledge entries and can flexibly adapt to different LLMs. Our code is available at https://github.com/CrashBugger/MindBridge.",
        "arxiv_id": "2503.02701",
        "ARXIVID": "2503.02701",
        "COMMENT": "This paper does not match any specific criteria but discusses knowledge editing in LLMs, which is tangentially relevant to your friend's interest in multi-modal and vision-language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01261": {
        "authors": [
            "Guotao Liang",
            "Baoquan Zhang",
            "Zhiyuan Wen",
            "Junteng Zhao",
            "Yunming Ye",
            "Kola Ye",
            "Yao He"
        ],
        "title": "Towards Improved Text-Aligned Codebook Learning: Multi-Hierarchical Codebook-Text Alignment with Long Text",
        "abstract": "arXiv:2503.01261v1 Announce Type: new  Abstract: Image quantization is a crucial technique in image generation, aimed at learning a codebook that encodes an image into a discrete token sequence. Recent advancements have seen researchers exploring learning multi-modal codebook (i.e., text-aligned codebook) by utilizing image caption semantics, aiming to enhance codebook performance in cross-modal tasks. However, existing image-text paired datasets exhibit a notable flaw in that the text descriptions tend to be overly concise, failing to adequately describe the images and provide sufficient semantic knowledge, resulting in limited alignment of text and codebook at a fine-grained level. In this paper, we propose a novel Text-Augmented Codebook Learning framework, named TA-VQ, which generates longer text for each image using the visual-language model for improved text-aligned codebook learning. However, the long text presents two key challenges: how to encode text and how to align codebook and text. To tackle two challenges, we propose to split the long text into multiple granularities for encoding, i.e., word, phrase, and sentence, so that the long text can be fully encoded without losing any key semantic knowledge. Following this, a hierarchical encoder and novel sampling-based alignment strategy are designed to achieve fine-grained codebook-text alignment. Additionally, our method can be seamlessly integrated into existing VQ models. Extensive experiments in reconstruction and various downstream tasks demonstrate its effectiveness compared to previous state-of-the-art approaches.",
        "arxiv_id": "2503.01261",
        "ARXIVID": "2503.01261",
        "COMMENT": "This paper does not match any specific criteria but is related to multi-modal learning and text-aligned codebook learning, which is tangentially relevant to your friend's interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01187": {
        "authors": [
            "Xingyuan Li",
            "Zirui Wang",
            "Yang Zou",
            "Zhixin Chen",
            "Jun Ma",
            "Zhiying Jiang",
            "Long Ma",
            "Jinyuan Liu"
        ],
        "title": "DifIISR: A Diffusion Model with Gradient Guidance for Infrared Image Super-Resolution",
        "abstract": "arXiv:2503.01187v1 Announce Type: new  Abstract: Infrared imaging is essential for autonomous driving and robotic operations as a supportive modality due to its reliable performance in challenging environments. Despite its popularity, the limitations of infrared cameras, such as low spatial resolution and complex degradations, consistently challenge imaging quality and subsequent visual tasks. Hence, infrared image super-resolution (IISR) has been developed to address this challenge. While recent developments in diffusion models have greatly advanced this field, current methods to solve it either ignore the unique modal characteristics of infrared imaging or overlook the machine perception requirements. To bridge these gaps, we propose DifIISR, an infrared image super-resolution diffusion model optimized for visual quality and perceptual performance. Our approach achieves task-based guidance for diffusion by injecting gradients derived from visual and perceptual priors into the noise during the reverse process. Specifically, we introduce an infrared thermal spectrum distribution regulation to preserve visual fidelity, ensuring that the reconstructed infrared images closely align with high-resolution images by matching their frequency components. Subsequently, we incorporate various visual foundational models as the perceptual guidance for downstream visual tasks, infusing generalizable perceptual features beneficial for detection and segmentation. As a result, our approach gains superior visual results while attaining State-Of-The-Art downstream task performance. Code is available at https://github.com/zirui0625/DifIISR",
        "arxiv_id": "2503.01187",
        "ARXIVID": "2503.01187",
        "COMMENT": "This paper focuses on infrared image super-resolution using diffusion models, which does not match any specific criteria but is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.02172": {
        "authors": [
            "Hongyu Lin",
            "Haoran Luo",
            "Hanghang Cao",
            "Yang Liu",
            "Shihao Gao",
            "Kaichun Yao",
            "Libo Zhang",
            "Mingjie Xing",
            "Yanjun Wu"
        ],
        "title": "KGCompiler: Deep Learning Compilation Optimization for Knowledge Graph Complex Logical Query Answering",
        "abstract": "arXiv:2503.02172v1 Announce Type: new  Abstract: Complex Logical Query Answering (CLQA) involves intricate multi-hop logical reasoning over large-scale and potentially incomplete Knowledge Graphs (KGs). Although existing CLQA algorithms achieve high accuracy in answering such queries, their reasoning time and memory usage scale significantly with the number of First-Order Logic (FOL) operators involved, creating serious challenges for practical deployment. In addition, current research primarily focuses on algorithm-level optimizations for CLQA tasks, often overlooking compiler-level optimizations, which can offer greater generality and scalability. To address these limitations, we introduce a Knowledge Graph Compiler, namely KGCompiler, the first deep learning compiler specifically designed for CLQA tasks. By incorporating KG-specific optimizations proposed in this paper, KGCompiler enhances the reasoning performance of CLQA algorithms without requiring additional manual modifications to their implementations. At the same time, it significantly reduces memory usage. Extensive experiments demonstrate that KGCompiler accelerates CLQA algorithms by factors ranging from 1.04x to 8.26x, with an average speedup of 3.71x. We also provide an interface to enable hands-on experience with KGCompiler.",
        "arxiv_id": "2503.02172",
        "ARXIVID": "2503.02172",
        "COMMENT": "Does not match any specific criteria but introduces a compiler optimization for knowledge graph query answering.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01333": {
        "authors": [
            "Xu Liang"
        ],
        "title": "Group Relative Policy Optimization for Image Captioning",
        "abstract": "arXiv:2503.01333v1 Announce Type: new  Abstract: Image captioning tasks usually use two-stage training to complete model optimization. The first stage uses cross-entropy as the loss function for optimization, and the second stage uses self-critical sequence training (SCST) for reinforcement learning optimization. However, the SCST algorithm has certain defects. SCST relies only on a single greedy decoding result as a baseline. If the model itself is not stable enough, the greedy decoding result may be relatively worst, which will lead to a high variance of advantage estimation, further leading to unstable policy updates. In addition, SCST only compares one sampling result with the greedy decoding result, and the generation diversity is limited, which may fall into a local optimum. In this paper, we propose using the latest Group Relative Policy Optimization (GRPO) reinforcement learning algorithm as an optimization solution for the second stage. GRPO generates multiple candidate captions for the input image and then continuously optimizes the model through intragroup comparison. By constraining the amplitude of policy updates and KL divergence, the stability of the model during training is greatly guaranteed. In addition, compared to SCST, which only samples one answer, GRPO samples and generates multiple answers. Multiple candidate answers in the group cover a wider solution space. Combined with KL divergence constraints, GRPO can improve diversity while ensuring model stability. The code for this article is available at https://github.com/liangxu-one/ms-models/tree/image_caption_grpo/research/arxiv_papers/Image_Caption_GRPO.",
        "arxiv_id": "2503.01333",
        "ARXIVID": "2503.01333",
        "COMMENT": "Does not match any specific criteria but proposes a novel reinforcement learning algorithm for image captioning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.02497": {
        "authors": [
            "Haider Asif",
            "Abdul Basit",
            "Nouhaila Innan",
            "Muhammad Kashif",
            "Alberto Marchisio",
            "Muhammad Shafique"
        ],
        "title": "PennyLang: Pioneering LLM-Based Quantum Code Generation with a Novel PennyLane-Centric Dataset",
        "abstract": "arXiv:2503.02497v1 Announce Type: new  Abstract: Large Language Models (LLMs) offer remarkable capabilities in code generation, natural language processing, and domain-specific reasoning. Their potential in aiding quantum software development remains underexplored, particularly for the PennyLane framework-a leading platform for hybrid quantum-classical computing. To address this gap, we introduce a novel, high-quality dataset comprising 3,347 PennyLane-specific code samples of quantum circuits and their contextual descriptions, specifically curated to train/fine-tune LLM-based quantum code assistance. Our key contributions are threefold: (1) the automatic creation and open-source release of a comprehensive PennyLane dataset leveraging quantum computing textbooks, official documentation, and open-source repositories; (2) the development of a systematic methodology for data refinement, annotation, and formatting to optimize LLM training efficiency; and (3) a thorough evaluation, based on a Retrieval-Augmented Generation (RAG) framework, demonstrating the effectiveness of our dataset in streamlining PennyLane code generation and improving quantum development workflows. Compared to existing efforts that predominantly focus on Qiskit, our dataset significantly broadens the spectrum of quantum frameworks covered in AI-driven code assistance. By bridging this gap and providing reproducible dataset-creation methodologies, we aim to advance the field of AI-assisted quantum programming, making quantum computing more accessible to both newcomers and experienced developers.",
        "arxiv_id": "2503.02497",
        "ARXIVID": "2503.02497",
        "COMMENT": "Does not match any specific criteria but involves LLMs in a niche domain (quantum computing).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01136": {
        "authors": [
            "Xiongfei Su",
            "Siyuan Li",
            "Yuning Cui",
            "Miao Cao",
            "Yulun Zhang",
            "Zheng Chen",
            "Zongliang Wu",
            "Zedong Wang",
            "Yuanlong Zhang",
            "Xin Yuan"
        ],
        "title": "Prior-guided Hierarchical Harmonization Network for Efficient Image Dehazing",
        "abstract": "arXiv:2503.01136v1 Announce Type: new  Abstract: Image dehazing is a crucial task that involves the enhancement of degraded images to recover their sharpness and textures. While vision Transformers have exhibited impressive results in diverse dehazing tasks, their quadratic complexity and lack of dehazing priors pose significant drawbacks for real-world applications.   In this paper, guided by triple priors, Bright Channel Prior (BCP), Dark Channel Prior (DCP), and Histogram Equalization (HE), we propose a \\textit{P}rior-\\textit{g}uided Hierarchical \\textit{H}armonization Network (PGH$^2$Net) for image dehazing. PGH$^2$Net is built upon the UNet-like architecture with an efficient encoder and decoder, consisting of two module types: (1) Prior aggregation module that injects B/DCP and selects diverse contexts with gating attention. (2) Feature harmonization modules that subtract low-frequency components from spatial and channel aspects and learn more informative feature distributions to equalize the feature maps.",
        "arxiv_id": "2503.01136",
        "ARXIVID": "2503.01136",
        "COMMENT": "Does not match any specific criterion but is related to image dehazing with prior-guided methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01387": {
        "authors": [
            "Siddhant Prakash",
            "David R. Walton",
            "Rafael K. dos Anjos",
            "Anthony Steed",
            "Tobias Ritschel"
        ],
        "title": "Blind Augmentation: Calibration-free Camera Distortion Model Estimation for Real-time Mixed-reality Consistency",
        "abstract": "arXiv:2503.01387v1 Announce Type: new  Abstract: Real camera footage is subject to noise, motion blur (MB) and depth of field (DoF). In some applications these might be considered distortions to be removed, but in others it is important to model them because it would be ineffective, or interfere with an aesthetic choice, to simply remove them. In augmented reality applications where virtual content is composed into a live video feed, we can model noise, MB and DoF to make the virtual content visually consistent with the video. Existing methods for this typically suffer two main limitations. First, they require a camera calibration step to relate a known calibration target to the specific cameras response. Second, existing work require methods that can be (differentiably) tuned to the calibration, such as slow and specialized neural networks. We propose a method which estimates parameters for noise, MB and DoF instantly, which allows using off-the-shelf real-time simulation methods from e.g., a game engine in compositing augmented content. Our main idea is to unlock both features by showing how to use modern computer vision methods that can remove noise, MB and DoF from the video stream, essentially providing self-calibration. This allows to auto-tune any black-box real-time noise+MB+DoF method to deliver fast and high-fidelity augmentation consistency.",
        "arxiv_id": "2503.01387",
        "ARXIVID": "2503.01387",
        "COMMENT": "Does not match any specific criterion but is related to camera distortion modeling for augmented reality.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01100": {
        "authors": [
            "Hanzhe Liang",
            "Jie Zhou",
            "Xuanxin Chen",
            "Tao Dai",
            "Jinbao Wang",
            "Can Gao"
        ],
        "title": "Fence Theorem: Towards Dual-Objective Semantic-Structure Isolation in Preprocessing Phase for 3D Anomaly Detection",
        "abstract": "arXiv:2503.01100v2 Announce Type: new  Abstract: 3D anomaly detection (AD) is prominent but difficult due to lacking a unified theoretical foundation for preprocessing design. We establish the Fence Theorem, formalizing preprocessing as a dual-objective semantic isolator: (1) mitigating cross-semantic interference to the greatest extent feasible and (2) confining anomaly judgments to aligned semantic spaces wherever viable, thereby establishing intra-semantic comparability. Any preprocessing approach achieves this goal through a two-stage process of Emantic-Division and Spatial-Constraints stage. Through systematic deconstruction, we theoretically and experimentally subsume existing preprocessing methods under this theorem via tripartite evidence: qualitative analyses, quantitative studies, and mathematical proofs. Guided by the Fence Theorem, we implement Patch3D, consisting of Patch-Cutting and Patch-Matching modules, to segment semantic spaces and consolidate similar ones while independently modeling normal features within each space. Experiments on Anomaly-ShapeNet and Real3D-AD with different settings demonstrate that progressively finer-grained semantic alignment in preprocessing directly enhances point-level AD accuracy, providing inverse validation of the theorem's causal logic.",
        "arxiv_id": "2503.01100",
        "ARXIVID": "2503.01100",
        "COMMENT": "Does not match any specific criterion but is related to 3D anomaly detection and preprocessing design.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.02053": {
        "authors": [
            "Zaifu Zhan",
            "Shuang Zhou",
            "Huixue Zhou",
            "Zirui Liu",
            "Rui Zhang"
        ],
        "title": "EPEE: Towards Efficient and Effective Foundation Models in Biomedicine",
        "abstract": "arXiv:2503.02053v1 Announce Type: new  Abstract: Foundation models, including language models, e.g., GPT, and vision models, e.g., CLIP, have significantly advanced numerous biomedical tasks. Despite these advancements, the high inference latency and the \"overthinking\" issues in model inference impair the efficiency and effectiveness of foundation models, thus limiting their application in real-time clinical settings. To address these challenges, we proposed EPEE (Entropy- and Patience-based Early Exiting), a novel hybrid strategy designed to improve the inference efficiency of foundation models. The core idea was to leverage the strengths of entropy-based and patience-based early exiting methods to overcome their respective weaknesses. To evaluate EPEE, we conducted experiments on three core biomedical tasks-classification, relation extraction, and event extraction-using four foundation models (BERT, ALBERT, GPT-2, and ViT) across twelve datasets, including clinical notes and medical images. The results showed that EPEE significantly reduced inference time while maintaining or improving accuracy, demonstrating its adaptability to diverse datasets and tasks. EPEE addressed critical barriers to deploying foundation models in healthcare by balancing efficiency and effectiveness. It potentially provided a practical solution for real-time clinical decision-making with foundation models, supporting reliable and efficient workflows.",
        "arxiv_id": "2503.02053",
        "ARXIVID": "2503.02053",
        "COMMENT": "Does not match any specific criteria but is relevant to foundation models and their efficiency in biomedical applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01257": {
        "authors": [
            "Xuan Zhu",
            "Jijun Xiang",
            "Xianqi Wang",
            "Longliang Liu",
            "Yu Wang",
            "Hong Zhang",
            "Fei Guo",
            "Xin Yang"
        ],
        "title": "SVDC: Consistent Direct Time-of-Flight Video Depth Completion with Frequency Selective Fusion",
        "abstract": "arXiv:2503.01257v1 Announce Type: new  Abstract: Lightweight direct Time-of-Flight (dToF) sensors are ideal for 3D sensing on mobile devices. However, due to the manufacturing constraints of compact devices and the inherent physical principles of imaging, dToF depth maps are sparse and noisy. In this paper, we propose a novel video depth completion method, called SVDC, by fusing the sparse dToF data with the corresponding RGB guidance. Our method employs a multi-frame fusion scheme to mitigate the spatial ambiguity resulting from the sparse dToF imaging. Misalignment between consecutive frames during multi-frame fusion could cause blending between object edges and the background, which results in a loss of detail. To address this, we introduce an adaptive frequency selective fusion (AFSF) module, which automatically selects convolution kernel sizes to fuse multi-frame features. Our AFSF utilizes a channel-spatial enhancement attention (CSEA) module to enhance features and generates an attention map as fusion weights. The AFSF ensures edge detail recovery while suppressing high-frequency noise in smooth regions. To further enhance temporal consistency, We propose a cross-window consistency loss to ensure consistent predictions across different windows, effectively reducing flickering. Our proposed SVDC achieves optimal accuracy and consistency on the TartanAir and Dynamic Replica datasets. Code is available at https://github.com/Lan1eve/SVDC.",
        "arxiv_id": "2503.01257",
        "ARXIVID": "2503.01257",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision and depth completion methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00515": {
        "authors": [
            "Songlin Dong",
            "Yuhang He",
            "Zhengdong Zhou",
            "Haoyu Luo",
            "Xing Wei",
            "Alex C. Kot",
            "Yihong Gong"
        ],
        "title": "Class-Independent Increment: An Efficient Approach for Multi-label Class-Incremental Learning",
        "abstract": "arXiv:2503.00515v1 Announce Type: new  Abstract: Current research on class-incremental learning primarily focuses on single-label classification tasks. However, real-world applications often involve multi-label scenarios, such as image retrieval and medical imaging. Therefore, this paper focuses on the challenging yet practical multi-label class-incremental learning (MLCIL) problem. In addition to the challenge of catastrophic forgetting, MLCIL encounters issues related to feature confusion, encompassing inter-session and intra-feature confusion. To address these problems, we propose a novel MLCIL approach called class-independent increment (CLIN). Specifically, in contrast to existing methods that extract image-level features, we propose a class-independent incremental network (CINet) to extract multiple class-level embeddings for multi-label samples. It learns and preserves the knowledge of different classes by constructing class-specific tokens. On this basis, we develop two novel loss functions, optimizing the learning of class-specific tokens and class-level embeddings, respectively. These losses aim to distinguish between new and old classes, further alleviating the problem of feature confusion. Extensive experiments on MS-COCO and PASCAL VOC datasets demonstrate the effectiveness of our method for improving recognition performance and mitigating forgetting on various MLCIL tasks.",
        "arxiv_id": "2503.00515",
        "ARXIVID": "2503.00515",
        "COMMENT": "Does not match any specific criteria but is relevant to incremental learning in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00450": {
        "authors": [
            "Joshua Talks",
            "Anna Kreshuk"
        ],
        "title": "Ranking pre-trained segmentation models for zero-shot transferability",
        "abstract": "arXiv:2503.00450v1 Announce Type: new  Abstract: Model transfer presents a solution to the challenges of segmentation in the microscopy community, where the immense cost of labelling sufficient training data is a major bottleneck in the use of deep learning. With large quantities of imaging data produced across a wide range of imaging conditions, institutes also produce many bespoke models trained on specific source data which then get collected in model banks or zoos. As the number of available models grows, so does the need for an efficient and reliable model selection method for a specific target dataset of interest. We focus on the unsupervised regime where no labels are available for the target dataset. Building on previous work linking model generalisation and consistency under perturbation, we propose the first unsupervised transferability estimator for semantic and instance segmentation tasks which doesn't require access to source training data or target domain labels. We evaluate the method on multiple segmentation problems across microscopy modalities, finding a strong correlation between the rankings based on our estimator and rankings based on target dataset performance.",
        "arxiv_id": "2503.00450",
        "ARXIVID": "2503.00450",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision and model transferability.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00861": {
        "authors": [
            "Sohyun Jeong",
            "Taewoong Kang",
            "Hyojin Jang",
            "Jaegul Choo"
        ],
        "title": "Zero-Shot Head Swapping in Real-World Scenarios",
        "abstract": "arXiv:2503.00861v1 Announce Type: new  Abstract: With growing demand in media and social networks for personalized images, the need for advanced head-swapping techniques, integrating an entire head from the head image with the body from the body image, has increased. However, traditional head swapping methods heavily rely on face-centered cropped data with primarily frontal facing views, which limits their effectiveness in real world applications. Additionally, their masking methods, designed to indicate regions requiring editing, are optimized for these types of dataset but struggle to achieve seamless blending in complex situations, such as when the original data includes features like long hair extending beyond the masked area. To overcome these limitations and enhance adaptability in diverse and complex scenarios, we propose a novel head swapping method, HID, that is robust to images including the full head and the upper body, and handles from frontal to side views, while automatically generating context aware masks. For automatic mask generation, we introduce the IOMask, which enables seamless blending of the head and body, effectively addressing integration challenges. We further introduce the hair injection module to capture hair details with greater precision. Our experiments demonstrate that the proposed approach achieves state-of-the-art performance in head swapping, providing visually consistent and realistic results across a wide range of challenging conditions.",
        "arxiv_id": "2503.00861",
        "ARXIVID": "2503.00861",
        "COMMENT": "Does not match any specific criterion but is generally relevant to generative modeling and image synthesis.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01715": {
        "authors": [
            "Antoni Bigata",
            "Micha{\\l} Stypu{\\l}kowski",
            "Rodrigo Mira",
            "Stella Bounareli",
            "Konstantinos Vougioukas",
            "Zoe Landgraf",
            "Nikita Drobyshev",
            "Maciej Zieba",
            "Stavros Petridis",
            "Maja Pantic"
        ],
        "title": "KeyFace: Expressive Audio-Driven Facial Animation for Long Sequences via KeyFrame Interpolation",
        "abstract": "arXiv:2503.01715v1 Announce Type: new  Abstract: Current audio-driven facial animation methods achieve impressive results for short videos but suffer from error accumulation and identity drift when extended to longer durations. Existing methods attempt to mitigate this through external spatial control, increasing long-term consistency but compromising the naturalness of motion. We propose KeyFace, a novel two-stage diffusion-based framework, to address these issues. In the first stage, keyframes are generated at a low frame rate, conditioned on audio input and an identity frame, to capture essential facial expressions and movements over extended periods of time. In the second stage, an interpolation model fills in the gaps between keyframes, ensuring smooth transitions and temporal coherence. To further enhance realism, we incorporate continuous emotion representations and handle a wide range of non-speech vocalizations (NSVs), such as laughter and sighs. We also introduce two new evaluation metrics for assessing lip synchronization and NSV generation. Experimental results show that KeyFace outperforms state-of-the-art methods in generating natural, coherent facial animations over extended durations, successfully encompassing NSVs and continuous emotions.",
        "arxiv_id": "2503.01715",
        "ARXIVID": "2503.01715",
        "COMMENT": "Does not match any specific criterion but is generally relevant to generative modeling and animation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01190": {
        "authors": [
            "Jonathan Fhima",
            "Jan Van Eijgen",
            "Lennert Beeckmans",
            "Thomas Jacobs",
            "Moti Freiman",
            "Luis Filipe Nakayama",
            "Ingeborg Stalmans",
            "Chaim Baskin",
            "Joachim A. Behar"
        ],
        "title": "Enhancing Retinal Vessel Segmentation Generalization via Layout-Aware Generative Modelling",
        "abstract": "arXiv:2503.01190v1 Announce Type: new  Abstract: Generalization in medical segmentation models is challenging due to limited annotated datasets and imaging variability. To address this, we propose Retinal Layout-Aware Diffusion (RLAD), a novel diffusion-based framework for generating controllable layout-aware images. RLAD conditions image generation on multiple key layout components extracted from real images, ensuring high structural fidelity while enabling diversity in other components. Applied to retinal fundus imaging, we augmented the training datasets by synthesizing paired retinal images and vessel segmentations conditioned on extracted blood vessels from real images, while varying other layout components such as lesions and the optic disc. Experiments demonstrated that RLAD-generated data improved generalization in retinal vessel segmentation by up to 8.1%. Furthermore, we present REYIA, a comprehensive dataset comprising 586 manually segmented retinal images. To foster reproducibility and drive innovation, both our code and dataset will be made publicly accessible.",
        "arxiv_id": "2503.01190",
        "ARXIVID": "2503.01190",
        "COMMENT": "Does not match any specific criterion but is generally relevant to generative modeling in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01569": {
        "authors": [
            "Muhammad Aqeel",
            "Shakiba Sharifi",
            "Marco Cristani",
            "Francesco Setti"
        ],
        "title": "Meta Learning-Driven Iterative Refinement for Robust Anomaly Detection in Industrial Inspection",
        "abstract": "arXiv:2503.01569v1 Announce Type: new  Abstract: This study investigates the performance of robust anomaly detection models in industrial inspection, focusing particularly on their ability to handle noisy data. We propose to leverage the adaptation ability of meta learning approaches to identify and reject noisy training data to improve the learning process. In our model, we employ Model Agnostic Meta Learning (MAML) and an iterative refinement process through an Inter-Quartile Range rejection scheme to enhance their adaptability and robustness. This approach significantly improves the models capability to distinguish between normal and defective conditions. Our results of experiments conducted on well known MVTec and KSDD2 datasets demonstrate that the proposed method not only excels in environments with substantial noise but can also contribute in case of a clear training set, isolating those samples that are relatively out of distribution, thus offering significant improvements over traditional models.",
        "arxiv_id": "2503.01569",
        "ARXIVID": "2503.01569",
        "COMMENT": "This paper proposes a meta-learning-driven approach for anomaly detection, which is not directly related to any specific criteria but is relevant to machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00250": {
        "authors": [
            "Yanan Niu",
            "Roy Sarkis",
            "Demetri Psaltis",
            "Mario Paolone",
            "Christophe Moser",
            "Luisa Lambertini"
        ],
        "title": "Solar Multimodal Transformer: Intraday Solar Irradiance Predictor using Public Cameras and Time Series",
        "abstract": "arXiv:2503.00250v1 Announce Type: new  Abstract: Accurate intraday solar irradiance forecasting is crucial for optimizing dispatch planning and electricity trading. For this purpose, we introduce a novel and effective approach that includes three distinguishing components from the literature: 1) the uncommon use of single-frame public camera imagery; 2) solar irradiance time series scaled with a proposed normalization step, which boosts performance; and 3) a lightweight multimodal model, called Solar Multimodal Transformer (SMT), that delivers accurate short-term solar irradiance forecasting by combining images and scaled time series. Benchmarking against Solcast, a leading solar forecasting service provider, our model improved prediction accuracy by 25.95%. Our approach allows for easy adaptation to various camera specifications, offering broad applicability for real-world solar forecasting challenges.",
        "arxiv_id": "2503.00250",
        "ARXIVID": "2503.00250",
        "COMMENT": "This paper introduces a multimodal transformer for solar irradiance prediction, which is not directly related to any specific criteria but involves multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00548": {
        "authors": [
            "Yanjun Li",
            "Zhaoyang Li",
            "Honghui Chen",
            "Lizhi Xu"
        ],
        "title": "Unbiased Video Scene Graph Generation via Visual and Semantic Dual Debiasing",
        "abstract": "arXiv:2503.00548v1 Announce Type: new  Abstract: Video Scene Graph Generation (VidSGG) aims to capture dynamic relationships among entities by sequentially analyzing video frames and integrating visual and semantic information. However, VidSGG is challenged by significant biases that skew predictions. To mitigate these biases, we propose a VIsual and Semantic Awareness (VISA) framework for unbiased VidSGG. VISA addresses visual bias through memory-enhanced temporal integration that enhances object representations and concurrently reduces semantic bias by iteratively integrating object features with comprehensive semantic information derived from triplet relationships. This visual-semantics dual debiasing approach results in more unbiased representations of complex scene dynamics. Extensive experiments demonstrate the effectiveness of our method, where VISA outperforms existing unbiased VidSGG approaches by a substantial margin (e.g., +13.1% improvement in mR@20 and mR@50 for the SGCLS task under Semi Constraint).",
        "arxiv_id": "2503.00548",
        "ARXIVID": "2503.00548",
        "COMMENT": "This paper addresses video scene graph generation with a debiasing framework, which is related to computer vision but does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01292": {
        "authors": [
            "Yurui Pan",
            "Lidong Wang",
            "Yuchao Chen",
            "Wenbing Zhu",
            "Bo Peng",
            "Mingmin Chi"
        ],
        "title": "PA-CLIP: Enhancing Zero-Shot Anomaly Detection through Pseudo-Anomaly Awareness",
        "abstract": "arXiv:2503.01292v1 Announce Type: new  Abstract: In industrial anomaly detection (IAD), accurately identifying defects amidst diverse anomalies and under varying imaging conditions remains a significant challenge. Traditional approaches often struggle with high false-positive rates, frequently misclassifying normal shadows and surface deformations as defects, an issue that becomes particularly pronounced in products with complex and intricate surface features. To address these challenges, we introduce PA-CLIP, a zero-shot anomaly detection method that reduces background noise and enhances defect detection through a pseudo-anomaly-based framework. The proposed method integrates a multiscale feature aggregation strategy for capturing detailed global and local information, two memory banks for distinguishing background information, including normal patterns and pseudo-anomalies, from true anomaly features, and a decision-making module designed to minimize false positives caused by environmental variations while maintaining high defect sensitivity. Demonstrated on the MVTec AD and VisA datasets, PA-CLIP outperforms existing zero-shot methods, providing a robust solution for industrial defect detection.",
        "arxiv_id": "2503.01292",
        "ARXIVID": "2503.01292",
        "COMMENT": "This paper introduces a zero-shot anomaly detection method, which is not directly related to any specific criteria but is relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00746": {
        "authors": [
            "Liao Shen",
            "Tianqi Liu",
            "Huiqiang Sun",
            "Jiaqi Li",
            "Zhiguo Cao",
            "Wei Li",
            "Chen Change Loy"
        ],
        "title": "DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting",
        "abstract": "arXiv:2503.00746v1 Announce Type: new  Abstract: Recent advances in 3D Gaussian Splatting (3D-GS) have shown remarkable success in representing 3D scenes and generating high-quality, novel views in real-time. However, 3D-GS and its variants assume that input images are captured based on pinhole imaging and are fully in focus. This assumption limits their applicability, as real-world images often feature shallow depth-of-field (DoF). In this paper, we introduce DoF-Gaussian, a controllable depth-of-field method for 3D-GS. We develop a lens-based imaging model based on geometric optics principles to control DoF effects. To ensure accurate scene geometry, we incorporate depth priors adjusted per scene, and we apply defocus-to-focus adaptation to minimize the gap in the circle of confusion. We also introduce a synthetic dataset to assess refocusing capabilities and the model's ability to learn precise lens parameters. Our framework is customizable and supports various interactive applications. Extensive experiments confirm the effectiveness of our method. Our project is available at https://dof-gaussian.github.io.",
        "arxiv_id": "2503.00746",
        "ARXIVID": "2503.00746",
        "COMMENT": "This paper does not match any specific criteria but is related to 3D scene representation and rendering, which is tangentially related to spatial understanding.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00086": {
        "authors": [
            "Zhenxing Cui",
            "Lu Chen",
            "Yunhai Wang",
            "Daniel Haehn",
            "Yong Wang",
            "Hanspeter Pfister"
        ],
        "title": "Generalization of CNNs on Relational Reasoning with Bar Charts",
        "abstract": "arXiv:2503.00086v1 Announce Type: new  Abstract: This paper presents a systematic study of the generalization of convolutional neural networks (CNNs) and humans on relational reasoning tasks with bar charts. We first revisit previous experiments on graphical perception and update the benchmark performance of CNNs. We then test the generalization performance of CNNs on a classic relational reasoning task: estimating bar length ratios in a bar chart, by progressively perturbing the standard visualizations. We further conduct a user study to compare the performance of CNNs and humans. Our results show that CNNs outperform humans only when the training and test data have the same visual encodings. Otherwise, they may perform worse. We also find that CNNs are sensitive to perturbations in various visual encodings, regardless of their relevance to the target bars. Yet, humans are mainly influenced by bar lengths. Our study suggests that robust relational reasoning with visualizations is challenging for CNNs. Improving CNNs' generalization performance may require training them to better recognize task-related visual properties.",
        "arxiv_id": "2503.00086",
        "ARXIVID": "2503.00086",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and relational reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.00046": {
        "authors": [
            "Zhaoyi Joey Hou",
            "Adriana Kovashka",
            "Xiang Lorraine Li"
        ],
        "title": "Leveraging Large Models for Evaluating Novel Content: A Case Study on Advertisement Creativity",
        "abstract": "arXiv:2503.00046v1 Announce Type: new  Abstract: Evaluating creativity is challenging, even for humans, not only because of its subjectivity but also because it involves complex cognitive processes. Inspired by work in marketing, we attempt to break down visual advertisement creativity into atypicality and originality. With fine-grained human annotations on these dimensions, we propose a suit of tasks specifically for such a subjective problem. We also evaluate the alignment between state-of-the-art (SoTA) vision language models (VLM) and humans on our proposed benchmark, demonstrating both the promises and challenges of using VLMs for automatic creativity assessment.",
        "arxiv_id": "2503.00046",
        "ARXIVID": "2503.00046",
        "COMMENT": "Does not match any specific criterion but evaluates vision-language models (VLMs) for creativity assessment, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.00516": {
        "authors": [
            "Jiawen Zhu",
            "Huayi Tang",
            "Xin Chen",
            "Xinying Wang",
            "Dong Wang",
            "Huchuan Lu"
        ],
        "title": "Two-stream Beats One-stream: Asymmetric Siamese Network for Efficient Visual Tracking",
        "abstract": "arXiv:2503.00516v1 Announce Type: new  Abstract: Efficient tracking has garnered attention for its ability to operate on resource-constrained platforms for real-world deployment beyond desktop GPUs. Current efficient trackers mainly follow precision-oriented trackers, adopting a one-stream framework with lightweight modules. However, blindly adhering to the one-stream paradigm may not be optimal, as incorporating template computation in every frame leads to redundancy, and pervasive semantic interaction between template and search region places stress on edge devices. In this work, we propose a novel asymmetric Siamese tracker named \\textbf{AsymTrack} for efficient tracking. AsymTrack disentangles template and search streams into separate branches, with template computing only once during initialization to generate modulation signals. Building on this architecture, we devise an efficient template modulation mechanism to unidirectional inject crucial cues into the search features, and design an object perception enhancement module that integrates abstract semantics and local details to overcome the limited representation in lightweight tracker. Extensive experiments demonstrate that AsymTrack offers superior speed-precision trade-offs across different platforms compared to the current state-of-the-arts. For instance, AsymTrack-T achieves 60.8\\% AUC on LaSOT and 224/81/84 FPS on GPU/CPU/AGX, surpassing HiT-Tiny by 6.0\\% AUC with higher speeds. The code is available at https://github.com/jiawen-zhu/AsymTrack.",
        "arxiv_id": "2503.00516",
        "ARXIVID": "2503.00516",
        "COMMENT": "This paper does not match any specific criteria but is related to efficient visual tracking, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.00325": {
        "authors": [
            "Zhiwei Ling",
            "Yachen Chang",
            "Hailiang Zhao",
            "Xinkui Zhao",
            "Kingsum Chow",
            "Shuiguang Deng"
        ],
        "title": "CADRef: Robust Out-of-Distribution Detection via Class-Aware Decoupled Relative Feature Leveraging",
        "abstract": "arXiv:2503.00325v1 Announce Type: new  Abstract: Deep neural networks (DNNs) have been widely criticized for their overconfidence when dealing with out-of-distribution (OOD) samples, highlighting the critical need for effective OOD detection to ensure the safe deployment of DNNs in real-world settings. Existing post-hoc OOD detection methods primarily enhance the discriminative power of logit-based approaches by reshaping sample features, yet they often neglect critical information inherent in the features themselves. In this paper, we propose the Class-Aware Relative Feature-based method (CARef), which utilizes the error between a sample's feature and its class-aware average feature as a discriminative criterion. To further refine this approach, we introduce the Class-Aware Decoupled Relative Feature-based method (CADRef), which decouples sample features based on the alignment of signs between the relative feature and corresponding model weights, enhancing the discriminative capabilities of CARef. Extensive experimental results across multiple datasets and models demonstrate that both proposed methods exhibit effectiveness and robustness in OOD detection compared to state-of-the-art methods. Specifically, our two methods outperform the best baseline by 2.82% and 3.27% in AUROC, with improvements of 4.03% and 6.32% in FPR95, respectively.",
        "arxiv_id": "2503.00325",
        "ARXIVID": "2503.00325",
        "COMMENT": "This paper does not match any of the specific criteria but is related to OOD detection, which is tangentially relevant to your friend's interest in robust AI systems.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.00202": {
        "authors": [
            "Ryosuke Kawamura",
            "Hideaki Hayashi",
            "Noriko Takemura",
            "Hajime Nagahara"
        ],
        "title": "MIDAS: Mixing Ambiguous Data with Soft Labels for Dynamic Facial Expression Recognition",
        "abstract": "arXiv:2503.00202v1 Announce Type: new  Abstract: Dynamic facial expression recognition (DFER) is an important task in the field of computer vision. To apply automatic DFER in practice, it is necessary to accurately recognize ambiguous facial expressions, which often appear in data in the wild. In this paper, we propose MIDAS, a data augmentation method for DFER, which augments ambiguous facial expression data with soft labels consisting of probabilities for multiple emotion classes. In MIDAS, the training data are augmented by convexly combining pairs of video frames and their corresponding emotion class labels, which can also be regarded as an extension of mixup to soft-labeled video data. This simple extension is remarkably effective in DFER with ambiguous facial expression data. To evaluate MIDAS, we conducted experiments on the DFEW dataset. The results demonstrate that the model trained on the data augmented by MIDAS outperforms the existing state-of-the-art method trained on the original dataset.",
        "arxiv_id": "2503.00202",
        "ARXIVID": "2503.00202",
        "COMMENT": "Does not match any specific criteria but is relevant to computer vision and machine learning in general.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.00545": {
        "authors": [
            "Yujie Lei",
            "Wenjie Sun",
            "Sen Jia",
            "Qingquan Li",
            "Jie Zhang"
        ],
        "title": "RFWNet: A Lightweight Remote Sensing Object Detector Integrating Multi-Scale Receptive Fields and Foreground Focus Mechanism",
        "abstract": "arXiv:2503.00545v1 Announce Type: new  Abstract: Challenges in remote sensing object detection (RSOD), such as high inter-class similarity, imbalanced foreground-background distribution, and the small size of objects in remote sensing images significantly hinder detection accuracy. Moreo-ver, the trade-off between model accuracy and computational complexity poses additional constraints on the application of RSOD algorithms. To address these issues, this study proposes an efficient and lightweight RSOD algorithm integrat-ing multi-scale receptive fields and foreground focus mechanism, named RFWNet. Specifically, we proposed a lightweight backbone network Receptive Field Adaptive Selection Network (RFASNet), leveraging the rich context infor-mation of remote sensing images to enhance class separability. Additionally, we developed a Foreground Background Separation Module (FBSM) consisting of a background redundant information filtering module and a foreground information enhancement module to emphasize critical regions within images while filtering redundant background information. Finally, we designed a loss function, the Weighted CIoU-Wasserstein (WCW) loss, which weights the IoU-based loss by using the Normalized Wasserstein Distance to mitigate model sensitivity to small object position deviations. Experimental evaluations on the DOTA V1.0 and NWPU VHR-10 datasets demonstrate that RFWNet achieves advanced perfor-mance with 6.0M parameters and can achieves 52 FPS.",
        "arxiv_id": "2503.00545",
        "ARXIVID": "2503.00545",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and lightweight model design.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.01654": {
        "authors": [
            "Shuvendu Roy",
            "Franklin Ogidi",
            "Ali Etemad",
            "Elham Dolatabadi",
            "Arash Afkanpour"
        ],
        "title": "A Shared Encoder Approach to Multimodal Representation Learning",
        "abstract": "arXiv:2503.01654v1 Announce Type: new  Abstract: Multimodal representation learning has demonstrated remarkable potential in enabling models to process and integrate diverse data modalities, such as text and images, for improved understanding and performance. While the medical domain can benefit significantly from this paradigm, the scarcity of paired multimodal data and reliance on proprietary or pretrained encoders pose significant challenges. In this work, we present a shared encoder framework for multimodal representation learning tailored to the medical domain. Our approach employs a single set of encoder parameters shared across modalities, augmented with learnable modality features. Empirical results demonstrate that our shared encoder idea achieves superior performance compared to separate modality-specific encoders, demonstrating improved generalization in data-constrained settings. Notably, the performance gains are more pronounced with fewer training examples, underscoring the efficiency of our shared encoder framework for real-world medical applications with limited data. Our code and experiment setup are available at https://github.com/VectorInstitute/shared_encoder.",
        "arxiv_id": "2503.01654",
        "ARXIVID": "2503.01654",
        "COMMENT": "Does not match any specific criterion but is generally relevant to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.01158": {
        "authors": [
            "Suzhen Wang",
            "Weijie Chen",
            "Wei Zhang",
            "Minda Zhao",
            "Lincheng Li",
            "Rongsheng Zhang",
            "Zhipeng Hu",
            "Xin Yu"
        ],
        "title": "EasyCraft: A Robust and Efficient Framework for Automatic Avatar Crafting",
        "abstract": "arXiv:2503.01158v1 Announce Type: new  Abstract: Character customization, or 'face crafting,' is a vital feature in role-playing games (RPGs), enhancing player engagement by enabling the creation of personalized avatars. Existing automated methods often struggle with generalizability across diverse game engines due to their reliance on the intermediate constraints of specific image domain and typically support only one type of input, either text or image. To overcome these challenges, we introduce EasyCraft, an innovative end-to-end feedforward framework that automates character crafting by uniquely supporting both text and image inputs. Our approach employs a translator capable of converting facial images of any style into crafting parameters. We first establish a unified feature distribution in the translator's image encoder through self-supervised learning on a large-scale dataset, enabling photos of any style to be embedded into a unified feature representation. Subsequently, we map this unified feature distribution to crafting parameters specific to a game engine, a process that can be easily adapted to most game engines and thus enhances EasyCraft's generalizability. By integrating text-to-image techniques with our translator, EasyCraft also facilitates precise, text-based character crafting. EasyCraft's ability to integrate diverse inputs significantly enhances the versatility and accuracy of avatar creation. Extensive experiments on two RPG games demonstrate the effectiveness of our method, achieving state-of-the-art results and facilitating adaptability across various avatar engines.",
        "arxiv_id": "2503.01158",
        "ARXIVID": "2503.01158",
        "COMMENT": "This paper introduces a framework for avatar crafting, which is not directly related to any specific criteria but is relevant to computer vision applications.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.01262": {
        "authors": [
            "Huayu Zhang",
            "Dongyue Wu",
            "Yuanjie Shao",
            "Nong Sang",
            "Changxin Gao"
        ],
        "title": "Object-Aware Video Matting with Cross-Frame Guidance",
        "abstract": "arXiv:2503.01262v1 Announce Type: new  Abstract: Recently, trimap-free methods have drawn increasing attention in human video matting due to their promising performance. Nevertheless, these methods still suffer from the lack of deterministic foreground-background cues, which impairs their ability to consistently identify and locate foreground targets over time and mine fine-grained details. In this paper, we present a trimap-free Object-Aware Video Matting (OAVM) framework, which can perceive different objects, enabling joint recognition of foreground objects and refinement of edge details. Specifically, we propose an Object-Guided Correction and Refinement (OGCR) module, which employs cross-frame guidance to aggregate object-level instance information into pixel-level detail features, thereby promoting their synergy. Furthermore, we design a Sequential Foreground Merging augmentation strategy to diversify sequential scenarios and enhance capacity of the network for object discrimination. Extensive experiments on recent widely used synthetic and real-world benchmarks demonstrate the state-of-the-art performance of our OAVM with only an initial coarse mask. The code and model will be available.",
        "arxiv_id": "2503.01262",
        "ARXIVID": "2503.01262",
        "COMMENT": "This paper focuses on video matting with object-aware methods, which does not directly match any specific criteria but is related to computer vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}