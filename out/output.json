{
    "2507.00790": {
        "authors": [
            "Huaqiu Li",
            "Yong Wang",
            "Tongwen Huang",
            "Hailang Huang",
            "Haoqian Wang",
            "Xiangxiang Chu"
        ],
        "title": "LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling",
        "abstract": "arXiv:2507.00790v1 Announce Type: new  Abstract: Unified image restoration is a significantly challenging task in low-level vision. Existing methods either make tailored designs for specific tasks, limiting their generalizability across various types of degradation, or rely on training with paired datasets, thereby suffering from closed-set constraints. To address these issues, we propose a novel, dataset-free, and unified approach through recurrent posterior sampling utilizing a pretrained latent diffusion model. Our method incorporates the multimodal understanding model to provide sematic priors for the generative model under a task-blind condition. Furthermore, it utilizes a lightweight module to align the degraded input with the generated preference of the diffusion model, and employs recurrent refinement for posterior sampling. Extensive experiments demonstrate that our method outperforms state-of-the-art methods, validating its effectiveness and robustness. Our code and data will be available at https://github.com/AMAP-ML/LD-RPS.",
        "arxiv_id": "2507.00790",
        "ARXIVID": "2507.00790",
        "COMMENT": "Matches criterion 2: Unified Diffusion Models for multiple vision tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.00377": {
        "authors": [
            "Jianhao Xie",
            "Ziang Zhang",
            "Zhenyu Weng",
            "Yuesheng Zhu",
            "Guibo Luo"
        ],
        "title": "MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis",
        "abstract": "arXiv:2507.00377v1 Announce Type: new  Abstract: Recent advancements in deep learning for medical image segmentation are often limited by the scarcity of high-quality training data.While diffusion models provide a potential solution by generating synthetic images, their effectiveness in medical imaging remains constrained due to their reliance on large-scale medical datasets and the need for higher image quality. To address these challenges, we present MedDiff-FT, a controllable medical image generation method that fine-tunes a diffusion foundation model to produce medical images with structural dependency and domain specificity in a data-efficient manner. During inference, a dynamic adaptive guiding mask enforces spatial constraints to ensure anatomically coherent synthesis, while a lightweight stochastic mask generator enhances diversity through hierarchical randomness injection. Additionally, an automated quality assessment protocol filters suboptimal outputs using feature-space metrics, followed by mask corrosion to refine fidelity. Evaluated on five medical segmentation datasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's segmentation performance by an average of 1% in Dice score. The framework effectively balances generation quality, diversity, and computational efficiency, offering a practical solution for medical data augmentation. The code is available at https://github.com/JianhaoXie1/MedDiff-FT.",
        "arxiv_id": "2507.00377",
        "ARXIVID": "2507.00377",
        "COMMENT": "Matches criterion 2: Unified Diffusion Models for multiple vision tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    }
}