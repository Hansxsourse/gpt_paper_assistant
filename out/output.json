{
    "2602.05218": {
        "authors": [
            "Jiahao Nie",
            "Yun Xing",
            "Wenbin An",
            "Qingsong Zhao",
            "Jiawei Shao",
            "Yap-Peng Tan",
            "Alex C. Kot",
            "Shijian Lu",
            "Xuelong Li"
        ],
        "title": "Boosting SAM for Cross-Domain Few-Shot Segmentation via Conditional Point Sparsification",
        "abstract": "arXiv:2602.05218v1 Announce Type: new  Abstract: Motivated by the success of the Segment Anything Model (SAM) in promptable segmentation, recent studies leverage SAM to develop training-free solutions for few-shot segmentation, which aims to predict object masks in the target image based on a few reference exemplars. These SAM-based methods typically rely on point matching between reference and target images and use the matched dense points as prompts for mask prediction. However, we observe that dense points perform poorly in Cross-Domain Few-Shot Segmentation (CD-FSS), where target images are from medical or satellite domains. We attribute this issue to large domain shifts that disrupt the point-image interactions learned by SAM, and find that point density plays a crucial role under such conditions. To address this challenge, we propose Conditional Point Sparsification (CPS), a training-free approach that adaptively guides SAM interactions for cross-domain images based on reference exemplars. Leveraging ground-truth masks, the reference images provide reliable guidance for adaptively sparsifying dense matched points, enabling more accurate segmentation results. Extensive experiments demonstrate that CPS outperforms existing training-free SAM-based methods across diverse CD-FSS datasets.",
        "arxiv_id": "2602.05218",
        "ARXIVID": "2602.05218",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.05966": {
        "authors": [
            "Mirlan Karimov",
            "Teodora Spasojevic",
            "Markus Braun",
            "Julian Wiederer",
            "Vasileios Belagiannis",
            "Marc Pollefeys"
        ],
        "title": "LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation",
        "abstract": "arXiv:2602.05966v1 Announce Type: new  Abstract: Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads.",
        "arxiv_id": "2602.05966",
        "ARXIVID": "2602.05966",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}