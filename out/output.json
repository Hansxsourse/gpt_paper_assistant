{
    "2511.20211": {
        "authors": [
            "Hao Yu",
            "Jiabo Zhan",
            "Zile Wang",
            "Jinglin Wang",
            "Huaisong Zhang",
            "Hongyu Li",
            "Xinrui Chen",
            "Yongxian Wei",
            "Chun Yuan"
        ],
        "title": "OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation",
        "abstract": "arXiv:2511.20211v1 Announce Type: new  Abstract: Generative models have excelled in RGB synthesis, but real-world applications require RGBA manipulation. This has led to a fragmented landscape: specialized, single-task models handle alpha but lack versatility, while unified multi-task frameworks are confined to the RGB domain. To bridge this critical gap, we propose OmniAlpha, the first unified, multi-task generative framework for sequence-to-sequence RGBA image generation and editing. Its architecture features MSRoPE-BiL, a novel RoPE method with a bi-directionally extendable layer axis for its Diffusion Transformer (DiT) backbone, enabling the concurrent processing of multiple input and target RGBA layers. To power this framework, we introduce AlphaLayers, a new dataset of 1,000 high-quality, multi-layer triplets, built via a novel automated synthesis and filter pipeline. Jointly training OmniAlpha on this dataset across a comprehensive suite of 21 diverse tasks, extensive experiments demonstrate that our unified approach consistently outperforms strong, specialized baselines. Most notably, OmniAlpha achieves a dramatic 84.8% relative reduction in SAD for mask-free matting on AIM-500 and wins over 90% of human preferences in layer-conditioned completion. Our work proves that a unified, multi-task model can learn a superior shared representation for RGBA, paving the way for more powerful, layer-aware generative systems.",
        "arxiv_id": "2511.20211",
        "ARXIVID": "2511.20211",
        "COMMENT": "The paper proposes a unified multi-task generative framework for RGBA image generation, which aligns with the criteria for unified image generation and segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2511.20646": {
        "authors": [
            "Xiaoye Wang",
            "Chen Tang",
            "Xiangyu Yue",
            "Wei-Hong Li"
        ],
        "title": "3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding",
        "abstract": "arXiv:2511.20646v1 Announce Type: new  Abstract: This paper addresses the challenge of training a single network to jointly perform multiple dense prediction tasks, such as segmentation and depth estimation, i.e., multi-task learning (MTL). Current approaches mainly capture cross-task relations in the 2D image space, often leading to unstructured features lacking 3D-awareness. We argue that 3D-awareness is vital for modeling cross-task correlations essential for comprehensive scene understanding. We propose to address this problem by integrating correlations across views, i.e., cost volume, as geometric consistency in the MTL network. Specifically, we introduce a lightweight Cross-view Module (CvM), shared across tasks, to exchange information across views and capture cross-view correlations, integrated with a feature from MTL encoder for multi-task predictions. This module is architecture-agnostic and can be applied to both single and multi-view data. Extensive results on NYUv2 and PASCAL-Context demonstrate that our method effectively injects geometric consistency into existing MTL methods to improve performance.",
        "arxiv_id": "2511.20646",
        "ARXIVID": "2511.20646",
        "COMMENT": "The paper addresses multi-task learning with a focus on segmentation and depth estimation, but does not propose a unified generative and segmentation model.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}