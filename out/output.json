{
    "2502.09980": {
        "authors": [
            "Hsu-kuang Chiu",
            "Ryo Hachiuma",
            "Chien-Yi Wang",
            "Stephen F. Smith",
            "Yu-Chiang Frank Wang",
            "Min-Hung Chen"
        ],
        "title": "V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models",
        "abstract": "arXiv:2502.09980v1 Announce Type: new  Abstract: Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicle-to-vehicle (V2V) communication have been proposed, but they have tended to focus on detection and tracking. How those approaches contribute to overall cooperative planning performance is still under-explored. Inspired by recent progress using Large Language Models (LLMs) to build autonomous driving systems, we propose a novel problem setting that integrates an LLM into cooperative autonomous driving, with the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and benchmark. We also propose our baseline method Vehicle-to-Vehicle Large Language Model (V2V-LLM), which uses an LLM to fuse perception information from multiple connected autonomous vehicles (CAVs) and answer driving-related questions: grounding, notable object identification, and planning. Experimental results show that our proposed V2V-LLM can be a promising unified model architecture for performing various tasks in cooperative autonomous driving, and outperforms other baseline methods that use different fusion approaches. Our work also creates a new research direction that can improve the safety of future autonomous driving systems. Our project website: https://eddyhkchiu.github.io/v2vllm.github.io/ .",
        "arxiv_id": "2502.09980",
        "ARXIVID": "2502.09980",
        "COMMENT": "This paper matches criteria 2 and 3 as it introduces a multi-modal large language model (V2V-LLM) for cooperative autonomous driving, focusing on novel methods for embodied AI.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2502.10248": {
        "authors": [
            "Guoqing Ma",
            "Haoyang Huang",
            "Kun Yan",
            "Liangyu Chen",
            "Nan Duan",
            "Shengming Yin",
            "Changyi Wan",
            "Ranchen Ming",
            "Xiaoniu Song",
            "Xing Chen",
            "Yu Zhou",
            "Deshan Sun",
            "Deyu Zhou",
            "Jian Zhou",
            "Kaijun Tan",
            "Kang An",
            "Mei Chen",
            "Wei Ji",
            "Qiling Wu",
            "Wen Sun",
            "Xin Han",
            "Yanan Wei",
            "Zheng Ge",
            "Aojie Li",
            "Bin Wang",
            "Bizhu Huang",
            "Bo Wang",
            "Brian Li",
            "Changxing Miao",
            "Chen Xu",
            "Chenfei Wu",
            "Chenguang Yu",
            "Dapeng Shi",
            "Dingyuan Hu",
            "Enle Liu",
            "Gang Yu",
            "Ge Yang",
            "Guanzhe Huang",
            "Gulin Yan",
            "Haiyang Feng",
            "Hao Nie",
            "Haonan Jia",
            "Hanpeng Hu",
            "Hanqi Chen",
            "Haolong Yan",
            "Heng Wang",
            "Hongcheng Guo",
            "Huilin Xiong",
            "Huixin Xiong",
            "Jiahao Gong",
            "Jianchang Wu",
            "Jiaoren Wu",
            "Jie Wu",
            "Jie Yang",
            "Jiashuai Liu",
            "Jiashuo Li",
            "Jingyang Zhang",
            "Junjing Guo",
            "Junzhe Lin",
            "Kaixiang Li",
            "Lei Liu",
            "Lei Xia",
            "Liang Zhao",
            "Liguo Tan",
            "Liwen Huang",
            "Liying Shi",
            "Ming Li",
            "Mingliang Li",
            "Muhua Cheng",
            "Na Wang",
            "Qiaohui Chen",
            "Qinglin He",
            "Qiuyan Liang",
            "Quan Sun",
            "Ran Sun",
            "Rui Wang",
            "Shaoliang Pang",
            "Shiliang Yang",
            "Sitong Liu",
            "Siqi Liu",
            "Shuli Gao",
            "Tiancheng Cao",
            "Tianyu Wang",
            "Weipeng Ming",
            "Wenqing He",
            "Xu Zhao",
            "Xuelin Zhang",
            "Xianfang Zeng",
            "Xiaojia Liu",
            "Xuan Yang",
            "Yaqi Dai",
            "Yanbo Yu",
            "Yang Li",
            "Yineng Deng",
            "Yingming Wang",
            "Yilei Wang",
            "Yuanwei Lu",
            "Yu Chen",
            "Yu Luo",
            "Yuchu Luo",
            "Yuhe Yin",
            "Yuheng Feng",
            "Yuxiang Yang",
            "Zecheng Tang",
            "Zekai Zhang",
            "Zidong Yang",
            "Binxing Jiao",
            "Jiansheng Chen",
            "Jing Li",
            "Shuchang Zhou",
            "Xiangyu Zhang",
            "Xinhao Zhang",
            "Yibo Zhu",
            "Heung-Yeung Shum",
            "Daxin Jiang"
        ],
        "title": "Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model",
        "abstract": "arXiv:2502.10248v1 Announce Type: new  Abstract: We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at https://github.com/stepfun-ai/Step-Video-T2V. The online version can be accessed from https://yuewen.cn/videos as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.",
        "arxiv_id": "2502.10248",
        "ARXIVID": "2502.10248",
        "COMMENT": "This paper matches criterion 4 as it discusses a video foundation model and its applications, including a novel benchmark for video generation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2502.09925": {
        "authors": [
            "Jiankang Chen",
            "Tianke Zhang",
            "Changyi Liu",
            "Haojie Ding",
            "Yaya Shi",
            "Feng Cheng",
            "Huihui Xiao",
            "Bin Wen",
            "Fan Yang",
            "Tingting Gao",
            "Di Zhang"
        ],
        "title": "TaskGalaxy: Scaling Multi-modal Instruction Fine-tuning with Tens of Thousands Vision Task Types",
        "abstract": "arXiv:2502.09925v1 Announce Type: new  Abstract: Multimodal visual language models are gaining prominence in open-world applications, driven by advancements in model architectures, training techniques, and high-quality data. However, their performance is often limited by insufficient task-specific data, leading to poor generalization and biased outputs. Existing efforts to increase task diversity in fine-tuning datasets are hindered by the labor-intensive process of manual task labeling, which typically produces only a few hundred task types. To address this, we propose TaskGalaxy, a large-scale multimodal instruction fine-tuning dataset comprising 19,227 hierarchical task types and 413,648 samples. TaskGalaxy utilizes GPT-4o to enrich task diversity by expanding from a small set of manually defined tasks, with CLIP and GPT-4o filtering those that best match open-source images, and generating relevant question-answer pairs. Multiple models are employed to ensure sample quality. This automated process enhances both task diversity and data quality, reducing manual intervention. Incorporating TaskGalaxy into LLaVA-v1.5 and InternVL-Chat-v1.0 models shows substantial performance improvements across 16 benchmarks, demonstrating the critical importance of task diversity. TaskGalaxy is publicly released at https://github.com/Kwai-YuanQi/TaskGalaxy.",
        "arxiv_id": "2502.09925",
        "ARXIVID": "2502.09925",
        "COMMENT": "This paper matches criterion 2 as it introduces a large-scale multi-modal instruction fine-tuning dataset and discusses advancements in multi-modal visual language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2502.09838": {
        "authors": [
            "Tianwei Lin",
            "Wenqiao Zhang",
            "Sijing Li",
            "Yuqian Yuan",
            "Binhe Yu",
            "Haoyuan Li",
            "Wanggui He",
            "Hao Jiang",
            "Mengze Li",
            "Xiaohui Song",
            "Siliang Tang",
            "Jun Xiao",
            "Hui Lin",
            "Yueting Zhuang",
            "Beng Chin Ooi"
        ],
        "title": "HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation",
        "abstract": "arXiv:2502.09838v1 Announce Type: new  Abstract: We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT.",
        "arxiv_id": "2502.09838",
        "ARXIVID": "2502.09838",
        "COMMENT": "Matches criterion 2 as it introduces HealthGPT, a medical large vision-language model (Med-LVLM) with novel techniques for integrating comprehension and generation tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2502.10148": {
        "authors": [
            "Zhiyuan Li",
            "Wenshuai Zhao",
            "Joni Pajarinen"
        ],
        "title": "Cooperative Multi-Agent Planning with Adaptive Skill Synthesis",
        "abstract": "arXiv:2502.10148v1 Announce Type: new  Abstract: Despite much progress in training distributed artificial intelligence (AI), building cooperative multi-agent systems with multi-agent reinforcement learning (MARL) faces challenges in sample efficiency, interpretability, and transferability. Unlike traditional learning-based methods that require extensive interaction with the environment, large language models (LLMs) demonstrate remarkable capabilities in zero-shot planning and complex reasoning. However, existing LLM-based approaches heavily rely on text-based observations and struggle with the non-Markovian nature of multi-agent interactions under partial observability. We present COMPASS, a novel multi-agent architecture that integrates vision-language models (VLMs) with a dynamic skill library and structured communication for decentralized closed-loop decision-making. The skill library, bootstrapped from demonstrations, evolves via planner-guided tasks to enable adaptive strategies. COMPASS propagates entity information through multi-hop communication under partial observability. Evaluations on the improved StarCraft Multi-Agent Challenge (SMACv2) demonstrate COMPASS achieves up to 30\\% higher win rates than state-of-the-art MARL algorithms in symmetric scenarios.",
        "arxiv_id": "2502.10148",
        "ARXIVID": "2502.10148",
        "COMMENT": "This paper matches criterion 3 as it proposes a novel multi-agent architecture integrating vision-language models for cooperative planning in embodied AI, focusing on new methods for multi-agent systems.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.10077": {
        "authors": [
            "Hongye Cao",
            "Fan Feng",
            "Meng Fang",
            "Shaokang Dong",
            "Tianpei Yang",
            "Jing Huo",
            "Yang Gao"
        ],
        "title": "Towards Empowerment Gain through Causal Structure Learning in Model-Based RL",
        "abstract": "arXiv:2502.10077v1 Announce Type: new  Abstract: In Model-Based Reinforcement Learning (MBRL), incorporating causal structures into dynamics models provides agents with a structured understanding of the environments, enabling efficient decision. Empowerment as an intrinsic motivation enhances the ability of agents to actively control their environments by maximizing the mutual information between future states and actions. We posit that empowerment coupled with causal understanding can improve controllability, while enhanced empowerment gain can further facilitate causal reasoning in MBRL. To improve learning efficiency and controllability, we propose a novel framework, Empowerment through Causal Learning (ECL), where an agent with the awareness of causal dynamics models achieves empowerment-driven exploration and optimizes its causal structure for task learning. Specifically, ECL operates by first training a causal dynamics model of the environment based on collected data. We then maximize empowerment under the causal structure for exploration, simultaneously using data gathered through exploration to update causal dynamics model to be more controllable than dense dynamics model without causal structure. In downstream task learning, an intrinsic curiosity reward is included to balance the causality, mitigating overfitting. Importantly, ECL is method-agnostic and is capable of integrating various causal discovery methods. We evaluate ECL combined with 3 causal discovery methods across 6 environments including pixel-based tasks, demonstrating its superior performance compared to other causal MBRL methods, in terms of causal discovery, sample efficiency, and asymptotic performance.",
        "arxiv_id": "2502.10077",
        "ARXIVID": "2502.10077",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework (ECL) for model-based reinforcement learning with causal structure learning, which is a new method for improving controllability and exploration.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.10028": {
        "authors": [
            "Yuxin He",
            "Qiang Nie"
        ],
        "title": "ManiTrend: Bridging Future Generation and Action Prediction with 3D Flow for Robotic Manipulation",
        "abstract": "arXiv:2502.10028v1 Announce Type: new  Abstract: Language-conditioned manipulation is a vital but challenging robotic task due to the high-level abstraction of language. To address this, researchers have sought improved goal representations derived from natural language. In this paper, we highlight 3D flow - representing the motion trend of 3D particles within a scene - as an effective bridge between language-based future image generation and fine-grained action prediction. To this end, we develop ManiTrend, a unified framework that models the dynamics of 3D particles, vision observations and manipulation actions with a causal transformer. Within this framework, features for 3D flow prediction serve as additional conditions for future image generation and action prediction, alleviating the complexity of pixel-wise spatiotemporal modeling and providing seamless action guidance. Furthermore, 3D flow can substitute missing or heterogeneous action labels during large-scale pretraining on cross-embodiment demonstrations. Experiments on two comprehensive benchmarks demonstrate that our method achieves state-of-the-art performance with high efficiency. Our code and model checkpoints will be available upon acceptance.",
        "arxiv_id": "2502.10028",
        "ARXIVID": "2502.10028",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework (ManiTrend) for robotic manipulation with a focus on 3D flow, which is a new angle for action prediction and future image generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.09696": {
        "authors": [
            "Jonathan Roberts",
            "Mohammad Reza Taesiri",
            "Ansh Sharma",
            "Akash Gupta",
            "Samuel Roberts",
            "Ioana Croitoru",
            "Simion-Vlad Bogolin",
            "Jialu Tang",
            "Florian Langer",
            "Vyas Raina",
            "Vatsal Raina",
            "Hanyi Xiong",
            "Vishaal Udandarao",
            "Jingyi Lu",
            "Shiyang Chen",
            "Sam Purkis",
            "Tianshuo Yan",
            "Wenye Lin",
            "Gyungin Shin",
            "Qiaochu Yang",
            "Anh Totti Nguyen",
            "Kai Han",
            "Samuel Albanie"
        ],
        "title": "ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models",
        "abstract": "arXiv:2502.09696v1 Announce Type: new  Abstract: Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting images and, by some measures, have poorer spatial cognition than small children or animals. Despite this, they attain high scores on many popular visual benchmarks, with headroom rapidly eroded by an ongoing surge of model progress. To address this, there is a pressing need for difficult benchmarks that remain relevant for longer. We take this idea to its limit by introducing ZeroBench-a lightweight visual reasoning benchmark that is entirely impossible for contemporary frontier LMMs. Our benchmark consists of 100 manually curated questions and 334 less difficult subquestions. We evaluate 20 LMMs on ZeroBench, all of which score 0.0%, and rigorously analyse the errors. To encourage progress in visual understanding, we publicly release ZeroBench.",
        "arxiv_id": "2502.09696",
        "ARXIVID": "2502.09696",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (ZeroBench) for evaluating large multimodal models with a focus on visual reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.09923": {
        "authors": [
            "Xinning Zhou",
            "Chengyang Ying",
            "Yao Feng",
            "Hang Su",
            "Jun Zhu"
        ],
        "title": "Self-Consistent Model-based Adaptation for Visual Reinforcement Learning",
        "abstract": "arXiv:2502.09923v1 Announce Type: new  Abstract: Visual reinforcement learning agents typically face serious performance declines in real-world applications caused by visual distractions. Existing methods rely on fine-tuning the policy's representations with hand-crafted augmentations. In this work, we propose Self-Consistent Model-based Adaptation (SCMA), a novel method that fosters robust adaptation without modifying the policy. By transferring cluttered observations to clean ones with a denoising model, SCMA can mitigate distractions for various policies as a plug-and-play enhancement. To optimize the denoising model in an unsupervised manner, we derive an unsupervised distribution matching objective with a theoretical analysis of its optimality. We further present a practical algorithm to optimize the objective by estimating the distribution of clean observations with a pre-trained world model. Extensive experiments on multiple visual generalization benchmarks and real robot data demonstrate that SCMA effectively boosts performance across various distractions and exhibits better sample efficiency.",
        "arxiv_id": "2502.09923",
        "ARXIVID": "2502.09923",
        "COMMENT": "Matches criterion 3 as it introduces a novel method (SCMA) for visual reinforcement learning with a focus on mitigating distractions using a denoising model.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2502.09795": {
        "authors": [
            "Dario Pisanti",
            "Robert Hewitt",
            "Roland Brockers",
            "Georgios Georgakis"
        ],
        "title": "Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging Illumination Conditions",
        "abstract": "arXiv:2502.09795v1 Announce Type: new  Abstract: Planetary exploration using aerial assets has the potential for unprecedented scientific discoveries on Mars. While NASA's Mars helicopter Ingenuity proved flight in Martian atmosphere is possible, future Mars rotocrafts will require advanced navigation capabilities for long-range flights. One such critical capability is Map-based Localization (MbL) which registers an onboard image to a reference map during flight in order to mitigate cumulative drift from visual odometry. However, significant illumination differences between rotocraft observations and a reference map prove challenging for traditional MbL systems, restricting the operational window of the vehicle. In this work, we investigate a new MbL system and propose Geo-LoFTR, a geometry-aided deep learning model for image registration that is more robust under large illumination differences than prior models. The system is supported by a custom simulation framework that uses real orbital maps to produce large amounts of realistic images of the Martian terrain. Comprehensive evaluations show that our proposed system outperforms prior MbL efforts in terms of localization accuracy under significant lighting and scale variations. Furthermore, we demonstrate the validity of our approach across a simulated Martian day.",
        "arxiv_id": "2502.09795",
        "ARXIVID": "2502.09795",
        "COMMENT": "Matches criterion 3 as it focuses on a novel method for embodied AI (Geo-LoFTR) and uses a custom simulation framework for Mars exploration.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2502.10012": {
        "authors": [
            "Asen Nachkov",
            "Danda Pani Paudel",
            "Jan-Nico Zaech",
            "Davide Scaramuzza",
            "Luc Van Gool"
        ],
        "title": "Dream to Drive: Model-Based Vehicle Control Using Analytic World Models",
        "abstract": "arXiv:2502.10012v1 Announce Type: new  Abstract: Differentiable simulators have recently shown great promise for training autonomous vehicle controllers. Being able to backpropagate through them, they can be placed into an end-to-end training loop where their known dynamics turn into useful priors for the policy to learn, removing the typical black box assumption of the environment. So far, these systems have only been used to train policies. However, this is not the end of the story in terms of what they can offer. Here, for the first time, we use them to train world models. Specifically, we present three new task setups that allow us to learn next state predictors, optimal planners, and optimal inverse states. Unlike analytic policy gradients (APG), which requires the gradient of the next simulator state with respect to the current actions, our proposed setups rely on the gradient of the next state with respect to the current state. We call this approach Analytic World Models (AWMs) and showcase its applications, including how to use it for planning in the Waymax simulator. Apart from pushing the limits of what is possible with such simulators, we offer an improved training recipe that increases performance on the large-scale Waymo Open Motion dataset by up to 12% compared to baselines at essentially no additional cost.",
        "arxiv_id": "2502.10012",
        "ARXIVID": "2502.10012",
        "COMMENT": "Matches criterion 3 as it introduces a novel method (Analytic World Models) for embodied AI and uses a simulator (Waymax) for training and evaluation.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2502.09927": {
        "authors": [
            "Granite Vision Team",
            "Leonid Karlinsky",
            "Assaf Arbelle",
            "Abraham Daniels",
            "Ahmed Nassar",
            "Amit Alfassi",
            "Bo Wu",
            "Eli Schwartz",
            "Dhiraj Joshi",
            "Jovana Kondic",
            "Nimrod Shabtay",
            "Pengyuan Li",
            "Roei Herzig",
            "Shafiq Abedin",
            "Shaked Perek",
            "Sivan Harary",
            "Udi Barzelay",
            "Adi Raz Goldfarb",
            "Aude Oliva",
            "Ben Wieles",
            "Bishwaranjan Bhattacharjee",
            "Brandon Huang",
            "Christoph Auer",
            "Dan Gutfreund",
            "David Beymer",
            "David Wood",
            "Hilde Kuehne",
            "Jacob Hansen",
            "Joseph Shtok",
            "Ken Wong",
            "Luis Angel Bathen",
            "Mayank Mishra",
            "Maksym Lysak",
            "Michele Dolfi",
            "Mikhail Yurochkin",
            "Nikolaos Livathinos",
            "Nimrod Harel",
            "Ophir Azulai",
            "Oshri Naparstek",
            "Rafael Teixeira de Lima",
            "Rameswar Panda",
            "Sivan Doveh",
            "Shubham Gupta",
            "Subhro Das",
            "Syed Zawad",
            "Yusik Kim",
            "Zexue He",
            "Alexander Brooks",
            "Gabe Goodhart",
            "Anita Govindjee",
            "Derek Leist",
            "Ibrahim Ibrahim",
            "Aya Soffer",
            "David Cox",
            "Kate Soule",
            "Luis Lastras",
            "Nirmit Desai",
            "Shila Ofek-koifman",
            "Sriram Raghavan",
            "Tanveer Syeda-Mahmood",
            "Peter Staar",
            "Tal Drory",
            "Rogerio Feris"
        ],
        "title": "Granite Vision: a lightweight, open-source multimodal model for enterprise Intelligence",
        "abstract": "arXiv:2502.09927v1 Announce Type: new  Abstract: We introduce Granite Vision, a lightweight large language model with vision capabilities, specifically designed to excel in enterprise use cases, particularly in visual document understanding. Our model is trained on a comprehensive instruction-following dataset, including document-related tasks, such as content extraction from tables, charts, diagrams, sketches, and infographics, as well as general image tasks. The architecture of Granite Vision is centered around visual modality alignment with a decoder-only, 2 billion parameter Granite large language model. Additionally, we introduce a dedicated safety classification approach in test-time that leverages a sparse set of attention vectors to identify potential harmful inputs. Despite its lightweight architecture, Granite Vision achieves strong results in standard benchmarks related to visual document understanding, as well as on the LiveXiv benchmark, which is designed to avoid test set contamination by using a constantly updated corpus of recently published Arxiv papers. We are releasing the model under the Apache-2 license, allowing for both research and commercial use, while offering complete visibility into the training data and other relevant details. See https://huggingface.co/ibm-granite/ for model weights.",
        "arxiv_id": "2502.09927",
        "ARXIVID": "2502.09927",
        "COMMENT": "Matches criterion 2 as it introduces a new visual large language model (Granite Vision) with specific enterprise use cases.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2502.09672": {
        "authors": [
            "Xiaohong Liu",
            "Xulong Zhao",
            "Gang Liu",
            "Zili Wu",
            "Tao Wang",
            "Lei Meng",
            "Yuhan Wang"
        ],
        "title": "IMM-MOT: A Novel 3D Multi-object Tracking Framework with Interacting Multiple Model Filter",
        "abstract": "arXiv:2502.09672v1 Announce Type: new  Abstract: 3D Multi-Object Tracking (MOT) provides the trajectories of surrounding objects, assisting robots or vehicles in smarter path planning and obstacle avoidance. Existing 3D MOT methods based on the Tracking-by-Detection framework typically use a single motion model to track an object throughout its entire tracking process. However, objects may change their motion patterns due to variations in the surrounding environment. In this paper, we introduce the Interacting Multiple Model filter in IMM-MOT, which accurately fits the complex motion patterns of individual objects, overcoming the limitation of single-model tracking in existing approaches. In addition, we incorporate a Damping Window mechanism into the trajectory lifecycle management, leveraging the continuous association status of trajectories to control their creation and termination, reducing the occurrence of overlooked low-confidence true targets. Furthermore, we propose the Distance-Based Score Enhancement module, which enhances the differentiation between false positives and true positives by adjusting detection scores, thereby improving the effectiveness of the Score Filter. On the NuScenes Val dataset, IMM-MOT outperforms most other single-modal models using 3D point clouds, achieving an AMOTA of 73.8%. Our project is available at https://github.com/Ap01lo/IMM-MOT.",
        "arxiv_id": "2502.09672",
        "ARXIVID": "2502.09672",
        "COMMENT": "Matches criterion 3 as it introduces IMM-MOT, a novel 3D multi-object tracking framework with a new approach to handling complex motion patterns.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.09818": {
        "authors": [
            "Ming Liu",
            "Hao Chen",
            "Jindong Wang",
            "Wensheng Zhang"
        ],
        "title": "On the robustness of multimodal language model towards distractions",
        "abstract": "arXiv:2502.09818v1 Announce Type: new  Abstract: Although vision-language models (VLMs) have achieved significant success in various applications such as visual question answering, their resilience to prompt variations remains an under-explored area. Understanding how distractions affect VLMs is crucial for improving their real-world applicability, as inputs could have noisy and irrelevant information in many practical scenarios. This paper aims to assess the robustness of VLMs against both visual and textual distractions in the context of science question answering. Built on the ScienceQA dataset, we developed a new benchmark that introduces distractions in both the visual and textual contexts to evaluate the reasoning capacity of VLMs amid these distractions. Our findings reveal that most-of-the-art VLMs, including GPT-4, are vulnerable to various types of distractions, experiencing noticeable degradation in reasoning capabilities when confronted with distractions. Notably, models such as InternVL2 demonstrate a higher degree of robustness to these distractions. We also found that models exhibit greater sensitivity to textual distractions than visual ones. Additionally, we explored various mitigation strategies, such as prompt engineering, to counteract the impact of distractions. While these strategies improved solution accuracy, our analysis shows that there remain significant opportunities for improvement.",
        "arxiv_id": "2502.09818",
        "ARXIVID": "2502.09818",
        "COMMENT": "Matches criterion 2 as it evaluates the robustness of vision-language models (VLMs) and introduces a new benchmark for testing their resilience to distractions.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.10059": {
        "authors": [
            "Teng Li",
            "Guangcong Zheng",
            "Rui Jiang",
            "Shuigenzhan",
            "Tao Wu",
            "Yehao Lu",
            "Yining Lin",
            "Xi Li"
        ],
        "title": "RealCam-I2V: Real-World Image-to-Video Generation with Interactive Complex Camera Control",
        "abstract": "arXiv:2502.10059v1 Announce Type: new  Abstract: Recent advancements in camera-trajectory-guided image-to-video generation offer higher precision and better support for complex camera control compared to text-based approaches. However, they also introduce significant usability challenges, as users often struggle to provide precise camera parameters when working with arbitrary real-world images without knowledge of their depth nor scene scale. To address these real-world application issues, we propose RealCam-I2V, a novel diffusion-based video generation framework that integrates monocular metric depth estimation to establish 3D scene reconstruction in a preprocessing step. During training, the reconstructed 3D scene enables scaling camera parameters from relative to absolute values, ensuring compatibility and scale consistency across diverse real-world images. In inference, RealCam-I2V offers an intuitive interface where users can precisely draw camera trajectories by dragging within the 3D scene. To further enhance precise camera control and scene consistency, we propose scene-constrained noise shaping, which shapes high-level noise and also allows the framework to maintain dynamic, coherent video generation in lower noise stages. RealCam-I2V achieves significant improvements in controllability and video quality on the RealEstate10K and out-of-domain images. We further enables applications like camera-controlled looping video generation and generative frame interpolation. We will release our absolute-scale annotation, codes, and all checkpoints. Please see dynamic results in https://zgctroy.github.io/RealCam-I2V.",
        "arxiv_id": "2502.10059",
        "ARXIVID": "2502.10059",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models and their application in video generation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2502.09933": {
        "authors": [
            "Kai Yan",
            "Zhan Ling",
            "Kang Liu",
            "Yifan Yang",
            "Ting-Han Fan",
            "Lingfeng Shen",
            "Zhengyin Du",
            "Jiecao Chen"
        ],
        "title": "MIR-Bench: Benchmarking LLM's Long-Context Intelligence via Many-Shot In-Context Inductive Reasoning",
        "abstract": "arXiv:2502.09933v1 Announce Type: new  Abstract: Inductive Reasoning (IR), the ability to summarize rules from examples and apply on new ones, has long been viewed as a primal ability for general intelligence and widely studied by cognitive science and AI researchers. Many benchmarks have been proposed to measure such ability for Large Language Models (LLMs); however, they focus on few-shot (usually $<$10) setting and lack evaluation for aggregating many pieces of information from long contexts. On the other hand, the ever-growing context length of LLMs have brought forth the novel paradigm of many-shot In-Context Learning (ICL), which addresses new tasks with hundreds to thousands of examples without expensive and inefficient fine-tuning. However, many-shot evaluations are mostly focused on classification (a very limited aspect of IR), and popular long-context LLM tasks such as Needle-In-A-Haystack (NIAH) seldom require complicated intelligence for integrating many pieces of information. To fix the issues from both worlds, we propose MIR-Bench, the first many-shot in-context inductive reasoning benchmark that asks LLM to induce output via input-output examples from underlying functions with diverse data format. Based on MIR-Bench, we study many novel problems for inductive reasoning and many-shot ICL, including robustness against erroneous shots and the effect of Chain-of-Thought (CoT), and acquired insightful findings.",
        "arxiv_id": "2502.09933",
        "ARXIVID": "2502.09933",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (MIR-Bench) for evaluating LLMs' long-context inductive reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2502.10120": {
        "authors": [
            "Xinfeng Zhao",
            "Yaoru Sun"
        ],
        "title": "Compress image to patches for Vision Transformer",
        "abstract": "arXiv:2502.10120v1 Announce Type: new  Abstract: The Vision Transformer (ViT) has made significant strides in the field of computer vision. However, as the depth of the model and the resolution of the input images increase, the computational cost associated with training and running ViT models has surged dramatically.This paper proposes a hybrid model based on CNN and Vision Transformer, named CI2P-ViT. The model incorporates a module called CI2P, which utilizes the CompressAI encoder to compress images and subsequently generates a sequence of patches through a series of convolutions. CI2P can replace the Patch Embedding component in the ViT model, enabling seamless integration into existing ViT models.Compared to ViT-B/16, CI2P-ViT has the number of patches input to the self-attention layer reduced to a quarter of the original.This design not only significantly reduces the computational cost of the ViT model but also effectively enhances the model's accuracy by introducing the inductive bias properties of CNN.The ViT model's precision is markedly enhanced.When trained from the ground up on the Animals-10 dataset, CI2P-ViT achieved an accuracy rate of 92.37%, representing a 3.3% improvement over the ViT-B/16 baseline. Additionally, the model's computational operations, measured in floating-point operations per second (FLOPs), were diminished by 63.35%, and it exhibited a 2-fold increase in training velocity on identical hardware configurations.",
        "arxiv_id": "2502.10120",
        "ARXIVID": "2502.10120",
        "COMMENT": "Matches criterion 4 as it proposes a hybrid model (CI2P-ViT) for vision transformers, improving computational efficiency and accuracy.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2502.10097": {
        "authors": [
            "Hongye Cao",
            "Fan Feng",
            "Tianpei Yang",
            "Jing Huo",
            "Yang Gao"
        ],
        "title": "Causal Information Prioritization for Efficient Reinforcement Learning",
        "abstract": "arXiv:2502.10097v1 Announce Type: new  Abstract: Current Reinforcement Learning (RL) methods often suffer from sample-inefficiency, resulting from blind exploration strategies that neglect causal relationships among states, actions, and rewards. Although recent causal approaches aim to address this problem, they lack grounded modeling of reward-guided causal understanding of states and actions for goal-orientation, thus impairing learning efficiency. To tackle this issue, we propose a novel method named Causal Information Prioritization (CIP) that improves sample efficiency by leveraging factored MDPs to infer causal relationships between different dimensions of states and actions with respect to rewards, enabling the prioritization of causal information. Specifically, CIP identifies and leverages causal relationships between states and rewards to execute counterfactual data augmentation to prioritize high-impact state features under the causal understanding of the environments. Moreover, CIP integrates a causality-aware empowerment learning objective, which significantly enhances the agent's execution of reward-guided actions for more efficient exploration in complex environments. To fully assess the effectiveness of CIP, we conduct extensive experiments across 39 tasks in 5 diverse continuous control environments, encompassing both locomotion and manipulation skills learning with pixel-based and sparse reward settings. Experimental results demonstrate that CIP consistently outperforms existing RL methods across a wide range of scenarios.",
        "arxiv_id": "2502.10097",
        "ARXIVID": "2502.10097",
        "COMMENT": "This paper matches criterion 3 as it proposes a novel method for improving reinforcement learning efficiency by leveraging causal relationships, which could be relevant to embodied AI and new methods.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.10127": {
        "authors": [
            "Gamal Elghazaly",
            "Raphael Frank"
        ],
        "title": "Leveraging V2X for Collaborative HD Maps Construction Using Scene Graph Generation",
        "abstract": "arXiv:2502.10127v1 Announce Type: new  Abstract: High-Definition (HD) maps play a crucial role in autonomous vehicle navigation, complementing onboard perception sensors for improved accuracy and safety. Traditional HD map generation relies on dedicated mapping vehicles, which are costly and fail to capture real-time infrastructure changes. This paper presents HDMapLaneNet, a novel framework leveraging V2X communication and Scene Graph Generation to collaboratively construct a localized geometric layer of HD maps. The approach extracts lane centerlines from front-facing camera images, represents them as graphs, and transmits the data for global aggregation to the cloud via V2X. Preliminary results on the nuScenes dataset demonstrate superior association prediction performance compared to a state-of-the-art method.",
        "arxiv_id": "2502.10127",
        "ARXIVID": "2502.10127",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework for collaborative HD map construction using scene graph generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.10038": {
        "authors": [
            "Jiawei Cheng",
            "Jingyuan Wang",
            "Yichuan Zhang",
            "Jiahao Ji",
            "Yuanshao Zhu",
            "Zhibo Zhang",
            "Xiangyu Zhao"
        ],
        "title": "POI-Enhancer: An LLM-based Semantic Enhancement Framework for POI Representation Learning",
        "abstract": "arXiv:2502.10038v1 Announce Type: new  Abstract: POI representation learning plays a crucial role in handling tasks related to user mobility data. Recent studies have shown that enriching POI representations with multimodal information can significantly enhance their task performance. Previously, the textual information incorporated into POI representations typically involved only POI categories or check-in content, leading to relatively weak textual features in existing methods. In contrast, large language models (LLMs) trained on extensive text data have been found to possess rich textual knowledge. However leveraging such knowledge to enhance POI representation learning presents two key challenges: first, how to extract POI-related knowledge from LLMs effectively, and second, how to integrate the extracted information to enhance POI representations. To address these challenges, we propose POI-Enhancer, a portable framework that leverages LLMs to improve POI representations produced by classic POI learning models. We first design three specialized prompts to extract semantic information from LLMs efficiently. Then, the Dual Feature Alignment module enhances the quality of the extracted information, while the Semantic Feature Fusion module preserves its integrity. The Cross Attention Fusion module then fully adaptively integrates such high-quality information into POI representations and Multi-View Contrastive Learning further injects human-understandable semantic information into these representations. Extensive experiments on three real-world datasets demonstrate the effectiveness of our framework, showing significant improvements across all baseline representations.",
        "arxiv_id": "2502.10038",
        "ARXIVID": "2502.10038",
        "COMMENT": "Matches criterion 2 as it leverages LLMs for semantic enhancement in POI representation learning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.09843": {
        "authors": [
            "Karan Taneja",
            "Ashok K. Goel"
        ],
        "title": "MuDoC: An Interactive Multimodal Document-grounded Conversational AI System",
        "abstract": "arXiv:2502.09843v1 Announce Type: new  Abstract: Multimodal AI is an important step towards building effective tools to leverage multiple modalities in human-AI communication. Building a multimodal document-grounded AI system to interact with long documents remains a challenge. Our work aims to fill the research gap of directly leveraging grounded visuals from documents alongside textual content in documents for response generation. We present an interactive conversational AI agent 'MuDoC' based on GPT-4o to generate document-grounded responses with interleaved text and figures. MuDoC's intelligent textbook interface promotes trustworthiness and enables verification of system responses by allowing instant navigation to source text and figures in the documents. We also discuss qualitative observations based on MuDoC responses highlighting its strengths and limitations.",
        "arxiv_id": "2502.09843",
        "ARXIVID": "2502.09843",
        "COMMENT": "This paper matches criterion 2 as it introduces a multimodal conversational AI system leveraging GPT-4o for document-grounded responses, which aligns with the interest in VLLMs or MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2502.09669": {
        "authors": [
            "Maizhe Yang",
            "Kaiyuan Tang",
            "Chaoli Wang"
        ],
        "title": "Meta-INR: Efficient Encoding of Volumetric Data via Meta-Learning Implicit Neural Representation",
        "abstract": "arXiv:2502.09669v1 Announce Type: new  Abstract: Implicit neural representation (INR) has emerged as a promising solution for encoding volumetric data, offering continuous representations and seamless compatibility with the volume rendering pipeline. However, optimizing an INR network from randomly initialized parameters for each new volume is computationally inefficient, especially for large-scale time-varying or ensemble volumetric datasets where volumes share similar structural patterns but require independent training. To close this gap, we propose Meta-INR, a pretraining strategy adapted from meta-learning algorithms to learn initial INR parameters from partial observation of a volumetric dataset. Compared to training an INR from scratch, the learned initial parameters provide a strong prior that enhances INR generalizability, allowing significantly faster convergence with just a few gradient updates when adapting to a new volume and better interpretability when analyzing the parameters of the adapted INRs. We demonstrate that Meta-INR can effectively extract high-quality generalizable features that help encode unseen similar volume data across diverse datasets. Furthermore, we highlight its utility in tasks such as simulation parameter analysis and representative timestep selection. The code is available at https://github.com/spacefarers/MetaINR.",
        "arxiv_id": "2502.09669",
        "ARXIVID": "2502.09669",
        "COMMENT": "This paper does not directly match any of the criteria but discusses a novel meta-learning approach for implicit neural representations, which could be tangentially related to spatial intelligence in embodied agents.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.09935": {
        "authors": [
            "{\\L}ukasz Staniszewski",
            "Bartosz Cywi\\'nski",
            "Franziska Boenisch",
            "Kamil Deja",
            "Adam Dziedzic"
        ],
        "title": "Precise Parameter Localization for Textual Generation in Diffusion Models",
        "abstract": "arXiv:2502.09935v1 Announce Type: new  Abstract: Novel diffusion models can synthesize photo-realistic images with integrated high-quality text. Surprisingly, we demonstrate through attention activation patching that only less than 1% of diffusion models' parameters, all contained in attention layers, influence the generation of textual content within the images. Building on this observation, we improve textual generation efficiency and performance by targeting cross and joint attention layers of diffusion models. We introduce several applications that benefit from localizing the layers responsible for textual content generation. We first show that a LoRA-based fine-tuning solely of the localized layers enhances, even more, the general text-generation capabilities of large diffusion models while preserving the quality and diversity of the diffusion models' generations. Then, we demonstrate how we can use the localized layers to edit textual content in generated images. Finally, we extend this idea to the practical use case of preventing the generation of toxic text in a cost-free manner. In contrast to prior work, our localization approach is broadly applicable across various diffusion model architectures, including U-Net (e.g., LDM and SDXL) and transformer-based (e.g., DeepFloyd IF and Stable Diffusion 3), utilizing diverse text encoders (e.g., from CLIP to the large language models like T5). Project page available at https://t2i-text-loc.github.io/.",
        "arxiv_id": "2502.09935",
        "ARXIVID": "2502.09935",
        "COMMENT": "This paper does not match any of the specified criteria. It focuses on parameter localization for textual generation in diffusion models, which is not directly related to the requested topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.09928": {
        "authors": [
            "Chang Nie",
            "Junfang Chen",
            "Yajie Chen"
        ],
        "title": "Deep Tree Tensor Networks for Image Recognition",
        "abstract": "arXiv:2502.09928v1 Announce Type: new  Abstract: Originating in quantum physics, tensor networks (TNs) have been widely adopted as exponential machines and parameter decomposers for recognition tasks. Typical TN models, such as Matrix Product States (MPS), have not yet achieved successful application in natural image processing. When employed, they primarily serve to compress parameters within off-the-shelf networks, thus losing their distinctive capability to enhance exponential-order feature interactions. This paper introduces a novel architecture named \\textit{\\textbf{D}eep \\textbf{T}ree \\textbf{T}ensor \\textbf{N}etwork} (DTTN), which captures $2^L$-order multiplicative interactions across features through multilinear operations, while essentially unfolding into a \\emph{tree}-like TN topology with the parameter-sharing property. DTTN is stacked with multiple antisymmetric interacting modules (AIMs), and this design facilitates efficient implementation. Moreover, we theoretically reveal the equivalency among quantum-inspired TN models and polynomial and multilinear networks under certain conditions, and we believe that DTTN can inspire more interpretable studies in this field. We evaluate the proposed model against a series of benchmarks and achieve excellent performance compared to its peers and cutting-edge architectures. Our code will soon be publicly available.",
        "arxiv_id": "2502.09928",
        "ARXIVID": "2502.09928",
        "COMMENT": "Does not match any specific criterion but introduces a novel tensor network architecture for image recognition.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.09688": {
        "authors": [
            "Benjamin D. Killeen",
            "Bohua Wan",
            "Aditya V. Kulkarni",
            "Nathan Drenkow",
            "Michael Oberst",
            "Paul H. Yi",
            "Mathias Unberath"
        ],
        "title": "Towards Virtual Clinical Trials of Radiology AI with Conditional Generative Modeling",
        "abstract": "arXiv:2502.09688v1 Announce Type: new  Abstract: Artificial intelligence (AI) is poised to transform healthcare by enabling personalized and efficient care through data-driven insights. Although radiology is at the forefront of AI adoption, in practice, the potential of AI models is often overshadowed by severe failures to generalize: AI models can have performance degradation of up to 20% when transitioning from controlled test environments to clinical use by radiologists. This mismatch raises concerns that radiologists will be misled by incorrect AI predictions in practice and/or grow to distrust AI, rendering these promising technologies practically ineffectual. Exhaustive clinical trials of AI models on abundant and diverse data is thus critical to anticipate AI model degradation when encountering varied data samples. Achieving these goals, however, is challenging due to the high costs of collecting diverse data samples and corresponding annotations. To overcome these limitations, we introduce a novel conditional generative AI model designed for virtual clinical trials (VCTs) of radiology AI, capable of realistically synthesizing full-body CT images of patients with specified attributes. By learning the joint distribution of images and anatomical structures, our model enables precise replication of real-world patient populations with unprecedented detail at this scale. We demonstrate meaningful evaluation of radiology AI models through VCTs powered by our synthetic CT study populations, revealing model degradation and facilitating algorithmic auditing for bias-inducing data attributes. Our generative AI approach to VCTs is a promising avenue towards a scalable solution to assess model robustness, mitigate biases, and safeguard patient care by enabling simpler testing and evaluation of AI models in any desired range of diverse patient populations.",
        "arxiv_id": "2502.09688",
        "ARXIVID": "2502.09688",
        "COMMENT": "Does not directly match any specific criterion but is relevant to the general interest area of generative modeling and healthcare applications.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.10060": {
        "authors": [
            "Utkarsh Mall",
            "Cheng Perng Phoo",
            "Mia Chiquier",
            "Bharath Hariharan",
            "Kavita Bala",
            "Carl Vondrick"
        ],
        "title": "DiSciPLE: Learning Interpretable Programs for Scientific Visual Discovery",
        "abstract": "arXiv:2502.10060v1 Announce Type: new  Abstract: Visual data is used in numerous different scientific workflows ranging from remote sensing to ecology. As the amount of observation data increases, the challenge is not just to make accurate predictions but also to understand the underlying mechanisms for those predictions. Good interpretation is important in scientific workflows, as it allows for better decision-making by providing insights into the data. This paper introduces an automatic way of obtaining such interpretable-by-design models, by learning programs that interleave neural networks. We propose DiSciPLE (Discovering Scientific Programs using LLMs and Evolution) an evolutionary algorithm that leverages common sense and prior knowledge of large language models (LLMs) to create Python programs explaining visual data. Additionally, we propose two improvements: a program critic and a program simplifier to improve our method further to synthesize good programs. On three different real-world problems, DiSciPLE learns state-of-the-art programs on novel tasks with no prior literature. For example, we can learn programs with 35% lower error than the closest non-interpretable baseline for population density estimation.",
        "arxiv_id": "2502.10060",
        "ARXIVID": "2502.10060",
        "COMMENT": "Does not directly match any specific criterion but is relevant to the general interest area of interpretable models and visual data analysis.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.09663": {
        "authors": [
            "Anis Bourou",
            "Saranga Kingkor Mahanta",
            "Thomas Boyer",
            "Val\\'erie Mezger",
            "Auguste Genovesio"
        ],
        "title": "DiffEx: Explaining a Classifier with Diffusion Models to Identify Microscopic Cellular Variations",
        "abstract": "arXiv:2502.09663v1 Announce Type: new  Abstract: In recent years, deep learning models have been extensively applied to biological data across various modalities. Discriminative deep learning models have excelled at classifying images into categories (e.g., healthy versus diseased, treated versus untreated). However, these models are often perceived as black boxes due to their complexity and lack of interpretability, limiting their application in real-world biological contexts. In biological research, explainability is essential: understanding classifier decisions and identifying subtle differences between conditions are critical for elucidating the effects of treatments, disease progression, and biological processes. To address this challenge, we propose DiffEx, a method for generating visually interpretable attributes to explain classifiers and identify microscopic cellular variations between different conditions. We demonstrate the effectiveness of DiffEx in explaining classifiers trained on natural and biological images. Furthermore, we use DiffEx to uncover phenotypic differences within microscopy datasets. By offering insights into cellular variations through classifier explanations, DiffEx has the potential to advance the understanding of diseases and aid drug discovery by identifying novel biomarkers.",
        "arxiv_id": "2502.09663",
        "ARXIVID": "2502.09663",
        "COMMENT": "This paper does not match any of the specified criteria. It focuses on explainability in biological image classification using diffusion models, which is not directly related to the requested topics.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.09955": {
        "authors": [
            "Iddo Drori",
            "Gaston Longhitano",
            "Mao Mao",
            "Seunghwan Hyun",
            "Yuke Zhang",
            "Sungjun Park",
            "Zachary Meeks",
            "Xin-Yu Zhang",
            "Ben Segev",
            "Howard Yong",
            "Nakul Verma",
            "Avi Shporer",
            "Alon Amit",
            "Madeleine Udell"
        ],
        "title": "Diverse Inference and Verification for Advanced Reasoning",
        "abstract": "arXiv:2502.09955v1 Announce Type: new  Abstract: Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions. We use a diverse inference approach that combines multiple models and methods at test time. We find that verifying mathematics and code problems, and rejection sampling on other problems is simple and effective. We automatically verify correctness of solutions to IMO problems by Lean, and ARC puzzles by code, and find that best-of-N effectively answers HLE questions. Our approach increases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%, accuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that 948 humans could not and 26.5% of ARC puzzles that o3 high compute does not. Test-time simulations, reinforcement learning, and meta-learning with inference feedback improve generalization by adapting agent graph representations and varying prompts, code, and datasets. Our approach is reliable, robust, and scalable, and in the spirit of reproducible research, we will make it publicly available upon publication.",
        "arxiv_id": "2502.09955",
        "ARXIVID": "2502.09955",
        "COMMENT": "Does not match any specific criteria. Focuses on reasoning LLMs and diverse inference approaches, which are not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.09657": {
        "authors": [
            "Wenjing Gong",
            "Xinyue Ye",
            "Keshu Wu",
            "Suphanut Jamonnak",
            "Wenyu Zhang",
            "Yifan Yang",
            "Xiao Huang"
        ],
        "title": "Integrating Spatiotemporal Vision Transformer into Digital Twins for High-Resolution Heat Stress Forecasting in Campus Environments",
        "abstract": "arXiv:2502.09657v1 Announce Type: new  Abstract: Extreme heat events exacerbated by climate change pose significant challenges to urban resilience and planning. This study introduces a climate-responsive digital twin framework integrating the Spatiotemporal Vision Transformer (ST-ViT) model to enhance heat stress forecasting and decision-making. Using a Texas campus as a testbed, we synthesized high-resolution physical model simulations with spatial and meteorological data to develop fine-scale human thermal predictions. The ST-ViT-powered digital twin enables efficient, data-driven insights for planners, policymakers, and campus stakeholders, supporting targeted heat mitigation strategies and advancing climate-adaptive urban design.",
        "arxiv_id": "2502.09657",
        "ARXIVID": "2502.09657",
        "COMMENT": "Does not match any specific criteria. Focuses on heat stress forecasting using a Spatiotemporal Vision Transformer, which is unrelated to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.09897": {
        "authors": [
            "Kehan Guo",
            "Yili Shen",
            "Gisela Abigail Gonzalez-Montiel",
            "Yue Huang",
            "Yujun Zhou",
            "Mihir Surve",
            "Zhichun Guo",
            "Prayel Das",
            "Nitesh V Chawla",
            "Olaf Wiest",
            "Xiangliang Zhang"
        ],
        "title": "Artificial Intelligence in Spectroscopy: Advancing Chemistry from Prediction to Generation and Beyond",
        "abstract": "arXiv:2502.09897v1 Announce Type: new  Abstract: The rapid advent of machine learning (ML) and artificial intelligence (AI) has catalyzed major transformations in chemistry, yet the application of these methods to spectroscopic and spectrometric data, referred to as Spectroscopy Machine Learning (SpectraML), remains relatively underexplored. Modern spectroscopic techniques (MS, NMR, IR, Raman, UV-Vis) generate an ever-growing volume of high-dimensional data, creating a pressing need for automated and intelligent analysis beyond traditional expert-based workflows. In this survey, we provide a unified review of SpectraML, systematically examining state-of-the-art approaches for both forward tasks (molecule-to-spectrum prediction) and inverse tasks (spectrum-to-molecule inference). We trace the historical evolution of ML in spectroscopy, from early pattern recognition to the latest foundation models capable of advanced reasoning, and offer a taxonomy of representative neural architectures, including graph-based and transformer-based methods. Addressing key challenges such as data quality, multimodal integration, and computational scalability, we highlight emerging directions such as synthetic data generation, large-scale pretraining, and few- or zero-shot learning. To foster reproducible research, we also release an open-source repository containing recent papers and their corresponding curated datasets (https://github.com/MINE-Lab-ND/SpectrumML_Survey_Papers). Our survey serves as a roadmap for researchers, guiding progress at the intersection of spectroscopy and AI.",
        "arxiv_id": "2502.09897",
        "ARXIVID": "2502.09897",
        "COMMENT": "Does not match any specific criterion but is relevant to AI applications in spectroscopy and chemistry.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.09931": {
        "authors": [
            "Ju-Hyeon Nam",
            "Nur Suriza Syazwany",
            "Sang-Chul Lee"
        ],
        "title": "TransGUNet: Transformer Meets Graph-based Skip Connection for Medical Image Segmentation",
        "abstract": "arXiv:2502.09931v1 Announce Type: new  Abstract: Skip connection engineering is primarily employed to address the semantic gap between the encoder and decoder, while also integrating global dependencies to understand the relationships among complex anatomical structures in medical image segmentation. Although several models have proposed transformer-based approaches to incorporate global dependencies within skip connections, they often face limitations in capturing detailed local features with high computational complexity. In contrast, graph neural networks (GNNs) exploit graph structures to effectively capture local and global features. Leveraging these properties, we introduce an attentional cross-scale graph neural network (ACS-GNN), which enhances the skip connection framework by converting cross-scale feature maps into a graph structure and capturing complex anatomical structures through node attention. Additionally, we observed that deep learning models often produce uninformative feature maps, which degrades the quality of spatial attention maps. To address this problem, we integrated entropy-driven feature selection (EFS) with spatial attention, calculating an entropy score for each channel and filtering out high-entropy feature maps. Our innovative framework, TransGUNet, comprises ACS-GNN and EFS-based spatial attentio} to effectively enhance domain generalizability across various modalities by leveraging GNNs alongside a reliable spatial attention map, ensuring more robust features within the skip connection. Through comprehensive experiments and analysis, TransGUNet achieved superior segmentation performance on six seen and eight unseen datasets, demonstrating significantly higher efficiency compared to previous methods.",
        "arxiv_id": "2502.09931",
        "ARXIVID": "2502.09931",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and segmentation tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.09873": {
        "authors": [
            "Jinpei Guo",
            "Zheng Chen",
            "Wenbo Li",
            "Yong Guo",
            "Yulun Zhang"
        ],
        "title": "Compression-Aware One-Step Diffusion Model for JPEG Artifact Removal",
        "abstract": "arXiv:2502.09873v1 Announce Type: new  Abstract: Diffusion models have demonstrated remarkable success in image restoration tasks. However, their multi-step denoising process introduces significant computational overhead, limiting their practical deployment. Furthermore, existing methods struggle to effectively remove severe JPEG artifact, especially in highly compressed images. To address these challenges, we propose CODiff, a compression-aware one-step diffusion model for JPEG artifact removal. The core of CODiff is the compression-aware visual embedder (CaVE), which extracts and leverages JPEG compression priors to guide the diffusion model. We propose a dual learning strategy that combines explicit and implicit learning. Specifically, explicit learning enforces a quality prediction objective to differentiate low-quality images with different compression levels. Implicit learning employs a reconstruction objective that enhances the model's generalization. This dual learning allows for a deeper and more comprehensive understanding of JPEG compression. Experimental results demonstrate that CODiff surpasses recent leading methods in both quantitative and visual quality metrics. The code and models will be released at https://github.com/jp-guo/CODiff.",
        "arxiv_id": "2502.09873",
        "ARXIVID": "2502.09873",
        "COMMENT": "Does not match any specific criterion but is relevant to image restoration and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.10258": {
        "authors": [
            "Kunal Swami",
            "Raghu Chittersu",
            "Pranav Adlinge",
            "Rajeev Irny",
            "Shashavali Doodekula",
            "Alok Shukla"
        ],
        "title": "PromptArtisan: Multi-instruction Image Editing in Single Pass with Complete Attention Control",
        "abstract": "arXiv:2502.10258v1 Announce Type: new  Abstract: We present PromptArtisan, a groundbreaking approach to multi-instruction image editing that achieves remarkable results in a single pass, eliminating the need for time-consuming iterative refinement. Our method empowers users to provide multiple editing instructions, each associated with a specific mask within the image. This flexibility allows for complex edits involving mask intersections or overlaps, enabling the realization of intricate and nuanced image transformations. PromptArtisan leverages a pre-trained InstructPix2Pix model in conjunction with a novel Complete Attention Control Mechanism (CACM). This mechanism ensures precise adherence to user instructions, granting fine-grained control over the editing process. Furthermore, our approach is zero-shot, requiring no additional training, and boasts improved processing complexity compared to traditional iterative methods. By seamlessly integrating multi-instruction capabilities, single-pass efficiency, and complete attention control, PromptArtisan unlocks new possibilities for creative and efficient image editing workflows, catering to both novice and expert users alike.",
        "arxiv_id": "2502.10258",
        "ARXIVID": "2502.10258",
        "COMMENT": "Does not directly match any specific criterion but is relevant to the general interest area of image editing and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.09793": {
        "authors": [
            "Yuang Wang",
            "Siyeop Yoon",
            "Rui Hu",
            "Baihui Yu",
            "Duhgoon Lee",
            "Rajiv Gupta",
            "Li Zhang",
            "Zhiqiang Chen",
            "Dufan Wu"
        ],
        "title": "Noise Controlled CT Super-Resolution with Conditional Diffusion Model",
        "abstract": "arXiv:2502.09793v1 Announce Type: new  Abstract: Improving the spatial resolution of CT images is a meaningful yet challenging task, often accompanied by the issue of noise amplification. This article introduces an innovative framework for noise-controlled CT super-resolution utilizing the conditional diffusion model. The model is trained on hybrid datasets, combining noise-matched simulation data with segmented details from real data. Experimental results with real CT images validate the effectiveness of our proposed framework, showing its potential for practical applications in CT imaging.",
        "arxiv_id": "2502.09793",
        "ARXIVID": "2502.09793",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling and medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.09947": {
        "authors": [
            "Jin Cui",
            "Alexander Capstick",
            "Payam Barnaghi",
            "Gregory Scott"
        ],
        "title": "Analyzing Patient Daily Movement Behavior Dynamics Using Two-Stage Encoding Model",
        "abstract": "arXiv:2502.09947v1 Announce Type: new  Abstract: In the analysis of remote healthcare monitoring data, time series representation learning offers substantial value in uncovering deeper patterns of patient behavior, especially given the fine temporal granularity of the data. In this study, we focus on a dataset of home activity records from people living with Dementia. We propose a two-stage self-supervised learning approach. The first stage involves converting time-series activities into text strings, which are then encoded by a fine-tuned language model. In the second stage, these time-series vectors are bi-dimensionalized for applying PageRank method, to analyze latent state transitions to quantitatively assess participants behavioral patterns and identify activity biases. These insights, combined with diagnostic data, aim to support personalized care interventions.",
        "arxiv_id": "2502.09947",
        "ARXIVID": "2502.09947",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of time-series analysis and healthcare applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.10195": {
        "authors": [
            "Myungseo Song",
            "Jin-Woo Park",
            "Jong-Seok Lee"
        ],
        "title": "Exploring the Camera Bias of Person Re-identification",
        "abstract": "arXiv:2502.10195v1 Announce Type: new  Abstract: We empirically investigate the camera bias of person re-identification (ReID) models. Previously, camera-aware methods have been proposed to address this issue, but they are largely confined to training domains of the models. We measure the camera bias of ReID models on unseen domains and reveal that camera bias becomes more pronounced under data distribution shifts. As a debiasing method for unseen domain data, we revisit feature normalization on embedding vectors. While the normalization has been used as a straightforward solution, its underlying causes and broader applicability remain unexplored. We analyze why this simple method is effective at reducing bias and show that it can be applied to detailed bias factors such as low-level image properties and body angle. Furthermore, we validate its generalizability across various models and benchmarks, highlighting its potential as a simple yet effective test-time postprocessing method for ReID. In addition, we explore the inherent risk of camera bias in unsupervised learning of ReID models. The unsupervised models remain highly biased towards camera labels even for seen domain data, indicating substantial room for improvement. Based on observations of the negative impact of camera-biased pseudo labels on training, we suggest simple training strategies to mitigate the bias. By applying these strategies to existing unsupervised learning algorithms, we show that significant performance improvements can be achieved with minor modifications.",
        "arxiv_id": "2502.10195",
        "ARXIVID": "2502.10195",
        "COMMENT": "This paper does not match any specific criteria but explores camera bias in person re-identification, which is tangentially related to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.09939": {
        "authors": [
            "Xi Shen",
            "Julian Gamboa",
            "Tabassom Hamidfar",
            "Shamima A. Mitu",
            "Selim M. Shahriar"
        ],
        "title": "Temporal Scale and Shift Invariant Automatic Event Recognition using the Mellin Transform",
        "abstract": "arXiv:2502.09939v1 Announce Type: new  Abstract: The Spatio-temporal holographic correlator combines the traditional 2D optical image correlation techniques with inhomogeneously broadened arrays of cold atoms to achieve 3D time-space correlation to realize automatic event recognition at an ultra-high speed. Here we propose a method to realize such event recognition for videos running at different speeds. With this method, we can highly improve recognition accuracy and filter almost all the unwanted events in the video database.",
        "arxiv_id": "2502.09939",
        "ARXIVID": "2502.09939",
        "COMMENT": "Does not match any specific criterion but is relevant to spatio-temporal event recognition.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.10277": {
        "authors": [
            "Yin-Chih Chelsea Wang",
            "Tsao-Lun Chen",
            "Shankeeth Vinayahalingam",
            "Tai-Hsien Wu",
            "Chu Wei Chang",
            "Hsuan Hao Chang",
            "Hung-Jen Wei",
            "Mu-Hsiung Chen",
            "Ching-Chang Ko",
            "David Anssari Moin",
            "Bram van Ginneken",
            "Tong Xi",
            "Hsiao-Cheng Tsai",
            "Min-Huey Chen",
            "Tzu-Ming Harry Hsu",
            "Hye Chou"
        ],
        "title": "Artificial Intelligence to Assess Dental Findings from Panoramic Radiographs -- A Multinational Study",
        "abstract": "arXiv:2502.10277v1 Announce Type: new  Abstract: Dental panoramic radiographs (DPRs) are widely used in clinical practice for comprehensive oral assessment but present challenges due to overlapping structures and time constraints in interpretation.   This study aimed to establish a solid baseline for the AI-automated assessment of findings in DPRs by developing, evaluating an AI system, and comparing its performance with that of human readers across multinational data sets.   We analyzed 6,669 DPRs from three data sets (the Netherlands, Brazil, and Taiwan), focusing on 8 types of dental findings. The AI system combined object detection and semantic segmentation techniques for per-tooth finding identification. Performance metrics included sensitivity, specificity, and area under the receiver operating characteristic curve (AUC-ROC). AI generalizability was tested across data sets, and performance was compared with human dental practitioners.   The AI system demonstrated comparable or superior performance to human readers, particularly +67.9% (95% CI: 54.0%-81.9%; p < .001) sensitivity for identifying periapical radiolucencies and +4.7% (95% CI: 1.4%-8.0%; p = .008) sensitivity for identifying missing teeth. The AI achieved a macro-averaged AUC-ROC of 96.2% (95% CI: 94.6%-97.8%) across 8 findings. AI agreements with the reference were comparable to inter-human agreements in 7 of 8 findings except for caries (p = .024). The AI system demonstrated robust generalization across diverse imaging and demographic settings and processed images 79 times faster (95% CI: 75-82) than human readers.   The AI system effectively assessed findings in DPRs, achieving performance on par with or better than human experts while significantly reducing interpretation time. These results highlight the potential for integrating AI into clinical workflows to improve diagnostic efficiency and accuracy, and patient management.",
        "arxiv_id": "2502.10277",
        "ARXIVID": "2502.10277",
        "COMMENT": "Does not match any specific criterion but is relevant to AI applications in medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}