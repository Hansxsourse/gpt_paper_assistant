{
    "2512.15577": {
        "authors": [
            "Zhipeng Du",
            "Duolikun Danier",
            "Jan Eric Lenssen",
            "Hakan Bilen"
        ],
        "title": "MoonSeg3R: Monocular Online Zero-Shot Segment Anything in 3D with Reconstructive Foundation Priors",
        "abstract": "arXiv:2512.15577v1 Announce Type: new  Abstract: In this paper, we focus on online zero-shot monocular 3D instance segmentation, a novel practical setting where existing approaches fail to perform because they rely on posed RGB-D sequences. To overcome this limitation, we leverage CUT3R, a recent Reconstructive Foundation Model (RFM), to provide reliable geometric priors from a single RGB stream. We propose MoonSeg3R, which introduces three key components: (1) a self-supervised query refinement module with spatial-semantic distillation that transforms segmentation masks from 2D visual foundation models (VFMs) into discriminative 3D queries; (2) a 3D query index memory that provides temporal consistency by retrieving contextual queries; and (3) a state-distribution token from CUT3R that acts as a mask identity descriptor to strengthen cross-frame fusion. Experiments on ScanNet200 and SceneNN show that MoonSeg3R is the first method to enable online monocular 3D segmentation and achieves performance competitive with state-of-the-art RGB-D-based systems. Code and models will be released.",
        "arxiv_id": "2512.15577",
        "ARXIVID": "2512.15577",
        "COMMENT": "The paper does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.15110": {
        "authors": [
            "Jialong Zuo",
            "Haoyou Deng",
            "Hanyu Zhou",
            "Jiaxin Zhu",
            "Yicheng Zhang",
            "Yiwei Zhang",
            "Yongxin Yan",
            "Kaixing Huang",
            "Weisen Chen",
            "Yongtai Deng",
            "Rui Jin",
            "Nong Sang",
            "Changxin Gao"
        ],
        "title": "Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets",
        "abstract": "arXiv:2512.15110v1 Announce Type: new  Abstract: The rapid evolution of text-to-image generation models has revolutionized visual content creation. While commercial products like Nano Banana Pro have garnered significant attention, their potential as generalist solvers for traditional low-level vision challenges remains largely underexplored. In this study, we investigate the critical question: Is Nano Banana Pro a Low-Level Vision All-Rounder? We conducted a comprehensive zero-shot evaluation across 14 distinct low-level tasks spanning 40 diverse datasets. By utilizing simple textual prompts without fine-tuning, we benchmarked Nano Banana Pro against state-of-the-art specialist models. Our extensive analysis reveals a distinct performance dichotomy: while \\textbf{Nano Banana Pro demonstrates superior subjective visual quality}, often hallucinating plausible high-frequency details that surpass specialist models, it lags behind in traditional reference-based quantitative metrics. We attribute this discrepancy to the inherent stochasticity of generative models, which struggle to maintain the strict pixel-level consistency required by conventional metrics. This report identifies Nano Banana Pro as a capable zero-shot contender for low-level vision tasks, while highlighting that achieving the high fidelity of domain specialists remains a significant hurdle.",
        "arxiv_id": "2512.15110",
        "ARXIVID": "2512.15110",
        "COMMENT": "The paper does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.15702": {
        "authors": [
            "Yuwei Guo",
            "Ceyuan Yang",
            "Hao He",
            "Yang Zhao",
            "Meng Wei",
            "Zhenheng Yang",
            "Weilin Huang",
            "Dahua Lin"
        ],
        "title": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
        "abstract": "arXiv:2512.15702v1 Announce Type: new  Abstract: Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.",
        "arxiv_id": "2512.15702",
        "ARXIVID": "2512.15702",
        "COMMENT": "The paper does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}