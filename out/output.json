{
    "2510.19329": {
        "authors": [
            "Panagiotis Agrafiotis",
            "Beg\\\"um Demir"
        ],
        "title": "Seabed-Net: A multi-task network for joint bathymetry estimation and seabed classification from remote sensing imagery in shallow waters",
        "abstract": "arXiv:2510.19329v1 Announce Type: new  Abstract: Accurate, detailed, and regularly updated bathymetry, coupled with complex semantic content, is essential for under-mapped shallow-water environments facing increasing climatological and anthropogenic pressures. However, existing approaches that derive either depth or seabed classes from remote sensing imagery treat these tasks in isolation, forfeiting the mutual benefits of their interaction and hindering the broader adoption of deep learning methods. To address these limitations, we introduce Seabed-Net, a unified multi-task framework that simultaneously predicts bathymetry and pixel-based seabed classification from remote sensing imagery of various resolutions. Seabed-Net employs dual-branch encoders for bathymetry estimation and pixel-based seabed classification, integrates cross-task features via an Attention Feature Fusion module and a windowed Swin-Transformer fusion block, and balances objectives through dynamic task uncertainty weighting. In extensive evaluations at two heterogeneous coastal sites, it consistently outperforms traditional empirical models and traditional machine learning regression methods, achieving up to 75\\% lower RMSE. It also reduces bathymetric RMSE by 10-30\\% compared to state-of-the-art single-task and multi-task baselines and improves seabed classification accuracy up to 8\\%. Qualitative analyses further demonstrate enhanced spatial consistency, sharper habitat boundaries, and corrected depth biases in low-contrast regions. These results confirm that jointly modeling depth with both substrate and seabed habitats yields synergistic gains, offering a robust, open solution for integrated shallow-water mapping. Code and pretrained weights are available at https://github.com/pagraf/Seabed-Net.",
        "arxiv_id": "2510.19329",
        "ARXIVID": "2510.19329",
        "COMMENT": "The paper matches criterion 1 closely as it proposes a unified multi-task framework for joint bathymetry estimation and seabed classification, which is similar to joint generation and segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2510.19574": {
        "authors": [
            "Ariana Yi",
            "Ce Zhou",
            "Liyang Xiao",
            "Qiben Yan"
        ],
        "title": "Can You Trust What You See? Alpha Channel No-Box Attacks on Video Object Detection",
        "abstract": "arXiv:2510.19574v1 Announce Type: new  Abstract: As object detection models are increasingly deployed in cyber-physical systems such as autonomous vehicles (AVs) and surveillance platforms, ensuring their security against adversarial threats is essential. While prior work has explored adversarial attacks in the image domain, those attacks in the video domain remain largely unexamined, especially in the no-box setting. In this paper, we present {\\alpha}-Cloak, the first no-box adversarial attack on object detectors that operates entirely through the alpha channel of RGBA videos. {\\alpha}-Cloak exploits the alpha channel to fuse a malicious target video with a benign video, resulting in a fused video that appears innocuous to human viewers but consistently fools object detectors. Our attack requires no access to model architecture, parameters, or outputs, and introduces no perceptible artifacts. We systematically study the support for alpha channels across common video formats and playback applications, and design a fusion algorithm that ensures visual stealth and compatibility. We evaluate {\\alpha}-Cloak on five state-of-the-art object detectors, a vision-language model, and a multi-modal large language model (Gemini-2.0-Flash), demonstrating a 100% attack success rate across all scenarios. Our findings reveal a previously unexplored vulnerability in video-based perception systems, highlighting the urgent need for defenses that account for the alpha channel in adversarial settings.",
        "arxiv_id": "2510.19574",
        "ARXIVID": "2510.19574",
        "COMMENT": "The paper does not match any specific criteria closely. It focuses on adversarial attacks on video object detection using the alpha channel, which is not related to joint generation and segmentation or diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.19597": {
        "authors": [
            "Zhou Lei",
            "Pan Gang",
            "Wang Jiahao",
            "Sun Di"
        ],
        "title": "CBDiff:Conditional Bernoulli Diffusion Models for Image Forgery Localization",
        "abstract": "arXiv:2510.19597v1 Announce Type: new  Abstract: Image Forgery Localization (IFL) is a crucial task in image forensics, aimed at accurately identifying manipulated or tampered regions within an image at the pixel level. Existing methods typically generate a single deterministic localization map, which often lacks the precision and reliability required for high-stakes applications such as forensic analysis and security surveillance. To enhance the credibility of predictions and mitigate the risk of errors, we introduce an advanced Conditional Bernoulli Diffusion Model (CBDiff). Given a forged image, CBDiff generates multiple diverse and plausible localization maps, thereby offering a richer and more comprehensive representation of the forgery distribution. This approach addresses the uncertainty and variability inherent in tampered regions. Furthermore, CBDiff innovatively incorporates Bernoulli noise into the diffusion process to more faithfully reflect the inherent binary and sparse properties of forgery masks. Additionally, CBDiff introduces a Time-Step Cross-Attention (TSCAttention), which is specifically designed to leverage semantic feature guidance with temporal steps to improve manipulation detection. Extensive experiments on eight publicly benchmark datasets demonstrate that CBDiff significantly outperforms existing state-of-the-art methods, highlighting its strong potential for real-world deployment.",
        "arxiv_id": "2510.19597",
        "ARXIVID": "2510.19597",
        "COMMENT": "The paper does not match any specific criteria closely. It focuses on image forgery localization using a diffusion model, but does not propose a multi-task diffusion framework.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.19022": {
        "authors": [
            "Aritra Bhowmik",
            "Denis Korzhenkov",
            "Cees G. M. Snoek",
            "Amirhossein Habibian",
            "Mohsen Ghafoorian"
        ],
        "title": "MoAlign: Motion-Centric Representation Alignment for Video Diffusion Models",
        "abstract": "arXiv:2510.19022v1 Announce Type: new  Abstract: Text-to-video diffusion models have enabled high-quality video synthesis, yet often fail to generate temporally coherent and physically plausible motion. A key reason is the models' insufficient understanding of complex motions that natural videos often entail. Recent works tackle this problem by aligning diffusion model features with those from pretrained video encoders. However, these encoders mix video appearance and dynamics into entangled features, limiting the benefit of such alignment. In this paper, we propose a motion-centric alignment framework that learns a disentangled motion subspace from a pretrained video encoder. This subspace is optimized to predict ground-truth optical flow, ensuring it captures true motion dynamics. We then align the latent features of a text-to-video diffusion model to this new subspace, enabling the generative model to internalize motion knowledge and generate more plausible videos. Our method improves the physical commonsense in a state-of-the-art video diffusion model, while preserving adherence to textual prompts, as evidenced by empirical evaluations on VideoPhy, VideoPhy2, VBench, and VBench-2.0, along with a user study.",
        "arxiv_id": "2510.19022",
        "ARXIVID": "2510.19022",
        "COMMENT": "The paper does not match any specific criteria closely. It focuses on improving motion understanding in video diffusion models, but does not propose a multi-task diffusion framework.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.19465": {
        "authors": [
            "Ali Sadeghkhani",
            "Brandon Bennett",
            "Masoud Babaei",
            "Arash Rabbani"
        ],
        "title": "PCP-GAN: Property-Constrained Pore-scale image reconstruction via conditional Generative Adversarial Networks",
        "abstract": "arXiv:2510.19465v1 Announce Type: new  Abstract: Obtaining truly representative pore-scale images that match bulk formation properties remains a fundamental challenge in subsurface characterization, as natural spatial heterogeneity causes extracted sub-images to deviate significantly from core-measured values. This challenge is compounded by data scarcity, where physical samples are only available at sparse well locations. This study presents a multi-conditional Generative Adversarial Network (cGAN) framework that generates representative pore-scale images with precisely controlled properties, addressing both the representativeness challenge and data availability constraints. The framework was trained on thin section samples from four depths (1879.50-1943.50 m) of a carbonate formation, simultaneously conditioning on porosity values and depth parameters within a single unified model. This approach captures both universal pore network principles and depth-specific geological characteristics, from grainstone fabrics with interparticle-intercrystalline porosity to crystalline textures with anhydrite inclusions. The model achieved exceptional porosity control (R^2=0.95) across all formations with mean absolute errors of 0.0099-0.0197. Morphological validation confirmed preservation of critical pore network characteristics including average pore radius, specific surface area, and tortuosity, with statistical differences remaining within acceptable geological tolerances. Most significantly, generated images demonstrated superior representativeness with dual-constraint errors of 1.9-11.3% compared to 36.4-578% for randomly extracted real sub-images. This capability provides transformative tools for subsurface characterization, particularly valuable for carbon storage, geothermal energy, and groundwater management applications where knowing the representative morphology of the pore space is critical for implementing digital rock physics.",
        "arxiv_id": "2510.19465",
        "ARXIVID": "2510.19465",
        "COMMENT": "The paper does not match any specific criteria closely. It focuses on pore-scale image reconstruction using GANs, which is not directly related to joint generation and segmentation or diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.19195": {
        "authors": [
            "Kai Zeng",
            "Zhanqian Wu",
            "Kaixin Xiong",
            "Xiaobao Wei",
            "Xiangyu Guo",
            "Zhenxin Zhu",
            "Kalok Ho",
            "Lijun Zhou",
            "Bohan Zeng",
            "Ming Lu",
            "Haiyang Sun",
            "Bing Wang",
            "Guang Chen",
            "Hangjun Ye",
            "Wentao Zhang"
        ],
        "title": "Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks",
        "abstract": "arXiv:2510.19195v1 Announce Type: new  Abstract: Recent advancements in driving world models enable controllable generation of high-quality RGB videos or multimodal videos. Existing methods primarily focus on metrics related to generation quality and controllability. However, they often overlook the evaluation of downstream perception tasks, which are $\\mathbf{really\\ crucial}$ for the performance of autonomous driving. Existing methods usually leverage a training strategy that first pretrains on synthetic data and finetunes on real data, resulting in twice the epochs compared to the baseline (real data only). When we double the epochs in the baseline, the benefit of synthetic data becomes negligible. To thoroughly demonstrate the benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data generation framework designed for enhancing the downstream perception tasks. Dream4Drive first decomposes the input video into several 3D-aware guidance maps and subsequently renders the 3D assets onto these guidance maps. Finally, the driving world model is fine-tuned to produce the edited, multi-view photorealistic videos, which can be used to train the downstream perception models. Dream4Drive enables unprecedented flexibility in generating multi-view corner cases at scale, significantly boosting corner case perception in autonomous driving. To facilitate future research, we also contribute a large-scale 3D asset dataset named DriveObj3D, covering the typical categories in driving scenarios and enabling diverse 3D-aware video editing. We conduct comprehensive experiments to show that Dream4Drive can effectively boost the performance of downstream perception models under various training epochs. Project: $\\href{https://wm-research.github.io/Dream4Drive/}{this\\ https\\ URL}$",
        "arxiv_id": "2510.19195",
        "ARXIVID": "2510.19195",
        "COMMENT": "The paper does not match any specific criteria closely. It focuses on synthetic data generation for perception tasks in autonomous driving, which is not directly related to joint generation and segmentation or diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}