{
    "2505.07538": {
        "authors": [
            "Bohan Wang",
            "Zhongqi Yue",
            "Fengda Zhang",
            "Shuo Chen",
            "Li'an Bi",
            "Junzhe Zhang",
            "Xue Song",
            "Kennard Yanting Chan",
            "Jiachun Pan",
            "Weijia Wu",
            "Mingze Zhou",
            "Wang Lin",
            "Kaihang Pan",
            "Saining Zhang",
            "Liyu Jia",
            "Wentao Hu",
            "Wei Zhao",
            "Hanwang Zhang"
        ],
        "title": "Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning",
        "abstract": "arXiv:2505.07538v1 Announce Type: new  Abstract: We completely discard the conventional spatial prior in image representation and introduce a novel discrete visual tokenizer: Self-consistency Tokenizer (Selftok). At its design core, we compose an autoregressive (AR) prior -- mirroring the causal structure of language -- into visual tokens by using the reverse diffusion process of image generation. The AR property makes Selftok fundamentally distinct from traditional spatial tokens in the following two key ways: - Selftok offers an elegant and minimalist approach to unify diffusion and AR for vision-language models (VLMs): By representing images with Selftok tokens, we can train a VLM using a purely discrete autoregressive architecture -- like that in LLMs -- without requiring additional modules or training objectives. - We theoretically show that the AR prior satisfies the Bellman equation, whereas the spatial prior does not. Therefore, Selftok supports reinforcement learning (RL) for visual generation with effectiveness comparable to that achieved in LLMs. Besides the AR property, Selftok is also a SoTA tokenizer that achieves a favorable trade-off between high-quality reconstruction and compression rate. We use Selftok to build a pure AR VLM for both visual comprehension and generation tasks. Impressively, without using any text-image training pairs, a simple policy gradient RL working in the visual tokens can significantly boost the visual generation benchmark, surpassing all the existing models by a large margin. Therefore, we believe that Selftok effectively addresses the long-standing challenge that visual tokens cannot support effective RL. When combined with the well-established strengths of RL in LLMs, this brings us one step closer to realizing a truly multimodal LLM. Project Page: https://selftok-team.github.io/report/.",
        "arxiv_id": "2505.07538",
        "ARXIVID": "2505.07538",
        "COMMENT": "This paper introduces a novel discrete visual tokenizer for vision-language models, which matches criterion 2 (new VLLMs or MLLMs) and criterion 4 (vision foundation models).",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2505.07500": {
        "authors": [
            "Bahram Mohammadi",
            "Ehsan Abbasnejad",
            "Yuankai Qi",
            "Qi Wu",
            "Anton Van Den Hengel",
            "Javen Qinfeng Shi"
        ],
        "title": "Learning to Reason and Navigate: Parameter Efficient Action Planning with Large Language Models",
        "abstract": "arXiv:2505.07500v1 Announce Type: new  Abstract: The remote embodied referring expression (REVERIE) task requires an agent to navigate through complex indoor environments and localize a remote object specified by high-level instructions, such as \"bring me a spoon\", without pre-exploration. Hence, an efficient navigation plan is essential for the final success. This paper proposes a novel parameter-efficient action planner using large language models (PEAP-LLM) to generate a single-step instruction at each location. The proposed model consists of two modules, LLM goal planner (LGP) and LoRA action planner (LAP). Initially, LGP extracts the goal-oriented plan from REVERIE instructions, including the target object and room. Then, LAP generates a single-step instruction with the goal-oriented plan, high-level instruction, and current visual observation as input. PEAP-LLM enables the embodied agent to interact with LAP as the path planner on the fly. A simple direct application of LLMs hardly achieves good performance. Also, existing hard-prompt-based methods are error-prone in complicated scenarios and need human intervention. To address these issues and prevent the LLM from generating hallucinations and biased information, we propose a novel two-stage method for fine-tuning the LLM, consisting of supervised fine-tuning (STF) and direct preference optimization (DPO). SFT improves the quality of generated instructions, while DPO utilizes environmental feedback. Experimental results show the superiority of our proposed model on REVERIE compared to the previous state-of-the-art.",
        "arxiv_id": "2505.07500",
        "ARXIVID": "2505.07500",
        "COMMENT": "Matches criterion 1 as it proposes a novel method for spatial understanding and navigation using large language models in embodied AI.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2505.07001": {
        "authors": [
            "Bidur Khanal",
            "Sandesh Pokhrel",
            "Sanjay Bhandari",
            "Ramesh Rana",
            "Nikesh Shrestha",
            "Ram Bahadur Gurung",
            "Cristian Linte",
            "Angus Watson",
            "Yash Raj Shrestha",
            "Binod Bhattarai"
        ],
        "title": "Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models",
        "abstract": "arXiv:2505.07001v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) are becoming increasingly popular in the medical domain, bridging the gap between medical images and clinical language. Existing VLMs demonstrate an impressive ability to comprehend medical images and text queries to generate detailed, descriptive diagnostic medical reports. However, hallucination--the tendency to generate descriptions that are inconsistent with the visual content--remains a significant issue in VLMs, with particularly severe implications in the medical field. To facilitate VLM research on gastrointestinal (GI) image analysis and study hallucination, we curate a multimodal image-text GI dataset: Gut-VLM. This dataset is created using a two-stage pipeline: first, descriptive medical reports of Kvasir-v2 images are generated using ChatGPT, which introduces some hallucinated or incorrect texts. In the second stage, medical experts systematically review these reports, and identify and correct potential inaccuracies to ensure high-quality, clinically reliable annotations. Unlike traditional datasets that contain only descriptive texts, our dataset also features tags identifying hallucinated sentences and their corresponding corrections. A common approach to reducing hallucination in VLM is to finetune the model on a small-scale, problem-specific dataset. However, we take a different strategy using our dataset. Instead of finetuning the VLM solely for generating textual reports, we finetune it to detect and correct hallucinations, an approach we call hallucination-aware finetuning. Our results show that this approach is better than simply finetuning for descriptive report generation. Additionally, we conduct an extensive evaluation of state-of-the-art VLMs across several metrics, establishing a benchmark. GitHub Repo: https://github.com/bhattarailab/Hallucination-Aware-VLM.",
        "arxiv_id": "2505.07001",
        "ARXIVID": "2505.07001",
        "COMMENT": "Matches criterion 2 as it evaluates vision-language models (VLMs) in the medical domain and introduces a novel benchmark for hallucination detection.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2505.07062": {
        "authors": [
            "Dong Guo",
            "Faming Wu",
            "Feida Zhu",
            "Fuxing Leng",
            "Guang Shi",
            "Haobin Chen",
            "Haoqi Fan",
            "Jian Wang",
            "Jianyu Jiang",
            "Jiawei Wang",
            "Jingji Chen",
            "Jingjia Huang",
            "Kang Lei",
            "Liping Yuan",
            "Lishu Luo",
            "Pengfei Liu",
            "Qinghao Ye",
            "Rui Qian",
            "Shen Yan",
            "Shixiong Zhao",
            "Shuai Peng",
            "Shuangye Li",
            "Sihang Yuan",
            "Sijin Wu",
            "Tianheng Cheng",
            "Weiwei Liu",
            "Wenqian Wang",
            "Xianhan Zeng",
            "Xiao Liu",
            "Xiaobo Qin",
            "Xiaohan Ding",
            "Xiaojun Xiao",
            "Xiaoying Zhang",
            "Xuanwei Zhang",
            "Xuehan Xiong",
            "Yanghua Peng",
            "Yangrui Chen",
            "Yanwei Li",
            "Yanxu Hu",
            "Yi Lin",
            "Yiyuan Hu",
            "Yiyuan Zhang",
            "Youbin Wu",
            "Yu Li",
            "Yudong Liu",
            "Yue Ling",
            "Yujia Qin",
            "Zanbo Wang",
            "Zhiwu He",
            "Aoxue Zhang",
            "Bairen Yi",
            "Bencheng Liao",
            "Can Huang",
            "Can Zhang",
            "Chaorui Deng",
            "Chaoyi Deng",
            "Cheng Lin",
            "Cheng Yuan",
            "Chenggang Li",
            "Chenhui Gou",
            "Chenwei Lou",
            "Chengzhi Wei",
            "Chundian Liu",
            "Chunyuan Li",
            "Deyao Zhu",
            "Donghong Zhong",
            "Feng Li",
            "Feng Zhang",
            "Gang Wu",
            "Guodong Li",
            "Guohong Xiao",
            "Haibin Lin",
            "Haihua Yang",
            "Haoming Wang",
            "Heng Ji",
            "Hongxiang Hao",
            "Hui Shen",
            "Huixia Li",
            "Jiahao Li",
            "Jialong Wu",
            "Jianhua Zhu",
            "Jianpeng Jiao",
            "Jiashi Feng",
            "Jiaze Chen",
            "Jianhui Duan",
            "Jihao Liu",
            "Jin Zeng",
            "Jingqun Tang",
            "Jingyu Sun",
            "Joya Chen",
            "Jun Long",
            "Junda Feng",
            "Junfeng Zhan",
            "Junjie Fang",
            "Junting Lu",
            "Kai Hua",
            "Kai Liu",
            "Kai Shen",
            "Kaiyuan Zhang",
            "Ke Shen",
            "Ke Wang",
            "Keyu Pan",
            "Kun Zhang",
            "Kunchang Li",
            "Lanxin Li",
            "Lei Li",
            "Lei Shi",
            "Li Han",
            "Liang Xiang",
            "Liangqiang Chen",
            "Lin Chen",
            "Lin Li",
            "Lin Yan",
            "Liying Chi",
            "Longxiang Liu",
            "Mengfei Du",
            "Mingxuan Wang",
            "Ningxin Pan",
            "Peibin Chen",
            "Pengfei Chen",
            "Pengfei Wu",
            "Qingqing Yuan",
            "Qingyao Shuai",
            "Qiuyan Tao",
            "Renjie Zheng",
            "Renrui Zhang",
            "Ru Zhang",
            "Rui Wang",
            "Rui Yang",
            "Rui Zhao",
            "Shaoqiang Xu",
            "Shihao Liang",
            "Shipeng Yan",
            "Shu Zhong",
            "Shuaishuai Cao",
            "Shuangzhi Wu",
            "Shufan Liu",
            "Shuhan Chang",
            "Songhua Cai",
            "Tenglong Ao",
            "Tianhao Yang",
            "Tingting Zhang",
            "Wanjun Zhong",
            "Wei Jia",
            "Wei Weng",
            "Weihao Yu",
            "Wenhao Huang",
            "Wenjia Zhu",
            "Wenli Yang",
            "Wenzhi Wang",
            "Xiang Long",
            "XiangRui Yin",
            "Xiao Li",
            "Xiaolei Zhu",
            "Xiaoying Jia",
            "Xijin Zhang",
            "Xin Liu",
            "Xinchen Zhang",
            "Xinyu Yang",
            "Xiongcai Luo",
            "Xiuli Chen",
            "Xuantong Zhong",
            "Xuefeng Xiao",
            "Xujing Li",
            "Yan Wu",
            "Yawei Wen",
            "Yifan Du",
            "Yihao Zhang",
            "Yining Ye",
            "Yonghui Wu",
            "Yu Liu",
            "Yu Yue",
            "Yufeng Zhou",
            "Yufeng Yuan",
            "Yuhang Xu",
            "Yuhong Yang",
            "Yun Zhang",
            "Yunhao Fang",
            "Yuntao Li",
            "Yurui Ren",
            "Yuwen Xiong",
            "Zehua Hong",
            "Zehua Wang",
            "Zewei Sun",
            "Zeyu Wang",
            "Zhao Cai",
            "Zhaoyue Zha",
            "Zhecheng An",
            "Zhehui Zhao",
            "Zhengzhuo Xu",
            "Zhipeng Chen",
            "Zhiyong Wu",
            "Zhuofan Zheng",
            "Zihao Wang",
            "Zilong Huang",
            "Ziyu Zhu",
            "Zuquan Song"
        ],
        "title": "Seed1.5-VL Technical Report",
        "abstract": "arXiv:2505.07062v1 Announce Type: new  Abstract: We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at https://www.volcengine.com/ (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)",
        "arxiv_id": "2505.07062",
        "ARXIVID": "2505.07062",
        "COMMENT": "This paper matches criterion 2 as it introduces Seed1.5-VL, a vision-language foundation model with strong multimodal reasoning capabilities.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2505.06524": {
        "authors": [
            "Jingyao Wang",
            "Jianqi Zhang",
            "Wenwen Qiang",
            "Changwen Zheng"
        ],
        "title": "Causal Prompt Calibration Guided Segment Anything Model for Open-Vocabulary Multi-Entity Segmentation",
        "abstract": "arXiv:2505.06524v1 Announce Type: new  Abstract: Despite the strength of the Segment Anything Model (SAM), it struggles with generalization issues in open-vocabulary multi-entity segmentation (OVMS). Through empirical and causal analyses, we find that (i) the prompt bias is the primary cause of the generalization issues; (ii) this bias is closely tied to the task-irrelevant generating factors within the prompts, which act as confounders and affect generalization. To address the generalization issues, we aim to propose a method that can calibrate prompts to eliminate confounders for accurate OVMS. Building upon the causal analysis, we propose that the optimal prompt for OVMS should contain only task-relevant causal factors. We define it as the causal prompt, serving as the goal of calibration. Next, our theoretical analysis, grounded by causal multi-distribution consistency theory, proves that this prompt can be obtained by enforcing segmentation consistency and optimality. Inspired by this, we propose CPC-SAM, a Causal Prompt Calibration method for SAM to achieve accurate OVMS. It integrates a lightweight causal prompt learner (CaPL) into SAM to obtain causal prompts. Specifically, we first generate multiple prompts using random annotations to simulate diverse distributions and then reweight them via CaPL by enforcing causal multi-distribution consistency in both task and entity levels. To ensure obtaining causal prompts, CaPL is optimized by minimizing the cumulative segmentation loss across the reweighted prompts to achieve consistency and optimality. A bi-level optimization strategy alternates between optimizing CaPL and SAM, ensuring accurate OVMS. Extensive experiments validate its superiority.",
        "arxiv_id": "2505.06524",
        "ARXIVID": "2505.06524",
        "COMMENT": "This paper matches criterion 2 as it proposes a novel method to improve the Segment Anything Model (SAM) for open-vocabulary multi-entity segmentation, which is relevant to visual large language models (VLLMs).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.07396": {
        "authors": [
            "Olaf Wysocki",
            "Benedikt Schwab",
            "Manoj Kumar Biswanath",
            "Qilin Zhang",
            "Jingwei Zhu",
            "Thomas Froech",
            "Medhini Heeramaglore",
            "Ihab Hijazi",
            "Khaoula Kanna",
            "Mathias Pechinger",
            "Zhaiyu Chen",
            "Yao Sun",
            "Alejandro Rueda Segura",
            "Ziyang Xu",
            "Omar AbdelGafar",
            "Mansour Mehranfar",
            "Chandan Yeshwanth",
            "Yueh-Cheng Liu",
            "Hadi Yazdi",
            "Jiapan Wang",
            "Stefan Auer",
            "Katharina Anders",
            "Klaus Bogenberger",
            "Andre Borrmann",
            "Angela Dai",
            "Ludwig Hoegner",
            "Christoph Holst",
            "Thomas H. Kolbe",
            "Ferdinand Ludwig",
            "Matthias Nie{\\ss}ner",
            "Frank Petzold",
            "Xiao Xiang Zhu",
            "Boris Jutzi"
        ],
        "title": "TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset",
        "abstract": "arXiv:2505.07396v1 Announce Type: new  Abstract: Urban Digital Twins (UDTs) have become essential for managing cities and integrating complex, heterogeneous data from diverse sources. Creating UDTs involves challenges at multiple process stages, including acquiring accurate 3D source data, reconstructing high-fidelity 3D models, maintaining models' updates, and ensuring seamless interoperability to downstream tasks. Current datasets are usually limited to one part of the processing chain, hampering comprehensive UDTs validation. To address these challenges, we introduce the first comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN. This dataset includes georeferenced, semantically aligned 3D models and networks along with various terrestrial, mobile, aerial, and satellite observations boasting 32 data subsets over roughly 100,000 $m^2$ and currently 767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high accuracy, and multimodal data integration, the benchmark supports robust analysis of sensors and the development of advanced reconstruction methods. Additionally, we explore downstream tasks demonstrating the potential of TUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar potential analysis, point cloud semantic segmentation, and LoD3 building reconstruction. We are convinced this contribution lays a foundation for overcoming current limitations in UDT creation, fostering new research directions and practical solutions for smarter, data-driven urban environments. The project is available under: https://tum2t.win",
        "arxiv_id": "2505.07396",
        "ARXIVID": "2505.07396",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for urban digital twins, which is relevant to embodied AI and simulation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.06997": {
        "authors": [
            "Wenhao Lu",
            "Zhengqiu Zhu",
            "Yong Zhao",
            "Yonglin Tian",
            "Junjie Zeng",
            "Jun Zhang",
            "Zhong Liu",
            "Fei-Yue Wang"
        ],
        "title": "A Multi-Agent Reinforcement Learning Approach for Cooperative Air-Ground-Human Crowdsensing in Emergency Rescue",
        "abstract": "arXiv:2505.06997v1 Announce Type: new  Abstract: Mobile crowdsensing is evolving beyond traditional human-centric models by integrating heterogeneous entities like unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs). Optimizing task allocation among these diverse agents is critical, particularly in challenging emergency rescue scenarios characterized by complex environments, limited communication, and partial observability. This paper tackles the Heterogeneous-Entity Collaborative-Sensing Task Allocation (HECTA) problem specifically for emergency rescue, considering humans, UAVs, and UGVs. We introduce a novel ``Hard-Cooperative'' policy where UGVs prioritize recharging low-battery UAVs, alongside performing their sensing tasks. The primary objective is maximizing the task completion rate (TCR) under strict time constraints. We rigorously formulate this NP-hard problem as a decentralized partially observable Markov decision process (Dec-POMDP) to effectively handle sequential decision-making under uncertainty. To solve this, we propose HECTA4ER, a novel multi-agent reinforcement learning algorithm built upon a Centralized Training with Decentralized Execution architecture. HECTA4ER incorporates tailored designs, including specialized modules for complex feature extraction, utilization of action-observation history via hidden states, and a mixing network integrating global and local information, specifically addressing the challenges of partial observability. Furthermore, theoretical analysis confirms the algorithm's convergence properties. Extensive simulations demonstrate that HECTA4ER significantly outperforms baseline algorithms, achieving an average 18.42% increase in TCR. Crucially, a real-world case study validates the algorithm's effectiveness and robustness in dynamic sensing scenarios, highlighting its strong potential for practical application in emergency response.",
        "arxiv_id": "2505.06997",
        "ARXIVID": "2505.06997",
        "COMMENT": "Matches criterion 3 as it focuses on a novel multi-agent reinforcement learning method for embodied AI in emergency rescue scenarios.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.07263": {
        "authors": [
            "Xiaokun Wang",
            "Chris",
            "Jiangbo Pei",
            "Wei Shen",
            "Yi Peng",
            "Yunzhuo Hao",
            "Weijie Qiu",
            "Ai Jian",
            "Tianyidan Xie",
            "Xuchen Song",
            "Yang Liu",
            "Yahui Zhou"
        ],
        "title": "Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning",
        "abstract": "arXiv:2505.07263v1 Announce Type: new  Abstract: We propose Skywork-VL Reward, a multimodal reward model that provides reward signals for both multimodal understanding and reasoning tasks. Our technical approach comprises two key components: First, we construct a large-scale multimodal preference dataset that covers a wide range of tasks and scenarios, with responses collected from both standard vision-language models (VLMs) and advanced VLM reasoners. Second, we design a reward model architecture based on Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage fine-tuning using pairwise ranking loss on pairwise preference data. Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art results on multimodal VL-RewardBench and exhibits competitive performance on the text-only RewardBench benchmark. Furthermore, preference data constructed based on our Skywork-VL Reward proves highly effective for training Mixed Preference Optimization (MPO), leading to significant improvements in multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as a significant advancement toward general-purpose, reliable reward models for multimodal alignment. Our model has been publicly released to promote transparency and reproducibility.",
        "arxiv_id": "2505.07263",
        "ARXIVID": "2505.07263",
        "COMMENT": "This paper matches criterion 2 as it introduces Skywork-VL Reward, a multimodal reward model for understanding and reasoning tasks.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2505.07581": {
        "authors": [
            "Lei Wang",
            "Heyang Gao",
            "Xiaohe Bo",
            "Xu Chen",
            "Ji-Rong Wen"
        ],
        "title": "YuLan-OneSim: Towards the Next Generation of Social Simulator with Large Language Models",
        "abstract": "arXiv:2505.07581v1 Announce Type: new  Abstract: Leveraging large language model (LLM) based agents to simulate human social behaviors has recently gained significant attention. In this paper, we introduce a novel social simulator called YuLan-OneSim. Compared to previous works, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free scenario construction: Users can simply describe and refine their simulation scenarios through natural language interactions with our simulator. All simulation code is automatically generated, significantly reducing the need for programming expertise. (2) Comprehensive default scenarios: We implement 50 default simulation scenarios spanning 8 domains, including economics, sociology, politics, psychology, organization, demographics, law, and communication, broadening access for a diverse range of social researchers. (3) Evolvable simulation: Our simulator is capable of receiving external feedback and automatically fine-tuning the backbone LLMs, significantly enhancing the simulation quality. (4) Large-scale simulation: By developing a fully responsive agent framework and a distributed simulation architecture, our simulator can handle up to 100,000 agents, ensuring more stable and reliable simulation results. (5) AI social researcher: Leveraging the above features, we develop an AI social researcher. Users only need to propose a research topic, and the AI researcher will automatically analyze the input, construct simulation environments, summarize results, generate technical reports, review and refine the reports--completing the social science research loop. To demonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate the quality of the automatically generated scenarios, the reliability, efficiency, and scalability of the simulation process, as well as the performance of the AI social researcher.",
        "arxiv_id": "2505.07581",
        "ARXIVID": "2505.07581",
        "COMMENT": "Matches criterion 3 as it introduces a new social simulator (YuLan-OneSim) leveraging large language models, focusing on novel aspects like code-free scenario construction and large-scale simulation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.07347": {
        "authors": [
            "Jiewen Yang",
            "Taoran Huang",
            "Shangwei Ding",
            "Xiaowei Xu",
            "Qinhua Zhao",
            "Yong Jiang",
            "Jiarong Guo",
            "Bin Pu",
            "Jiexuan Zheng",
            "Caojin Zhang",
            "Hongwen Fei",
            "Xiaomeng Li"
        ],
        "title": "AI-Enabled Accurate Non-Invasive Assessment of Pulmonary Hypertension Progression via Multi-Modal Echocardiography",
        "abstract": "arXiv:2505.07347v1 Announce Type: new  Abstract: Echocardiographers can detect pulmonary hypertension using Doppler echocardiography; however, accurately assessing its progression often proves challenging. Right heart catheterization (RHC), the gold standard for precise evaluation, is invasive and unsuitable for routine use, limiting its practicality for timely diagnosis and monitoring of pulmonary hypertension progression. Here, we propose MePH, a multi-view, multi-modal vision-language model to accurately assess pulmonary hypertension progression using non-invasive echocardiography. We constructed a large dataset comprising paired standardized echocardiogram videos, spectral images and RHC data, covering 1,237 patient cases from 12 medical centers. For the first time, MePH precisely models the correlation between non-invasive multi-view, multi-modal echocardiography and the pressure and resistance obtained via RHC. We show that MePH significantly outperforms echocardiographers' assessments using echocardiography, reducing the mean absolute error in estimating mean pulmonary arterial pressure (mPAP) and pulmonary vascular resistance (PVR) by 49.73% and 43.81%, respectively. In eight independent external hospitals, MePH achieved a mean absolute error of 3.147 for PVR assessment. Furthermore, MePH achieved an area under the curve of 0.921, surpassing echocardiographers (area under the curve of 0.842) in accurately predicting the severity of pulmonary hypertension, whether mild or severe. A prospective study demonstrated that MePH can predict treatment efficacy for patients. Our work provides pulmonary hypertension patients with a non-invasive and timely method for monitoring disease progression, improving the accuracy and efficiency of pulmonary hypertension management while enabling earlier interventions and more personalized treatment decisions.",
        "arxiv_id": "2505.07347",
        "ARXIVID": "2505.07347",
        "COMMENT": "Matches criterion 2 as it introduces a multi-modal vision-language model (MePH) for echocardiography analysis.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.06535": {
        "authors": [
            "Anindya Sarkar",
            "Binglin Ji",
            "Yevgeniy Vorobeychik"
        ],
        "title": "Online Feedback Efficient Active Target Discovery in Partially Observable Environments",
        "abstract": "arXiv:2505.06535v1 Announce Type: new  Abstract: In various scientific and engineering domains, where data acquisition is costly, such as in medical imaging, environmental monitoring, or remote sensing, strategic sampling from unobserved regions, guided by prior observations, is essential to maximize target discovery within a limited sampling budget. In this work, we introduce Diffusion-guided Active Target Discovery (DiffATD), a novel method that leverages diffusion dynamics for active target discovery. DiffATD maintains a belief distribution over each unobserved state in the environment, using this distribution to dynamically balance exploration-exploitation. Exploration reduces uncertainty by sampling regions with the highest expected entropy, while exploitation targets areas with the highest likelihood of discovering the target, indicated by the belief distribution and an incrementally trained reward model designed to learn the characteristics of the target. DiffATD enables efficient target discovery in a partially observable environment within a fixed sampling budget, all without relying on any prior supervised training. Furthermore, DiffATD offers interpretability, unlike existing black-box policies that require extensive supervised training. Through extensive experiments and ablation studies across diverse domains, including medical imaging and remote sensing, we show that DiffATD performs significantly better than baselines and competitively with supervised methods that operate under full environmental observability.",
        "arxiv_id": "2505.06535",
        "ARXIVID": "2505.06535",
        "COMMENT": "Matches criterion 1 as it introduces a novel method (DiffATD) for active target discovery in partially observable environments, which relates to spatial intelligence in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2505.07652": {
        "authors": [
            "Ozgur Kara",
            "Krishna Kumar Singh",
            "Feng Liu",
            "Duygu Ceylan",
            "James M. Rehg",
            "Tobias Hinz"
        ],
        "title": "ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models",
        "abstract": "arXiv:2505.07652v1 Announce Type: new  Abstract: Current diffusion-based text-to-video methods are limited to producing short video clips of a single shot and lack the capability to generate multi-shot videos with discrete transitions where the same character performs distinct activities across the same or different backgrounds. To address this limitation we propose a framework that includes a dataset collection pipeline and architectural extensions to video diffusion models to enable text-to-multi-shot video generation. Our approach enables generation of multi-shot videos as a single video with full attention across all frames of all shots, ensuring character and background consistency, and allows users to control the number, duration, and content of shots through shot-specific conditioning. This is achieved by incorporating a transition token into the text-to-video model to control at which frames a new shot begins and a local attention masking strategy which controls the transition token's effect and allows shot-specific prompting. To obtain training data we propose a novel data collection pipeline to construct a multi-shot video dataset from existing single-shot video datasets. Extensive experiments demonstrate that fine-tuning a pre-trained text-to-video model for a few thousand iterations is enough for the model to subsequently be able to generate multi-shot videos with shot-specific control, outperforming the baselines. You can find more details in https://shotadapter.github.io/",
        "arxiv_id": "2505.07652",
        "ARXIVID": "2505.07652",
        "COMMENT": "Matches criterion 4. Proposes a novel framework for text-to-multi-shot video generation using diffusion models, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2505.07256": {
        "authors": [
            "Christoph Huber",
            "Ludwig Schleeh",
            "Dino Knoll",
            "Michael Guthe"
        ],
        "title": "Synthetic Similarity Search in Automotive Production",
        "abstract": "arXiv:2505.07256v1 Announce Type: new  Abstract: Visual quality inspection in automotive production is essential for ensuring the safety and reliability of vehicles. Computer vision (CV) has become a popular solution for these inspections due to its cost-effectiveness and reliability. However, CV models require large, annotated datasets, which are costly and time-consuming to collect. To reduce the need for extensive training data, we propose a novel image classification pipeline that combines similarity search using a vision-based foundation model with synthetic data. Our approach leverages a DINOv2 model to transform input images into feature vectors, which are then compared to pre-classified reference images using cosine distance measurements. By utilizing synthetic data instead of real images as references, our pipeline achieves high classification accuracy without relying on real data. We evaluate this approach in eight real-world inspection scenarios and demonstrate that it meets the high performance requirements of production environments.",
        "arxiv_id": "2505.07256",
        "ARXIVID": "2505.07256",
        "COMMENT": "Matches criterion 4 as it applies vision foundation models (DINOv2) for similarity search in automotive production.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.06668": {
        "authors": [
            "Ziyi Wang",
            "Haipeng Li",
            "Lin Sui",
            "Tianhao Zhou",
            "Hai Jiang",
            "Lang Nie",
            "Shuaicheng Liu"
        ],
        "title": "StableMotion: Repurposing Diffusion-Based Image Priors for Motion Estimation",
        "abstract": "arXiv:2505.06668v1 Announce Type: new  Abstract: We present StableMotion, a novel framework leverages knowledge (geometry and content priors) from pretrained large-scale image diffusion models to perform motion estimation, solving single-image-based image rectification tasks such as Stitched Image Rectangling (SIR) and Rolling Shutter Correction (RSC). Specifically, StableMotion framework takes text-to-image Stable Diffusion (SD) models as backbone and repurposes it into an image-to-motion estimator. To mitigate inconsistent output produced by diffusion models, we propose Adaptive Ensemble Strategy (AES) that consolidates multiple outputs into a cohesive, high-fidelity result. Additionally, we present the concept of Sampling Steps Disaster (SSD), the counterintuitive scenario where increasing the number of sampling steps can lead to poorer outcomes, which enables our framework to achieve one-step inference. StableMotion is verified on two image rectification tasks and delivers state-of-the-art performance in both, as well as showing strong generalizability. Supported by SSD, StableMotion offers a speedup of 200 times compared to previous diffusion model-based methods.",
        "arxiv_id": "2505.06668",
        "ARXIVID": "2505.06668",
        "COMMENT": "Matches criterion 4 as it repurposes diffusion-based image priors for motion estimation, showcasing an application of vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.07622": {
        "authors": [
            "Zhuo Song",
            "Ye Zhang",
            "Kunhong Li",
            "Longguang Wang",
            "Yulan Guo"
        ],
        "title": "A Unified Hierarchical Framework for Fine-grained Cross-view Geo-localization over Large-scale Scenarios",
        "abstract": "arXiv:2505.07622v1 Announce Type: new  Abstract: Cross-view geo-localization is a promising solution for large-scale localization problems, requiring the sequential execution of retrieval and metric localization tasks to achieve fine-grained predictions. However, existing methods typically focus on designing standalone models for these two tasks, resulting in inefficient collaboration and increased training overhead. In this paper, we propose UnifyGeo, a novel unified hierarchical geo-localization framework that integrates retrieval and metric localization tasks into a single network. Specifically, we first employ a unified learning strategy with shared parameters to jointly learn multi-granularity representation, facilitating mutual reinforcement between these two tasks. Subsequently, we design a re-ranking mechanism guided by a dedicated loss function, which enhances geo-localization performance by improving both retrieval accuracy and metric localization references. Extensive experiments demonstrate that UnifyGeo significantly outperforms the state-of-the-arts in both task-isolated and task-associated settings. Remarkably, on the challenging VIGOR benchmark, which supports fine-grained localization evaluation, the 1-meter-level localization recall rate improves from 1.53\\% to 39.64\\% and from 0.43\\% to 25.58\\% under same-area and cross-area evaluations, respectively. Code will be made publicly available.",
        "arxiv_id": "2505.07622",
        "ARXIVID": "2505.07622",
        "COMMENT": "Matches criterion 3 as it proposes a unified hierarchical framework for fine-grained cross-view geo-localization, which is a novel method in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.07715": {
        "authors": [
            "Qi Xu",
            "Jie Deng",
            "Jiangrong Shen",
            "Biwu Chen",
            "Huajin Tang",
            "Gang Pan"
        ],
        "title": "Hybrid Spiking Vision Transformer for Object Detection with Event Cameras",
        "abstract": "arXiv:2505.07715v1 Announce Type: new  Abstract: Event-based object detection has gained increasing attention due to its advantages such as high temporal resolution, wide dynamic range, and asynchronous address-event representation. Leveraging these advantages, Spiking Neural Networks (SNNs) have emerged as a promising approach, offering low energy consumption and rich spatiotemporal dynamics. To further enhance the performance of event-based object detection, this study proposes a novel hybrid spike vision Transformer (HsVT) model. The HsVT model integrates a spatial feature extraction module to capture local and global features, and a temporal feature extraction module to model time dependencies and long-term patterns in event sequences. This combination enables HsVT to capture spatiotemporal features, improving its capability to handle complex event-based object detection tasks. To support research in this area, we developed and publicly released The Fall Detection Dataset as a benchmark for event-based object detection tasks. This dataset, captured using an event-based camera, ensures facial privacy protection and reduces memory usage due to the event representation format. We evaluated the HsVT model on GEN1 and Fall Detection datasets across various model sizes. Experimental results demonstrate that HsVT achieves significant performance improvements in event detection with fewer parameters.",
        "arxiv_id": "2505.07715",
        "ARXIVID": "2505.07715",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (Fall Detection Dataset) and a novel hybrid spiking vision transformer for event-based object detection.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.06328": {
        "authors": [
            "Felix Ocker",
            "J\\\"org Deigm\\\"oller",
            "Pavel Smirnov",
            "Julian Eggert"
        ],
        "title": "A Grounded Memory System For Smart Personal Assistants",
        "abstract": "arXiv:2505.06328v1 Announce Type: new  Abstract: A wide variety of agentic AI applications - ranging from cognitive assistants for dementia patients to robotics - demand a robust memory system grounded in reality. In this paper, we propose such a memory system consisting of three components. First, we combine Vision Language Models for image captioning and entity disambiguation with Large Language Models for consistent information extraction during perception. Second, the extracted information is represented in a memory consisting of a knowledge graph enhanced by vector embeddings to efficiently manage relational information. Third, we combine semantic search and graph query generation for question answering via Retrieval Augmented Generation. We illustrate the system's working and potential using a real-world example.",
        "arxiv_id": "2505.06328",
        "ARXIVID": "2505.06328",
        "COMMENT": "Matches criterion 1. Proposes a grounded memory system for smart personal assistants, which is relevant to spatial understanding and intelligence in embodied agents.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2505.07812": {
        "authors": [
            "Chenze Shao",
            "Fandong Meng",
            "Jie Zhou"
        ],
        "title": "Continuous Visual Autoregressive Generation via Score Maximization",
        "abstract": "arXiv:2505.07812v1 Announce Type: new  Abstract: Conventional wisdom suggests that autoregressive models are used to process discrete data. When applied to continuous modalities such as visual data, Visual AutoRegressive modeling (VAR) typically resorts to quantization-based approaches to cast the data into a discrete space, which can introduce significant information loss. To tackle this issue, we introduce a Continuous VAR framework that enables direct visual autoregressive generation without vector quantization. The underlying theoretical foundation is strictly proper scoring rules, which provide powerful statistical tools capable of evaluating how well a generative model approximates the true distribution. Within this framework, all we need is to select a strictly proper score and set it as the training objective to optimize. We primarily explore a class of training objectives based on the energy score, which is likelihood-free and thus overcomes the difficulty of making probabilistic predictions in the continuous space. Previous efforts on continuous autoregressive generation, such as GIVT and diffusion loss, can also be derived from our framework using other strictly proper scores. Source code: https://github.com/shaochenze/EAR.",
        "arxiv_id": "2505.07812",
        "ARXIVID": "2505.07812",
        "COMMENT": "Matches criterion 4. Proposes a novel framework for continuous visual autoregressive generation, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2505.07375": {
        "authors": [
            "Yuqi Cheng",
            "Yunkang Cao",
            "Dongfang Wang",
            "Weiming Shen",
            "Wenlong Li"
        ],
        "title": "Boosting Global-Local Feature Matching via Anomaly Synthesis for Multi-Class Point Cloud Anomaly Detection",
        "abstract": "arXiv:2505.07375v1 Announce Type: new  Abstract: Point cloud anomaly detection is essential for various industrial applications. The huge computation and storage costs caused by the increasing product classes limit the application of single-class unsupervised methods, necessitating the development of multi-class unsupervised methods. However, the feature similarity between normal and anomalous points from different class data leads to the feature confusion problem, which greatly hinders the performance of multi-class methods. Therefore, we introduce a multi-class point cloud anomaly detection method, named GLFM, leveraging global-local feature matching to progressively separate data that are prone to confusion across multiple classes. Specifically, GLFM is structured into three stages: Stage-I proposes an anomaly synthesis pipeline that stretches point clouds to create abundant anomaly data that are utilized to adapt the point cloud feature extractor for better feature representation. Stage-II establishes the global and local memory banks according to the global and local feature distributions of all the training data, weakening the impact of feature confusion on the establishment of the memory bank. Stage-III implements anomaly detection of test data leveraging its feature distance from global and local memory banks. Extensive experiments on the MVTec 3D-AD, Real3D-AD and actual industry parts dataset showcase our proposed GLFM's superior point cloud anomaly detection performance. The code is available at https://github.com/hustCYQ/GLFM-Multi-class-3DAD.",
        "arxiv_id": "2505.07375",
        "ARXIVID": "2505.07375",
        "COMMENT": "Matches criterion 3. Proposes a novel multi-class point cloud anomaly detection method with a focus on global-local feature matching, which is relevant to embodied AI and new methods.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.07019": {
        "authors": [
            "Khang Nguyen Quoc",
            "Lan Le Thi Thu",
            "Luyl-Da Quach"
        ],
        "title": "A Vision-Language Foundation Model for Leaf Disease Identification",
        "abstract": "arXiv:2505.07019v1 Announce Type: new  Abstract: Leaf disease identification plays a pivotal role in smart agriculture. However, many existing studies still struggle to integrate image and textual modalities to compensate for each other's limitations. Furthermore, many of these approaches rely on pretraining with constrained datasets such as ImageNet, which lack domain-specific information. We propose SCOLD (Soft-target COntrastive learning for Leaf Disease identification), a context-aware vision-language foundation model tailored to address these challenges for agricultural tasks. SCOLD is developed using a diverse corpus of plant leaf images and corresponding symptom descriptions, comprising over 186,000 image-caption pairs aligned with 97 unique concepts. Through task-agnostic pretraining, SCOLD leverages contextual soft targets to mitigate overconfidence in contrastive learning by smoothing labels, thereby improving model generalization and robustness on fine-grained classification tasks. Experimental results demonstrate that SCOLD outperforms existing vision-language models such as OpenAI-CLIP-L, BioCLIP, and SigLIP2 across several benchmarks, including zero-shot and few-shot classification, image-text retrieval, and image classification, while maintaining a competitive parameter footprint. Ablation studies further highlight SCOLD's effectiveness in contrast to its counterparts. The proposed approach significantly advances the agricultural vision-language foundation model, offering strong performance with minimal or no supervised fine-tuning. This work lays a solid groundwork for future research on models trained with long-form and simplified contexts, tasks involving class ambiguity, and multi-modal systems for intelligent plant disease diagnostics. The code for this study is available at https://huggingface.co/enalis/scold",
        "arxiv_id": "2505.07019",
        "ARXIVID": "2505.07019",
        "COMMENT": "This paper introduces a vision-language foundation model for leaf disease identification, which matches criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.07496": {
        "authors": [
            "Mohamed Ali Souibgui",
            "Changkyu Choi",
            "Andrey Barsky",
            "Kangsoo Jung",
            "Ernest Valveny",
            "Dimosthenis Karatzas"
        ],
        "title": "DocVXQA: Context-Aware Visual Explanations for Document Question Answering",
        "abstract": "arXiv:2505.07496v1 Announce Type: new  Abstract: We propose DocVXQA, a novel framework for visually self-explainable document question answering. The framework is designed not only to produce accurate answers to questions but also to learn visual heatmaps that highlight contextually critical regions, thereby offering interpretable justifications for the model's decisions. To integrate explanations into the learning process, we quantitatively formulate explainability principles as explicit learning objectives. Unlike conventional methods that emphasize only the regions pertinent to the answer, our framework delivers explanations that are \\textit{contextually sufficient} while remaining \\textit{representation-efficient}. This fosters user trust while achieving a balance between predictive performance and interpretability in DocVQA applications. Extensive experiments, including human evaluation, provide strong evidence supporting the effectiveness of our method. The code is available at https://github.com/dali92002/DocVXQA.",
        "arxiv_id": "2505.07496",
        "ARXIVID": "2505.07496",
        "COMMENT": "This paper partially matches criterion 4 as it introduces DocVXQA, a framework for visually explainable document question answering, which is related to vision-language models.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2505.06898": {
        "authors": [
            "Honglong Yang",
            "Shanshan Song",
            "Yi Qin",
            "Lehan Wang",
            "Haonan Wang",
            "Xinpeng Ding",
            "Qixiang Zhang",
            "Bodong Du",
            "Xiaomeng Li"
        ],
        "title": "Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration",
        "abstract": "arXiv:2505.06898v1 Announce Type: new  Abstract: Generalist Medical AI (GMAI) systems have demonstrated expert-level performance in biomedical perception tasks, yet their clinical utility remains limited by inadequate multi-modal explainability and suboptimal prognostic capabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI assistant that integrates textual and visual interpretability to support transparent and trustworthy medical decision-making. XMedGPT not only produces accurate diagnostic and descriptive outputs, but also grounds referenced anatomical sites within medical images, bridging critical gaps in interpretability and enhancing clinician usability. To support real-world deployment, we introduce a reliability indexing mechanism that quantifies uncertainty through consistency-based assessment via interactive question-answering. We validate XMedGPT across four pillars: multi-modal interpretability, uncertainty quantification, and prognostic modeling, and rigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical regions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between visual rationales and clinical outcomes. For uncertainty estimation, it attains an AUC of 0.862 on visual question answering and 0.764 on radiology report generation. In survival and recurrence prediction for lung and glioma cancers, it surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%. Rigorous benchmarking across 347 datasets covers 40 imaging modalities and external validation spans 4 anatomical systems confirming exceptional generalizability, with performance gains surpassing existing GMAI by 20.7% for in-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together, XMedGPT represents a significant leap forward in clinician-centric AI integration, offering trustworthy and scalable support for diverse healthcare applications.",
        "arxiv_id": "2505.06898",
        "ARXIVID": "2505.06898",
        "COMMENT": "This paper partially matches criterion 2 as it introduces XMedGPT, a multi-modal AI assistant with visual and textual interpretability, but it is focused on medical applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.07215": {
        "authors": [
            "Vivek Verma",
            "David Huang",
            "William Chen",
            "Dan Klein",
            "Nicholas Tomlin"
        ],
        "title": "Measuring General Intelligence with Generated Games",
        "abstract": "arXiv:2505.07215v1 Announce Type: new  Abstract: We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically generated by (1) using a large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as a Gym environment, and (3) training reinforcement learning (RL) agents via self-play on the generated games. We evaluate language models by their winrate against these RL agents by prompting models with the game description, current board state, and a list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as o1, o3-mini and DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark.",
        "arxiv_id": "2505.07215",
        "ARXIVID": "2505.07215",
        "COMMENT": "This paper partially matches criterion 3 as it introduces a new benchmark (gg-bench) for evaluating general reasoning in language models, but it is not specific to embodied AI.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2505.06516": {
        "authors": [
            "Yilin Dong",
            "Tianyun Zhu",
            "Xinde Li",
            "Jean Dezert",
            "Rigui Zhou",
            "Changming Zhu",
            "Lei Cao",
            "Shuzhi Sam Ge"
        ],
        "title": "Quantum Conflict Measurement in Decision Making for Out-of-Distribution Detection",
        "abstract": "arXiv:2505.06516v1 Announce Type: new  Abstract: Quantum Dempster-Shafer Theory (QDST) uses quantum interference effects to derive a quantum mass function (QMF) as a fuzzy metric type from information obtained from various data sources. In addition, QDST uses quantum parallel computing to speed up computation. Nevertheless, the effective management of conflicts between multiple QMFs in QDST is a challenging question. This work aims to address this problem by proposing a Quantum Conflict Indicator (QCI) that measures the conflict between two QMFs in decision-making. Then, the properties of the QCI are carefully investigated. The obtained results validate its compliance with desirable conflict measurement properties such as non-negativity, symmetry, boundedness, extreme consistency and insensitivity to refinement. We then apply the proposed QCI in conflict fusion methods and compare its performance with several commonly used fusion approaches. This comparison demonstrates the superiority of the QCI-based conflict fusion method. Moreover, the Class Description Domain Space (C-DDS) and its optimized version, C-DDS+ by utilizing the QCI-based fusion method, are proposed to address the Out-of-Distribution (OOD) detection task. The experimental results show that the proposed approach gives better OOD performance with respect to several state-of-the-art baseline OOD detection methods. Specifically, it achieves an average increase in Area Under the Receiver Operating Characteristic Curve (AUC) of 1.2% and a corresponding average decrease in False Positive Rate at 95% True Negative Rate (FPR95) of 5.4% compared to the optimal baseline method.",
        "arxiv_id": "2505.06516",
        "ARXIVID": "2505.06516",
        "COMMENT": "This paper does not match any of the specific criteria. It focuses on quantum conflict measurement and out-of-distribution detection, which is not directly related to spatial understanding, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.07773": {
        "authors": [
            "Xinji Mai",
            "Haotian Xu",
            "Xing W",
            "Weinong Wang",
            "Yingying Zhang",
            "Wenqiang Zhang"
        ],
        "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving",
        "abstract": "arXiv:2505.07773v1 Announce Type: new  Abstract: Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \\href{https://github.com/Anonymize-Author/AgentRL}{https://github.com/Anonymize-Author/AgentRL}.",
        "arxiv_id": "2505.07773",
        "ARXIVID": "2505.07773",
        "COMMENT": "Does not match any specific criterion but explores reinforcement learning for mathematical problem solving, which is tangentially related to general AI interests.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.06903": {
        "authors": [
            "Yuanzhuo Wang",
            "Junwen Duan",
            "Xinyu Li",
            "Jianxin Wang"
        ],
        "title": "CheXLearner: Text-Guided Fine-Grained Representation Learning for Progression Detection",
        "abstract": "arXiv:2505.06903v1 Announce Type: new  Abstract: Temporal medical image analysis is essential for clinical decision-making, yet existing methods either align images and text at a coarse level - causing potential semantic mismatches - or depend solely on visual information, lacking medical semantic integration. We present CheXLearner, the first end-to-end framework that unifies anatomical region detection, Riemannian manifold-based structure alignment, and fine-grained regional semantic guidance. Our proposed Med-Manifold Alignment Module (Med-MAM) leverages hyperbolic geometry to robustly align anatomical structures and capture pathologically meaningful discrepancies across temporal chest X-rays. By introducing regional progression descriptions as supervision, CheXLearner achieves enhanced cross-modal representation learning and supports dynamic low-level feature optimization. Experiments show that CheXLearner achieves 81.12% (+17.2%) average accuracy and 80.32% (+11.05%) F1-score on anatomical region progression detection - substantially outperforming state-of-the-art baselines, especially in structurally complex regions. Additionally, our model attains a 91.52% average AUC score in downstream disease classification, validating its superior feature representation.",
        "arxiv_id": "2505.06903",
        "ARXIVID": "2505.06903",
        "COMMENT": "This paper introduces a novel framework for temporal medical image analysis with cross-modal representation learning, which is tangentially related to vision-language models but does not directly match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.06436": {
        "authors": [
            "Jingrui He",
            "Andrew Stephen McGough"
        ],
        "title": "My Emotion on your face: The use of Facial Keypoint Detection to preserve Emotions in Latent Space Editing",
        "abstract": "arXiv:2505.06436v1 Announce Type: new  Abstract: Generative Adversarial Network approaches such as StyleGAN/2 provide two key benefits: the ability to generate photo-realistic face images and possessing a semantically structured latent space from which these images are created. Many approaches have emerged for editing images derived from vectors in the latent space of a pre-trained StyleGAN/2 models by identifying semantically meaningful directions (e.g., gender or age) in the latent space. By moving the vector in a specific direction, the ideal result would only change the target feature while preserving all the other features. Providing an ideal data augmentation approach for gesture research as it could be used to generate numerous image variations whilst keeping the facial expressions intact. However, entanglement issues, where changing one feature inevitably affects other features, impacts the ability to preserve facial expressions. To address this, we propose the use of an addition to the loss function of a Facial Keypoint Detection model to restrict changes to the facial expressions. Building on top of an existing model, adding the proposed Human Face Landmark Detection (HFLD) loss, provided by a pre-trained Facial Keypoint Detection model, to the original loss function. We quantitatively and qualitatively evaluate the existing and our extended model, showing the effectiveness of our approach in addressing the entanglement issue and maintaining the facial expression. Our approach achieves up to 49% reduction in the change of emotion in our experiments. Moreover, we show the benefit of our approach by comparing with state-of-the-art models. By increasing the ability to preserve the facial gesture and expression during facial transformation, we present a way to create human face images with fixed expression but different appearances, making it a reliable data augmentation approach for Facial Gesture and Expression research.",
        "arxiv_id": "2505.06436",
        "ARXIVID": "2505.06436",
        "COMMENT": "This paper does not match any of the specific criteria. It focuses on latent space editing in StyleGAN/2 for preserving facial expressions, which is not directly related to spatial understanding, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.07336": {
        "authors": [
            "Zhixuan Zhang",
            "Xiaopeng Li",
            "Qi Liu"
        ],
        "title": "SAEN-BGS: Energy-Efficient Spiking AutoEncoder Network for Background Subtraction",
        "abstract": "arXiv:2505.07336v1 Announce Type: new  Abstract: Background subtraction (BGS) is utilized to detect moving objects in a video and is commonly employed at the onset of object tracking and human recognition processes. Nevertheless, existing BGS techniques utilizing deep learning still encounter challenges with various background noises in videos, including variations in lighting, shifts in camera angles, and disturbances like air turbulence or swaying trees. To address this problem, we design a spiking autoencoder network, termed SAEN-BGS, based on noise resilience and time-sequence sensitivity of spiking neural networks (SNNs) to enhance the separation of foreground and background. To eliminate unnecessary background noise and preserve the important foreground elements, we begin by creating the continuous spiking conv-and-dconv block, which serves as the fundamental building block for the decoder in SAEN-BGS. Moreover, in striving for enhanced energy efficiency, we introduce a novel self-distillation spiking supervised learning method grounded in ANN-to-SNN frameworks, resulting in decreased power consumption. In extensive experiments conducted on CDnet-2014 and DAVIS-2016 datasets, our approach demonstrates superior segmentation performance relative to other baseline methods, even when challenged by complex scenarios with dynamic backgrounds.",
        "arxiv_id": "2505.07336",
        "ARXIVID": "2505.07336",
        "COMMENT": "This paper does not match any of the specific criteria. It focuses on spiking neural networks for background subtraction, which is not directly related to spatial understanding, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.07073": {
        "authors": [
            "Payal Varshney",
            "Adriano Lucieri",
            "Christoph Balada",
            "Andreas Dengel",
            "Sheraz Ahmed"
        ],
        "title": "Discovering Concept Directions from Diffusion-based Counterfactuals via Latent Clustering",
        "abstract": "arXiv:2505.07073v1 Announce Type: new  Abstract: Concept-based explanations have emerged as an effective approach within Explainable Artificial Intelligence, enabling interpretable insights by aligning model decisions with human-understandable concepts. However, existing methods rely on computationally intensive procedures and struggle to efficiently capture complex, semantic concepts. Recently, the Concept Discovery through Latent Diffusion-based Counterfactual Trajectories (CDCT) framework, introduced by Varshney et al. (2025), attempts to identify concepts via dimension-wise traversal of the latent space of a Variational Autoencoder trained on counterfactual trajectories. Extending the CDCT framework, this work introduces Concept Directions via Latent Clustering (CDLC), which extracts global, class-specific concept directions by clustering latent difference vectors derived from factual and diffusion-generated counterfactual image pairs. CDLC substantially reduces computational complexity by eliminating the exhaustive latent dimension traversal required in CDCT and enables the extraction of multidimensional semantic concepts encoded across the latent dimensions. This approach is validated on a real-world skin lesion dataset, demonstrating that the extracted concept directions align with clinically recognized dermoscopic features and, in some cases, reveal dataset-specific biases or unknown biomarkers. These results highlight that CDLC is interpretable, scalable, and applicable across high-stakes domains and diverse data modalities.",
        "arxiv_id": "2505.07073",
        "ARXIVID": "2505.07073",
        "COMMENT": "Does not match any specific criteria. Focuses on concept-based explanations and latent clustering, which is not directly related to the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.07049": {
        "authors": [
            "Yubo Shu",
            "Zhewei Huang",
            "Xin Wu",
            "Chen Hu",
            "Shuchang Zhou",
            "Daxin Jiang"
        ],
        "title": "DialogueReason: Rule-Based RL Sparks Dialogue Reasoning in LLMs",
        "abstract": "arXiv:2505.07049v1 Announce Type: new  Abstract: We propose DialogueReason, a reasoning paradigm that uncovers the lost roles in monologue-style reasoning models, aiming to boost diversity and coherency of the reasoning process. Recent advances in RL-based large reasoning models have led to impressive long CoT capabilities and high performance on math and science benchmarks. However, these reasoning models rely mainly on monologue-style reasoning, which often limits reasoning diversity and coherency, frequently recycling fixed strategies or exhibiting unnecessary shifts in attention. Our work consists of an analysis of monologue reasoning patterns and the development of a dialogue-based reasoning approach. We first introduce the Compound-QA task, which concatenates multiple problems into a single prompt to assess both diversity and coherency of reasoning. Our analysis shows that Compound-QA exposes weaknesses in monologue reasoning, evidenced by both quantitative metrics and qualitative reasoning traces. Building on the analysis, we propose a dialogue-based reasoning, named DialogueReason, structured around agents, environment, and interactions. Using PPO with rule-based rewards, we train open-source LLMs (Qwen-QWQ and Qwen-Base) to adopt dialogue reasoning. We evaluate trained models on MATH, AIME, and GPQA datasets, showing that the dialogue reasoning model outperforms monologue models under more complex compound questions. Additionally, we discuss how dialogue-based reasoning helps enhance interpretability, facilitate more intuitive human interaction, and inspire advances in multi-agent system design.",
        "arxiv_id": "2505.07049",
        "ARXIVID": "2505.07049",
        "COMMENT": "Does not match any specific criteria. Focuses on dialogue-based reasoning in LLMs, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.06905": {
        "authors": [
            "Jian Song",
            "Hongruixuan Chen",
            "Naoto Yokoya"
        ],
        "title": "Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction",
        "abstract": "arXiv:2505.06905v1 Announce Type: new  Abstract: Monocular height estimation (MHE) from very-high-resolution (VHR) remote sensing imagery via deep learning is notoriously challenging due to the lack of sufficient structural information. Conventional digital elevation models (DEMs), typically derived from airborne LiDAR or multi-view stereo, remain costly and geographically limited. Recently, models trained on synthetic data and refined through domain adaptation have shown remarkable performance in MHE, yet it remains unclear how these models make predictions or how reliable they truly are. In this paper, we investigate a state-of-the-art MHE model trained purely on synthetic data to explore where the model looks when making height predictions. Through systematic analyses, we find that the model relies heavily on shadow cues, a factor that can lead to overestimation or underestimation of heights when shadows deviate from expected norms. Furthermore, the inherent difficulty of evaluating regression tasks with the human eye underscores additional limitations of purely synthetic training. To address these issues, we propose a novel correction pipeline that integrates sparse, imperfect global LiDAR measurements (ICESat-2) with deep-learning outputs to improve local accuracy and achieve spatially consistent corrections. Our method comprises two stages: pre-processing raw ICESat-2 data, followed by a random forest-based approach to densely refine height estimates. Experiments in three representative urban regions -- Saint-Omer, Tokyo, and Sao Paulo -- reveal substantial error reductions, with mean absolute error (MAE) decreased by 22.8\\%, 6.9\\%, and 4.9\\%, respectively. These findings highlight the critical role of shadow awareness in synthetic data-driven models and demonstrate how fusing imperfect real-world LiDAR data can bolster the robustness of MHE, paving the way for more reliable and scalable 3D mapping solutions.",
        "arxiv_id": "2505.06905",
        "ARXIVID": "2505.06905",
        "COMMENT": "Does not match any specific criteria. Focuses on monocular height estimation and LiDAR-guided correction, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.06920": {
        "authors": [
            "Timing Li",
            "Bing Cao",
            "Pengfei Zhu",
            "Bin Xiao",
            "Qinghua Hu"
        ],
        "title": "Bi-directional Self-Registration for Misaligned Infrared-Visible Image Fusion",
        "abstract": "arXiv:2505.06920v1 Announce Type: new  Abstract: Acquiring accurately aligned multi-modal image pairs is fundamental for achieving high-quality multi-modal image fusion. To address the lack of ground truth in current multi-modal image registration and fusion methods, we propose a novel self-supervised \\textbf{B}i-directional \\textbf{S}elf-\\textbf{R}egistration framework (\\textbf{B-SR}). Specifically, B-SR utilizes a proxy data generator (PDG) and an inverse proxy data generator (IPDG) to achieve self-supervised global-local registration. Visible-infrared image pairs with spatially misaligned differences are aligned to obtain global differences through the registration module. The same image pairs are processed by PDG, such as cropping, flipping, stitching, etc., and then aligned to obtain local differences. IPDG converts the obtained local differences into pseudo-global differences, which are used to perform global-local difference consistency with the global differences. Furthermore, aiming at eliminating the effect of modal gaps on the registration module, we design a neighborhood dynamic alignment loss to achieve cross-modal image edge alignment. Extensive experiments on misaligned multi-modal images demonstrate the effectiveness of the proposed method in multi-modal image alignment and fusion against the competing methods. Our code will be publicly available.",
        "arxiv_id": "2505.06920",
        "ARXIVID": "2505.06920",
        "COMMENT": "Does not match any specific criteria. Focuses on multi-modal image registration and fusion, which is not directly related to spatial intelligence, VLLMs/MLLMs, embodied AI benchmarks, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.06948": {
        "authors": [
            "Pan Du",
            "Wangbo Zhao",
            "Xinai Lu",
            "Nian Liu",
            "Zhikai Li",
            "Chaoyu Gong",
            "Suyun Zhao",
            "Hong Chen",
            "Cuiping Li",
            "Kai Wang",
            "Yang You"
        ],
        "title": "Unsupervised Learning for Class Distribution Mismatch",
        "abstract": "arXiv:2505.06948v1 Announce Type: new  Abstract: Class distribution mismatch (CDM) refers to the discrepancy between class distributions in training data and target tasks. Previous methods address this by designing classifiers to categorize classes known during training, while grouping unknown or new classes into an \"other\" category. However, they focus on semi-supervised scenarios and heavily rely on labeled data, limiting their applicability and performance. To address this, we propose Unsupervised Learning for Class Distribution Mismatch (UCDM), which constructs positive-negative pairs from unlabeled data for classifier training. Our approach randomly samples images and uses a diffusion model to add or erase semantic classes, synthesizing diverse training pairs. Additionally, we introduce a confidence-based labeling mechanism that iteratively assigns pseudo-labels to valuable real-world data and incorporates them into the training process. Extensive experiments on three datasets demonstrate UCDM's superiority over previous semi-supervised methods. Specifically, with a 60% mismatch proportion on Tiny-ImageNet dataset, our approach, without relying on labeled data, surpasses OpenMatch (with 40 labels per class) by 35.1%, 63.7%, and 72.5% in classifying known, unknown, and new classes.",
        "arxiv_id": "2505.06948",
        "ARXIVID": "2505.06948",
        "COMMENT": "Does not match any specific criterion but explores unsupervised learning for class distribution mismatch, which is tangentially related to general machine learning interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.07087": {
        "authors": [
            "Robert E. Wray",
            "James R. Kirk",
            "John E. Laird"
        ],
        "title": "Architectural Precedents for General Agents using Large Language Models",
        "abstract": "arXiv:2505.07087v1 Announce Type: new  Abstract: One goal of AI (and AGI) is to identify and understand specific mechanisms and representations sufficient for general intelligence. Often, this work manifests in research focused on architectures and many cognitive architectures have been explored in AI/AGI. However, different research groups and even different research traditions have somewhat independently identified similar/common patterns of processes and representations or cognitive design patterns that are manifest in existing architectures. Today, AI systems exploiting large language models (LLMs) offer a relatively new combination of mechanism and representation available for exploring the possibilities of general intelligence. In this paper, we summarize a few recurring cognitive design patterns that have appeared in various pre-transformer AI architectures. We then explore how these patterns are evident in systems using LLMs, especially for reasoning and interactive (\"agentic\") use cases. By examining and applying these recurring patterns, we can also predict gaps or deficiencies in today's Agentic LLM Systems and identify likely subjects of future research towards general intelligence using LLMs and other generative foundation models.",
        "arxiv_id": "2505.07087",
        "ARXIVID": "2505.07087",
        "COMMENT": "This paper explores cognitive design patterns in LLM-based systems, which is tangentially related to vision-language models but does not directly match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.06907": {
        "authors": [
            "Yu Qiao",
            "Huy Q. Le",
            "Avi Deb Raha",
            "Phuong-Nam Tran",
            "Apurba Adhikary",
            "Mengchun Zhang",
            "Loc X. Nguyen",
            "Eui-Nam Huh",
            "Dusit Niyato",
            "Choong Seon Hong"
        ],
        "title": "Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence",
        "abstract": "arXiv:2505.06907v1 Announce Type: new  Abstract: The rise of large language models (LLMs), such as ChatGPT, DeepSeek, and Grok-3, has reshaped the artificial intelligence landscape. As prominent examples of foundational models (FMs) built on LLMs, these models exhibit remarkable capabilities in generating human-like content, bringing us closer to achieving artificial general intelligence (AGI). However, their large-scale nature, sensitivity to privacy concerns, and substantial computational demands present significant challenges to personalized customization for end users. To bridge this gap, this paper presents the vision of artificial personalized intelligence (API), focusing on adapting these powerful models to meet the specific needs and preferences of users while maintaining privacy and efficiency. Specifically, this paper proposes personalized federated intelligence (PFI), which integrates the privacy-preserving advantages of federated learning (FL) with the zero-shot generalization capabilities of FMs, enabling personalized, efficient, and privacy-protective deployment at the edge. We first review recent advances in both FL and FMs, and discuss the potential of leveraging FMs to enhance federated systems. We then present the key motivations behind realizing PFI and explore promising opportunities in this space, including efficient PFI, trustworthy PFI, and PFI empowered by retrieval-augmented generation (RAG). Finally, we outline key challenges and future research directions for deploying FM-powered FL systems at the edge with improved personalization, computational efficiency, and privacy guarantees. Overall, this survey aims to lay the groundwork for the development of API as a complement to AGI, with a particular focus on PFI as a key enabling technique.",
        "arxiv_id": "2505.06907",
        "ARXIVID": "2505.06907",
        "COMMENT": "This paper discusses foundation models and their adaptation for personalized federated intelligence, which is tangentially related to vision foundation models but does not directly match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.06937": {
        "authors": [
            "Fei Zhou",
            "Yi Li",
            "Mingqing Zhu"
        ],
        "title": "Transformer-Based Dual-Optical Attention Fusion Crowd Head Point Counting and Localization Network",
        "abstract": "arXiv:2505.06937v1 Announce Type: new  Abstract: In this paper, the dual-optical attention fusion crowd head point counting model (TAPNet) is proposed to address the problem of the difficulty of accurate counting in complex scenes such as crowd dense occlusion and low light in crowd counting tasks under UAV view. The model designs a dual-optical attention fusion module (DAFP) by introducing complementary information from infrared images to improve the accuracy and robustness of all-day crowd counting. In order to fully utilize different modal information and solve the problem of inaccurate localization caused by systematic misalignment between image pairs, this paper also proposes an adaptive two-optical feature decomposition fusion module (AFDF). In addition, we optimize the training strategy to improve the model robustness through spatial random offset data augmentation. Experiments on two challenging public datasets, DroneRGBT and GAIIC2, show that the proposed method outperforms existing techniques in terms of performance, especially in challenging dense low-light scenes. Code is available at https://github.com/zz-zik/TAPNet",
        "arxiv_id": "2505.06937",
        "ARXIVID": "2505.06937",
        "COMMENT": "Does not match any specific criteria but focuses on a vision-based task (crowd counting) with transformer-based methods, which is tangentially relevant to your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.06769": {
        "authors": [
            "Krishnendu Chatterjee",
            "Mahdi JafariRaviz",
            "Raimundo Saona",
            "Jakub Svoboda"
        ],
        "title": "Value Iteration with Guessing for Markov Chains and Markov Decision Processes",
        "abstract": "arXiv:2505.06769v1 Announce Type: new  Abstract: Two standard models for probabilistic systems are Markov chains (MCs) and Markov decision processes (MDPs). Classic objectives for such probabilistic models for control and planning problems are reachability and stochastic shortest path. The widely studied algorithmic approach for these problems is the Value Iteration (VI) algorithm which iteratively applies local updates called Bellman updates. There are many practical approaches for VI in the literature but they all require exponentially many Bellman updates for MCs in the worst case. A preprocessing step is an algorithm that is discrete, graph-theoretical, and requires linear space. An important open question is whether, after a polynomial-time preprocessing, VI can be achieved with sub-exponentially many Bellman updates. In this work, we present a new approach for VI based on guessing values. Our theoretical contributions are twofold. First, for MCs, we present an almost-linear-time preprocessing algorithm after which, along with guessing values, VI requires only subexponentially many Bellman updates. Second, we present an improved analysis of the speed of convergence of VI for MDPs. Finally, we present a practical algorithm for MDPs based on our new approach. Experimental results show that our approach provides a considerable improvement over existing VI-based approaches on several benchmark examples from the literature.",
        "arxiv_id": "2505.06769",
        "ARXIVID": "2505.06769",
        "COMMENT": "Does not match any specific criteria but is related to algorithmic improvements in Markov decision processes, which is tangentially relevant to your friend's interest in embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.07374": {
        "authors": [
            "Zhiye Xie",
            "Enmei Tu",
            "Xianping Fu",
            "Guoliang Yuan",
            "Yi Han"
        ],
        "title": "AIS Data-Driven Maritime Monitoring Based on Transformer: A Comprehensive Review",
        "abstract": "arXiv:2505.07374v1 Announce Type: new  Abstract: With the increasing demands for safety, efficiency, and sustainability in global shipping, Automatic Identification System (AIS) data plays an increasingly important role in maritime monitoring. AIS data contains spatial-temporal variation patterns of vessels that hold significant research value in the marine domain. However, due to its massive scale, the full potential of AIS data has long remained untapped. With its powerful sequence modeling capabilities, particularly its ability to capture long-range dependencies and complex temporal dynamics, the Transformer model has emerged as an effective tool for processing AIS data. Therefore, this paper reviews the research on Transformer-based AIS data-driven maritime monitoring, providing a comprehensive overview of the current applications of Transformer models in the marine field. The focus is on Transformer-based trajectory prediction methods, behavior detection, and prediction techniques. Additionally, this paper collects and organizes publicly available AIS datasets from the reviewed papers, performing data filtering, cleaning, and statistical analysis. The statistical results reveal the operational characteristics of different vessel types, providing data support for further research on maritime monitoring tasks. Finally, we offer valuable suggestions for future research, identifying two promising research directions. Datasets are available at https://github.com/eyesofworld/Maritime-Monitoring.",
        "arxiv_id": "2505.07374",
        "ARXIVID": "2505.07374",
        "COMMENT": "Does not match any specific criteria but reviews transformer-based methods for maritime monitoring, which is tangentially relevant to your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.07552": {
        "authors": [
            "Efe Bozkir",
            "Christian Kosel",
            "Tina Seidel",
            "Enkelejda Kasneci"
        ],
        "title": "Automated Visual Attention Detection using Mobile Eye Tracking in Behavioral Classroom Studies",
        "abstract": "arXiv:2505.07552v1 Announce Type: new  Abstract: Teachers' visual attention and its distribution across the students in classrooms can constitute important implications for student engagement, achievement, and professional teacher training. Despite that, inferring the information about where and which student teachers focus on is not trivial. Mobile eye tracking can provide vital help to solve this issue; however, the use of mobile eye tracking alone requires a significant amount of manual annotations. To address this limitation, we present an automated processing pipeline concept that requires minimal manually annotated data to recognize which student the teachers focus on. To this end, we utilize state-of-the-art face detection models and face recognition feature embeddings to train face recognition models with transfer learning in the classroom context and combine these models with the teachers' gaze from mobile eye trackers. We evaluated our approach with data collected from four different classrooms, and our results show that while it is possible to estimate the visually focused students with reasonable performance in all of our classroom setups, U-shaped and small classrooms led to the best results with accuracies of approximately 0.7 and 0.9, respectively. While we did not evaluate our method for teacher-student interactions and focused on the validity of the technical approach, as our methodology does not require a vast amount of manually annotated data and offers a non-intrusive way of handling teachers' visual attention, it could help improve instructional strategies, enhance classroom management, and provide feedback for professional teacher development.",
        "arxiv_id": "2505.07552",
        "ARXIVID": "2505.07552",
        "COMMENT": "Does not match any specific criteria but is related to computer vision applications in education, which is tangentially relevant to your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}