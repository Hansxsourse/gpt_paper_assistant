{
    "2507.13942": {
        "authors": [
            "Jacob C Walker",
            "Pedro V\\'elez",
            "Luisa Polania Cabrera",
            "Guangyao Zhou",
            "Rishabh Kabra",
            "Carl Doersch",
            "Maks Ovsjanikov",
            "Jo\\~ao Carreira",
            "Shiry Ginosar"
        ],
        "title": "Generalist Forecasting with Frozen Video Models via Latent Diffusion",
        "abstract": "arXiv:2507.13942v1 Announce Type: new  Abstract: Forecasting what will happen next is a critical skill for general-purpose systems that plan or act in the world at different levels of abstraction. In this paper, we identify a strong correlation between a vision model's perceptual ability and its generalist forecasting performance over short time horizons. This trend holds across a diverse set of pretrained models-including those trained generatively-and across multiple levels of abstraction, from raw pixels to depth, point tracks, and object motion. The result is made possible by a novel generalist forecasting framework that operates on any frozen vision backbone: we train latent diffusion models to forecast future features in the frozen representation space, which are then decoded via lightweight, task-specific readouts. To enable consistent evaluation across tasks, we introduce distributional metrics that compare distributional properties directly in the space of downstream tasks and apply this framework to nine models and four tasks. Our results highlight the value of bridging representation learning and generative modeling for temporally grounded video understanding.",
        "arxiv_id": "2507.13942",
        "ARXIVID": "2507.13942",
        "COMMENT": "Matches criterion 2: Unified diffusion models for multiple vision tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2507.13934": {
        "authors": [
            "Marzieh Gheisari",
            "Auguste Genovesio"
        ],
        "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization",
        "abstract": "arXiv:2507.13934v1 Announce Type: new  Abstract: Unsupervised disentanglement of static appearance and dynamic motion in video remains a fundamental challenge, often hindered by information leakage and blurry reconstructions in existing VAE- and GAN-based approaches. We introduce DiViD, the first end-to-end video diffusion framework for explicit static-dynamic factorization. DiViD's sequence encoder extracts a global static token from the first frame and per-frame dynamic tokens, explicitly removing static content from the motion code. Its conditional DDPM decoder incorporates three key inductive biases: a shared-noise schedule for temporal consistency, a time-varying KL-based bottleneck that tightens at early timesteps (compressing static information) and relaxes later (enriching dynamics), and cross-attention that routes the global static token to all frames while keeping dynamic tokens frame-specific. An orthogonality regularizer further prevents residual static-dynamic leakage. We evaluate DiViD on real-world benchmarks using swap-based accuracy and cross-leakage metrics. DiViD outperforms state-of-the-art sequential disentanglement methods: it achieves the highest swap-based joint accuracy, preserves static fidelity while improving dynamic transfer, and reduces average cross-leakage.",
        "arxiv_id": "2507.13934",
        "ARXIVID": "2507.13934",
        "COMMENT": "Matches criterion 2: Unified diffusion models for multiple vision tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    }
}