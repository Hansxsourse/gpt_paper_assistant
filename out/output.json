{
    "2502.13923": {
        "authors": [
            "Shuai Bai",
            "Keqin Chen",
            "Xuejing Liu",
            "Jialin Wang",
            "Wenbin Ge",
            "Sibo Song",
            "Kai Dang",
            "Peng Wang",
            "Shijie Wang",
            "Jun Tang",
            "Humen Zhong",
            "Yuanzhi Zhu",
            "Mingkun Yang",
            "Zhaohai Li",
            "Jianqiang Wan",
            "Pengfei Wang",
            "Wei Ding",
            "Zheren Fu",
            "Yiheng Xu",
            "Jiabo Ye",
            "Xi Zhang",
            "Tianbao Xie",
            "Zesen Cheng",
            "Hang Zhang",
            "Zhibo Yang",
            "Haiyang Xu",
            "Junyang Lin"
        ],
        "title": "Qwen2.5-VL Technical Report",
        "abstract": "arXiv:2502.13923v1 Announce Type: new  Abstract: We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language series, which demonstrates significant advancements in both foundational capabilities and innovative functionalities. Qwen2.5-VL achieves a major leap forward in understanding and interacting with the world through enhanced visual recognition, precise object localization, robust document parsing, and long-video comprehension. A standout feature of Qwen2.5-VL is its ability to localize objects using bounding boxes or points accurately. It provides robust structured data extraction from invoices, forms, and tables, as well as detailed analysis of charts, diagrams, and layouts. To handle complex inputs, Qwen2.5-VL introduces dynamic resolution processing and absolute time encoding, enabling it to process images of varying sizes and videos of extended durations (up to hours) with second-level event localization. This allows the model to natively perceive spatial scales and temporal dynamics without relying on traditional normalization techniques. By training a native dynamic-resolution Vision Transformer (ViT) from scratch and incorporating Window Attention, we reduce computational overhead while maintaining native resolution. As a result, Qwen2.5-VL excels not only in static image and document understanding but also as an interactive visual agent capable of reasoning, tool usage, and task execution in real-world scenarios such as operating computers and mobile devices. Qwen2.5-VL is available in three sizes, addressing diverse use cases from edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model matches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly excelling in document and diagram understanding. Additionally, Qwen2.5-VL maintains robust linguistic performance, preserving the core language competencies of the Qwen2.5 LLM.",
        "arxiv_id": "2502.13923",
        "ARXIVID": "2502.13923",
        "COMMENT": "Matches criterion 2 as it introduces Qwen2.5-VL, a new visual large language model with advanced capabilities in visual recognition and multi-modal tasks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2502.13430": {
        "authors": [
            "Hao Ma",
            "Shijie Wang",
            "Zhiqiang Pu",
            "Siyao Zhao",
            "Xiaolin Ai"
        ],
        "title": "Vision-Based Generic Potential Function for Policy Alignment in Multi-Agent Reinforcement Learning",
        "abstract": "arXiv:2502.13430v1 Announce Type: new  Abstract: Guiding the policy of multi-agent reinforcement learning to align with human common sense is a difficult problem, largely due to the complexity of modeling common sense as a reward, especially in complex and long-horizon multi-agent tasks. Recent works have shown the effectiveness of reward shaping, such as potential-based rewards, to enhance policy alignment. The existing works, however, primarily rely on experts to design rule-based rewards, which are often labor-intensive and lack a high-level semantic understanding of common sense. To solve this problem, we propose a hierarchical vision-based reward shaping method. At the bottom layer, a visual-language model (VLM) serves as a generic potential function, guiding the policy to align with human common sense through its intrinsic semantic understanding. To help the policy adapts to uncertainty and changes in long-horizon tasks, the top layer features an adaptive skill selection module based on a visual large language model (vLLM). The module uses instructions, video replays, and training records to dynamically select suitable potential function from a pre-designed pool. Besides, our method is theoretically proven to preserve the optimal policy. Extensive experiments conducted in the Google Research Football environment demonstrate that our method not only achieves a higher win rate but also effectively aligns the policy with human common sense.",
        "arxiv_id": "2502.13430",
        "ARXIVID": "2502.13430",
        "COMMENT": "Matches criterion 1 and 3 as it proposes a novel hierarchical vision-based reward shaping method for multi-agent reinforcement learning, leveraging visual-language models for spatial understanding and policy alignment.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2502.13928": {
        "authors": [
            "Shengguang Wu",
            "Fan-Yun Sun",
            "Kaiyue Wen",
            "Nick Haber"
        ],
        "title": "Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images",
        "abstract": "arXiv:2502.13928v1 Announce Type: new  Abstract: Recent studies have shown that Large Vision-Language Models (VLMs) tend to neglect image content and over-rely on language-model priors, resulting in errors in visually grounded tasks and hallucinations. We hypothesize that this issue arises because existing VLMs are not explicitly trained to generate texts that are accurately grounded in fine-grained image details. To enhance visual feedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive Optimization), a novel finetuning objective that steers the model toward capturing important visual details and aligning them with corresponding text tokens. To further facilitate this detailed alignment, we introduce MVC, a paired image-text dataset built by automatically filtering and augmenting visual counterfactual data to challenge the model with hard contrastive cases involving Minimal Visual Contrasts. Experiments show that our method consistently improves VLM performance across diverse benchmarks covering various abilities and domains, achieving up to a 22% reduction in hallucinations, and significant gains in vision-centric and general tasks. Notably, these improvements become increasingly pronounced in benchmarks with higher visual dependency. In short, S-VCO offers a significant enhancement of VLM's visually-dependent task performance while retaining or even improving the model's general abilities. We opensource our code at https://s-vco.github.io/",
        "arxiv_id": "2502.13928",
        "ARXIVID": "2502.13928",
        "COMMENT": "This paper matches criterion 2 as it proposes a novel finetuning objective (S-VCO) to improve visual grounding in Vision-Language Models (VLMs) and addresses hallucination issues. It also introduces a new dataset (MVC) for challenging visual contrastive cases.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.13967": {
        "authors": [
            "Roman Bachmann",
            "Jesse Allardice",
            "David Mizrahi",
            "Enrico Fini",
            "O\\u{g}uzhan Fatih Kar",
            "Elmira Amirloo",
            "Alaaeldin El-Nouby",
            "Amir Zamir",
            "Afshin Dehghan"
        ],
        "title": "FlexTok: Resampling Images into 1D Token Sequences of Flexible Length",
        "abstract": "arXiv:2502.13967v1 Announce Type: new  Abstract: Image tokenization has enabled major advances in autoregressive image generation by providing compressed, discrete representations that are more efficient to process than raw pixels. While traditional approaches use 2D grid tokenization, recent methods like TiTok have shown that 1D tokenization can achieve high generation quality by eliminating grid redundancies. However, these methods typically use a fixed number of tokens and thus cannot adapt to an image's inherent complexity. We introduce FlexTok, a tokenizer that projects 2D images into variable-length, ordered 1D token sequences. For example, a 256x256 image can be resampled into anywhere from 1 to 256 discrete tokens, hierarchically and semantically compressing its information. By training a rectified flow model as the decoder and using nested dropout, FlexTok produces plausible reconstructions regardless of the chosen token sequence length. We evaluate our approach in an autoregressive generation setting using a simple GPT-style Transformer. On ImageNet, this approach achieves an FID<2 across 8 to 128 tokens, outperforming TiTok and matching state-of-the-art methods with far fewer tokens. We further extend the model to support to text-conditioned image generation and examine how FlexTok relates to traditional 2D tokenization. A key finding is that FlexTok enables next-token prediction to describe images in a coarse-to-fine \"visual vocabulary\", and that the number of tokens to generate depends on the complexity of the generation task.",
        "arxiv_id": "2502.13967",
        "ARXIVID": "2502.13967",
        "COMMENT": "Matches criterion 4 as it introduces a novel tokenization method for vision foundation models and its applications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.13569": {
        "authors": [
            "Yan Yu",
            "Wengang Zhou",
            "Yaodong Yang",
            "Wanxuan Lu",
            "Yingyan Hou",
            "Houqiang Li"
        ],
        "title": "Model Evolution Framework with Genetic Algorithm for Multi-Task Reinforcement Learning",
        "abstract": "arXiv:2502.13569v1 Announce Type: new  Abstract: Multi-task reinforcement learning employs a single policy to complete various tasks, aiming to develop an agent with generalizability across different scenarios. Given the shared characteristics of tasks, the agent's learning efficiency can be enhanced through parameter sharing. Existing approaches typically use a routing network to generate specific routes for each task and reconstruct a set of modules into diverse models to complete multiple tasks simultaneously. However, due to the inherent difference between tasks, it is crucial to allocate resources based on task difficulty, which is constrained by the model's structure. To this end, we propose a Model Evolution framework with Genetic Algorithm (MEGA), which enables the model to evolve during training according to the difficulty of the tasks. When the current model is insufficient for certain tasks, the framework will automatically incorporate additional modules, enhancing the model's capabilities. Moreover, to adapt to our model evolution framework, we introduce a genotype module-level model, using binary sequences as genotype policies for model reconstruction, while leveraging a non-gradient genetic algorithm to optimize these genotype policies. Unlike routing networks with fixed output dimensions, our approach allows for the dynamic adjustment of the genotype policy length, enabling it to accommodate models with a varying number of modules. We conducted experiments on various robotics manipulation tasks in the Meta-World benchmark. Our state-of-the-art performance demonstrated the effectiveness of the MEGA framework. We will release our source code to the public.",
        "arxiv_id": "2502.13569",
        "ARXIVID": "2502.13569",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework (MEGA) for multi-task reinforcement learning with dynamic model evolution, which is a new angle in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.13968": {
        "authors": [
            "Suhas Gopal",
            "Rishabh Dabral",
            "Vladislav Golyanik",
            "Christian Theobalt"
        ],
        "title": "Betsu-Betsu: Multi-View Separable 3D Reconstruction of Two Interacting Objects",
        "abstract": "arXiv:2502.13968v1 Announce Type: new  Abstract: Separable 3D reconstruction of multiple objects from multi-view RGB images -- resulting in two different 3D shapes for the two objects with a clear separation between them -- remains a sparsely researched problem. It is challenging due to severe mutual occlusions and ambiguities along the objects' interaction boundaries. This paper investigates the setting and introduces a new neuro-implicit method that can reconstruct the geometry and appearance of two objects undergoing close interactions while disjoining both in 3D, avoiding surface inter-penetrations and enabling novel-view synthesis of the observed scene. The framework is end-to-end trainable and supervised using a novel alpha-blending regularisation that ensures that the two geometries are well separated even under extreme occlusions. Our reconstruction method is markerless and can be applied to rigid as well as articulated objects. We introduce a new dataset consisting of close interactions between a human and an object and also evaluate on two scenes of humans performing martial arts. The experiments confirm the effectiveness of our framework and substantial improvements using 3D and novel view synthesis metrics compared to several existing approaches applicable in our setting.",
        "arxiv_id": "2502.13968",
        "ARXIVID": "2502.13968",
        "COMMENT": "Matches criterion 1 as it focuses on a novel method for spatial understanding in 3D reconstruction of interacting objects.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2502.13759": {
        "authors": [
            "Zirui Song",
            "Jingpu Yang",
            "Yuan Huang",
            "Jonathan Tonglet",
            "Zeyu Zhang",
            "Tao Cheng",
            "Meng Fang",
            "Iryna Gurevych",
            "Xiuying Chen"
        ],
        "title": "Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and Human-Like Reasoning Framework",
        "abstract": "arXiv:2502.13759v1 Announce Type: new  Abstract: Geolocation, the task of identifying an image's location, requires complex reasoning and is crucial for navigation, monitoring, and cultural preservation. However, current methods often produce coarse, imprecise, and non-interpretable localization. A major challenge lies in the quality and scale of existing geolocation datasets. These datasets are typically small-scale and automatically constructed, leading to noisy data and inconsistent task difficulty, with images that either reveal answers too easily or lack sufficient clues for reliable inference. To address these challenges, we introduce a comprehensive geolocation framework with three key components: GeoComp, a large-scale dataset; GeoCoT, a novel reasoning method; and GeoEval, an evaluation metric, collectively designed to address critical challenges and drive advancements in geolocation research. At the core of this framework is GeoComp (Geolocation Competition Dataset), a large-scale dataset collected from a geolocation game platform involving 740K users over two years. It comprises 25 million entries of metadata and 3 million geo-tagged locations spanning much of the globe, with each location annotated thousands to tens of thousands of times by human users. The dataset offers diverse difficulty levels for detailed analysis and highlights key gaps in current models. Building on this dataset, we propose Geographical Chain-of-Thought (GeoCoT), a novel multi-step reasoning framework designed to enhance the reasoning capabilities of Large Vision Models (LVMs) in geolocation tasks. GeoCoT improves performance by integrating contextual and spatial cues through a multi-step process that mimics human geolocation reasoning. Finally, using the GeoEval metric, we demonstrate that GeoCoT significantly boosts geolocation accuracy by up to 25% while enhancing interpretability.",
        "arxiv_id": "2502.13759",
        "ARXIVID": "2502.13759",
        "COMMENT": "Matches criterion 3 as it introduces a new large-scale geolocation dataset and a novel reasoning framework for spatial intelligence.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2502.13637": {
        "authors": [
            "Prasun Roy",
            "Saumik Bhattacharya",
            "Subhankar Ghosh",
            "Umapada Pal",
            "Michael Blumenstein"
        ],
        "title": "Exploring Mutual Cross-Modal Attention for Context-Aware Human Affordance Generation",
        "abstract": "arXiv:2502.13637v1 Announce Type: new  Abstract: Human affordance learning investigates contextually relevant novel pose prediction such that the estimated pose represents a valid human action within the scene. While the task is fundamental to machine perception and automated interactive navigation agents, the exponentially large number of probable pose and action variations make the problem challenging and non-trivial. However, the existing datasets and methods for human affordance prediction in 2D scenes are significantly limited in the literature. In this paper, we propose a novel cross-attention mechanism to encode the scene context for affordance prediction by mutually attending spatial feature maps from two different modalities. The proposed method is disentangled among individual subtasks to efficiently reduce the problem complexity. First, we sample a probable location for a person within the scene using a variational autoencoder (VAE) conditioned on the global scene context encoding. Next, we predict a potential pose template from a set of existing human pose candidates using a classifier on the local context encoding around the predicted location. In the subsequent steps, we use two VAEs to sample the scale and deformation parameters for the predicted pose template by conditioning on the local context and template class. Our experiments show significant improvements over the previous baseline of human affordance injection into complex 2D scenes.",
        "arxiv_id": "2502.13637",
        "ARXIVID": "2502.13637",
        "COMMENT": "Matches criterion 1 as it proposes a novel cross-modal attention mechanism for human affordance prediction in spatial contexts.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.13875": {
        "authors": [
            "Huu-Thien Tran",
            "Phuoc-Sang Pham",
            "Thai-Son Tran",
            "Khoa Luu"
        ],
        "title": "MEX: Memory-efficient Approach to Referring Multi-Object Tracking",
        "abstract": "arXiv:2502.13875v1 Announce Type: new  Abstract: Referring Multi-Object Tracking (RMOT) is a relatively new concept that has rapidly gained traction as a promising research direction at the intersection of computer vision and natural language processing. Unlike traditional multi-object tracking, RMOT identifies and tracks objects and incorporates textual descriptions for object class names, making the approach more intuitive. Various techniques have been proposed to address this challenging problem; however, most require the training of the entire network due to their end-to-end nature. Among these methods, iKUN has emerged as a particularly promising solution. Therefore, we further explore its pipeline and enhance its performance. In this paper, we introduce a practical module dubbed Memory-Efficient Cross-modality -- MEX. This memory-efficient technique can be directly applied to off-the-shelf trackers like iKUN, resulting in significant architectural improvements. Our method proves effective during inference on a single GPU with 4 GB of memory. Among the various benchmarks, the Refer-KITTI dataset, which offers diverse autonomous driving scenes with relevant language expressions, is particularly useful for studying this problem. Empirically, our method demonstrates effectiveness and efficiency regarding HOTA tracking scores, substantially improving memory allocation and processing speed.",
        "arxiv_id": "2502.13875",
        "ARXIVID": "2502.13875",
        "COMMENT": "Matches criterion 3 as it focuses on a novel memory-efficient module for referring multi-object tracking, which is a new method in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.13883": {
        "authors": [
            "Idris Hamoud",
            "Vinkle Srivastav",
            "Muhammad Abdullah Jamal",
            "Didier Mutter",
            "Omid Mohareri",
            "Nicolas Padoy"
        ],
        "title": "Multi-view Video-Pose Pretraining for Operating Room Surgical Activity Recognition",
        "abstract": "arXiv:2502.13883v1 Announce Type: new  Abstract: Understanding the workflow of surgical procedures in complex operating rooms requires a deep understanding of the interactions between clinicians and their environment. Surgical activity recognition (SAR) is a key computer vision task that detects activities or phases from multi-view camera recordings. Existing SAR models often fail to account for fine-grained clinician movements and multi-view knowledge, or they require calibrated multi-view camera setups and advanced point-cloud processing to obtain better results. In this work, we propose a novel calibration-free multi-view multi-modal pretraining framework called Multiview Pretraining for Video-Pose Surgical Activity Recognition PreViPS, which aligns 2D pose and vision embeddings across camera views. Our model follows CLIP-style dual-encoder architecture: one encoder processes visual features, while the other encodes human pose embeddings. To handle the continuous 2D human pose coordinates, we introduce a tokenized discrete representation to convert the continuous 2D pose coordinates into discrete pose embeddings, thereby enabling efficient integration within the dual-encoder framework. To bridge the gap between these two modalities, we propose several pretraining objectives using cross- and in-modality geometric constraints within the embedding space and incorporating masked pose token prediction strategy to enhance representation learning. Extensive experiments and ablation studies demonstrate improvements over the strong baselines, while data-efficiency experiments on two distinct operating room datasets further highlight the effectiveness of our approach. We highlight the benefits of our approach for surgical activity recognition in both multi-view and single-view settings, showcasing its practical applicability in complex surgical environments. Code will be made available at: https://github.com/CAMMA-public/PreViPS.",
        "arxiv_id": "2502.13883",
        "ARXIVID": "2502.13883",
        "COMMENT": "Matches criterion 3 as it proposes a novel multi-modal pretraining framework for surgical activity recognition, which is a new method in embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2502.13818": {
        "authors": [
            "Nikolaos Dionelis",
            "Nicolas Long\\'ep\\'e",
            "Alessandra Feliciotti",
            "Mattia Marconcini",
            "Devis Peressutti",
            "Nika Oman Kadunc",
            "JaeWan Park",
            "Hagai Raja Sinulingga",
            "Steve Andreas Immanuel",
            "Ba Tran",
            "Caroline Arnold"
        ],
        "title": "Building Age Estimation: A New Multi-Modal Benchmark Dataset and Community Challenge",
        "abstract": "arXiv:2502.13818v1 Announce Type: new  Abstract: Estimating the construction year of buildings is of great importance for sustainability. Sustainable buildings minimize energy consumption and are a key part of responsible and sustainable urban planning and development to effectively combat climate change. By using Artificial Intelligence (AI) and recently proposed Transformer models, we are able to estimate the construction epoch of buildings from a multi-modal dataset. In this paper, we introduce a new benchmark multi-modal dataset, i.e. the Map your City Dataset (MyCD), containing top-view Very High Resolution (VHR) images, Earth Observation (EO) multi-spectral data from the Copernicus Sentinel-2 satellite constellation, and street-view images in many different cities in Europe, co-localized with respect to the building under study and labelled with the construction epoch. We assess EO generalization performance on new/ previously unseen cities that have been held-out from training and appear only during inference. In this work, we present the community-based data challenge we organized based on MyCD. The ESA AI4EO Challenge MapYourCity was opened in 2024 for 4 months. Here, we present the Top-4 performing models, and the main evaluation results. During inference, the performance of the models using both all three input modalities and only the two top-view modalities, i.e. without the street-view images, is examined. The evaluation results show that the models are effective and can achieve good performance on this difficult real-world task of estimating the age of buildings, even on previously unseen cities, as well as even using only the two top-view modalities (i.e. VHR and Sentinel-2) during inference.",
        "arxiv_id": "2502.13818",
        "ARXIVID": "2502.13818",
        "COMMENT": "Matches criterion 3 as it introduces a new multi-modal benchmark dataset for building age estimation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.13693": {
        "authors": [
            "Omid Nejati Manzari",
            "Hojat Asgariandehkordi",
            "Taha Koleilat",
            "Yiming Xiao",
            "Hassan Rivaz"
        ],
        "title": "Medical Image Classification with KAN-Integrated Transformers and Dilated Neighborhood Attention",
        "abstract": "arXiv:2502.13693v1 Announce Type: new  Abstract: Convolutional networks, transformers, hybrid models, and Mamba-based architectures have demonstrated strong performance across various medical image classification tasks. However, these methods were primarily designed to classify clean images using labeled data. In contrast, real-world clinical data often involve image corruptions that are unique to multi-center studies and stem from variations in imaging equipment across manufacturers. In this paper, we introduce the Medical Vision Transformer (MedViTV2), a novel architecture incorporating Kolmogorov-Arnold Network (KAN) layers into the transformer architecture for the first time, aiming for generalized medical image classification. We have developed an efficient KAN block to reduce computational load while enhancing the accuracy of the original MedViT. Additionally, to counteract the fragility of our MedViT when scaled up, we propose an enhanced Dilated Neighborhood Attention (DiNA), an adaptation of the efficient fused dot-product attention kernel capable of capturing global context and expanding receptive fields to scale the model effectively and addressing feature collapse issues. Moreover, a hierarchical hybrid strategy is introduced to stack our Local Feature Perception and Global Feature Perception blocks in an efficient manner, which balances local and global feature perceptions to boost performance. Extensive experiments on 17 medical image classification datasets and 12 corrupted medical image datasets demonstrate that MedViTV2 achieved state-of-the-art results in 27 out of 29 experiments with reduced computational complexity. MedViTV2 is 44\\% more computationally efficient than the previous version and significantly enhances accuracy, achieving improvements of 4.6\\% on MedMNIST, 5.8\\% on NonMNIST, and 13.4\\% on the MedMNIST-C benchmark.",
        "arxiv_id": "2502.13693",
        "ARXIVID": "2502.13693",
        "COMMENT": "Does not match any specific criteria but introduces a novel transformer-based architecture for medical image classification, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.13170": {
        "authors": [
            "Yuze Zhao",
            "Tianyun Ji",
            "Wenjun Feng",
            "Zhenya Huang",
            "Qi Liu",
            "Zhiding Liu",
            "Yixiao Ma",
            "Kai Zhang",
            "Enhong Chen"
        ],
        "title": "Unveiling the Magic of Code Reasoning through Hypothesis Decomposition and Amendment",
        "abstract": "arXiv:2502.13170v1 Announce Type: new  Abstract: The reasoning abilities are one of the most enigmatic and captivating aspects of large language models (LLMs). Numerous studies are dedicated to exploring and expanding the boundaries of this reasoning capability. However, tasks that embody both reasoning and recall characteristics are often overlooked. In this paper, we introduce such a novel task, code reasoning, to provide a new perspective for the reasoning abilities of LLMs. We summarize three meta-benchmarks based on established forms of logical reasoning, and instantiate these into eight specific benchmark tasks. Our testing on these benchmarks reveals that LLMs continue to struggle with identifying satisfactory reasoning pathways. Additionally, we present a new pathway exploration pipeline inspired by human intricate problem-solving methods. This Reflective Hypothesis Decomposition and Amendment (RHDA) pipeline consists of the following iterative steps: (1) Proposing potential hypotheses based on observations and decomposing them; (2) Utilizing tools to validate hypotheses and reflection outcomes; (3) Revising hypothesis in light of observations. Our approach effectively mitigates logical chain collapses arising from forgetting or hallucination issues in multi-step reasoning, resulting in performance gains of up to $3\\times$. Finally, we expanded this pipeline by applying it to simulate complex household tasks in real-world scenarios, specifically in VirtualHome, enhancing the handling of failure cases. We release our code and all of results at https://github.com/TnTWoW/code_reasoning.",
        "arxiv_id": "2502.13170",
        "ARXIVID": "2502.13170",
        "COMMENT": "Does not match any specific criteria but explores reasoning in LLMs with a novel pipeline, which is tangentially relevant to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.13389": {
        "authors": [
            "Kongcheng Zhang",
            "Qi Yao",
            "Baisheng Lai",
            "Jiaxing Huang",
            "Wenkai Fang",
            "Dacheng Tao",
            "Mingli Song",
            "Shunyu Liu"
        ],
        "title": "Reasoning with Reinforced Functional Token Tuning",
        "abstract": "arXiv:2502.13389v1 Announce Type: new  Abstract: In this work, we propose Reinforced Functional Token Tuning (RFTT), a novel reinforced fine-tuning framework that empowers Large Language Models (LLMs) with self-play learn-to-reason capabilities. Unlike prior prompt-driven reasoning efforts, RFTT embeds a rich set of learnable functional tokens (e.g., , , ) directly into the model vocabulary, enabling chain-of-thought construction with diverse human-like reasoning behaviors. Specifically, RFTT comprises two phases: (1) supervised fine-tuning performs prompt-driven tree search to obtain self-generated training data annotated with functional tokens, which warms up the model to learn these tokens for reasoning; and (2) online reinforcement learning further allows the model to explore different reasoning pathways through functional token sampling without relying on prompts, thereby facilitating effective self-improvement for functional reasoning. Extensive experiments demonstrate the superiority of the proposed RFTT on mathematical benchmarks, significantly boosting Qwen-2.5-7B-Instruct (70.6% to 79.8%) and LLaMA-3.1-8B-Instruct (32.2% to 60.2%) on the MATH dataset. Moreover, the performance of RFTT consistently improves with more search rollouts at inference time. Our code is available at https://github.com/sastpg/RFTT.",
        "arxiv_id": "2502.13389",
        "ARXIVID": "2502.13389",
        "COMMENT": "Does not match any specific criteria but introduces a novel fine-tuning framework for reasoning in LLMs, which is tangentially relevant to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.13516": {
        "authors": [
            "Hao Yi",
            "Qingyang Li",
            "Yulan Hu",
            "Fuzheng Zhang",
            "Di Zhang",
            "Yong Liu"
        ],
        "title": "SPPD: Self-training with Process Preference Learning Using Dynamic Value Margin",
        "abstract": "arXiv:2502.13516v1 Announce Type: new  Abstract: Recently, enhancing the numerical and logical reasoning capability of Large Language Models (LLMs) has emerged as a research hotspot. Existing methods face several limitations: inference-phase techniques (e.g., Chain of Thoughts) rely on prompt selection and the pretrained knowledge; sentence-level Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) struggle with step-wise mathematical correctness and depend on stronger models distillation or human annotations; while Reinforcement Learning (RL) approaches incur high GPU memory costs and unstable training. To address these, we propose \\textbf{S}elf-training framework integrating \\textbf{P}rocess \\textbf{P}reference learning using \\textbf{D}ynamic value margin (SPPD). SPPD leverages a process-based Markov Decision Process (MDP) and Bellman optimality equation to derive \\textbf{dynamic value margin} on step-level preference optimization, which employs tree-based self-sampling on model responses \\textbf{without any distillation} from other models. Furthermore, we theoretically prove that SPPD is \\textbf{equivalent to on-policy policy gradient methods} under reward constraints. Experiments on 7B-scale models demonstrate superior performance across in-domain and out-domain mathematical benchmarks. We open-source our code at \\href{https://anonymous.4open.science/r/SSDPO-D-DCDD}{https://anonymous.4open.science/r/SPPD-DCDD}.",
        "arxiv_id": "2502.13516",
        "ARXIVID": "2502.13516",
        "COMMENT": "Does not match any specific criteria but is related to enhancing reasoning capabilities in LLMs, which is tangentially relevant to your friend's interest in generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.13834": {
        "authors": [
            "Zenan Li",
            "Zhaoyu Li",
            "Wen Tang",
            "Xian Zhang",
            "Yuan Yao",
            "Xujie Si",
            "Fan Yang",
            "Kaiyu Yang",
            "Xiaoxing Ma"
        ],
        "title": "Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning",
        "abstract": "arXiv:2502.13834v1 Announce Type: new  Abstract: Large language models (LLMs) can prove mathematical theorems formally by generating proof steps (\\textit{a.k.a.} tactics) within a proof system. However, the space of possible tactics is vast and complex, while the available training data for formal proofs is limited, posing a significant challenge to LLM-based tactic generation. To address this, we introduce a neuro-symbolic tactic generator that synergizes the mathematical intuition learned by LLMs with domain-specific insights encoded by symbolic methods. The key aspect of this integration is identifying which parts of mathematical reasoning are best suited to LLMs and which to symbolic methods. While the high-level idea of neuro-symbolic integration is broadly applicable to various mathematical problems, in this paper, we focus specifically on Olympiad inequalities (Figure~1). We analyze how humans solve these problems and distill the techniques into two types of tactics: (1) scaling, handled by symbolic methods, and (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with LLMs to prune and rank the proof goals for efficient proof search. We evaluate our framework on 161 challenging inequalities from multiple mathematics competitions, achieving state-of-the-art performance and significantly outperforming existing LLM and symbolic approaches without requiring additional training data.",
        "arxiv_id": "2502.13834",
        "ARXIVID": "2502.13834",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of large language models and symbolic reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.13234": {
        "authors": [
            "Yen-Siang Wu",
            "Chi-Pin Huang",
            "Fu-En Yang",
            "Yu-Chiang Frank Wang"
        ],
        "title": "MotionMatcher: Motion Customization of Text-to-Video Diffusion Models via Motion Feature Matching",
        "abstract": "arXiv:2502.13234v1 Announce Type: new  Abstract: Text-to-video (T2V) diffusion models have shown promising capabilities in synthesizing realistic videos from input text prompts. However, the input text description alone provides limited control over the precise objects movements and camera framing. In this work, we tackle the motion customization problem, where a reference video is provided as motion guidance. While most existing methods choose to fine-tune pre-trained diffusion models to reconstruct the frame differences of the reference video, we observe that such strategy suffer from content leakage from the reference video, and they cannot capture complex motion accurately. To address this issue, we propose MotionMatcher, a motion customization framework that fine-tunes the pre-trained T2V diffusion model at the feature level. Instead of using pixel-level objectives, MotionMatcher compares high-level, spatio-temporal motion features to fine-tune diffusion models, ensuring precise motion learning. For the sake of memory efficiency and accessibility, we utilize a pre-trained T2V diffusion model, which contains considerable prior knowledge about video motion, to compute these motion features. In our experiments, we demonstrate state-of-the-art motion customization performances, validating the design of our framework.",
        "arxiv_id": "2502.13234",
        "ARXIVID": "2502.13234",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.13859": {
        "authors": [
            "Shuyong Gao",
            "Yu'ang Feng",
            "Qishan Wang",
            "Lingyi Hong",
            "Xinyu Zhou",
            "Liu Fei",
            "Yan Wang",
            "Wenqiang Zhang"
        ],
        "title": "MSVCOD:A Large-Scale Multi-Scene Dataset for Video Camouflage Object Detection",
        "abstract": "arXiv:2502.13859v1 Announce Type: new  Abstract: Video Camouflaged Object Detection (VCOD) is a challenging task which aims to identify objects that seamlessly concealed within the background in videos. The dynamic properties of video enable detection of camouflaged objects through motion cues or varied perspectives. Previous VCOD datasets primarily contain animal objects, limiting the scope of research to wildlife scenarios. However, the applications of VCOD extend beyond wildlife and have significant implications in security, art, and medical fields. Addressing this problem, we construct a new large-scale multi-domain VCOD dataset MSVCOD. To achieve high-quality annotations, we design a semi-automatic iterative annotation pipeline that reduces costs while maintaining annotation accuracy. Our MSVCOD is the largest VCOD dataset to date, introducing multiple object categories including human, animal, medical, and vehicle objects for the first time, while also expanding background diversity across various environments. This expanded scope increases the practical applicability of the VCOD task in camouflaged object detection. Alongside this dataset, we introduce a one-steam video camouflage object detection model that performs both feature extraction and information fusion without additional motion feature fusion modules. Our framework achieves state-of-the-art results on the existing VCOD animal dataset and the proposed MSVCOD. The dataset and code will be made publicly available.",
        "arxiv_id": "2502.13859",
        "ARXIVID": "2502.13859",
        "COMMENT": "Does not match any specific criteria but introduces a new dataset and model for video camouflaged object detection, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.13476": {
        "authors": [
            "Sunder Ali Khowaja",
            "Kapal Dev",
            "Muhammad Salman Pathan",
            "Engin Zeydan",
            "Merouane Debbah"
        ],
        "title": "Integration of Agentic AI with 6G Networks for Mission-Critical Applications: Use-case and Challenges",
        "abstract": "arXiv:2502.13476v1 Announce Type: new  Abstract: We are in a transformative era, and advances in Artificial Intelligence (AI), especially the foundational models, are constantly in the news. AI has been an integral part of many applications that rely on automation for service delivery, and one of them is mission-critical public safety applications. The problem with AI-oriented mission-critical applications is the humanin-the-loop system and the lack of adaptability to dynamic conditions while maintaining situational awareness. Agentic AI (AAI) has gained a lot of attention recently due to its ability to analyze textual data through a contextual lens while quickly adapting to conditions. In this context, this paper proposes an AAI framework for mission-critical applications. We propose a novel framework with a multi-layer architecture to realize the AAI. We also present a detailed implementation of AAI layer that bridges the gap between network infrastructure and missioncritical applications. Our preliminary analysis shows that the AAI reduces initial response time by 5.6 minutes on average, while alert generation time is reduced by 15.6 seconds on average and resource allocation is improved by up to 13.4%. We also show that the AAI methods improve the number of concurrent operations by 40, which reduces the recovery time by up to 5.2 minutes. Finally, we highlight some of the issues and challenges that need to be considered when implementing AAI frameworks.",
        "arxiv_id": "2502.13476",
        "ARXIVID": "2502.13476",
        "COMMENT": "Does not match any specific criteria but discusses Agentic AI in mission-critical applications, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.13624": {
        "authors": [
            "Zheng Wu",
            "Yiping Xie",
            "Bo Zhao",
            "Jiguang He",
            "Fei Luo",
            "Ning Deng",
            "Zitong Yu"
        ],
        "title": "CardiacMamba: A Multimodal RGB-RF Fusion Framework with State Space Models for Remote Physiological Measurement",
        "abstract": "arXiv:2502.13624v1 Announce Type: new  Abstract: Heart rate (HR) estimation via remote photoplethysmography (rPPG) offers a non-invasive solution for health monitoring. However, traditional single-modality approaches (RGB or Radio Frequency (RF)) face challenges in balancing robustness and accuracy due to lighting variations, motion artifacts, and skin tone bias. In this paper, we propose CardiacMamba, a multimodal RGB-RF fusion framework that leverages the complementary strengths of both modalities. It introduces the Temporal Difference Mamba Module (TDMM) to capture dynamic changes in RF signals using timing differences between frames, enhancing the extraction of local and global features. Additionally, CardiacMamba employs a Bidirectional SSM for cross-modal alignment and a Channel-wise Fast Fourier Transform (CFFT) to effectively capture and refine the frequency domain characteristics of RGB and RF signals, ultimately improving heart rate estimation accuracy and periodicity detection. Extensive experiments on the EquiPleth dataset demonstrate state-of-the-art performance, achieving marked improvements in accuracy and robustness. CardiacMamba significantly mitigates skin tone bias, reducing performance disparities across demographic groups, and maintains resilience under missing-modality scenarios. By addressing critical challenges in fairness, adaptability, and precision, the framework advances rPPG technology toward reliable real-world deployment in healthcare. The codes are available at: https://github.com/WuZheng42/CardiacMamba.",
        "arxiv_id": "2502.13624",
        "ARXIVID": "2502.13624",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of multi-modal learning and healthcare applications.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.13524": {
        "authors": [
            "Wei Dai",
            "Steven Wang",
            "Jun Liu"
        ],
        "title": "MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D Medical Image Analysis",
        "abstract": "arXiv:2502.13524v1 Announce Type: new  Abstract: Efficient evaluation of three-dimensional (3D) medical images is crucial for diagnostic and therapeutic practices in healthcare. Recent years have seen a substantial uptake in applying deep learning and computer vision to analyse and interpret medical images. Traditional approaches, such as convolutional neural networks (CNNs) and vision transformers (ViTs), face significant computational challenges, prompting the need for architectural advancements. Recent efforts have led to the introduction of novel architectures like the ``Mamba'' model as alternative solutions to traditional CNNs or ViTs. The Mamba model excels in the linear processing of one-dimensional data with low computational demands. However, Mamba's potential for 3D medical image analysis remains underexplored and could face significant computational challenges as the dimension increases. This manuscript presents MobileViM, a streamlined architecture for efficient segmentation of 3D medical images. In the MobileViM network, we invent a new dimension-independent mechanism and a dual-direction traversing approach to incorporate with a vision-Mamba-based framework. MobileViM also features a cross-scale bridging technique to improve efficiency and accuracy across various medical imaging modalities. With these enhancements, MobileViM achieves segmentation speeds exceeding 90 frames per second (FPS) on a single graphics processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster than the state-of-the-art deep learning models for processing 3D images with the same computational resources. In addition, experimental evaluations demonstrate that MobileViM delivers superior performance, with Dice similarity scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024, ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses existing models.",
        "arxiv_id": "2502.13524",
        "ARXIVID": "2502.13524",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of computer vision and medical imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}