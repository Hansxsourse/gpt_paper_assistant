{
    "2512.04821": {
        "authors": [
            "Huynh Trinh Ngoc",
            "Hoang Anh Nguyen Kim",
            "Toan Nguyen Hai",
            "Long Tran Quoc"
        ],
        "title": "LatentFM: A Latent Flow Matching Approach for Generative Medical Image Segmentation",
        "abstract": "arXiv:2512.04821v1 Announce Type: new  Abstract: Generative models have achieved remarkable progress with the emergence of flow matching (FM). It has demonstrated strong generative capabilities and attracted significant attention as a simulation-free flow-based framework capable of learning exact data densities. Motivated by these advances, we propose LatentFM, a flow-based model operating in the latent space for medical image segmentation. To model the data distribution, we first design two variational autoencoders (VAEs) to encode both medical images and their corresponding masks into a lower-dimensional latent space. We then estimate a conditional velocity field that guides the flow based on the input image. By sampling multiple latent representations, our method synthesizes diverse segmentation outputs whose pixel-wise variance reliably captures the underlying data distribution, enabling both highly accurate and uncertainty-aware predictions. Furthermore, we generate confidence maps that quantify the model certainty, providing clinicians with richer information for deeper analysis. We conduct experiments on two datasets, ISIC-2018 and CVC-Clinic, and compare our method with several prior baselines, including both deterministic and generative approach models. Through comprehensive evaluations, both qualitative and quantitative results show that our approach achieves superior segmentation accuracy while remaining highly efficient in the latent space.",
        "arxiv_id": "2512.04821",
        "ARXIVID": "2512.04821",
        "COMMENT": "Matches criterion 1: Unified Image/Video Generation and Segmentation",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.04926": {
        "authors": [
            "Yueming Pan",
            "Ruoyu Feng",
            "Qi Dai",
            "Yuqi Wang",
            "Wenfeng Lin",
            "Mingyu Guo",
            "Chong Luo",
            "Nanning Zheng"
        ],
        "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
        "abstract": "arXiv:2512.04926v1 Announce Type: new  Abstract: Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.",
        "arxiv_id": "2512.04926",
        "ARXIVID": "2512.04926",
        "COMMENT": "Matches criterion 2: Unified Diffusion Models for multiple vision tasks",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2512.05044": {
        "authors": [
            "Yanran Zhang",
            "Ziyi Wang",
            "Wenzhao Zheng",
            "Zheng Zhu",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image",
        "abstract": "arXiv:2512.05044v1 Announce Type: new  Abstract: Generating interactive and dynamic 4D scenes from a single static image remains a core challenge. Most existing generate-then-reconstruct and reconstruct-then-generate methods decouple geometry from motion, causing spatiotemporal inconsistencies and poor generalization. To address these, we extend the reconstruct-then-generate framework to jointly perform Motion generation and geometric Reconstruction for 4D Synthesis (MoRe4D). We first introduce TrajScene-60K, a large-scale dataset of 60,000 video samples with dense point trajectories, addressing the scarcity of high-quality 4D scene data. Based on this, we propose a diffusion-based 4D Scene Trajectory Generator (4D-STraG) to jointly generate geometrically consistent and motion-plausible 4D point trajectories. To leverage single-view priors, we design a depth-guided motion normalization strategy and a motion-aware module for effective geometry and dynamics integration. We then propose a 4D View Synthesis Module (4D-ViSM) to render videos with arbitrary camera trajectories from 4D point track representations. Experiments show that MoRe4D generates high-quality 4D scenes with multi-view consistency and rich dynamic details from a single image. Code: https://github.com/Zhangyr2022/MoRe4D.",
        "arxiv_id": "2512.05044",
        "ARXIVID": "2512.05044",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.04830": {
        "authors": [
            "Shijie Chen",
            "Peixi Peng"
        ],
        "title": "FreeGen: Feed-Forward Reconstruction-Generation Co-Training for Free-Viewpoint Driving Scene Synthesis",
        "abstract": "arXiv:2512.04830v1 Announce Type: new  Abstract: Closed-loop simulation and scalable pre-training for autonomous driving require synthesizing free-viewpoint driving scenes. However, existing datasets and generative pipelines rarely provide consistent off-trajectory observations, limiting large-scale evaluation and training. While recent generative models demonstrate strong visual realism, they struggle to jointly achieve interpolation consistency and extrapolation realism without per-scene optimization. To address this, we propose FreeGen, a feed-forward reconstruction-generation co-training framework for free-viewpoint driving scene synthesis. The reconstruction model provides stable geometric representations to ensure interpolation consistency, while the generation model performs geometry-aware enhancement to improve realism at unseen viewpoints. Through co-training, generative priors are distilled into the reconstruction model to improve off-trajectory rendering, and the refined geometry in turn offers stronger structural guidance for generation. Experiments demonstrate that FreeGen achieves state-of-the-art performance for free-viewpoint driving scene synthesis.",
        "arxiv_id": "2512.04830",
        "ARXIVID": "2512.04830",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.04810": {
        "authors": [
            "Xin He",
            "Longhui Wei",
            "Jianbo Ouyang",
            "Lingxi Xie",
            "Qi Tian"
        ],
        "title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture",
        "abstract": "arXiv:2512.04810v1 Announce Type: new  Abstract: We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.",
        "arxiv_id": "2512.04810",
        "ARXIVID": "2512.04810",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.04515": {
        "authors": [
            "Liuzhou Zhang",
            "Jiarui Ye",
            "Yuanlei Wang",
            "Ming Zhong",
            "Mingju Cao",
            "Wanke Xia",
            "Bowen Zeng",
            "Zeyu Zhang",
            "Hao Tang"
        ],
        "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
        "abstract": "arXiv:2512.04515v1 Announce Type: new  Abstract: Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.",
        "arxiv_id": "2512.04515",
        "ARXIVID": "2512.04515",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.05000": {
        "authors": [
            "Daniyar Zakarin",
            "Thiemo Wandel",
            "Anton Obukhov",
            "Dengxin Dai"
        ],
        "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
        "abstract": "arXiv:2512.05000v1 Announce Type: new  Abstract: We introduce a diffusion-transformer (DiT) framework for single-image reflection removal that leverages the generalization strengths of foundation diffusion models in the restoration setting. Rather than relying on task-specific architectures, we repurpose a pre-trained DiT-based foundation model by conditioning it on reflection-contaminated inputs and guiding it toward clean transmission layers. We systematically analyze existing reflection removal data sources for diversity, scalability, and photorealism. To address the shortage of suitable data, we construct a physically based rendering (PBR) pipeline in Blender, built around the Principled BSDF, to synthesize realistic glass materials and reflection effects. Efficient LoRA-based adaptation of the foundation model, combined with the proposed synthetic data, achieves state-of-the-art performance on in-domain and zero-shot benchmarks. These results demonstrate that pretrained diffusion transformers, when paired with physically grounded data synthesis and efficient adaptation, offer a scalable and high-fidelity solution for reflection removal. Project page: https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
        "arxiv_id": "2512.05000",
        "ARXIVID": "2512.05000",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.04563": {
        "authors": [
            "Zefeng Zhang",
            "Xiangzhao Hao",
            "Hengzhu Tang",
            "Zhenyu Zhang",
            "Jiawei Sheng",
            "Xiaodong Li",
            "Zhenyang Li",
            "Li Gao",
            "Daiting Shi",
            "Dawei Yin",
            "Tingwen Liu"
        ],
        "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence",
        "abstract": "arXiv:2512.04563v1 Announce Type: new  Abstract: Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \\textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \\textbf{6.91\\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \\textbf{7.92\\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.",
        "arxiv_id": "2512.04563",
        "ARXIVID": "2512.04563",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2512.05106": {
        "authors": [
            "Yu Zeng",
            "Charles Ochoa",
            "Mingyuan Zhou",
            "Vishal M. Patel",
            "Vitor Guizilini",
            "Rowan McAllister"
        ],
        "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
        "abstract": "arXiv:2512.05106v1 Announce Type: new  Abstract: Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion {\\phi}-PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. {\\phi}-PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, {\\phi}-PD produces controllable, spatially aligned results. When applied to the CARLA simulator, {\\phi}-PD improves CARLA-to-Waymo planner performance by 50\\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our \\href{https://yuzeng-at-tri.github.io/ppd-page/}{project page}.",
        "arxiv_id": "2512.05106",
        "ARXIVID": "2512.05106",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}