{
    "2511.08258": {
        "authors": [
            "Jae Joong Lee",
            "Bedrich Benes"
        ],
        "title": "Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation",
        "abstract": "arXiv:2511.08258v1 Announce Type: new  Abstract: Generating ground-level images from aerial views is a challenging task due to extreme viewpoint disparity, occlusions, and a limited field of view. We introduce Top2Ground, a novel diffusion-based method that directly generates photorealistic ground-view images from aerial input images without relying on intermediate representations such as depth maps or 3D voxels. Specifically, we condition the denoising process on a joint representation of VAE-encoded spatial features (derived from aerial RGB images and an estimated height map) and CLIP-based semantic embeddings. This design ensures the generation is both geometrically constrained by the scene's 3D structure and semantically consistent with its content. We evaluate Top2Ground on three diverse datasets: CVUSA, CVACT, and the Auto Arborist. Our approach shows 7.3% average improvement in SSIM across three benchmark datasets, showing Top2Ground can robustly handle both wide and narrow fields of view, highlighting its strong generalization capabilities.",
        "arxiv_id": "2511.08258",
        "ARXIVID": "2511.08258",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.07756": {
        "authors": [
            "Song Yan",
            "Min Li",
            "Bi Xinliang",
            "Jian Yang",
            "Yusen Zhang",
            "Guanye Xiong",
            "Yunwei Lan",
            "Tao Zhang",
            "Wei Zhai",
            "Zheng-Jun Zha"
        ],
        "title": "Beyond Randomness: Understand the Order of the Noise in Diffusion",
        "abstract": "arXiv:2511.07756v1 Announce Type: new  Abstract: In text-driven content generation (T2C) diffusion model, semantic of generated content is mostly attributed to the process of text embedding and attention mechanism interaction. The initial noise of the generation process is typically characterized as a random element that contributes to the diversity of the generated content. Contrary to this view, this paper reveals that beneath the random surface of noise lies strong analyzable patterns. Specifically, this paper first conducts a comprehensive analysis of the impact of random noise on the model's generation. We found that noise not only contains rich semantic information, but also allows for the erasure of unwanted semantics from it in an extremely simple way based on information theory, and using the equivalence between the generation process of diffusion model and semantic injection to inject semantics into the cleaned noise. Then, we mathematically decipher these observations and propose a simple but efficient training-free and universal two-step \"Semantic Erasure-Injection\" process to modulate the initial noise in T2C diffusion model. Experimental results demonstrate that our method is consistently effective across various T2C models based on both DiT and UNet architectures and presents a novel perspective for optimizing the generation of diffusion model, providing a universal tool for consistent generation.",
        "arxiv_id": "2511.07756",
        "ARXIVID": "2511.07756",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}