{
    "2511.16917": {
        "authors": [
            "Chi Zhang",
            "Jiepeng Wang",
            "Youming Wang",
            "Yuanzhi Liang",
            "Xiaoyan Yang",
            "Zuoxin Li",
            "Haibin Huang",
            "Xuelong Li"
        ],
        "title": "UniModel: A Visual-Only Framework for Unified Multimodal Understanding and Generation",
        "abstract": "arXiv:2511.16917v1 Announce Type: new  Abstract: We present UniModel, a unified generative model that jointly supports visual understanding and visual generation within a single pixel-to-pixel diffusion framework. Our goal is to achieve unification along three axes: the model, the tasks, and the representations. At the representation level, we eliminate modality discrepancies by mapping both text and images into a shared visual space: textual prompts are rendered as painted text images on a clean canvas, and all inputs and outputs are treated purely as RGB pixels. This yields a fully vision-native formulation of multimodal learning. At the task level, a broad range of vision-language problems are cast as pixel-to-pixel transformations in this visual space. For understanding tasks, the model takes an RGB image and produces a painted text image that visually encodes the semantic prediction. For generation tasks, painted text images serve as visual conditions that guide realistic and semantically aligned image synthesis. Captioning and text-to-image generation thus become different directions of the same underlying visual translation process. At the model level, we instantiate a single Unified Diffusion Transformer trained with rectified flow in pixel space. A shared backbone jointly learns bidirectional mappings between natural images and painted text images, with lightweight task embeddings to specify the desired direction. Experiments on text-to-image synthesis and image-to-text understanding demonstrate strong cross-modal alignment and emergent controllability such as cycle-consistent image-caption-image loops. Our initial exploration suggests that unifying model, tasks, and representations in a single visual space is a promising paradigm for general-purpose multimodal intelligence.",
        "arxiv_id": "2511.16917",
        "ARXIVID": "2511.16917",
        "COMMENT": "Matches criterion 2 closely with a unified diffusion model for multiple vision tasks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2511.17448": {
        "authors": [
            "Yuqi Li",
            "Junhao Dong",
            "Chuanguang Yang",
            "Shiping Wen",
            "Piotr Koniusz",
            "Tingwen Huang",
            "Yingli Tian",
            "Yew-Soon Ong"
        ],
        "title": "MMT-ARD: Multimodal Multi-Teacher Adversarial Distillation for Robust Vision-Language Models",
        "abstract": "arXiv:2511.17448v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) are increasingly deployed in safety-critical applications, making their adversarial robustness a crucial concern. While adversarial knowledge distillation has shown promise in transferring robustness from teacher to student models, traditional single-teacher approaches suffer from limited knowledge diversity, slow convergence, and difficulty in balancing robustness and accuracy. To address these challenges, we propose MMT-ARD: a Multimodal Multi-Teacher Adversarial Robust Distillation framework. Our key innovation is a dual-teacher knowledge fusion architecture that collaboratively optimizes clean feature preservation and robust feature enhancement. To better handle challenging adversarial examples, we introduce a dynamic weight allocation strategy based on teacher confidence, enabling adaptive focus on harder samples. Moreover, to mitigate bias among teachers, we design an adaptive sigmoid-based weighting function that balances the strength of knowledge transfer across modalities. Extensive experiments on ImageNet and zero-shot benchmarks demonstrate that MMT-ARD improves robust accuracy by +4.32% and zero-shot accuracy by +3.5% on the ViT-B-32 model, while achieving a 2.3x increase in training efficiency over traditional single-teacher methods. These results highlight the effectiveness and scalability of MMT-ARD in enhancing the adversarial robustness of multimodal large models. Our codes are available at https://github.com/itsnotacie/MMT-ARD.",
        "arxiv_id": "2511.17448",
        "ARXIVID": "2511.17448",
        "COMMENT": "The paper does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.16719": {
        "authors": [
            "Nicolas Carion",
            "Laura Gustafson",
            "Yuan-Ting Hu",
            "Shoubhik Debnath",
            "Ronghang Hu",
            "Didac Suris",
            "Chaitanya Ryali",
            "Kalyan Vasudev Alwala",
            "Haitham Khedr",
            "Andrew Huang",
            "Jie Lei",
            "Tengyu Ma",
            "Baishan Guo",
            "Arpit Kalla",
            "Markus Marks",
            "Joseph Greer",
            "Meng Wang",
            "Peize Sun",
            "Roman R\\\"adle",
            "Triantafyllos Afouras",
            "Effrosyni Mavroudi",
            "Katherine Xu",
            "Tsung-Han Wu",
            "Yu Zhou",
            "Liliane Momeni",
            "Rishi Hazra",
            "Shuangrui Ding",
            "Sagar Vaze",
            "Francois Porcher",
            "Feng Li",
            "Siyuan Li",
            "Aishwarya Kamath",
            "Ho Kei Cheng",
            "Piotr Doll\\'ar",
            "Nikhila Ravi",
            "Kate Saenko",
            "Pengchuan Zhang",
            "Christoph Feichtenhofer"
        ],
        "title": "SAM 3: Segment Anything with Concepts",
        "abstract": "arXiv:2511.16719v1 Announce Type: new  Abstract: We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.",
        "arxiv_id": "2511.16719",
        "ARXIVID": "2511.16719",
        "COMMENT": "The paper does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}