{
    "2508.05399": {
        "authors": [
            "Wonjun Kang",
            "Byeongkeun Ahn",
            "Minjae Lee",
            "Kevin Galim",
            "Seunghyuk Oh",
            "Hyung Il Koo",
            "Nam Ik Cho"
        ],
        "title": "UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation",
        "abstract": "arXiv:2508.05399v1 Announce Type: new  Abstract: Text-to-image (T2I) generation has been actively studied using Diffusion Models and Autoregressive Models. Recently, Masked Generative Transformers have gained attention as an alternative to Autoregressive Models to overcome the inherent limitations of causal attention and autoregressive decoding through bidirectional attention and parallel decoding, enabling efficient and high-quality image generation. However, compositional T2I generation remains challenging, as even state-of-the-art Diffusion Models often fail to accurately bind attributes and achieve proper text-image alignment. While Diffusion Models have been extensively studied for this issue, Masked Generative Transformers exhibit similar limitations but have not been explored in this context. To address this, we propose Unmasking with Contrastive Attention Guidance (UNCAGE), a novel training-free method that improves compositional fidelity by leveraging attention maps to prioritize the unmasking of tokens that clearly represent individual objects. UNCAGE consistently improves performance in both quantitative and qualitative evaluations across multiple benchmarks and metrics, with negligible inference overhead. Our code is available at https://github.com/furiosa-ai/uncage.",
        "arxiv_id": "2508.05399",
        "ARXIVID": "2508.05399",
        "COMMENT": "The paper does not match any specific criteria closely. It focuses on text-to-image generation using masked generative transformers, but not in a unified multi-task framework.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.04818": {
        "authors": [
            "Mehrdad Moradi",
            "Marco Grasso",
            "Bianca Maria Colosimo",
            "Kamran Paynabar"
        ],
        "title": "Single-Step Reconstruction-Free Anomaly Detection and Segmentation via Diffusion Models",
        "abstract": "arXiv:2508.04818v1 Announce Type: new  Abstract: Generative models have demonstrated significant success in anomaly detection and segmentation over the past decade. Recently, diffusion models have emerged as a powerful alternative, outperforming previous approaches such as GANs and VAEs. In typical diffusion-based anomaly detection, a model is trained on normal data, and during inference, anomalous images are perturbed to a predefined intermediate step in the forward diffusion process. The corresponding normal image is then reconstructed through iterative reverse sampling.   However, reconstruction-based approaches present three major challenges: (1) the reconstruction process is computationally expensive due to multiple sampling steps, making real-time applications impractical; (2) for complex or subtle patterns, the reconstructed image may correspond to a different normal pattern rather than the original input; and (3) Choosing an appropriate intermediate noise level is challenging because it is application-dependent and often assumes prior knowledge of anomalies, an assumption that does not hold in unsupervised settings.   We introduce Reconstruction-free Anomaly Detection with Attention-based diffusion models in Real-time (RADAR), which overcomes the limitations of reconstruction-based anomaly detection. Unlike current SOTA methods that reconstruct the input image, RADAR directly produces anomaly maps from the diffusion model, improving both detection accuracy and computational efficiency. We evaluate RADAR on real-world 3D-printed material and the MVTec-AD dataset. Our approach surpasses state-of-the-art diffusion-based and statistical machine learning models across all key metrics, including accuracy, precision, recall, and F1 score. Specifically, RADAR improves F1 score by 7% on MVTec-AD and 13% on the 3D-printed material dataset compared to the next best model.   Code available at: https://github.com/mehrdadmoradi124/RADAR",
        "arxiv_id": "2508.04818",
        "ARXIVID": "2508.04818",
        "COMMENT": "The paper does not match any specific criteria closely. It focuses on anomaly detection and segmentation using diffusion models, but not in a unified multi-task framework.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}