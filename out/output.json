{
    "2601.08881": {
        "authors": [
            "Yu Xu",
            "Hongbin Yan",
            "Juan Cao",
            "Yiji Cheng",
            "Tiankai Hang",
            "Runze He",
            "Zijin Yin",
            "Shiyi Zhang",
            "Yuxin Zhang",
            "Jintao Li",
            "Chunyu Wang",
            "Qinglin Lu",
            "Tong-Yee Lee",
            "Fan Tang"
        ],
        "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
        "abstract": "arXiv:2601.08881v1 Announce Type: new  Abstract: Unified image generation and editing models suffer from severe task interference in dense diffusion transformers architectures, where a shared parameter space must compromise between conflicting objectives (e.g., local editing v.s. subject-driven generation). While the sparse Mixture-of-Experts (MoE) paradigm is a promising solution, its gating networks remain task-agnostic, operating based on local features, unaware of global task intent. This task-agnostic nature prevents meaningful specialization and fails to resolve the underlying task interference. In this paper, we propose a novel framework to inject semantic intent into MoE routing. We introduce a Hierarchical Task Semantic Annotation scheme to create structured task descriptors (e.g., scope, type, preservation). We then design Predictive Alignment Regularization to align internal routing decisions with the task's high-level semantics. This regularization evolves the gating network from a task-agnostic executor to a dispatch center. Our model effectively mitigates task interference, outperforming dense baselines in fidelity and quality, and our analysis shows that experts naturally develop clear and semantically correlated specializations.",
        "arxiv_id": "2601.08881",
        "ARXIVID": "2601.08881",
        "COMMENT": "Matches criterion 2: Unified Diffusion Models for multi-task vision tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2601.09213": {
        "authors": [
            "Jialu Li",
            "Taiyan Zhou"
        ],
        "title": "SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion",
        "abstract": "arXiv:2601.09213v1 Announce Type: new  Abstract: Reconstructing natural visual scenes from neural activity is a key challenge in neuroscience and computer vision. We present SpikeVAEDiff, a novel two-stage framework that combines a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to generate high-resolution and semantically meaningful image reconstructions from neural spike data. In the first stage, VDVAE produces low-resolution preliminary reconstructions by mapping neural spike signals to latent representations. In the second stage, regression models map neural spike signals to CLIP-Vision and CLIP-Text features, enabling Versatile Diffusion to refine the images via image-to-image generation.   We evaluate our approach on the Allen Visual Coding-Neuropixels dataset and analyze different brain regions. Our results show that the VISI region exhibits the most prominent activation and plays a key role in reconstruction quality. We present both successful and unsuccessful reconstruction examples, reflecting the challenges of decoding neural activity. Compared with fMRI-based approaches, spike data provides superior temporal and spatial resolution. We further validate the effectiveness of the VDVAE model and conduct ablation studies demonstrating that data from specific brain regions significantly enhances reconstruction performance.",
        "arxiv_id": "2601.09213",
        "ARXIVID": "2601.09213",
        "COMMENT": "Does not match any specific criteria closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}