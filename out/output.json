{
    "2504.14151": {
        "authors": [
            "Sergio Arnaud",
            "Paul McVay",
            "Ada Martin",
            "Arjun Majumdar",
            "Krishna Murthy Jatavallabhula",
            "Phillip Thomas",
            "Ruslan Partsey",
            "Daniel Dugas",
            "Abha Gejji",
            "Alexander Sax",
            "Vincent-Pierre Berges",
            "Mikael Henaff",
            "Ayush Jain",
            "Ang Cao",
            "Ishita Prasad",
            "Mrinal Kalakrishnan",
            "Michael Rabbat",
            "Nicolas Ballas",
            "Mido Assran",
            "Oleksandr Maksymets",
            "Aravind Rajeswaran",
            "Franziska Meier"
        ],
        "title": "Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D",
        "abstract": "arXiv:2504.14151v1 Announce Type: new  Abstract: We present LOCATE 3D, a model for localizing objects in 3D scenes from referring expressions like \"the small coffee table between the sofa and the lamp.\" LOCATE 3D sets a new state-of-the-art on standard referential grounding benchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D operates directly on sensor observation streams (posed RGB-D frames), enabling real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA, a novel self-supervised learning (SSL) algorithm applicable to sensor point clouds. It takes as input a 3D pointcloud featurized using 2D foundation models (CLIP, DINO). Subsequently, masked prediction in latent space is employed as a pretext task to aid the self-supervised learning of contextualized pointcloud features. Once trained, the 3D-JEPA encoder is finetuned alongside a language-conditioned decoder to jointly predict 3D masks and bounding boxes. Additionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential grounding, spanning multiple capture setups with over 130K annotations. This enables a systematic study of generalization capabilities as well as a stronger model.",
        "arxiv_id": "2504.14151",
        "ARXIVID": "2504.14151",
        "COMMENT": "Matches criteria 1 and 3 as it introduces a new method (3D-JEPA) for spatial understanding and a new benchmark dataset (LOCATE 3D DATASET) for referential grounding in 3D scenes.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2504.13995": {
        "authors": [
            "Andrea Amaduzzi",
            "Pierluigi Zama Ramirez",
            "Giuseppe Lisanti",
            "Samuele Salti",
            "Luigi Di Stefano"
        ],
        "title": "Scaling LLaNA: Advancing NeRF-Language Understanding Through Large-Scale Training",
        "abstract": "arXiv:2504.13995v1 Announce Type: new  Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have shown remarkable capabilities in understanding both images and 3D data, yet these modalities face inherent limitations in comprehensively representing object geometry and appearance. Neural Radiance Fields (NeRFs) have emerged as a promising alternative, encoding both geometric and photorealistic properties within the weights of a simple Multi-Layer Perceptron (MLP). This work investigates the feasibility and effectiveness of ingesting NeRFs into an MLLM. We introduce LLaNA, the first MLLM able to perform new tasks such as NeRF captioning and Q\\&A, by directly processing the weights of a NeRF's MLP. Notably, LLaNA is able to extract information about the represented objects without the need to render images or materialize 3D data structures. In addition, we build the first large-scale NeRF-language dataset, composed by more than 300K NeRFs trained on ShapeNet and Objaverse, with paired textual annotations that enable various NeRF-language tasks. Based on this dataset, we develop a benchmark to evaluate the NeRF understanding capability of our method. Results show that directly processing NeRF weights leads to better performance on NeRF-Language tasks compared to approaches that rely on either 2D or 3D representations derived from NeRFs.",
        "arxiv_id": "2504.13995",
        "ARXIVID": "2504.13995",
        "COMMENT": "Matches criterion 2 as it introduces a new MLLM (LLaNA) that processes NeRF weights directly for NeRF-language tasks.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2504.15271": {
        "authors": [
            "Guo Chen",
            "Zhiqi Li",
            "Shihao Wang",
            "Jindong Jiang",
            "Yicheng Liu",
            "Lidong Lu",
            "De-An Huang",
            "Wonmin Byeon",
            "Matthieu Le",
            "Tuomas Rintamaki",
            "Tyler Poon",
            "Max Ehrlich",
            "Tuomas Rintamaki",
            "Tyler Poon",
            "Tong Lu",
            "Limin Wang",
            "Bryan Catanzaro",
            "Jan Kautz",
            "Andrew Tao",
            "Zhiding Yu",
            "Guilin Liu"
        ],
        "title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models",
        "abstract": "arXiv:2504.15271v1 Announce Type: new  Abstract: We introduce Eagle 2.5, a family of frontier vision-language models (VLMs) for long-context multimodal learning. Our work addresses the challenges in long video comprehension and high-resolution image understanding, introducing a generalist framework for both tasks. The proposed training framework incorporates Automatic Degrade Sampling and Image Area Preservation, two techniques that preserve contextual integrity and visual details. The framework also includes numerous efficiency optimizations in the pipeline for long-context data training. Finally, we propose Eagle-Video-110K, a novel dataset that integrates both story-level and clip-level annotations, facilitating long-video understanding. Eagle 2.5 demonstrates substantial improvements on long-context multimodal benchmarks, providing a robust solution to the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B achieves 72.4% on Video-MME with 512 input frames, matching the results of top-tier commercial model such as GPT-4o and large-scale open-source models like Qwen2.5-VL-72B and InternVL2.5-78B.",
        "arxiv_id": "2504.15271",
        "ARXIVID": "2504.15271",
        "COMMENT": "Matches criterion 2 as it introduces a new vision-language model (Eagle 2.5) for long-context multimodal learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2504.14032": {
        "authors": [
            "Haiwen Huang",
            "Anpei Chen",
            "Volodymyr Havrylov",
            "Andreas Geiger",
            "Dan Zhang"
        ],
        "title": "LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models",
        "abstract": "arXiv:2504.14032v1 Announce Type: new  Abstract: Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved impressive results on various downstream tasks, but their limited feature resolution hampers performance in applications requiring pixel-level understanding. Feature upsampling offers a promising direction to address this challenge. In this work, we identify two critical factors for enhancing feature upsampling: the upsampler architecture and the training objective. For the upsampler architecture, we introduce a coordinate-based cross-attention transformer that integrates the high-resolution images with coordinates and low-resolution VFM features to generate sharp, high-quality features. For the training objective, we propose constructing high-resolution pseudo-groundtruth features by leveraging class-agnostic masks and self-distillation. Our approach effectively captures fine-grained details and adapts flexibly to various input and feature resolutions. Through experiments, we demonstrate that our approach significantly outperforms existing feature upsampling techniques across various downstream tasks. Our code is released at https://github.com/andrehuang/loftup.",
        "arxiv_id": "2504.14032",
        "ARXIVID": "2504.14032",
        "COMMENT": "This paper proposes a feature upsampler for vision foundation models, directly matching criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2504.14395": {
        "authors": [
            "Chung-En (Johnny)",
            "Yu (Neil)",
            "Hsuan-Chih (Neil)",
            "Chen",
            "Brian Jalaian",
            "Nathaniel D. Bastian"
        ],
        "title": "Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models",
        "abstract": "arXiv:2504.14395v1 Announce Type: new  Abstract: To develop trustworthy Vision-Language Models (VLMs), it is essential to address adversarial robustness and hallucination mitigation, both of which impact factual accuracy in high-stakes applications such as defense and healthcare. Existing methods primarily focus on either adversarial defense or hallucination post-hoc correction, leaving a gap in unified robustness strategies. We introduce \\textbf{Hydra}, an adaptive agentic framework that enhances plug-in VLMs through iterative reasoning, structured critiques, and cross-model verification, improving both resilience to adversarial perturbations and intrinsic model errors. Hydra employs an Action-Critique Loop, where it retrieves and critiques visual information, leveraging Chain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine outputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to both adversarial manipulations and intrinsic model errors, making it robust to malicious perturbations and hallucination-related inaccuracies. We evaluate Hydra on four VLMs, three hallucination benchmarks, two adversarial attack strategies, and two adversarial defense methods, assessing performance on both clean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs and state-of-the-art (SOTA) dehallucination methods, even without explicit adversarial defenses, demonstrating enhanced robustness and factual consistency. By bridging adversarial resistance and hallucination mitigation, Hydra provides a scalable, training-free solution for improving the reliability of VLMs in real-world applications.",
        "arxiv_id": "2504.14395",
        "ARXIVID": "2504.14395",
        "COMMENT": "Matches criterion 2 as it introduces Hydra, a framework for improving VLMs with iterative reasoning and cross-model verification.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2504.14128": {
        "authors": [
            "Christopher Zhang Cui",
            "Xingdi Yuan",
            "Zhang Xiao",
            "Prithviraj Ammanabrolu",
            "Marc-Alexandre C\\^ot\\'e"
        ],
        "title": "TALES: Text Adventure Learning Environment Suite",
        "abstract": "arXiv:2504.14128v1 Announce Type: new  Abstract: Reasoning is an essential skill to enable Large Language Models (LLMs) to interact with the world. As tasks become more complex, they demand increasingly sophisticated and diverse reasoning capabilities for sequential decision-making, requiring structured reasoning over the context history to determine the next best action. We introduce TALES, a diverse collection of synthetic and human-written text-adventure games designed to challenge and evaluate diverse reasoning capabilities. We present results over a range of LLMs, open- and closed-weights, performing a qualitative analysis on the top performing models. Despite an impressive showing on synthetic games, even the top LLM-driven agents fail to achieve 15% on games designed for human enjoyment. Code and visualization of the experiments can be found at https://microsoft.github.io/tales.",
        "arxiv_id": "2504.14128",
        "ARXIVID": "2504.14128",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (TALES) for reasoning in embodied AI with text-adventure games.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2504.15134": {
        "authors": [
            "Xiao Zhang",
            "Lu Zou",
            "Tao Lu",
            "Yuan Yao",
            "Zhangjin Huang",
            "Guoping Wang"
        ],
        "title": "Instance-Adaptive Keypoint Learning with Local-to-Global Geometric Aggregation for Category-Level Object Pose Estimation",
        "abstract": "arXiv:2504.15134v1 Announce Type: new  Abstract: Category-level object pose estimation aims to predict the 6D pose and size of previously unseen instances from predefined categories, requiring strong generalization across diverse object instances. Although many previous methods attempt to mitigate intra-class variations, they often struggle with instances exhibiting complex geometries or significant deviations from canonical shapes. To address this challenge, we propose INKL-Pose, a novel category-level object pose estimation framework that enables INstance-adaptive Keypoint Learning with local-to-global geometric aggregation. Specifically, our approach first predicts semantically consistent and geometric informative keypoints through an Instance-Adaptive Keypoint Generator, then refines them with: (1) a Local Keypoint Feature Aggregator capturing fine-grained geometries, and (2) a Global Keypoint Feature Aggregator using bidirectional Mamba for structural consistency. To enable bidirectional modeling in Mamba, we introduce a Feature Sequence Flipping strategy that preserves spatial coherence while constructing backward feature sequences. Additionally, we design a surface loss and a separation loss to enforce uniform coverage and spatial diversity in keypoint distribution. The generated keypoints are finally mapped to a canonical space for regressing the object's 6D pose and size. Extensive experiments on CAMERA25, REAL275, and HouseCat6D demonstrate that INKL-Pose achieves state-of-the-art performance and significantly outperforms existing methods.",
        "arxiv_id": "2504.15134",
        "ARXIVID": "2504.15134",
        "COMMENT": "Matches criterion 1 as it proposes a novel method for spatial understanding in category-level object pose estimation using instance-adaptive keypoint learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.15032": {
        "authors": [
            "Weijie He",
            "Mushui Liu",
            "Yunlong Yu",
            "Zhao Wang",
            "Chao Wu"
        ],
        "title": "DyST-XL: Dynamic Layout Planning and Content Control for Compositional Text-to-Video Generation",
        "abstract": "arXiv:2504.15032v1 Announce Type: new  Abstract: Compositional text-to-video generation, which requires synthesizing dynamic scenes with multiple interacting entities and precise spatial-temporal relationships, remains a critical challenge for diffusion-based models. Existing methods struggle with layout discontinuity, entity identity drift, and implausible interaction dynamics due to unconstrained cross-attention mechanisms and inadequate physics-aware reasoning. To address these limitations, we propose DyST-XL, a \\textbf{training-free} framework that enhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through frame-aware control. DyST-XL integrates three key innovations: (1) A Dynamic Layout Planner that leverages large language models (LLMs) to parse input prompts into entity-attribute graphs and generates physics-aware keyframe layouts, with intermediate frames interpolated via trajectory optimization; (2) A Dual-Prompt Controlled Attention Mechanism that enforces localized text-video alignment through frame-aware attention masking, achieving the precise control over individual entities; and (3) An Entity-Consistency Constraint strategy that propagates first-frame feature embeddings to subsequent frames during denoising, preserving object identity without manual annotation. Experiments demonstrate that DyST-XL excels in compositional text-to-video generation, significantly improving performance on complex prompts and bridging a crucial gap in training-free video synthesis.",
        "arxiv_id": "2504.15032",
        "ARXIVID": "2504.15032",
        "COMMENT": "Matches criterion 2 as it discusses compositional text-to-video generation using large language models and visual reasoning.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2504.14553": {
        "authors": [
            "Weijun Zhuang",
            "Qizhang Li",
            "Xin Li",
            "Ming Liu",
            "Xiaopeng Hong",
            "Feng Gao",
            "Fan Yang",
            "Wangmeng Zuo"
        ],
        "title": "Grounding-MD: Grounded Video-language Pre-training for Open-World Moment Detection",
        "abstract": "arXiv:2504.14553v1 Announce Type: new  Abstract: Temporal Action Detection and Moment Retrieval constitute two pivotal tasks in video understanding, focusing on precisely localizing temporal segments corresponding to specific actions or events. Recent advancements introduced Moment Detection to unify these two tasks, yet existing approaches remain confined to closed-set scenarios, limiting their applicability in open-world contexts. To bridge this gap, we present Grounding-MD, an innovative, grounded video-language pre-training framework tailored for open-world moment detection. Our framework incorporates an arbitrary number of open-ended natural language queries through a structured prompt mechanism, enabling flexible and scalable moment detection. Grounding-MD leverages a Cross-Modality Fusion Encoder and a Text-Guided Fusion Decoder to facilitate comprehensive video-text alignment and enable effective cross-task collaboration. Through large-scale pre-training on temporal action detection and moment retrieval datasets, Grounding-MD demonstrates exceptional semantic representation learning capabilities, effectively handling diverse and complex query conditions. Comprehensive evaluations across four benchmark datasets including ActivityNet, THUMOS14, ActivityNet-Captions, and Charades-STA demonstrate that Grounding-MD establishes new state-of-the-art performance in zero-shot and supervised settings in open-world moment detection scenarios. All source code and trained models will be released.",
        "arxiv_id": "2504.14553",
        "ARXIVID": "2504.14553",
        "COMMENT": "This paper presents a grounded video-language pre-training framework for open-world moment detection, aligning with criterion 2 (new MLLMs).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.14467": {
        "authors": [
            "Jiachen Li",
            "Qing Xie",
            "Xiaohan Yu",
            "Hongyun Wang",
            "Jinyu Xu",
            "Yongjian Liu",
            "Yongsheng Gao"
        ],
        "title": "LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image Segmentation",
        "abstract": "arXiv:2504.14467v1 Announce Type: new  Abstract: Zero-shot referring image segmentation aims to locate and segment the target region based on a referring expression, with the primary challenge of aligning and matching semantics across visual and textual modalities without training. Previous works address this challenge by utilizing Vision-Language Models and mask proposal networks for region-text matching. However, this paradigm may lead to incorrect target localization due to the inherent ambiguity and diversity of free-form referring expressions. To alleviate this issue, we present LGD (Leveraging Generative Descriptions), a framework that utilizes the advanced language generation capabilities of Multi-Modal Large Language Models to enhance region-text matching performance in Vision-Language Models. Specifically, we first design two kinds of prompts, the attribute prompt and the surrounding prompt, to guide the Multi-Modal Large Language Models in generating descriptions related to the crucial attributes of the referent object and the details of surrounding objects, referred to as attribute description and surrounding description, respectively. Secondly, three visual-text matching scores are introduced to evaluate the similarity between instance-level visual features and textual features, which determines the mask most associated with the referring expression. The proposed method achieves new state-of-the-art performance on three public datasets RefCOCO, RefCOCO+ and RefCOCOg, with maximum improvements of 9.97% in oIoU and 11.29% in mIoU compared to previous methods.",
        "arxiv_id": "2504.14467",
        "ARXIVID": "2504.14467",
        "COMMENT": "This paper introduces a framework leveraging multi-modal large language models for zero-shot referring image segmentation, matching criterion 2 (new MLLMs).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.14391": {
        "authors": [
            "Rahul Thapa",
            "Andrew Li",
            "Qingyang Wu",
            "Bryan He",
            "Yuki Sahashi",
            "Christina Binder",
            "Angela Zhang",
            "Ben Athiwaratkun",
            "Shuaiwen Leon Song",
            "David Ouyang",
            "James Zou"
        ],
        "title": "How Well Can General Vision-Language Models Learn Medicine By Watching Public Educational Videos?",
        "abstract": "arXiv:2504.14391v1 Announce Type: new  Abstract: Publicly available biomedical videos, such as those on YouTube, serve as valuable educational resources for medical students. Unlike standard machine learning datasets, these videos are designed for human learners, often mixing medical imagery with narration, explanatory diagrams, and contextual framing. In this work, we investigate whether such pedagogically rich, yet non-standardized and heterogeneous videos can effectively teach general-domain vision-language models biomedical knowledge. To this end, we introduce OpenBiomedVi, a biomedical video instruction tuning dataset comprising 1031 hours of video-caption and Q/A pairs, curated through a multi-step human-in-the-loop pipeline. Diverse biomedical video datasets are rare, and OpenBiomedVid fills an important gap by providing instruction-style supervision grounded in real-world educational content. Surprisingly, despite the informal and heterogeneous nature of these videos, the fine-tuned Qwen-2-VL models exhibit substantial performance improvements across most benchmarks. The 2B model achieves gains of 98.7% on video tasks, 71.2% on image tasks, and 0.2% on text tasks. The 7B model shows improvements of 37.09% on video and 11.2% on image tasks, with a slight degradation of 2.7% on text tasks compared to their respective base models. To address the lack of standardized biomedical video evaluation datasets, we also introduce two new expert curated benchmarks, MIMICEchoQA and SurgeryVideoQA. On these benchmarks, the 2B model achieves gains of 99.1% and 98.1%, while the 7B model shows gains of 22.5% and 52.1%, respectively, demonstrating the models' ability to generalize and perform biomedical video understanding on cleaner and more standardized datasets than those seen during training. These results suggest that educational videos created for human learning offer a surprisingly effective training signal for biomedical VLMs.",
        "arxiv_id": "2504.14391",
        "ARXIVID": "2504.14391",
        "COMMENT": "Matches criterion 2 as it discusses visual large language models (VLLMs) and their application in biomedical video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2504.14848": {
        "authors": [
            "Yunpu Zhao",
            "Rui Zhang",
            "Junbin Xiao",
            "Ruibo Hou",
            "Jiaming Guo",
            "Zihao Zhang",
            "Yifan Hao",
            "Yunji Chen"
        ],
        "title": "Object-Level Verbalized Confidence Calibration in Vision-Language Models via Semantic Perturbation",
        "abstract": "arXiv:2504.14848v1 Announce Type: new  Abstract: Vision-language models (VLMs) excel in various multimodal tasks but frequently suffer from poor calibration, resulting in misalignment between their verbalized confidence and response correctness. This miscalibration undermines user trust, especially when models confidently provide incorrect or fabricated information. In this work, we propose a novel Confidence Calibration through Semantic Perturbation (CSP) framework to improve the calibration of verbalized confidence for VLMs in response to object-centric queries. We first introduce a perturbed dataset where Gaussian noise is applied to the key object regions to simulate visual uncertainty at different confidence levels, establishing an explicit mapping between visual ambiguity and confidence levels. We further enhance calibration through a two-stage training process combining supervised fine-tuning on the perturbed dataset with subsequent preference optimization. Extensive experiments on popular benchmarks demonstrate that our method significantly improves the alignment between verbalized confidence and response correctness while maintaining or enhancing overall task performance. These results highlight the potential of semantic perturbation as a practical tool for improving the reliability and interpretability of VLMs.",
        "arxiv_id": "2504.14848",
        "ARXIVID": "2504.14848",
        "COMMENT": "Matches criterion 2 as it focuses on improving calibration in vision-language models (VLMs) through a novel semantic perturbation framework.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2504.14687": {
        "authors": [
            "Seokju Cho",
            "Jiahui Huang",
            "Seungryong Kim",
            "Joon-Young Lee"
        ],
        "title": "Seurat: From Moving Points to Depth",
        "abstract": "arXiv:2504.14687v1 Announce Type: new  Abstract: Accurate depth estimation from monocular videos remains challenging due to ambiguities inherent in single-view geometry, as crucial depth cues like stereopsis are absent. However, humans often perceive relative depth intuitively by observing variations in the size and spacing of objects as they move. Inspired by this, we propose a novel method that infers relative depth by examining the spatial relationships and temporal evolution of a set of tracked 2D trajectories. Specifically, we use off-the-shelf point tracking models to capture 2D trajectories. Then, our approach employs spatial and temporal transformers to process these trajectories and directly infer depth changes over time. Evaluated on the TAPVid-3D benchmark, our method demonstrates robust zero-shot performance, generalizing effectively from synthetic to real-world datasets. Results indicate that our approach achieves temporally smooth, high-accuracy depth predictions across diverse domains.",
        "arxiv_id": "2504.14687",
        "ARXIVID": "2504.14687",
        "COMMENT": "Matches criterion 1 as it proposes a novel method for depth estimation using spatial and temporal relationships, which relates to spatial understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2504.14267": {
        "authors": [
            "Li Yu",
            "Xuanzhe Sun",
            "Wei Zhou",
            "Moncef Gabbouj"
        ],
        "title": "Text-Audio-Visual-conditioned Diffusion Model for Video Saliency Prediction",
        "abstract": "arXiv:2504.14267v1 Announce Type: new  Abstract: Video saliency prediction is crucial for downstream applications, such as video compression and human-computer interaction. With the flourishing of multimodal learning, researchers started to explore multimodal video saliency prediction, including audio-visual and text-visual approaches. Auditory cues guide the gaze of viewers to sound sources, while textual cues provide semantic guidance for understanding video content. Integrating these complementary cues can improve the accuracy of saliency prediction. Therefore, we attempt to simultaneously analyze visual, auditory, and textual modalities in this paper, and propose TAVDiff, a Text-Audio-Visual-conditioned Diffusion Model for video saliency prediction. TAVDiff treats video saliency prediction as an image generation task conditioned on textual, audio, and visual inputs, and predicts saliency maps through stepwise denoising. To effectively utilize text, a large multimodal model is used to generate textual descriptions for video frames and introduce a saliency-oriented image-text response (SITR) mechanism to generate image-text response maps. It is used as conditional information to guide the model to localize the visual regions that are semantically related to the textual description. Regarding the auditory modality, it is used as another conditional information for directing the model to focus on salient regions indicated by sounds. At the same time, since the diffusion transformer (DiT) directly concatenates the conditional information with the timestep, which may affect the estimation of the noise level. To achieve effective conditional guidance, we propose Saliency-DiT, which decouples the conditional information from the timestep. Experimental results show that TAVDiff outperforms existing methods, improving 1.03\\%, 2.35\\%, 2.71\\% and 0.33\\% on SIM, CC, NSS and AUC-J metrics, respectively.",
        "arxiv_id": "2504.14267",
        "ARXIVID": "2504.14267",
        "COMMENT": "Matches criterion 2 as it introduces a multimodal diffusion model for video saliency prediction, leveraging text, audio, and visual inputs.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2504.14870": {
        "authors": [
            "Hongru Wang",
            "Cheng Qian",
            "Wanjun Zhong",
            "Xiusi Chen",
            "Jiahao Qiu",
            "Shijue Huang",
            "Bowen Jin",
            "Mengdi Wang",
            "Kam-Fai Wong",
            "Heng Ji"
        ],
        "title": "OTC: Optimal Tool Calls via Reinforcement Learning",
        "abstract": "arXiv:2504.14870v1 Announce Type: new  Abstract: Tool-integrated reasoning (TIR) augments large language models (LLMs) with the ability to invoke external tools, such as search engines and code interpreters, to solve tasks beyond the capabilities of language-only reasoning. While reinforcement learning (RL) has shown promise in improving TIR by optimizing final answer correctness, existing approaches often overlook the efficiency and cost associated with tool usage. This can lead to suboptimal behavior, including excessive tool calls that increase computational and financial overhead, or insufficient tool use that compromises answer quality. In this work, we propose Optimal Tool Call-controlled Policy Optimization (OTC-PO), a simple yet effective RL-based framework that encourages models to produce accurate answers with minimal tool calls. Our method introduces a tool-integrated reward that jointly considers correctness and tool efficiency, promoting high tool productivity. We instantiate this framework within both Proximal Policy Optimization (PPO) and Group Relative Preference Optimization (GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and Qwen-Math across multiple QA benchmarks show that our approach reduces tool calls by up to 73.1\\% and improves tool productivity by up to 229.4\\%, while maintaining comparable answer accuracy. To the best of our knowledge, this is the first RL-based framework that explicitly optimizes tool-use efficiency in TIR.",
        "arxiv_id": "2504.14870",
        "ARXIVID": "2504.14870",
        "COMMENT": "Matches criterion 2 as it focuses on tool-integrated reasoning with LLMs and optimizing their efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2504.14526": {
        "authors": [
            "Tong Zeng",
            "Longfeng Wu",
            "Liang Shi",
            "Dawei Zhou",
            "Feng Guo"
        ],
        "title": "Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding",
        "abstract": "arXiv:2504.14526v1 Announce Type: new  Abstract: Vision Large Language Models (VLLMs) have demonstrated impressive capabilities in general visual tasks such as image captioning and visual question answering. However, their effectiveness in specialized, safety-critical domains like autonomous driving remains largely unexplored. Autonomous driving systems require sophisticated scene understanding in complex environments, yet existing multimodal benchmarks primarily focus on normal driving conditions, failing to adequately assess VLLMs' performance in safety-critical scenarios. To address this, we introduce DVBench, a pioneering benchmark designed to evaluate the performance of VLLMs in understanding safety-critical driving videos. Built around a hierarchical ability taxonomy that aligns with widely adopted frameworks for describing driving scenarios used in assessing highly automated driving systems, DVBench features 10,000 multiple-choice questions with human-annotated ground-truth answers, enabling a comprehensive evaluation of VLLMs' capabilities in perception and reasoning. Experiments on 14 SOTA VLLMs, ranging from 0.5B to 72B parameters, reveal significant performance gaps, with no model achieving over 40% accuracy, highlighting critical limitations in understanding complex driving scenarios. To probe adaptability, we fine-tuned selected models using domain-specific data from DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage points, with relative improvements of up to 43.59%. This improvement underscores the necessity of targeted adaptation to bridge the gap between general-purpose VLLMs and mission-critical driving applications. DVBench establishes an essential evaluation framework and research roadmap for developing VLLMs that meet the safety and robustness requirements for real-world autonomous systems. We released the benchmark toolbox and the fine-tuned model at: https://github.com/tong-zeng/DVBench.git.",
        "arxiv_id": "2504.14526",
        "ARXIVID": "2504.14526",
        "COMMENT": "Matches criterion 4 as it introduces a benchmark for VLLMs in safety-critical driving scenarios.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2504.15046": {
        "authors": [
            "Shilin Zhang",
            "Zican Hu",
            "Wenhao Wu",
            "Xinyi Xie",
            "Jianxiang Tang",
            "Chunlin Chen",
            "Daoyi Dong",
            "Yu Cheng",
            "Zhenhong Sun",
            "Zhi Wang"
        ],
        "title": "Text-to-Decision Agent: Learning Generalist Policies from Natural Language Supervision",
        "abstract": "arXiv:2504.15046v1 Announce Type: new  Abstract: RL systems usually tackle generalization by inferring task beliefs from high-quality samples or warmup explorations. The restricted form limits their generality and usability since these supervision signals are expensive and even infeasible to acquire in advance for unseen tasks. Learning directly from the raw text about decision tasks is a promising alternative to leverage a much broader source of supervision. In the paper, we propose Text-to-Decision Agent (T2DA), a simple and scalable framework that supervises generalist policy learning with natural language. We first introduce a generalized world model to encode multi-task decision data into a dynamics-aware embedding space. Then, inspired by CLIP, we predict which textual description goes with which decision embedding, effectively bridging their semantic gap via contrastive language-decision pre-training and aligning the text embeddings to comprehend the environment dynamics. After training the text-conditioned generalist policy, the agent can directly realize zero-shot text-to-decision generation in response to language instructions. Comprehensive experiments on MuJoCo and Meta-World benchmarks show that T2DA facilitates high-capacity zero-shot generalization and outperforms various types of baselines.",
        "arxiv_id": "2504.15046",
        "ARXIVID": "2504.15046",
        "COMMENT": "This paper introduces a framework for learning generalist policies from natural language supervision, which aligns with criterion 3 (new methods in embodied AI) as it focuses on zero-shot generalization in decision-making tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2504.14975": {
        "authors": [
            "Hongbin Xu",
            "Chaohui Yu",
            "Feng Xiao",
            "Jiazheng Xing",
            "Hai Ci",
            "Weitao Chen",
            "Ming Li"
        ],
        "title": "Cyc3D: Fine-grained Controllable 3D Generation via Cycle Consistency Regularization",
        "abstract": "arXiv:2504.14975v1 Announce Type: new  Abstract: Despite the remarkable progress of 3D generation, achieving controllability, i.e., ensuring consistency between generated 3D content and input conditions like edge and depth, remains a significant challenge. Existing methods often struggle to maintain accurate alignment, leading to noticeable discrepancies. To address this issue, we propose \\name{}, a new framework that enhances controllable 3D generation by explicitly encouraging cyclic consistency between the second-order 3D content, generated based on extracted signals from the first-order generation, and its original input controls. Specifically, we employ an efficient feed-forward backbone that can generate a 3D object from an input condition and a text prompt. Given an initial viewpoint and a control signal, a novel view is rendered from the generated 3D content, from which the extracted condition is used to regenerate the 3D content. This re-generated output is then rendered back to the initial viewpoint, followed by another round of control signal extraction, forming a cyclic process with two consistency constraints. \\emph{View consistency} ensures coherence between the two generated 3D objects, measured by semantic similarity to accommodate generative diversity. \\emph{Condition consistency} aligns the final extracted signal with the original input control, preserving structural or geometric details throughout the process. Extensive experiments on popular benchmarks demonstrate that \\name{} significantly improves controllability, especially for fine-grained details, outperforming existing methods across various conditions (e.g., +14.17\\% PSNR for edge, +6.26\\% PSNR for sketch).",
        "arxiv_id": "2504.14975",
        "ARXIVID": "2504.14975",
        "COMMENT": "Matches criterion 4 as it discusses controllable 3D generation and aligns with vision foundation models and their applications.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2504.14603": {
        "authors": [
            "Chaoyun Zhang",
            "He Huang",
            "Chiming Ni",
            "Jian Mu",
            "Si Qin",
            "Shilin He",
            "Lu Wang",
            "Fangkai Yang",
            "Pu Zhao",
            "Chao Du",
            "Liqun Li",
            "Yu Kang",
            "Zhao Jiang",
            "Suzhen Zheng",
            "Rujia Wang",
            "Jiaxu Qian",
            "Minghua Ma",
            "Jian-Guang Lou",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang"
        ],
        "title": "UFO2: The Desktop AgentOS",
        "abstract": "arXiv:2504.14603v1 Announce Type: new  Abstract: Recent Computer-Using Agents (CUAs), powered by multimodal large language models (LLMs), offer a promising direction for automating complex desktop workflows through natural language. However, most existing CUAs remain conceptual prototypes, hindered by shallow OS integration, fragile screenshot-based interaction, and disruptive execution.   We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs into practical, system-level automation. UFO2 features a centralized HostAgent for task decomposition and coordination, alongside a collection of application-specialized AppAgent equipped with native APIs, domain-specific knowledge, and a unified GUI--API action layer. This architecture enables robust task execution while preserving modularity and extensibility. A hybrid control detection pipeline fuses Windows UI Automation (UIA) with vision-based parsing to support diverse interface styles. Runtime efficiency is further enhanced through speculative multi-action planning, reducing per-step LLM overhead. Finally, a Picture-in-Picture (PiP) interface enables automation within an isolated virtual desktop, allowing agents and users to operate concurrently without interference.   We evaluate UFO2 across over 20 real-world Windows applications, demonstrating substantial improvements in robustness and execution accuracy over prior CUAs. Our results show that deep OS integration unlocks a scalable path toward reliable, user-aligned desktop automation.",
        "arxiv_id": "2504.14603",
        "ARXIVID": "2504.14603",
        "COMMENT": "Matches criterion 3 as it introduces a new system-level framework (UFO2) for desktop automation, which involves embodied agents.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.14693": {
        "authors": [
            "Enxin Song",
            "Wenhao Chai",
            "Weili Xu",
            "Jianwen Xie",
            "Yuxuan Liu",
            "Gaoang Wang"
        ],
        "title": "Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark",
        "abstract": "arXiv:2504.14693v1 Announce Type: new  Abstract: Recent advancements in language multimodal models (LMMs) for video have demonstrated their potential for understanding video content, yet the task of comprehending multi-discipline lectures remains largely unexplored. We introduce Video-MMLU, a massive benchmark designed to evaluate the capabilities of LMMs in understanding Multi-Discipline Lectures. We evaluate over 90 open-source and proprietary models, ranging from 0.5B to 40B parameters. Our results highlight the limitations of current models in addressing the cognitive challenges presented by these lectures, especially in tasks requiring both perception and reasoning. Additionally, we explore how the number of visual tokens and the large language models influence performance, offering insights into the interplay between multimodal perception and reasoning in lecture comprehension.",
        "arxiv_id": "2504.14693",
        "ARXIVID": "2504.14693",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (Video-MMLU) for evaluating multimodal models in lecture understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2504.14658": {
        "authors": [
            "Jing Zhang",
            "Dan Guo",
            "Zhangbin Li",
            "Meng Wang"
        ],
        "title": "EmoSEM: Segment and Explain Emotion Stimuli in Visual Art",
        "abstract": "arXiv:2504.14658v1 Announce Type: new  Abstract: This paper focuses on a key challenge in visual art understanding: given an art image, the model pinpoints pixel regions that trigger a specific human emotion, and generates linguistic explanations for the emotional arousal. Despite recent advances in art understanding, pixel-level emotion understanding still faces a dual challenge: first, the subjectivity of emotion makes it difficult for general segmentation models like SAM to adapt to emotion-oriented segmentation tasks; and second, the abstract nature of art expression makes it difficult for captioning models to balance pixel-level semantic understanding and emotion reasoning. To solve the above problems, this paper proposes the Emotion stimuli Segmentation and Explanation Model (EmoSEM) to endow the segmentation model SAM with emotion comprehension capability. First, to enable the model to perform segmentation under the guidance of emotional intent well, we introduce an emotional prompt with a learnable mask token as the conditional input for segmentation decoding. Then, we design an emotion projector to establish the association between emotion and visual features. Next, more importantly, to address emotion-visual stimuli alignment, we develop a lightweight prefix projector, a module that fuses the learned emotional mask with the corresponding emotion into a unified representation compatible with the language model.Finally, we input the joint visual, mask, and emotional tokens into the language model and output the emotional explanations. It ensures that the generated interpretations remain semantically and emotionally coherent with the visual stimuli. The method innovatively realizes end-to-end modeling from low-level pixel features to high-level emotion interpretation, providing the first interpretable fine-grained analysis framework for artistic emotion computing. Extensive experiments validate the effectiveness of our model.",
        "arxiv_id": "2504.14658",
        "ARXIVID": "2504.14658",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models for emotion understanding in visual art.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2504.14129": {
        "authors": [
            "Yaning Zhang",
            "Jiahe Zhang",
            "Chunjie Ma",
            "Weili Guan",
            "Tian Gan",
            "Zan Gao"
        ],
        "title": "BMRL: Bi-Modal Guided Multi-Perspective Representation Learning for Zero-Shot Deepfake Attribution",
        "abstract": "arXiv:2504.14129v1 Announce Type: new  Abstract: The challenge of tracing the source attribution of forged faces has gained significant attention due to the rapid advancement of generative models. However, existing deepfake attribution (DFA) works primarily focus on the interaction among various domains in vision modality, and other modalities such as texts and face parsing are not fully explored. Besides, they tend to fail to assess the generalization performance of deepfake attributors to unseen generators in a fine-grained manner. In this paper, we propose a novel bi-modal guided multi-perspective representation learning (BMRL) framework for zero-shot deepfake attribution (ZS-DFA), which facilitates effective traceability to unseen generators. Specifically, we design a multi-perspective visual encoder (MPVE) to explore general deepfake attribution visual characteristics across three views (i.e., image, noise, and edge). We devise a novel parsing encoder to focus on global face attribute embeddings, enabling parsing-guided DFA representation learning via vision-parsing matching. A language encoder is proposed to capture fine-grained language embeddings, facilitating language-guided general visual forgery representation learning through vision-language alignment. Additionally, we present a novel deepfake attribution contrastive center (DFACC) loss, to pull relevant generators closer and push irrelevant ones away, which can be introduced into DFA models to enhance traceability. Experimental results demonstrate that our method outperforms the state-of-the-art on the ZS-DFA task through various protocols evaluation.",
        "arxiv_id": "2504.14129",
        "ARXIVID": "2504.14129",
        "COMMENT": "Matches criterion 2 as it introduces a bi-modal framework for deepfake attribution, involving vision and language modalities.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2504.14245": {
        "authors": [
            "Yikun Ji",
            "Yan Hong",
            "Jiahui Zhan",
            "Haoxing Chen",
            "jun lan",
            "Huijia Zhu",
            "Weiqiang Wang",
            "Liqing Zhang",
            "Jianfu Zhang"
        ],
        "title": "Towards Explainable Fake Image Detection with Multi-Modal Large Language Models",
        "abstract": "arXiv:2504.14245v1 Announce Type: new  Abstract: Progress in image generation raises significant public security concerns. We argue that fake image detection should not operate as a \"black box\". Instead, an ideal approach must ensure both strong generalization and transparency. Recent progress in Multi-modal Large Language Models (MLLMs) offers new opportunities for reasoning-based AI-generated image detection. In this work, we evaluate the capabilities of MLLMs in comparison to traditional detection methods and human evaluators, highlighting their strengths and limitations. Furthermore, we design six distinct prompts and propose a framework that integrates these prompts to develop a more robust, explainable, and reasoning-driven detection system. The code is available at https://github.com/Gennadiyev/mllm-defake.",
        "arxiv_id": "2504.14245",
        "ARXIVID": "2504.14245",
        "COMMENT": "Matches criterion 2 as it explores the use of multi-modal large language models (MLLMs) for explainable fake image detection, focusing on reasoning and transparency.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2504.15152": {
        "authors": [
            "Jun Zhou",
            "Bingchen Gao",
            "Kai Wang",
            "Jialun Pei",
            "Pheng-Ann Heng",
            "Jing Qin"
        ],
        "title": "Landmark-Free Preoperative-to-Intraoperative Registration in Laparoscopic Liver Resection",
        "abstract": "arXiv:2504.15152v1 Announce Type: new  Abstract: Liver registration by overlaying preoperative 3D models onto intraoperative 2D frames can assist surgeons in perceiving the spatial anatomy of the liver clearly for a higher surgical success rate. Existing registration methods rely heavily on anatomical landmark-based workflows, which encounter two major limitations: 1) ambiguous landmark definitions fail to provide efficient markers for registration; 2) insufficient integration of intraoperative liver visual information in shape deformation modeling. To address these challenges, in this paper, we propose a landmark-free preoperative-to-intraoperative registration framework utilizing effective self-supervised learning, termed \\ourmodel. This framework transforms the conventional 3D-2D workflow into a 3D-3D registration pipeline, which is then decoupled into rigid and non-rigid registration subtasks. \\ourmodel~first introduces a feature-disentangled transformer to learn robust correspondences for recovering rigid transformations. Further, a structure-regularized deformation network is designed to adjust the preoperative model to align with the intraoperative liver surface. This network captures structural correlations through geometry similarity modeling in a low-rank transformer network. To facilitate the validation of the registration performance, we also construct an in-vivo registration dataset containing liver resection videos of 21 patients, called \\emph{P2I-LReg}, which contains 346 keyframes that provide a global view of the liver together with liver mask annotations and calibrated camera intrinsic parameters. Extensive experiments and user studies on both synthetic and in-vivo datasets demonstrate the superiority and potential clinical applicability of our method.",
        "arxiv_id": "2504.15152",
        "ARXIVID": "2504.15152",
        "COMMENT": "Matches criterion 1 as it involves spatial understanding in the context of liver registration and spatial anatomy.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2504.13924": {
        "authors": [
            "Akash V. Maharaj",
            "David Arbour",
            "Daniel Lee",
            "Uttaran Bhattacharya",
            "Anup Rao",
            "Austin Zane",
            "Avi Feller",
            "Kun Qian",
            "Yunyao Li"
        ],
        "title": "Evaluation and Incident Prevention in an Enterprise AI Assistant",
        "abstract": "arXiv:2504.13924v1 Announce Type: new  Abstract: Enterprise AI Assistants are increasingly deployed in domains where accuracy is paramount, making each erroneous output a potentially significant incident. This paper presents a comprehensive framework for monitoring, benchmarking, and continuously improving such complex, multi-component systems under active development by multiple teams. Our approach encompasses three key elements: (1) a hierarchical ``severity'' framework for incident detection that identifies and categorizes errors while attributing component-specific error rates, facilitating targeted improvements; (2) a scalable and principled methodology for benchmark construction, evaluation, and deployment, designed to accommodate multiple development teams, mitigate overfitting risks, and assess the downstream impact of system modifications; and (3) a continual improvement strategy leveraging multidimensional evaluation, enabling the identification and implementation of diverse enhancement opportunities. By adopting this holistic framework, organizations can systematically enhance the reliability and performance of their AI Assistants, ensuring their efficacy in critical enterprise environments. We conclude by discussing how this multifaceted evaluation approach opens avenues for various classes of enhancements, paving the way for more robust and trustworthy AI systems.",
        "arxiv_id": "2504.13924",
        "ARXIVID": "2504.13924",
        "COMMENT": "Matches criterion 3 as it discusses a new benchmarking framework for enterprise AI assistants.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.14977": {
        "authors": [
            "Jingkai Zhou",
            "Yifan Wu",
            "Shikai Li",
            "Min Wei",
            "Chao Fan",
            "Weihua Chen",
            "Wei Jiang",
            "Fan Wang"
        ],
        "title": "RealisDance-DiT: Simple yet Strong Baseline towards Controllable Character Animation in the Wild",
        "abstract": "arXiv:2504.14977v1 Announce Type: new  Abstract: Controllable character animation remains a challenging problem, particularly in handling rare poses, stylized characters, character-object interactions, complex illumination, and dynamic scenes. To tackle these issues, prior work has largely focused on injecting pose and appearance guidance via elaborate bypass networks, but often struggles to generalize to open-world scenarios. In this paper, we propose a new perspective that, as long as the foundation model is powerful enough, straightforward model modifications with flexible fine-tuning strategies can largely address the above challenges, taking a step towards controllable character animation in the wild. Specifically, we introduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our sufficient analysis reveals that the widely adopted Reference Net design is suboptimal for large-scale DiT models. Instead, we demonstrate that minimal modifications to the foundation model architecture yield a surprisingly strong baseline. We further propose the low-noise warmup and \"large batches and small iterations\" strategies to accelerate model convergence during fine-tuning while maximally preserving the priors of the foundation model. In addition, we introduce a new test dataset that captures diverse real-world challenges, complementing existing benchmarks such as TikTok dataset and UBC fashion video dataset, to comprehensively evaluate the proposed method. Extensive experiments show that RealisDance-DiT outperforms existing methods by a large margin.",
        "arxiv_id": "2504.14977",
        "ARXIVID": "2504.14977",
        "COMMENT": "This paper proposes a method for controllable character animation using a video foundation model, which aligns with criterion 4 (vision foundation models and applications).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2504.14240": {
        "authors": [
            "Xie Liang",
            "Gao Wei",
            "Zhenghui Ming",
            "Li Ge"
        ],
        "title": "ROI-Guided Point Cloud Geometry Compression Towards Human and Machine Vision",
        "abstract": "arXiv:2504.14240v1 Announce Type: new  Abstract: Point cloud data is pivotal in applications like autonomous driving, virtual reality, and robotics. However, its substantial volume poses significant challenges in storage and transmission. In order to obtain a high compression ratio, crucial semantic details usually confront severe damage, leading to difficulties in guaranteeing the accuracy of downstream tasks. To tackle this problem, we are the first to introduce a novel Region of Interest (ROI)-guided Point Cloud Geometry Compression (RPCGC) method for human and machine vision. Our framework employs a dual-branch parallel structure, where the base layer encodes and decodes a simplified version of the point cloud, and the enhancement layer refines this by focusing on geometry details. Furthermore, the residual information of the enhancement layer undergoes refinement through an ROI prediction network. This network generates mask information, which is then incorporated into the residuals, serving as a strong supervision signal. Additionally, we intricately apply these mask details in the Rate-Distortion (RD) optimization process, with each point weighted in the distortion calculation. Our loss function includes RD loss and detection loss to better guide point cloud encoding for the machine. Experiment results demonstrate that RPCGC achieves exceptional compression performance and better detection accuracy (10% gain) than some learning-based compression methods at high bitrates in ScanNet and SUN RGB-D datasets.",
        "arxiv_id": "2504.14240",
        "ARXIVID": "2504.14240",
        "COMMENT": "This paper introduces a novel ROI-guided point cloud compression method, which is tangentially related to spatial intelligence (criterion 1) but does not directly match any specific criterion.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2504.14309": {
        "authors": [
            "Ruoyan Xiong",
            "Huanbin Zhang",
            "Shentao Wang",
            "Hui He",
            "Yuke Hou",
            "Yue Zhang",
            "Yujie Cui",
            "Huipan Guan",
            "Shang Zhang"
        ],
        "title": "FGSGT: Saliency-Guided Siamese Network Tracker Based on Key Fine-Grained Feature Information for Thermal Infrared Target Tracking",
        "abstract": "arXiv:2504.14309v1 Announce Type: new  Abstract: Thermal infrared (TIR) images typically lack detailed features and have low contrast, making it challenging for conventional feature extraction models to capture discriminative target characteristics. As a result, trackers are often affected by interference from visually similar objects and are susceptible to tracking drift. To address these challenges, we propose a novel saliency-guided Siamese network tracker based on key fine-grained feature infor-mation. First, we introduce a fine-grained feature parallel learning convolu-tional block with a dual-stream architecture and convolutional kernels of varying sizes. This design captures essential global features from shallow layers, enhances feature diversity, and minimizes the loss of fine-grained in-formation typically encountered in residual connections. In addition, we propose a multi-layer fine-grained feature fusion module that uses bilinear matrix multiplication to effectively integrate features across both deep and shallow layers. Next, we introduce a Siamese residual refinement block that corrects saliency map prediction errors using residual learning. Combined with deep supervision, this mechanism progressively refines predictions, ap-plying supervision at each recursive step to ensure consistent improvements in accuracy. Finally, we present a saliency loss function to constrain the sali-ency predictions, directing the network to focus on highly discriminative fi-ne-grained features. Extensive experiment results demonstrate that the pro-posed tracker achieves the highest precision and success rates on the PTB-TIR and LSOTB-TIR benchmarks. It also achieves a top accuracy of 0.78 on the VOT-TIR 2015 benchmark and 0.75 on the VOT-TIR 2017 benchmark.",
        "arxiv_id": "2504.14309",
        "ARXIVID": "2504.14309",
        "COMMENT": "This paper introduces a novel saliency-guided Siamese network for thermal infrared target tracking, which is tangentially related to spatial intelligence (criterion 1) but does not directly match any specific criterion.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2504.14491": {
        "authors": [
            "Shang Zhang",
            "Xiaobo Ding",
            "Huanbin Zhang",
            "Ruoyan Xiong",
            "Yue Zhang"
        ],
        "title": "STARS: Sparse Learning Correlation Filter with Spatio-temporal Regularization and Super-resolution Reconstruction for Thermal Infrared Target Tracking",
        "abstract": "arXiv:2504.14491v1 Announce Type: new  Abstract: Thermal infrared (TIR) target tracking methods often adopt the correlation filter (CF) framework due to its computational efficiency. However, the low resolution of TIR images, along with tracking interference, significantly limits the perfor-mance of TIR trackers. To address these challenges, we introduce STARS, a novel sparse learning-based CF tracker that incorporates spatio-temporal regulari-zation and super-resolution reconstruction. First, we apply adaptive sparse filter-ing and temporal domain filtering to extract key features of the target while reduc-ing interference from background clutter and noise. Next, we introduce an edge-preserving sparse regularization method to stabilize target features and prevent excessive blurring. This regularization integrates multiple terms and employs the alternating direction method of multipliers to optimize the solution. Finally, we propose a gradient-enhanced super-resolution method to extract fine-grained TIR target features and improve the resolution of TIR images, addressing performance degradation in tracking caused by low-resolution sequences. To the best of our knowledge, STARS is the first to integrate super-resolution methods within a sparse learning-based CF framework. Extensive experiments on the LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR2017 benchmarks demonstrate that STARS outperforms state-of-the-art trackers in terms of robustness.",
        "arxiv_id": "2504.14491",
        "ARXIVID": "2504.14491",
        "COMMENT": "This paper proposes a novel method for thermal infrared target tracking with spatio-temporal regularization and super-resolution reconstruction. It does not directly match any of the criteria but is tangentially related to spatial intelligence (criterion 1).",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2504.14737": {
        "authors": [
            "Shuang Zeng",
            "Lei Zhu",
            "Xinliang Zhang",
            "Hangzhou He",
            "Yanye Lu"
        ],
        "title": "SuperCL: Superpixel Guided Contrastive Learning for Medical Image Segmentation Pre-training",
        "abstract": "arXiv:2504.14737v1 Announce Type: new  Abstract: Medical image segmentation is a critical yet challenging task, primarily due to the difficulty of obtaining extensive datasets of high-quality, expert-annotated images. Contrastive learning presents a potential but still problematic solution to this issue. Because most existing methods focus on extracting instance-level or pixel-to-pixel representation, which ignores the characteristics between intra-image similar pixel groups. Moreover, when considering contrastive pairs generation, most SOTA methods mainly rely on manually setting thresholds, which requires a large number of gradient experiments and lacks efficiency and generalization. To address these issues, we propose a novel contrastive learning approach named SuperCL for medical image segmentation pre-training. Specifically, our SuperCL exploits the structural prior and pixel correlation of images by introducing two novel contrastive pairs generation strategies: Intra-image Local Contrastive Pairs (ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation. Considering superpixel cluster aligns well with the concept of contrastive pairs generation, we utilize the superpixel map to generate pseudo masks for both ILCP and IGCP to guide supervised contrastive learning. Moreover, we also propose two modules named Average SuperPixel Feature Map Generation (ASP) and Connected Components Label Generation (CCL) to better exploit the prior structural information for IGCP. Finally, experiments on 8 medical image datasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL achieves a superior performance with more precise predictions from visualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best results on MMWHS, CHAOS, Spleen with 10% annotations. Our code will be released after acceptance.",
        "arxiv_id": "2504.14737",
        "ARXIVID": "2504.14737",
        "COMMENT": "Does not match any specific criterion but discusses contrastive learning for medical image segmentation, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.14785": {
        "authors": [
            "Zhenyu Yu",
            "Mohd Yamani Idna Idris",
            "Pei Wang"
        ],
        "title": "When Cloud Removal Meets Diffusion Model in Remote Sensing",
        "abstract": "arXiv:2504.14785v1 Announce Type: new  Abstract: Cloud occlusion significantly hinders remote sensing applications by obstructing surface information and complicating analysis. To address this, we propose DC4CR (Diffusion Control for Cloud Removal), a novel multimodal diffusion-based framework for cloud removal in remote sensing imagery. Our method introduces prompt-driven control, allowing selective removal of thin and thick clouds without relying on pre-generated cloud masks, thereby enhancing preprocessing efficiency and model adaptability. Additionally, we integrate low-rank adaptation for computational efficiency, subject-driven generation for improved generalization, and grouped learning to enhance performance on small datasets. Designed as a plug-and-play module, DC4CR seamlessly integrates into existing cloud removal models, providing a scalable and robust solution. Extensive experiments on the RICE and CUHK-CR datasets demonstrate state-of-the-art performance, achieving superior cloud removal across diverse conditions. This work presents a practical and efficient approach for remote sensing image processing with broad real-world applications.",
        "arxiv_id": "2504.14785",
        "ARXIVID": "2504.14785",
        "COMMENT": "Does not match any specific criterion but discusses multimodal diffusion models for cloud removal, which is tangentially related to vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.14826": {
        "authors": [
            "Zhuoran Zheng",
            "Xin Su",
            "Chen Wu",
            "Xiuyi Jia"
        ],
        "title": "Distribution-aware Dataset Distillation for Efficient Image Restoration",
        "abstract": "arXiv:2504.14826v1 Announce Type: new  Abstract: With the exponential increase in image data, training an image restoration model is laborious. Dataset distillation is a potential solution to this problem, yet current distillation techniques are a blank canvas in the field of image restoration. To fill this gap, we propose the Distribution-aware Dataset Distillation method (TripleD), a new framework that extends the principles of dataset distillation to image restoration. Specifically, TripleD uses a pre-trained vision Transformer to extract features from images for complexity evaluation, and the subset (the number of samples is much smaller than the original training set) is selected based on complexity. The selected subset is then fed through a lightweight CNN that fine-tunes the image distribution to align with the distribution of the original dataset at the feature level. To efficiently condense knowledge, the training is divided into two stages. Early stages focus on simpler, low-complexity samples to build foundational knowledge, while later stages select more complex and uncertain samples as the model matures. Our method achieves promising performance on multiple image restoration tasks, including multi-task image restoration, all-in-one image restoration, and ultra-high-definition image restoration tasks. Note that we can train a state-of-the-art image restoration model on an ultra-high-definition (4K resolution) dataset using only one consumer-grade GPU in less than 8 hours (500 savings in computing resources and immeasurable training time).",
        "arxiv_id": "2504.14826",
        "ARXIVID": "2504.14826",
        "COMMENT": "Does not match any specific criterion but discusses dataset distillation for image restoration, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.15179": {
        "authors": [
            "Fei Yin",
            "Mallikarjun B R",
            "Chun-Han Yao",
            "Rafa{\\l} Mantiuk",
            "Varun Jampani"
        ],
        "title": "FaceCraft4D: Animated 3D Facial Avatar Generation from a Single Image",
        "abstract": "arXiv:2504.15179v1 Announce Type: new  Abstract: We present a novel framework for generating high-quality, animatable 4D avatar from a single image. While recent advances have shown promising results in 4D avatar creation, existing methods either require extensive multiview data or struggle with shape accuracy and identity consistency. To address these limitations, we propose a comprehensive system that leverages shape, image, and video priors to create full-view, animatable avatars. Our approach first obtains initial coarse shape through 3D-GAN inversion. Then, it enhances multiview textures using depth-guided warping signals for cross-view consistency with the help of the image diffusion model. To handle expression animation, we incorporate a video prior with synchronized driving signals across viewpoints. We further introduce a Consistent-Inconsistent training to effectively handle data inconsistencies during 4D reconstruction. Experimental results demonstrate that our method achieves superior quality compared to the prior art, while maintaining consistency across different viewpoints and expressions.",
        "arxiv_id": "2504.15179",
        "ARXIVID": "2504.15179",
        "COMMENT": "Does not match any specific criteria but is related to 3D avatar generation, which is tangentially relevant to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.14606": {
        "authors": [
            "Siyi Jiao",
            "Wenzheng Zeng",
            "Yerong Li",
            "Huayu Zhang",
            "Changxin Gao",
            "Nong Sang",
            "Mike Zheng Shou"
        ],
        "title": "MP-Mat: A 3D-and-Instance-Aware Human Matting and Editing Framework with Multiplane Representation",
        "abstract": "arXiv:2504.14606v1 Announce Type: new  Abstract: Human instance matting aims to estimate an alpha matte for each human instance in an image, which is challenging as it easily fails in complex cases requiring disentangling mingled pixels belonging to multiple instances along hairy and thin boundary structures. In this work, we address this by introducing MP-Mat, a novel 3D-and-instance-aware matting framework with multiplane representation, where the multiplane concept is designed from two different perspectives: scene geometry level and instance level. Specifically, we first build feature-level multiplane representations to split the scene into multiple planes based on depth differences. This approach makes the scene representation 3D-aware, and can serve as an effective clue for splitting instances in different 3D positions, thereby improving interpretability and boundary handling ability especially in occlusion areas. Then, we introduce another multiplane representation that splits the scene in an instance-level perspective, and represents each instance with both matte and color. We also treat background as a special instance, which is often overlooked by existing methods. Such an instance-level representation facilitates both foreground and background content awareness, and is useful for other down-stream tasks like image editing. Once built, the representation can be reused to realize controllable instance-level image editing with high efficiency. Extensive experiments validate the clear advantage of MP-Mat in matting task. We also demonstrate its superiority in image editing tasks, an area under-explored by existing matting-focused methods, where our approach under zero-shot inference even outperforms trained specialized image editing techniques by large margins. Code is open-sourced at https://github.com/JiaoSiyi/MPMat.git}.",
        "arxiv_id": "2504.14606",
        "ARXIVID": "2504.14606",
        "COMMENT": "This paper introduces a 3D-and-instance-aware human matting framework, which is related to computer vision but does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.15054": {
        "authors": [
            "Xiangchen Yin",
            "Zhenda Yu",
            "Longtao Jiang",
            "Xin Gao",
            "Xiao Sun",
            "Zhi Liu",
            "Xun Yang"
        ],
        "title": "Structure-guided Diffusion Transformer for Low-Light Image Enhancement",
        "abstract": "arXiv:2504.15054v1 Announce Type: new  Abstract: While the diffusion transformer (DiT) has become a focal point of interest in recent years, its application in low-light image enhancement remains a blank area for exploration. Current methods recover the details from low-light images while inevitably amplifying the noise in images, resulting in poor visual quality. In this paper, we firstly introduce DiT into the low-light enhancement task and design a novel Structure-guided Diffusion Transformer based Low-light image enhancement (SDTL) framework. We compress the feature through wavelet transform to improve the inference efficiency of the model and capture the multi-directional frequency band. Then we propose a Structure Enhancement Module (SEM) that uses structural prior to enhance the texture and leverages an adaptive fusion strategy to achieve more accurate enhancement effect. In Addition, we propose a Structure-guided Attention Block (SAB) to pay more attention to texture-riched tokens and avoid interference from noisy areas in noise prediction. Extensive qualitative and quantitative experiments demonstrate that our method achieves SOTA performance on several popular datasets, validating the effectiveness of SDTL in improving image quality and the potential of DiT in low-light enhancement tasks.",
        "arxiv_id": "2504.15054",
        "ARXIVID": "2504.15054",
        "COMMENT": "This paper introduces a novel method for low-light image enhancement using a diffusion transformer, which is related to computer vision but does not match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.14108": {
        "authors": [
            "Zhenyu Yu",
            "Mohd Yamani Idna Idris",
            "Pei Wang",
            "Yuelong Xia"
        ],
        "title": "Point-Driven Interactive Text and Image Layer Editing Using Diffusion Models",
        "abstract": "arXiv:2504.14108v1 Announce Type: new  Abstract: We present DanceText, a training-free framework for multilingual text editing in images, designed to support complex geometric transformations and achieve seamless foreground-background integration. While diffusion-based generative models have shown promise in text-guided image synthesis, they often lack controllability and fail to preserve layout consistency under non-trivial manipulations such as rotation, translation, scaling, and warping. To address these limitations, DanceText introduces a layered editing strategy that separates text from the background, allowing geometric transformations to be performed in a modular and controllable manner. A depth-aware module is further proposed to align appearance and perspective between the transformed text and the reconstructed background, enhancing photorealism and spatial consistency. Importantly, DanceText adopts a fully training-free design by integrating pretrained modules, allowing flexible deployment without task-specific fine-tuning. Extensive experiments on the AnyWord-3M benchmark demonstrate that our method achieves superior performance in visual quality, especially under large-scale and complex transformation scenarios.",
        "arxiv_id": "2504.14108",
        "ARXIVID": "2504.14108",
        "COMMENT": "This paper introduces a training-free framework for text and image layer editing using diffusion models, which is relevant to generative modeling but does not directly match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.14294": {
        "authors": [
            "Pourya Shamsolmoali",
            "Masoumeh Zareapoor",
            "Huiyu Zhou",
            "Michael Felsberg",
            "Dacheng Tao",
            "Xuelong Li"
        ],
        "title": "From Missing Pieces to Masterpieces: Image Completion with Context-Adaptive Diffusion",
        "abstract": "arXiv:2504.14294v1 Announce Type: new  Abstract: Image completion is a challenging task, particularly when ensuring that generated content seamlessly integrates with existing parts of an image. While recent diffusion models have shown promise, they often struggle with maintaining coherence between known and unknown (missing) regions. This issue arises from the lack of explicit spatial and semantic alignment during the diffusion process, resulting in content that does not smoothly integrate with the original image. Additionally, diffusion models typically rely on global learned distributions rather than localized features, leading to inconsistencies between the generated and existing image parts. In this work, we propose ConFill, a novel framework that introduces a Context-Adaptive Discrepancy (CAD) model to ensure that intermediate distributions of known and unknown regions are closely aligned throughout the diffusion process. By incorporating CAD, our model progressively reduces discrepancies between generated and original images at each diffusion step, leading to contextually aligned completion. Moreover, ConFill uses a new Dynamic Sampling mechanism that adaptively increases the sampling rate in regions with high reconstruction complexity. This approach enables precise adjustments, enhancing detail and integration in restored areas. Extensive experiments demonstrate that ConFill outperforms current methods, setting a new benchmark in image completion.",
        "arxiv_id": "2504.14294",
        "ARXIVID": "2504.14294",
        "COMMENT": "This paper proposes a novel framework for image completion using context-adaptive diffusion, which does not directly match any of the criteria but is relevant to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.15176": {
        "authors": [
            "Miaomiao Cai",
            "Simiao Li",
            "Wei Li",
            "Xudong Huang",
            "Hanting Chen",
            "Jie Hu",
            "Yunhe Wang"
        ],
        "title": "DSPO: Direct Semantic Preference Optimization for Real-World Image Super-Resolution",
        "abstract": "arXiv:2504.15176v1 Announce Type: new  Abstract: Recent advances in diffusion models have improved Real-World Image Super-Resolution (Real-ISR), but existing methods lack human feedback integration, risking misalignment with human preference and may leading to artifacts, hallucinations and harmful content generation. To this end, we are the first to introduce human preference alignment into Real-ISR, a technique that has been successfully applied in Large Language Models and Text-to-Image tasks to effectively enhance the alignment of generated outputs with human preferences. Specifically, we introduce Direct Preference Optimization (DPO) into Real-ISR to achieve alignment, where DPO serves as a general alignment technique that directly learns from the human preference dataset. Nevertheless, unlike high-level tasks, the pixel-level reconstruction objectives of Real-ISR are difficult to reconcile with the image-level preferences of DPO, which can lead to the DPO being overly sensitive to local anomalies, leading to reduced generation quality. To resolve this dichotomy, we propose Direct Semantic Preference Optimization (DSPO) to align instance-level human preferences by incorporating semantic guidance, which is through two strategies: (a) semantic instance alignment strategy, implementing instance-level alignment to ensure fine-grained perceptual consistency, and (b) user description feedback strategy, mitigating hallucinations through semantic textual feedback on instance-level images. As a plug-and-play solution, DSPO proves highly effective in both one-step and multi-step SR frameworks.",
        "arxiv_id": "2504.15176",
        "ARXIVID": "2504.15176",
        "COMMENT": "Does not match any specific criterion but discusses human preference alignment in image super-resolution, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.14868": {
        "authors": [
            "Jianhui Wang",
            "Yangfan He",
            "Yan Zhong",
            "Xinyuan Song",
            "Jiayi Su",
            "Yuheng Feng",
            "Hongyang He",
            "Wenyu Zhu",
            "Xinhang Yuan",
            "Kuan Lu",
            "Menghao Huo",
            "Miao Zhang",
            "Keqin Li",
            "Jiaqi Chen",
            "Tianyu Shi",
            "Xueqian Wang"
        ],
        "title": "Twin Co-Adaptive Dialogue for Progressive Image Generation",
        "abstract": "arXiv:2504.14868v1 Announce Type: new  Abstract: Modern text-to-image generation systems have enabled the creation of remarkably realistic and high-quality visuals, yet they often falter when handling the inherent ambiguities in user prompts. In this work, we present Twin-Co, a framework that leverages synchronized, co-adaptive dialogue to progressively refine image generation. Instead of a static generation process, Twin-Co employs a dynamic, iterative workflow where an intelligent dialogue agent continuously interacts with the user. Initially, a base image is generated from the user's prompt. Then, through a series of synchronized dialogue exchanges, the system adapts and optimizes the image according to evolving user feedback. The co-adaptive process allows the system to progressively narrow down ambiguities and better align with user intent. Experiments demonstrate that Twin-Co not only enhances user experience by reducing trial-and-error iterations but also improves the quality of the generated images, streamlining the creative process across various applications.",
        "arxiv_id": "2504.14868",
        "ARXIVID": "2504.14868",
        "COMMENT": "Does not match any specific criterion but discusses iterative refinement in text-to-image generation, which aligns with your friend's general interest in generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.14470": {
        "authors": [
            "Jingjing Ren",
            "Wenbo Li",
            "Zhongdao Wang",
            "Haoze Sun",
            "Bangzhen Liu",
            "Haoyu Chen",
            "Jiaqi Xu",
            "Aoxue Li",
            "Shifeng Zhang",
            "Bin Shao",
            "Yong Guo",
            "Lei Zhu"
        ],
        "title": "Turbo2K: Towards Ultra-Efficient and High-Quality 2K Video Synthesis",
        "abstract": "arXiv:2504.14470v1 Announce Type: new  Abstract: Demand for 2K video synthesis is rising with increasing consumer expectations for ultra-clear visuals. While diffusion transformers (DiTs) have demonstrated remarkable capabilities in high-quality video generation, scaling them to 2K resolution remains computationally prohibitive due to quadratic growth in memory and processing costs. In this work, we propose Turbo2K, an efficient and practical framework for generating detail-rich 2K videos while significantly improving training and inference efficiency. First, Turbo2K operates in a highly compressed latent space, reducing computational complexity and memory footprint, making high-resolution video synthesis feasible. However, the high compression ratio of the VAE and limited model size impose constraints on generative quality. To mitigate this, we introduce a knowledge distillation strategy that enables a smaller student model to inherit the generative capacity of a larger, more powerful teacher model. Our analysis reveals that, despite differences in latent spaces and architectures, DiTs exhibit structural similarities in their internal representations, facilitating effective knowledge transfer. Second, we design a hierarchical two-stage synthesis framework that first generates multi-level feature at lower resolutions before guiding high-resolution video generation. This approach ensures structural coherence and fine-grained detail refinement while eliminating redundant encoding-decoding overhead, further enhancing computational efficiency.Turbo2K achieves state-of-the-art efficiency, generating 5-second, 24fps, 2K videos with significantly reduced computational cost. Compared to existing methods, Turbo2K is up to 20$\\times$ faster for inference, making high-resolution video generation more scalable and practical for real-world applications.",
        "arxiv_id": "2504.14470",
        "ARXIVID": "2504.14470",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in video synthesis, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2504.14092": {
        "authors": [
            "Wei Dong",
            "Han Zhou",
            "Seyed Amirreza Mousavi",
            "Jun Chen"
        ],
        "title": "Retinex-guided Histogram Transformer for Mask-free Shadow Removal",
        "abstract": "arXiv:2504.14092v1 Announce Type: new  Abstract: While deep learning methods have achieved notable progress in shadow removal, many existing approaches rely on shadow masks that are difficult to obtain, limiting their generalization to real-world scenes. In this work, we propose ReHiT, an efficient mask-free shadow removal framework based on a hybrid CNN-Transformer architecture guided by Retinex theory. We first introduce a dual-branch pipeline to separately model reflectance and illumination components, and each is restored by our developed Illumination-Guided Hybrid CNN-Transformer (IG-HCT) module. Second, besides the CNN-based blocks that are capable of learning residual dense features and performing multi-scale semantic fusion, multi-scale semantic fusion, we develop the Illumination-Guided Histogram Transformer Block (IGHB) to effectively handle non-uniform illumination and spatially complex shadows. Extensive experiments on several benchmark datasets validate the effectiveness of our approach over existing mask-free methods. Trained solely on the NTIRE 2025 Shadow Removal Challenge dataset, our solution delivers competitive results with one of the smallest parameter sizes and fastest inference speeds among top-ranked entries, highlighting its applicability for real-world applications with limited computational resources. The code is available at https://github.com/dongw22/oath.",
        "arxiv_id": "2504.14092",
        "ARXIVID": "2504.14092",
        "COMMENT": "Does not match any specific criteria. Focuses on shadow removal using a hybrid CNN-Transformer architecture, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.14177": {
        "authors": [
            "Li He",
            "He Zhao",
            "Stephen Wan",
            "Dadong Wang",
            "Lina Yao",
            "Tongliang Liu"
        ],
        "title": "Direct Advantage Regression: Aligning LLMs with Online AI Reward",
        "abstract": "arXiv:2504.14177v1 Announce Type: new  Abstract: Online AI Feedback (OAIF) presents a promising alternative to Reinforcement Learning from Human Feedback (RLHF) by utilizing online AI preference in aligning language models (LLMs). However, the straightforward replacement of humans with AI deprives LLMs from learning more fine-grained AI supervision beyond binary signals. In this paper, we propose Direct Advantage Regression (DAR), a simple alignment algorithm using online AI reward to optimize policy improvement through weighted supervised fine-tuning. As an RL-free approach, DAR maintains theoretical consistency with online RLHF pipelines while significantly reducing implementation complexity and improving learning efficiency. Our empirical results underscore that AI reward is a better form of AI supervision consistently achieving higher human-AI agreement as opposed to AI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show that DAR outperforms both OAIF and online RLHF baselines.",
        "arxiv_id": "2504.14177",
        "ARXIVID": "2504.14177",
        "COMMENT": "Does not match any specific criteria. Focuses on aligning LLMs with AI reward, which is not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.14664": {
        "authors": [
            "Jixiang Sun",
            "Fei Lei",
            "Jiawei Zhang",
            "Wenxiu Sun",
            "Yujiu Yang"
        ],
        "title": "Frequency-domain Learning with Kernel Prior for Blind Image Deblurring",
        "abstract": "arXiv:2504.14664v1 Announce Type: new  Abstract: While achieving excellent results on various datasets, many deep learning methods for image deblurring suffer from limited generalization capabilities with out-of-domain data. This limitation is likely caused by their dependence on certain domain-specific datasets. To address this challenge, we argue that it is necessary to introduce the kernel prior into deep learning methods, as the kernel prior remains independent of the image context. For effective fusion of kernel prior information, we adopt a rational implementation method inspired by traditional deblurring algorithms that perform deconvolution in the frequency domain. We propose a module called Frequency Integration Module (FIM) for fusing the kernel prior and combine it with a frequency-based deblurring Transfomer network. Experimental results demonstrate that our method outperforms state-of-the-art methods on multiple blind image deblurring tasks, showcasing robust generalization abilities. Source code will be available soon.",
        "arxiv_id": "2504.14664",
        "ARXIVID": "2504.14664",
        "COMMENT": "Does not match any specific criteria. Focuses on image deblurring with kernel priors, which is not directly related to spatial understanding, VLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.14137": {
        "authors": [
            "Hangyu Liu",
            "Bo Peng",
            "Pengxiang Ding",
            "Donglin Wang"
        ],
        "title": "Rethinking Target Label Conditioning in Adversarial Attacks: A 2D Tensor-Guided Generative Approach",
        "abstract": "arXiv:2504.14137v1 Announce Type: new  Abstract: Compared to single-target adversarial attacks, multi-target attacks have garnered significant attention due to their ability to generate adversarial images for multiple target classes simultaneously. Existing generative approaches for multi-target attacks mainly analyze the effect of the use of target labels on noise generation from a theoretical perspective, lacking practical validation and comprehensive summarization. To address this gap, we first identify and validate that the semantic feature quality and quantity are critical factors affecting the transferability of targeted attacks: 1) Feature quality refers to the structural and detailed completeness of the implanted target features, as deficiencies may result in the loss of key discriminative information; 2) Feature quantity refers to the spatial sufficiency of the implanted target features, as inadequacy limits the victim model's attention to this feature. Based on these findings, we propose the 2D Tensor-Guided Adversarial Fusion (2D-TGAF) framework, which leverages the powerful generative capabilities of diffusion models to encode target labels into two-dimensional semantic tensors for guiding adversarial noise generation. Additionally, we design a novel masking strategy tailored for the training process, ensuring that parts of the generated noise retain complete semantic information about the target class. Extensive experiments on the standard ImageNet dataset demonstrate that 2D-TGAF consistently surpasses state-of-the-art methods in attack success rates, both on normally trained models and across various defense mechanisms.",
        "arxiv_id": "2504.14137",
        "ARXIVID": "2504.14137",
        "COMMENT": "Does not match any specific criteria. Focuses on adversarial attacks and generative approaches, which are not directly related to the listed criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.14249": {
        "authors": [
            "Bin Ren",
            "Eduard Zamfir",
            "Zongwei Wu",
            "Yawei Li",
            "Yidi Li",
            "Danda Pani Paudel",
            "Radu Timofte",
            "Ming-Hsuan Yang",
            "Luc Van Gool",
            "Nicu Sebe"
        ],
        "title": "Any Image Restoration via Efficient Spatial-Frequency Degradation Adaptation",
        "abstract": "arXiv:2504.14249v1 Announce Type: new  Abstract: Restoring any degraded image efficiently via just one model has become increasingly significant and impactful, especially with the proliferation of mobile devices. Traditional solutions typically involve training dedicated models per degradation, resulting in inefficiency and redundancy. More recent approaches either introduce additional modules to learn visual prompts, significantly increasing model size, or incorporate cross-modal transfer from large language models trained on vast datasets, adding complexity to the system architecture. In contrast, our approach, termed AnyIR, takes a unified path that leverages inherent similarity across various degradations to enable both efficient and comprehensive restoration through a joint embedding mechanism, without scaling up the model or relying on large language models.Specifically, we examine the sub-latent space of each input, identifying key components and reweighting them first in a gated manner. To fuse the intrinsic degradation awareness and the contextualized attention, a spatial-frequency parallel fusion strategy is proposed for enhancing spatial-aware local-global interactions and enriching the restoration details from the frequency perspective. Extensive benchmarking in the all-in-one restoration setting confirms AnyIR's SOTA performance, reducing model complexity by around 82\\% in parameters and 85\\% in FLOPs. Our code will be available at our Project page (https://amazingren.github.io/AnyIR/)",
        "arxiv_id": "2504.14249",
        "ARXIVID": "2504.14249",
        "COMMENT": "Does not match any specific criterion but discusses efficient image restoration, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.14045": {
        "authors": [
            "Mark Steyvers",
            "Megan A. K. Peters"
        ],
        "title": "Metacognition and Uncertainty Communication in Humans and Large Language Models",
        "abstract": "arXiv:2504.14045v1 Announce Type: new  Abstract: Metacognition, the capacity to monitor and evaluate one's own knowledge and performance, is foundational to human decision-making, learning, and communication. As large language models (LLMs) become increasingly embedded in high-stakes decision contexts, it is critical to assess whether, how, and to what extent they exhibit metacognitive abilities. Here, we provide an overview of current knowledge of LLMs' metacognitive capacities, how they might be studied, and how they relate to our knowledge of metacognition in humans. We show that while humans and LLMs can sometimes appear quite aligned in their metacognitive capacities and behaviors, it is clear many differences remain. Attending to these differences is crucial not only for enhancing human-AI collaboration, but also for promoting the development of more capable and trustworthy artificial systems. Finally, we discuss how endowing future LLMs with more sensitive and more calibrated metacognition may also help them develop new capacities such as more efficient learning, self-direction, and curiosity.",
        "arxiv_id": "2504.14045",
        "ARXIVID": "2504.14045",
        "COMMENT": "Does not match any specific criterion but discusses metacognition in LLMs, which is tangentially related to your friend's interest in VLLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.14379": {
        "authors": [
            "Andrew Lee",
            "Lihao Sun",
            "Chris Wendler",
            "Fernanda Vi\\'egas",
            "Martin Wattenberg"
        ],
        "title": "The Geometry of Self-Verification in a Task-Specific Reasoning Model",
        "abstract": "arXiv:2504.14379v1 Announce Type: new  Abstract: How do reasoning models verify their own answers? We study this question by training a model using DeepSeek R1's recipe on the CountDown task. We leverage the fact that preference tuning leads to mode collapse, resulting in a model that always produces highly structured and easily parse-able chain-of-thought sequences. With this setup, we do a top-down and bottom-up analysis to reverse-engineer how the model verifies its outputs. Our top-down analysis reveals Gated Linear Unit (GLU) weights encoding verification-related tokens, such as ``success'' or ``incorrect'', which activate according to the correctness of the model's reasoning steps. Our bottom-up analysis reveals that ``previous-token heads'' are mainly responsible for model verification. Our analyses meet in the middle: drawing inspiration from inter-layer communication channels, we use the identified GLU vectors to localize as few as three attention heads that can disable model verification, pointing to a necessary component of a potentially larger verification circuit.",
        "arxiv_id": "2504.14379",
        "ARXIVID": "2504.14379",
        "COMMENT": "Does not match any specific criteria but is related to reasoning models and self-verification, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.15257": {
        "authors": [
            "Hongcheng Gao",
            "Yue Liu",
            "Yufei He",
            "Longxu Dou",
            "Chao Du",
            "Zhijie Deng",
            "Bryan Hooi",
            "Min Lin",
            "Tianyu Pang"
        ],
        "title": "FlowReasoner: Reinforcing Query-Level Meta-Agents",
        "abstract": "arXiv:2504.15257v1 Announce Type: new  Abstract: This paper proposes a query-level meta-agent named FlowReasoner to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize a reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first endow the basic reasoning ability regarding the generation of multi-agent systems to FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with external execution feedback. A multi-purpose reward is designed to guide the RL training from aspects of performance, complexity, and efficiency. In this manner, FlowReasoner is enabled to generate a personalized multi-agent system for each user query via deliberative reasoning. Experiments on both engineering and competition code benchmarks demonstrate the superiority of FlowReasoner. Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks. The code is available at https://github.com/sail-sg/FlowReasoner.",
        "arxiv_id": "2504.15257",
        "ARXIVID": "2504.15257",
        "COMMENT": "Does not match any specific criteria but is related to query-level meta-agents, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.14548": {
        "authors": [
            "Lifeng Lin",
            "Rongfeng Lu",
            "Quan Chen",
            "Haofan Ren",
            "Ming Lu",
            "Yaoqi Sun",
            "Chenggang Yan",
            "Anke Xue"
        ],
        "title": "VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control",
        "abstract": "arXiv:2504.14548v1 Announce Type: new  Abstract: Sparse-view 3D reconstruction is a fundamental yet challenging task in practical 3D reconstruction applications. Recently, many methods based on the 3D Gaussian Splatting (3DGS) framework have been proposed to address sparse-view 3D reconstruction. Although these methods have made considerable advancements, they still show significant issues with overfitting. To reduce the overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number Control (VGNC) approach based on generative novel view synthesis (NVS) models. To the best of our knowledge, this is the first attempt to alleviate the overfitting issue of sparse-view 3DGS with generative validation images. Specifically, we first introduce a validation image generation method based on a generative NVS model. We then propose a Gaussian number control strategy that utilizes generated validation images to determine the optimal Gaussian numbers, thereby reducing the issue of overfitting. We conducted detailed experiments on various sparse-view 3DGS baselines and datasets to evaluate the effectiveness of VGNC. Extensive experiments show that our approach not only reduces overfitting but also improves rendering quality on the test set while decreasing the number of Gaussian points. This reduction lowers storage demands and accelerates both training and rendering. The code will be released.",
        "arxiv_id": "2504.14548",
        "ARXIVID": "2504.14548",
        "COMMENT": "Does not match any specific criteria but is related to sparse-view 3D reconstruction, which is tangentially relevant to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.14075": {
        "authors": [
            "Wei Dong",
            "Yan Min",
            "Han Zhou",
            "Jun Chen"
        ],
        "title": "Towards Scale-Aware Low-Light Enhancement via Structure-Guided Transformer Design",
        "abstract": "arXiv:2504.14075v1 Announce Type: new  Abstract: Current Low-light Image Enhancement (LLIE) techniques predominantly rely on either direct Low-Light (LL) to Normal-Light (NL) mappings or guidance from semantic features or illumination maps. Nonetheless, the intrinsic ill-posedness of LLIE and the difficulty in retrieving robust semantics from heavily corrupted images hinder their effectiveness in extremely low-light environments. To tackle this challenge, we present SG-LLIE, a new multi-scale CNN-Transformer hybrid framework guided by structure priors. Different from employing pre-trained models for the extraction of semantics or illumination maps, we choose to extract robust structure priors based on illumination-invariant edge detectors. Moreover, we develop a CNN-Transformer Hybrid Structure-Guided Feature Extractor (HSGFE) module at each scale with in the UNet encoder-decoder architecture. Besides the CNN blocks which excels in multi-scale feature extraction and fusion, we introduce a Structure-Guided Transformer Block (SGTB) in each HSGFE that incorporates structural priors to modulate the enhancement process. Extensive experiments show that our method achieves state-of-the-art performance on several LLIE benchmarks in both quantitative metrics and visual quality. Our solution ranks second in the NTIRE 2025 Low-Light Enhancement Challenge. Code is released at https://github.com/minyan8/imagine.",
        "arxiv_id": "2504.14075",
        "ARXIVID": "2504.14075",
        "COMMENT": "Does not match any specific criteria but is related to computer vision advancements in low-light enhancement.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.14618": {
        "authors": [
            "Han Bi",
            "Ge Yu",
            "Yu He",
            "Wenzhuo Liu",
            "Zijie Zheng"
        ],
        "title": "VM-BHINet:Vision Mamba Bimanual Hand Interaction Network for 3D Interacting Hand Mesh Recovery From a Single RGB Image",
        "abstract": "arXiv:2504.14618v1 Announce Type: new  Abstract: Understanding bimanual hand interactions is essential for realistic 3D pose and shape reconstruction. However, existing methods struggle with occlusions, ambiguous appearances, and computational inefficiencies. To address these challenges, we propose Vision Mamba Bimanual Hand Interaction Network (VM-BHINet), introducing state space models (SSMs) into hand reconstruction to enhance interaction modeling while improving computational efficiency. The core component, Vision Mamba Interaction Feature Extraction Block (VM-IFEBlock), combines SSMs with local and global feature operations, enabling deep understanding of hand interactions. Experiments on the InterHand2.6M dataset show that VM-BHINet reduces Mean per-joint position error (MPJPE) and Mean per-vertex position error (MPVPE) by 2-3%, significantly surpassing state-of-the-art methods.",
        "arxiv_id": "2504.14618",
        "ARXIVID": "2504.14618",
        "COMMENT": "This paper proposes a network for 3D interacting hand mesh recovery, which is related to spatial understanding but does not introduce new methodological improvements for embodied agents (criterion 1).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.14238": {
        "authors": [
            "Lu Pan",
            "Yu-Hsuan Huang",
            "Hongxia Xie",
            "Cheng Zhang",
            "Hongwei Zhao",
            "Hong-Han Shuai",
            "Wen-Huang Cheng"
        ],
        "title": "Single Document Image Highlight Removal via A Large-Scale Real-World Dataset and A Location-Aware Network",
        "abstract": "arXiv:2504.14238v1 Announce Type: new  Abstract: Reflective documents often suffer from specular highlights under ambient lighting, severely hindering text readability and degrading overall visual quality. Although recent deep learning methods show promise in highlight removal, they remain suboptimal for document images, primarily due to the lack of dedicated datasets and tailored architectural designs. To tackle these challenges, we present DocHR14K, a large-scale real-world dataset comprising 14,902 high-resolution image pairs across six document categories and various lighting conditions. To the best of our knowledge, this is the first high-resolution dataset for document highlight removal that captures a wide range of real-world lighting conditions. Additionally, motivated by the observation that the residual map between highlighted and clean images naturally reveals the spatial structure of highlight regions, we propose a simple yet effective Highlight Location Prior (HLP) to estimate highlight masks without human annotations. Building on this prior, we present the Location-Aware Laplacian Pyramid Highlight Removal Network (L2HRNet), which effectively removes highlights by leveraging estimated priors and incorporates diffusion module to restore details. Extensive experiments demonstrate that DocHR14K improves highlight removal under diverse lighting conditions. Our L2HRNet achieves state-of-the-art performance across three benchmark datasets, including a 5.01\\% increase in PSNR and a 13.17\\% reduction in RMSE on DocHR14K.",
        "arxiv_id": "2504.14238",
        "ARXIVID": "2504.14238",
        "COMMENT": "This paper introduces a new dataset and method for document highlight removal, which does not align with any of the specific criteria but is relevant to computer vision applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.15188": {
        "authors": [
            "Yizhu Jiao",
            "Xuchao Zhang",
            "Zhaoyang Wang",
            "Yubo Ma",
            "Zhun Deng",
            "Rujia Wang",
            "Chetan Bansal",
            "Saravan Rajmohan",
            "Jiawei Han",
            "Huaxiu Yao"
        ],
        "title": "Synergistic Weak-Strong Collaboration by Aligning Preferences",
        "abstract": "arXiv:2504.15188v1 Announce Type: new  Abstract: Current Large Language Models (LLMs) excel in general reasoning yet struggle with specialized tasks requiring proprietary or domain-specific knowledge. Fine-tuning large models for every niche application is often infeasible due to black-box constraints and high computational overhead. To address this, we propose a collaborative framework that pairs a specialized weak model with a general strong model. The weak model, tailored to specific domains, produces initial drafts and background information, while the strong model leverages its advanced reasoning to refine these drafts, extending LLMs' capabilities to critical yet specialized tasks. To optimize this collaboration, we introduce a collaborative feedback to fine-tunes the weak model, which quantifies the influence of the weak model's contributions in the collaboration procedure and establishes preference pairs to guide preference tuning of the weak model. We validate our framework through experiments on three domains. We find that the collaboration significantly outperforms each model alone by leveraging complementary strengths. Moreover, aligning the weak model with the collaborative preference further enhances overall performance.",
        "arxiv_id": "2504.15188",
        "ARXIVID": "2504.15188",
        "COMMENT": "This paper proposes a collaborative framework for specialized and general models, which does not directly match any of the criteria but is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.15003": {
        "authors": [
            "Xin Li",
            "Xijun Wang",
            "Bingchen Li",
            "Kun Yuan",
            "Yizhen Shao",
            "Suhang Yao",
            "Ming Sun",
            "Chao Zhou",
            "Radu Timofte",
            "Zhibo Chen"
        ],
        "title": "NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: KwaiSR Dataset and Study",
        "abstract": "arXiv:2504.15003v1 Announce Type: new  Abstract: In this work, we build the first benchmark dataset for short-form UGC Image Super-resolution in the wild, termed KwaiSR, intending to advance the research on developing image super-resolution algorithms for short-form UGC platforms. This dataset is collected from the Kwai Platform, which is composed of two parts, i.e., synthetic and wild parts. Among them, the synthetic dataset, including 1,900 image pairs, is produced by simulating the degradation following the distribution of real-world low-quality short-form UGC images, aiming to provide the ground truth for training and objective comparison in the validation/testing. The wild dataset contains low-quality images collected directly from the Kwai Platform, which are filtered using the quality assessment method KVQ from the Kwai Platform. As a result, the KwaiSR dataset contains 1800 synthetic image pairs and 1900 wild images, which are divided into training, validation, and testing parts with a ratio of 8:1:1. Based on the KwaiSR dataset, we organize the NTIRE 2025 challenge on a second short-form UGC Video quality assessment and enhancement, which attracts lots of researchers to develop the algorithm for it. The results of this competition have revealed that our KwaiSR dataset is pretty challenging for existing Image SR methods, which is expected to lead to a new direction in the image super-resolution field. The dataset can be found from https://lixinustc.github.io/NTIRE2025-KVQE-KwaSR-KVQ.github.io/.",
        "arxiv_id": "2504.15003",
        "ARXIVID": "2504.15003",
        "COMMENT": "This paper introduces a new benchmark dataset for image super-resolution, which could be tangentially related to criterion 3 (new benchmarks), but it is not focused on embodied AI or simulators.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.15041": {
        "authors": [
            "Shiben Liu",
            "Huijie Fan",
            "Qiang Wang",
            "Baojie Fan",
            "Yandong Tang",
            "Liangqiong Qu"
        ],
        "title": "Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong Person Re-identification",
        "abstract": "arXiv:2504.15041v1 Announce Type: new  Abstract: Lifelong Person Re-identification (LReID) suffers from a key challenge in preserving old knowledge while adapting to new information. The existing solutions include rehearsal-based and rehearsal-free methods to address this challenge. Rehearsal-based approaches rely on knowledge distillation, continuously accumulating forgetting during the distillation process. Rehearsal-free methods insufficiently learn the distribution of each domain, leading to forgetfulness over time. To solve these issues, we propose a novel Distribution-aware Forgetting Compensation (DAFC) model that explores cross-domain shared representation learning and domain-specific distribution integration without using old exemplars or knowledge distillation. We propose a Text-driven Prompt Aggregation (TPA) that utilizes text features to enrich prompt elements and guide the prompt model to learn fine-grained representations for each instance. This can enhance the differentiation of identity information and establish the foundation for domain distribution awareness. Then, Distribution-based Awareness and Integration (DAI) is designed to capture each domain-specific distribution by a dedicated expert network and adaptively consolidate them into a shared region in high-dimensional space. In this manner, DAI can consolidate and enhance cross-domain shared representation learning while alleviating catastrophic forgetting. Furthermore, we develop a Knowledge Consolidation Mechanism (KCM) that comprises instance-level discrimination and cross-domain consistency alignment strategies to facilitate model adaptive learning of new knowledge from the current domain and promote knowledge consolidation learning between acquired domain-specific distributions, respectively. Experimental results show that our DAFC outperform state-of-the-art methods by at least 9.8\\%/6.6\\% and 6.4\\%/6.2\\% of average mAP/R@1 on two training orders.",
        "arxiv_id": "2504.15041",
        "ARXIVID": "2504.15041",
        "COMMENT": "Does not match any specific criterion but is related to lifelong learning and domain adaptation, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2504.14600": {
        "authors": [
            "Zheng Chen",
            "Jingkai Wang",
            "Kai Liu",
            "Jue Gong",
            "Lei Sun",
            "Zongwei Wu",
            "Radu Timofte",
            "Yulun Zhang",
            "Jianxing Zhang",
            "Jinlong Wu",
            "Jun Wang",
            "Zheng Xie",
            "Hakjae Jeon",
            "Suejin Han",
            "Hyung-Ju Chun",
            "Hyunhee Park",
            "Zhicun Yin",
            "Junjie Chen",
            "Ming Liu",
            "Xiaoming Li",
            "Chao Zhou",
            "Wangmeng Zuo",
            "Weixia Zhang",
            "Dingquan Li",
            "Kede Ma",
            "Yun Zhang",
            "Zhuofan Zheng",
            "Yuyue Liu",
            "Shizhen Tang",
            "Zihao Zhang",
            "Yi Ning",
            "Hao Jiang",
            "Wenjie An",
            "Kangmeng Yu",
            "Chenyang Wang",
            "Kui Jiang",
            "Xianming Liu",
            "Junjun Jiang",
            "Yingfu Zhang",
            "Gang He",
            "Siqi Wang",
            "Kepeng Xu",
            "Zhenyang Liu",
            "Changxin Zhou",
            "Shanlan Shen",
            "Yubo Duan",
            "Yiang Chen",
            "Jin Guo",
            "Mengru Yang",
            "Jen-Wei Lee",
            "Chia-Ming Lee",
            "Chih-Chung Hsu",
            "Hu Peng",
            "Chunming He"
        ],
        "title": "NTIRE 2025 Challenge on Real-World Face Restoration: Methods and Results",
        "abstract": "arXiv:2504.14600v1 Announce Type: new  Abstract: This paper provides a review of the NTIRE 2025 challenge on real-world face restoration, highlighting the proposed solutions and the resulting outcomes. The challenge focuses on generating natural, realistic outputs while maintaining identity consistency. Its goal is to advance state-of-the-art solutions for perceptual quality and realism, without imposing constraints on computational resources or training data. The track of the challenge evaluates performance using a weighted image quality assessment (IQA) score and employs the AdaFace model as an identity checker. The competition attracted 141 registrants, with 13 teams submitting valid models, and ultimately, 10 teams achieved a valid score in the final ranking. This collaborative effort advances the performance of real-world face restoration while offering an in-depth overview of the latest trends in the field.",
        "arxiv_id": "2504.14600",
        "ARXIVID": "2504.14600",
        "COMMENT": "Does not match any specific criteria. Focuses on face restoration, which is not directly related to spatial understanding, VLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.14191": {
        "authors": [
            "Yansheng Qiu",
            "Haoquan Zhang",
            "Zhaopan Xu",
            "Ming Li",
            "Diping Song",
            "Zheng Wang",
            "Kaipeng Zhang"
        ],
        "title": "AI Idea Bench 2025: AI Research Idea Generation Benchmark",
        "abstract": "arXiv:2504.14191v1 Announce Type: new  Abstract: Large-scale Language Models (LLMs) have revolutionized human-AI interaction and achieved significant success in the generation of novel ideas. However, current assessments of idea generation overlook crucial factors such as knowledge leakage in LLMs, the absence of open-ended benchmarks with grounded truth, and the limited scope of feasibility analysis constrained by prompt design. These limitations hinder the potential of uncovering groundbreaking research ideas. In this paper, we present AI Idea Bench 2025, a framework designed to quantitatively evaluate and compare the ideas generated by LLMs within the domain of AI research from diverse perspectives. The framework comprises a comprehensive dataset of 3,495 AI papers and their associated inspired works, along with a robust evaluation methodology. This evaluation system gauges idea quality in two dimensions: alignment with the ground-truth content of the original papers and judgment based on general reference material. AI Idea Bench 2025's benchmarking system stands to be an invaluable resource for assessing and comparing idea-generation techniques, thereby facilitating the automation of scientific discovery.",
        "arxiv_id": "2504.14191",
        "ARXIVID": "2504.14191",
        "COMMENT": "Does not match any specific criteria but is related to idea generation benchmarks, which is tangentially relevant to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.14423": {
        "authors": [
            "Qiang Chen",
            "Xiao Wang",
            "Haowen Wang",
            "Bo Jiang",
            "Lin Zhu",
            "Dawei Zhang",
            "Yonghong Tian",
            "Jin Tang"
        ],
        "title": "Adversarial Attack for RGB-Event based Visual Object Tracking",
        "abstract": "arXiv:2504.14423v1 Announce Type: new  Abstract: Visual object tracking is a crucial research topic in the fields of computer vision and multi-modal fusion. Among various approaches, robust visual tracking that combines RGB frames with Event streams has attracted increasing attention from researchers. While striving for high accuracy and efficiency in tracking, it is also important to explore how to effectively conduct adversarial attacks and defenses on RGB-Event stream tracking algorithms, yet research in this area remains relatively scarce. To bridge this gap, in this paper, we propose a cross-modal adversarial attack algorithm for RGB-Event visual tracking. Because of the diverse representations of Event streams, and given that Event voxels and frames are more commonly used, this paper will focus on these two representations for an in-depth study. Specifically, for the RGB-Event voxel, we first optimize the perturbation by adversarial loss to generate RGB frame adversarial examples. For discrete Event voxel representations, we propose a two-step attack strategy, more in detail, we first inject Event voxels into the target region as initialized adversarial examples, then, conduct a gradient-guided optimization by perturbing the spatial location of the Event voxels. For the RGB-Event frame based tracking, we optimize the cross-modal universal perturbation by integrating the gradient information from multimodal data. We evaluate the proposed approach against attacks on three widely used RGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive experiments show that our method significantly reduces the performance of the tracker across numerous datasets in both unimodal and multimodal scenarios. The source code will be released on https://github.com/Event-AHU/Adversarial_Attack_Defense",
        "arxiv_id": "2504.14423",
        "ARXIVID": "2504.14423",
        "COMMENT": "Does not match any specific criteria but is related to adversarial attacks in multi-modal fusion, which is tangentially relevant to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2504.14582": {
        "authors": [
            "Zheng Chen",
            "Kai Liu",
            "Jue Gong",
            "Jingkai Wang",
            "Lei Sun",
            "Zongwei Wu",
            "Radu Timofte",
            "Yulun Zhang",
            "Xiangyu Kong",
            "Xiaoxuan Yu",
            "Hyunhee Park",
            "Suejin Han",
            "Hakjae Jeon",
            "Dafeng Zhang",
            "Hyung-Ju Chun",
            "Donghun Ryou",
            "Inju Ha",
            "Bohyung Han",
            "Lu Zhao",
            "Yuyi Zhang",
            "Pengyu Yan",
            "Jiawei Hu",
            "Pengwei Liu",
            "Fengjun Guo",
            "Hongyuan Yu",
            "Pufan Xu",
            "Zhijuan Huang",
            "Shuyuan Cui",
            "Peng Guo",
            "Jiahui Liu",
            "Dongkai Zhang",
            "Heng Zhang",
            "Huiyuan Fu",
            "Huadong Ma",
            "Yanhui Guo",
            "Sisi Tian",
            "Xin Liu",
            "Jinwen Liang",
            "Jie Liu",
            "Jie Tang",
            "Gangshan Wu",
            "Zeyu Xiao",
            "Zhuoyuan Li",
            "Yinxiang Zhang",
            "Wenxuan Cai",
            "Vijayalaxmi Ashok Aralikatti",
            "Nikhil Akalwadi",
            "G Gyaneshwar Rao",
            "Chaitra Desai",
            "Ramesh Ashok Tabib",
            "Uma Mudenagudi",
            "Marcos V. Conde",
            "Alejandro Merino",
            "Bruno Longarela",
            "Javier Abad",
            "Weijun Yuan",
            "Zhan Li",
            "Zhanglu Chen",
            "Boyang Yao",
            "Aagam Jain",
            "Milan Kumar Singh",
            "Ankit Kumar",
            "Shubh Kawa",
            "Divyavardhan Singh",
            "Anjali Sarvaiya",
            "Kishor Upla",
            "Raghavendra Ramachandra",
            "Chia-Ming Lee",
            "Yu-Fan Lin",
            "Chih-Chung Hsu",
            "Risheek V Hiremath",
            "Yashaswini Palani",
            "Yuxuan Jiang",
            "Qiang Zhu",
            "Siyue Teng",
            "Fan Zhang",
            "Shuyuan Zhu",
            "Bing Zeng",
            "David Bull",
            "Jingwei Liao",
            "Yuqing Yang",
            "Wenda Shao",
            "Junyi Zhao",
            "Qisheng Xu",
            "Kele Xu",
            "Sunder Ali Khowaja",
            "Ik Hyun Lee",
            "Snehal Singh Tomar",
            "Rajarshi Ray",
            "Klaus Mueller",
            "Sachin Chaudhary",
            "Surya Vashisth",
            "Akshay Dudhane",
            "Praful Hambarde",
            "Satya Naryan Tazi",
            "Prashant Patil",
            "Santosh Kumar Vipparthi",
            "Subrahmanyam Murala",
            "Bilel Benjdira",
            "Anas M. Ali",
            "Wadii Boulila",
            "Zahra Moammeri",
            "Ahmad Mahmoudi-Aznaveh",
            "Ali Karbasi",
            "Hossein Motamednia",
            "Liangyan Li",
            "Guanhua Zhao",
            "Kevin Le",
            "Yimo Ning",
            "Haoxuan Huang",
            "Jun Chen"
        ],
        "title": "NTIRE 2025 Challenge on Image Super-Resolution ($\\times$4): Methods and Results",
        "abstract": "arXiv:2504.14582v1 Announce Type: new  Abstract: This paper presents the NTIRE 2025 image super-resolution ($\\times$4) challenge, one of the associated competitions of the 10th NTIRE Workshop at CVPR 2025. The challenge aims to recover high-resolution (HR) images from low-resolution (LR) counterparts generated through bicubic downsampling with a $\\times$4 scaling factor. The objective is to develop effective network designs or solutions that achieve state-of-the-art SR performance. To reflect the dual objectives of image SR research, the challenge includes two sub-tracks: (1) a restoration track, emphasizes pixel-wise accuracy and ranks submissions based on PSNR; (2) a perceptual track, focuses on visual realism and ranks results by a perceptual score. A total of 286 participants registered for the competition, with 25 teams submitting valid entries. This report summarizes the challenge design, datasets, evaluation protocol, the main results, and methods of each team. The challenge serves as a benchmark to advance the state of the art and foster progress in image SR.",
        "arxiv_id": "2504.14582",
        "ARXIVID": "2504.14582",
        "COMMENT": "This paper discusses a challenge on image super-resolution but does not align with any of the specific criteria. It is more focused on benchmarking for image restoration.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}