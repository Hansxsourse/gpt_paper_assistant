{
    "2508.02129": {
        "authors": [
            "Yuru Xiao",
            "Zihan Lin",
            "Chao Lu",
            "Deming Zhai",
            "Kui Jiang",
            "Wenbo Zhao",
            "Wei Zhang",
            "Junjun Jiang",
            "Huanran Wang",
            "Xianming Liu"
        ],
        "title": "VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic Urban Scenes Modeling",
        "abstract": "arXiv:2508.02129v1 Announce Type: new  Abstract: Dynamic urban scene modeling is a rapidly evolving area with broad applications. While current approaches leveraging neural radiance fields or Gaussian Splatting have achieved fine-grained reconstruction and high-fidelity novel view synthesis, they still face significant limitations. These often stem from a dependence on pre-calibrated object tracks or difficulties in accurately modeling fast-moving objects from undersampled capture, particularly due to challenges in handling temporal discontinuities. To overcome these issues, we propose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our key insight is to distill robust, temporally consistent priors from a test-time adapted video diffusion model. To ensure precise pose alignment and effective integration of this denoised content, we introduce two core innovations: a joint timestamp optimization strategy that refines interpolated frame poses, and an uncertainty distillation method that adaptively extracts target content while preserving well-reconstructed regions. Extensive experiments demonstrate that our method significantly enhances dynamic modeling, especially for fast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view synthesis over baseline approaches.",
        "arxiv_id": "2508.02129",
        "ARXIVID": "2508.02129",
        "COMMENT": "Matches criterion 2",
        "RELEVANCE": 5,
        "NOVELTY": 6
    }
}