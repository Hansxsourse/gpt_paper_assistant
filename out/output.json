{
    "2505.05517": {
        "authors": [
            "Hongyi Chen",
            "Yunchao Yao",
            "Yufei Ye",
            "Zhixuan Xu",
            "Homanga Bharadhwaj",
            "Jiashun Wang",
            "Shubham Tulsiani",
            "Zackory Erickson",
            "Jeffrey Ichnowski"
        ],
        "title": "Web2Grasp: Learning Functional Grasps from Web Images of Hand-Object Interactions",
        "abstract": "arXiv:2505.05517v1 Announce Type: new  Abstract: Functional grasp is essential for enabling dexterous multi-finger robot hands to manipulate objects effectively. However, most prior work either focuses on power grasping, which simply involves holding an object still, or relies on costly teleoperated robot demonstrations to teach robots how to grasp each object functionally. Instead, we propose extracting human grasp information from web images since they depict natural and functional object interactions, thereby bypassing the need for curated demonstrations. We reconstruct human hand-object interaction (HOI) 3D meshes from RGB images, retarget the human hand to multi-finger robot hands, and align the noisy object mesh with its accurate 3D shape. We show that these relatively low-quality HOI data from inexpensive web sources can effectively train a functional grasping model. To further expand the grasp dataset for seen and unseen objects, we use the initially-trained grasping policy with web data in the IsaacGym simulator to generate physically feasible grasps while preserving functionality. We train the grasping model on 10 object categories and evaluate it on 9 unseen objects, including challenging items such as syringes, pens, spray bottles, and tongs, which are underrepresented in existing datasets. The model trained on the web HOI dataset, achieving a 75.8% success rate on seen objects and 61.8% across all objects in simulation, with a 6.7% improvement in success rate and a 1.8x increase in functionality ratings over baselines. Simulator-augmented data further boosts performance from 61.8% to 83.4%. The sim-to-real transfer to the LEAP Hand achieves a 85% success rate. Project website is at: https://webgrasp.github.io/.",
        "arxiv_id": "2505.05517",
        "ARXIVID": "2505.05517",
        "COMMENT": "Matches criterion 3 as it focuses on functional grasping for embodied AI using web images and introduces a novel dataset and method.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2505.05681": {
        "authors": [
            "Giulio Cesare Mastrocinque Santo",
            "Patr\\'icia Izar",
            "Irene Delval",
            "Victor de Napole Gregolin",
            "Nina S. T. Hirata"
        ],
        "title": "Fine-Tuning Video-Text Contrastive Model for Primate Behavior Retrieval from Unlabeled Raw Videos",
        "abstract": "arXiv:2505.05681v1 Announce Type: new  Abstract: Video recordings of nonhuman primates in their natural habitat are a common source for studying their behavior in the wild. We fine-tune pre-trained video-text foundational models for the specific domain of capuchin monkeys, with the goal of developing useful computational models to help researchers to retrieve useful clips from videos. We focus on the challenging problem of training a model based solely on raw, unlabeled video footage, using weak audio descriptions sometimes provided by field collaborators. We leverage recent advances in Multimodal Large Language Models (MLLMs) and Vision-Language Models (VLMs) to address the extremely noisy nature of both video and audio content. Specifically, we propose a two-folded approach: an agentic data treatment pipeline and a fine-tuning process. The data processing pipeline automatically extracts clean and semantically aligned video-text pairs from the raw videos, which are subsequently used to fine-tune a pre-trained Microsoft's X-CLIP model through Low-Rank Adaptation (LoRA). We obtained an uplift in $Hits@5$ of $167\\%$ for the 16 frames model and an uplift of $114\\%$ for the 8 frame model on our domain data. Moreover, based on $NDCG@K$ results, our model is able to rank well most of the considered behaviors, while the tested raw pre-trained models are not able to rank them at all. The code will be made available upon acceptance.",
        "arxiv_id": "2505.05681",
        "ARXIVID": "2505.05681",
        "COMMENT": "Matches criterion 2 as it fine-tunes a video-text foundational model and leverages VLMs/MLLMs for behavior retrieval.",
        "RELEVANCE": 9,
        "NOVELTY": 6
    },
    "2505.05591": {
        "authors": [
            "Yueh-Cheng Liu",
            "Lukas H\\\"ollein",
            "Matthias Nie{\\ss}ner",
            "Angela Dai"
        ],
        "title": "QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian Initialization",
        "abstract": "arXiv:2505.05591v1 Announce Type: new  Abstract: Surface reconstruction is fundamental to computer vision and graphics, enabling applications in 3D modeling, mixed reality, robotics, and more. Existing approaches based on volumetric rendering obtain promising results, but optimize on a per-scene basis, resulting in a slow optimization that can struggle to model under-observed or textureless regions. We introduce QuickSplat, which learns data-driven priors to generate dense initializations for 2D gaussian splatting optimization of large-scale indoor scenes. This provides a strong starting point for the reconstruction, which accelerates the convergence of the optimization and improves the geometry of flat wall structures. We further learn to jointly estimate the densification and update of the scene parameters during each iteration; our proposed densifier network predicts new Gaussians based on the rendering gradients of existing ones, removing the needs of heuristics for densification. Extensive experiments on large-scale indoor scene reconstruction demonstrate the superiority of our data-driven optimization. Concretely, we accelerate runtime by 8x, while decreasing depth errors by up to 48% in comparison to state of the art methods.",
        "arxiv_id": "2505.05591",
        "ARXIVID": "2505.05591",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for spatial understanding in 3D surface reconstruction.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.05512": {
        "authors": [
            "Zhang Zhang",
            "Qiang Zhang",
            "Wei Cui",
            "Shuai Shi",
            "Yijie Guo",
            "Gang Han",
            "Wen Zhao",
            "Jingkai Sun",
            "Jiahang Cao",
            "Jiaxu Wang",
            "Hao Cheng",
            "Xiaozhu Ju",
            "Zhengping Che",
            "Renjing Xu",
            "Jian Tang"
        ],
        "title": "Occupancy World Model for Robots",
        "abstract": "arXiv:2505.05512v1 Announce Type: new  Abstract: Understanding and forecasting the scene evolutions deeply affect the exploration and decision of embodied agents. While traditional methods simulate scene evolutions through trajectory prediction of potential instances, current works use the occupancy world model as a generative framework for describing fine-grained overall scene dynamics. However, existing methods cluster on the outdoor structured road scenes, while ignoring the exploration of forecasting 3D occupancy scene evolutions for robots in indoor scenes. In this work, we explore a new framework for learning the scene evolutions of observed fine-grained occupancy and propose an occupancy world model based on the combined spatio-temporal receptive field and guided autoregressive transformer to forecast the scene evolutions, called RoboOccWorld. We propose the Conditional Causal State Attention (CCSA), which utilizes camera poses of next state as conditions to guide the autoregressive transformer to adapt and understand the indoor robotics scenarios. In order to effectively exploit the spatio-temporal cues from historical observations, Hybrid Spatio-Temporal Aggregation (HSTA) is proposed to obtain the combined spatio-temporal receptive field based on multi-scale spatio-temporal windows. In addition, we restructure the OccWorld-ScanNet benchmark based on local annotations to facilitate the evaluation of the indoor 3D occupancy scene evolution prediction task. Experimental results demonstrate that our RoboOccWorld outperforms state-of-the-art methods in indoor 3D occupancy scene evolution prediction task. The code will be released soon.",
        "arxiv_id": "2505.05512",
        "ARXIVID": "2505.05512",
        "COMMENT": "Matches criterion 3 as it proposes a new framework for indoor 3D occupancy scene evolution prediction and introduces a new benchmark.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.06217": {
        "authors": [
            "Pengfei Gu",
            "Haoteng Tang",
            "Islam A. Ebeid",
            "Jose A. Nunez",
            "Fabian Vazquez",
            "Diego Adame",
            "Marcus Zhan",
            "Huimin Li",
            "Bin Fu",
            "Danny Z. Chen"
        ],
        "title": "Adapting a Segmentation Foundation Model for Medical Image Classification",
        "abstract": "arXiv:2505.06217v1 Announce Type: new  Abstract: Recent advancements in foundation models, such as the Segment Anything Model (SAM), have shown strong performance in various vision tasks, particularly image segmentation, due to their impressive zero-shot segmentation capabilities. However, effectively adapting such models for medical image classification is still a less explored topic. In this paper, we introduce a new framework to adapt SAM for medical image classification. First, we utilize the SAM image encoder as a feature extractor to capture segmentation-based features that convey important spatial and contextual details of the image, while freezing its weights to avoid unnecessary overhead during training. Next, we propose a novel Spatially Localized Channel Attention (SLCA) mechanism to compute spatially localized attention weights for the feature maps. The features extracted from SAM's image encoder are processed through SLCA to compute attention weights, which are then integrated into deep learning classification models to enhance their focus on spatially relevant or meaningful regions of the image, thus improving classification performance. Experimental results on three public medical image classification datasets demonstrate the effectiveness and data-efficiency of our approach.",
        "arxiv_id": "2505.06217",
        "ARXIVID": "2505.06217",
        "COMMENT": "Matches criterion 4 as it adapts a vision foundation model (SAM) for medical image classification.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2505.05501": {
        "authors": [
            "Pu Cao",
            "Feng Zhou",
            "Junyi Ji",
            "Qingye Kong",
            "Zhixiang Lv",
            "Mingjian Zhang",
            "Xuekun Zhao",
            "Siqi Wu",
            "Yinghui Lin",
            "Qing Song",
            "Lu Yang"
        ],
        "title": "Preliminary Explorations with GPT-4o(mni) Native Image Generation",
        "abstract": "arXiv:2505.05501v1 Announce Type: new  Abstract: Recently, the visual generation ability by GPT-4o(mni) has been unlocked by OpenAI. It demonstrates a very remarkable generation capability with excellent multimodal condition understanding and varied task instructions. In this paper, we aim to explore the capabilities of GPT-4o across various tasks. Inspired by previous study, we constructed a task taxonomy along with a carefully curated set of test samples to conduct a comprehensive qualitative test. Benefiting from GPT-4o's powerful multimodal comprehension, its image-generation process demonstrates abilities surpassing those of traditional image-generation tasks. Thus, regarding the dimensions of model capabilities, we evaluate its performance across six task categories: traditional image generation tasks, discriminative tasks, knowledge-based generation, commonsense-based generation, spatially-aware image generation, and temporally-aware image generation. These tasks not only assess the quality and conditional alignment of the model's outputs but also probe deeper into GPT-4o's understanding of real-world concepts. Our results reveal that GPT-4o performs impressively well in general-purpose synthesis tasks, showing strong capabilities in text-to-image generation, visual stylization, and low-level image processing. However, significant limitations remain in its ability to perform precise spatial reasoning, instruction-grounded generation, and consistent temporal prediction. Furthermore, when faced with knowledge-intensive or domain-specific scenarios, such as scientific illustrations or mathematical plots, the model often exhibits hallucinations, factual errors, or structural inconsistencies. These findings suggest that while GPT-4o marks a substantial advancement in unified multimodal generation, there is still a long way to go before it can be reliably applied to professional or safety-critical domains.",
        "arxiv_id": "2505.05501",
        "ARXIVID": "2505.05501",
        "COMMENT": "Matches criterion 2. Explores GPT-4o's multimodal capabilities, including visual generation tasks, which aligns with interest in VLLMs/MLLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.05505": {
        "authors": [
            "Yiming Qin",
            "Zhu Xu",
            "Yang Liu"
        ],
        "title": "Apply Hierarchical-Chain-of-Generation to Complex Attributes Text-to-3D Generation",
        "abstract": "arXiv:2505.05505v1 Announce Type: new  Abstract: Recent text-to-3D models can render high-quality assets, yet they still stumble on objects with complex attributes. The key obstacles are: (1) existing text-to-3D approaches typically lift text-to-image models to extract semantics via text encoders, while the text encoder exhibits limited comprehension ability for long descriptions, leading to deviated cross-attention focus, subsequently wrong attribute binding in generated results. (2) Occluded object parts demand a disciplined generation order and explicit part disentanglement. Though some works introduce manual efforts to alleviate the above issues, their quality is unstable and highly reliant on manual information. To tackle above problems, we propose a automated method Hierarchical-Chain-of-Generation (HCoG). It leverages a large language model to decompose the long description into blocks representing different object parts, and orders them from inside out according to occlusions, forming a hierarchical chain. Within each block we first coarsely create components, then precisely bind attributes via target-region localization and corresponding 3D Gaussian kernel optimization. Between blocks, we introduce Gaussian Extension and Label Elimination to seamlessly generate new parts by extending new Gaussian kernels, re-assigning semantic labels, and eliminating unnecessary kernels, ensuring that only relevant parts are added without disrupting previously optimized parts. Experiments confirm that HCoG yields structurally coherent, attribute-faithful 3D objects with complex attributes. The code is available at https://github.com/Wakals/GASCOL .",
        "arxiv_id": "2505.05505",
        "ARXIVID": "2505.05505",
        "COMMENT": "Matches criterion 4. Proposes a novel method for text-to-3D generation, which is related to vision foundation models and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2505.06219": {
        "authors": [
            "Noah Frahm",
            "Dongxu Zhao",
            "Andrea Dunn Beltran",
            "Ron Alterovitz",
            "Jan-Michael Frahm",
            "Junier Oliva",
            "Roni Sengupta"
        ],
        "title": "VIN-NBV: A View Introspection Network for Next-Best-View Selection for Resource-Efficient 3D Reconstruction",
        "abstract": "arXiv:2505.06219v1 Announce Type: new  Abstract: Next Best View (NBV) algorithms aim to acquire an optimal set of images using minimal resources, time, or number of captures to enable efficient 3D reconstruction of a scene. Existing approaches often rely on prior scene knowledge or additional image captures and often develop policies that maximize coverage. Yet, for many real scenes with complex geometry and self-occlusions, coverage maximization does not lead to better reconstruction quality directly. In this paper, we propose the View Introspection Network (VIN), which is trained to predict the reconstruction quality improvement of views directly, and the VIN-NBV policy. A greedy sequential sampling-based policy, where at each acquisition step, we sample multiple query views and choose the one with the highest VIN predicted improvement score. We design the VIN to perform 3D-aware featurization of the reconstruction built from prior acquisitions, and for each query view create a feature that can be decoded into an improvement score. We then train the VIN using imitation learning to predict the reconstruction improvement score. We show that VIN-NBV improves reconstruction quality by ~30% over a coverage maximization baseline when operating with constraints on the number of acquisitions or the time in motion.",
        "arxiv_id": "2505.06219",
        "ARXIVID": "2505.06219",
        "COMMENT": "Matches criterion 3 as it introduces a new method for next-best-view selection in 3D reconstruction, which is relevant to embodied AI benchmarks and methods.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2505.05666": {
        "authors": [
            "Alexander Most",
            "Joseph Winjum",
            "Ayan Biswas",
            "Shawn Jones",
            "Nishath Rajiv Ranasinghe",
            "Dan O'Malley",
            "Manish Bhattarai"
        ],
        "title": "Lost in OCR Translation? Vision-Based Approaches to Robust Document Retrieval",
        "abstract": "arXiv:2505.05666v1 Announce Type: new  Abstract: Retrieval-Augmented Generation (RAG) has become a popular technique for enhancing the reliability and utility of Large Language Models (LLMs) by grounding responses in external documents. Traditional RAG systems rely on Optical Character Recognition (OCR) to first process scanned documents into text. However, even state-of-the-art OCRs can introduce errors, especially in degraded or complex documents. Recent vision-language approaches, such as ColPali, propose direct visual embedding of documents, eliminating the need for OCR. This study presents a systematic comparison between a vision-based RAG system (ColPali) and more traditional OCR-based pipelines utilizing Llama 3.2 (90B) and Nougat OCR across varying document qualities. Beyond conventional retrieval accuracy metrics, we introduce a semantic answer evaluation benchmark to assess end-to-end question-answering performance. Our findings indicate that while vision-based RAG performs well on documents it has been fine-tuned on, OCR-based RAG is better able to generalize to unseen documents of varying quality. We highlight the key trade-offs between computational efficiency and semantic accuracy, offering practical guidance for RAG practitioners in selecting between OCR-dependent and vision-based document retrieval systems in production environments.",
        "arxiv_id": "2505.05666",
        "ARXIVID": "2505.05666",
        "COMMENT": "Matches criterion 4 as it explores vision-based approaches for document retrieval and compares them to OCR-based methods.",
        "RELEVANCE": 7,
        "NOVELTY": 5
    },
    "2505.06166": {
        "authors": [
            "Radu Alexandru Rosu",
            "Keyu Wu",
            "Yao Feng",
            "Youyi Zheng",
            "Michael J. Black"
        ],
        "title": "DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models",
        "abstract": "arXiv:2505.06166v1 Announce Type: new  Abstract: We address the task of generating 3D hair geometry from a single image, which is challenging due to the diversity of hairstyles and the lack of paired image-to-3D hair data. Previous methods are primarily trained on synthetic data and cope with the limited amount of such data by using low-dimensional intermediate representations, such as guide strands and scalp-level embeddings, that require post-processing to decode, upsample, and add realism. These approaches fail to reconstruct detailed hair, struggle with curly hair, or are limited to handling only a few hairstyles. To overcome these limitations, we propose DiffLocks, a novel framework that enables detailed reconstruction of a wide variety of hairstyles directly from a single image. First, we address the lack of 3D hair data by automating the creation of the largest synthetic hair dataset to date, containing 40K hairstyles. Second, we leverage the synthetic hair dataset to learn an image-conditioned diffusion-transfomer model that generates accurate 3D strands from a single frontal image. By using a pretrained image backbone, our method generalizes to in-the-wild images despite being trained only on synthetic data. Our diffusion model predicts a scalp texture map in which any point in the map contains the latent code for an individual hair strand. These codes are directly decoded to 3D strands without post-processing techniques. Representing individual strands, instead of guide strands, enables the transformer to model the detailed spatial structure of complex hairstyles. With this, DiffLocks can recover highly curled hair, like afro hairstyles, from a single image for the first time. Data and code is available at https://radualexandru.github.io/difflocks/",
        "arxiv_id": "2505.06166",
        "ARXIVID": "2505.06166",
        "COMMENT": "Matches criterion 4 as it introduces a novel method for generating 3D hair geometry using diffusion models, which is related to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2505.05612": {
        "authors": [
            "Qing Wang",
            "Yining Pan",
            "Minghao Zhou",
            "Zijia Tang",
            "Yanfei Wang",
            "Guangyu Wang",
            "Qianqian Song"
        ],
        "title": "scDrugMap: Benchmarking Large Foundation Models for Drug Response Prediction",
        "abstract": "arXiv:2505.05612v1 Announce Type: new  Abstract: Drug resistance presents a major challenge in cancer therapy. Single cell profiling offers insights into cellular heterogeneity, yet the application of large-scale foundation models for predicting drug response in single cell data remains underexplored. To address this, we developed scDrugMap, an integrated framework featuring both a Python command-line interface and a web server for drug response prediction. scDrugMap evaluates a wide range of foundation models, including eight single-cell models and two large language models, using a curated dataset of over 326,000 cells in the primary collection and 18,800 cells in the validation set, spanning 36 datasets and diverse tissue and cancer types. We benchmarked model performance under pooled-data and cross-data evaluation settings, employing both layer freezing and Low-Rank Adaptation (LoRA) fine-tuning strategies. In the pooled-data scenario, scFoundation achieved the best performance, with mean F1 scores of 0.971 (layer freezing) and 0.947 (fine-tuning), outperforming the lowest-performing model by over 50%. In the cross-data setting, UCE excelled post fine-tuning (mean F1: 0.774), while scGPT led in zero-shot learning (mean F1: 0.858). Overall, scDrugMap provides the first large-scale benchmark of foundation models for drug response prediction in single-cell data and serves as a user-friendly, flexible platform for advancing drug discovery and translational research.",
        "arxiv_id": "2505.05612",
        "ARXIVID": "2505.05612",
        "COMMENT": "Matches criterion 4. Benchmarks large foundation models for drug response prediction, which is related to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.05721": {
        "authors": [
            "Zixuan Li",
            "Lei Meng",
            "Guoqing Chao",
            "Wei Wu",
            "Xiaoshuo Yan",
            "Yimeng Yang",
            "Zhuang Qi",
            "Xiangxu Meng"
        ],
        "title": "Semantic-Space-Intervened Diffusive Alignment for Visual Classification",
        "abstract": "arXiv:2505.05721v1 Announce Type: new  Abstract: Cross-modal alignment is an effective approach to improving visual classification. Existing studies typically enforce a one-step mapping that uses deep neural networks to project the visual features to mimic the distribution of textual features. However, they typically face difficulties in finding such a projection due to the two modalities in both the distribution of class-wise samples and the range of their feature values. To address this issue, this paper proposes a novel Semantic-Space-Intervened Diffusive Alignment method, termed SeDA, models a semantic space as a bridge in the visual-to-textual projection, considering both types of features share the same class-level information in classification. More importantly, a bi-stage diffusion framework is developed to enable the progressive alignment between the two modalities. Specifically, SeDA first employs a Diffusion-Controlled Semantic Learner to model the semantic features space of visual features by constraining the interactive features of the diffusion model and the category centers of visual features. In the later stage of SeDA, the Diffusion-Controlled Semantic Translator focuses on learning the distribution of textual features from the semantic space. Meanwhile, the Progressive Feature Interaction Network introduces stepwise feature interactions at each alignment step, progressively integrating textual information into mapped features. Experimental results show that SeDA achieves stronger cross-modal feature alignment, leading to superior performance over existing methods across multiple scenarios.",
        "arxiv_id": "2505.05721",
        "ARXIVID": "2505.05721",
        "COMMENT": "Matches criterion 4 as it proposes a novel cross-modal alignment method for visual classification, which is related to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.05519": {
        "authors": [
            "Minkyu Choi",
            "Yunhao Yang",
            "Neel P. Bhatt",
            "Kushagra Gupta",
            "Sahil Shah",
            "Aditya Rai",
            "David Fridovich-Keil",
            "Ufuk Topcu",
            "Sandeep P. Chinchali"
        ],
        "title": "Real-Time Privacy Preservation for Robot Visual Perception",
        "abstract": "arXiv:2505.05519v1 Announce Type: new  Abstract: Many robots (e.g., iRobot's Roomba) operate based on visual observations from live video streams, and such observations may inadvertently include privacy-sensitive objects, such as personal identifiers. Existing approaches for preserving privacy rely on deep learning models, differential privacy, or cryptography. They lack guarantees for the complete concealment of all sensitive objects. Guaranteeing concealment requires post-processing techniques and thus is inadequate for real-time video streams. We develop a method for privacy-constrained video streaming, PCVS, that conceals sensitive objects within real-time video streams. PCVS takes a logical specification constraining the existence of privacy-sensitive objects, e.g., never show faces when a person exists. It uses a detection model to evaluate the existence of these objects in each incoming frame. Then, it blurs out a subset of objects such that the existence of the remaining objects satisfies the specification. We then propose a conformal prediction approach to (i) establish a theoretical lower bound on the probability of the existence of these objects in a sequence of frames satisfying the specification and (ii) update the bound with the arrival of each subsequent frame. Quantitative evaluations show that PCVS achieves over 95 percent specification satisfaction rate in multiple datasets, significantly outperforming other methods. The satisfaction rate is consistently above the theoretical bounds across all datasets, indicating that the established bounds hold. Additionally, we deploy PCVS on robots in real-time operation and show that the robots operate normally without being compromised when PCVS conceals objects.",
        "arxiv_id": "2505.05519",
        "ARXIVID": "2505.05519",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for privacy-preserving real-time video streaming in robots, which is related to embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.05804": {
        "authors": [
            "Xi Xiao",
            "Yunbei Zhang",
            "Thanh-Huy Nguyen",
            "Ba-Thinh Lam",
            "Janet Wang",
            "Jihun Hamm",
            "Tianyang Wang",
            "Xingjian Li",
            "Xiao Wang",
            "Hao Xu",
            "Tianming Liu",
            "Min Xu"
        ],
        "title": "Describe Anything in Medical Images",
        "abstract": "arXiv:2505.05804v1 Announce Type: new  Abstract: Localized image captioning has made significant progress with models like the Describe Anything Model (DAM), which can generate detailed region-specific descriptions without explicit region-text supervision. However, such capabilities have yet to be widely applied to specialized domains like medical imaging, where diagnostic interpretation relies on subtle regional findings rather than global understanding. To mitigate this gap, we propose MedDAM, the first comprehensive framework leveraging large vision-language models for region-specific captioning in medical images. MedDAM employs medical expert-designed prompts tailored to specific imaging modalities and establishes a robust evaluation benchmark comprising a customized assessment protocol, data pre-processing pipeline, and specialized QA template library. This benchmark evaluates both MedDAM and other adaptable large vision-language models, focusing on clinical factuality through attribute-level verification tasks, thereby circumventing the absence of ground-truth region-caption pairs in medical datasets. Extensive experiments on the VinDr-CXR, LIDC-IDRI, and SkinCon datasets demonstrate MedDAM's superiority over leading peers (including GPT-4o, Claude 3.7 Sonnet, LLaMA-3.2 Vision, Qwen2.5-VL, GPT-4Rol, and OMG-LLaVA) in the task, revealing the importance of region-level semantic alignment in medical image understanding and establishing MedDAM as a promising foundation for clinical vision-language integration.",
        "arxiv_id": "2505.05804",
        "ARXIVID": "2505.05804",
        "COMMENT": "Matches criterion 2 as it leverages large vision-language models (VLLMs) for medical image captioning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.06020": {
        "authors": [
            "Shuai Wang",
            "Ivona Najdenkoska",
            "Hongyi Zhu",
            "Stevan Rudinac",
            "Monika Kackovic",
            "Nachoem Wijnberg",
            "Marcel Worring"
        ],
        "title": "ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding",
        "abstract": "arXiv:2505.06020v1 Announce Type: new  Abstract: Understanding visual art requires reasoning across multiple perspectives -- cultural, historical, and stylistic -- beyond mere object recognition. While recent multimodal large language models (MLLMs) perform well on general image captioning, they often fail to capture the nuanced interpretations that fine art demands. We propose ArtRAG, a novel, training-free framework that combines structured knowledge with retrieval-augmented generation (RAG) for multi-perspective artwork explanation. ArtRAG automatically constructs an Art Context Knowledge Graph (ACKG) from domain-specific textual sources, organizing entities such as artists, movements, themes, and historical events into a rich, interpretable graph. At inference time, a multi-granular structured retriever selects semantically and topologically relevant subgraphs to guide generation. This enables MLLMs to produce contextually grounded, culturally informed art descriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG outperforms several heavily trained baselines. Human evaluations further confirm that ArtRAG generates coherent, insightful, and culturally enriched interpretations.",
        "arxiv_id": "2505.06020",
        "ARXIVID": "2505.06020",
        "COMMENT": "Matches criterion 2 as it proposes a novel framework for multi-modal large language models (MLLMs) in the context of visual art understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.06191": {
        "authors": [
            "Jiayuan Mao",
            "Joshua B. Tenenbaum",
            "Jiajun Wu"
        ],
        "title": "Neuro-Symbolic Concepts",
        "abstract": "arXiv:2505.06191v1 Announce Type: new  Abstract: This article presents a concept-centric paradigm for building agents that can learn continually and reason flexibly. The concept-centric agent utilizes a vocabulary of neuro-symbolic concepts. These concepts, such as object, relation, and action concepts, are grounded on sensory inputs and actuation outputs. They are also compositional, allowing for the creation of novel concepts through their structural combination. To facilitate learning and reasoning, the concepts are typed and represented using a combination of symbolic programs and neural network representations. Leveraging such neuro-symbolic concepts, the agent can efficiently learn and recombine them to solve various tasks across different domains, ranging from 2D images, videos, 3D scenes, and robotic manipulation tasks. This concept-centric framework offers several advantages, including data efficiency, compositional generalization, continual learning, and zero-shot transfer.",
        "arxiv_id": "2505.06191",
        "ARXIVID": "2505.06191",
        "COMMENT": "Does not match any specific criteria but discusses neuro-symbolic concepts, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.05710": {
        "authors": [
            "Wooyoung Jeong",
            "Hyun Jae Park",
            "Seonghun Jeong",
            "Jong Wook Jang",
            "Tae Hoon Lim",
            "Dae Seoung Kim"
        ],
        "title": "HyperspectralMAE: The Hyperspectral Imagery Classification Model using Fourier-Encoded Dual-Branch Masked Autoencoder",
        "abstract": "arXiv:2505.05710v1 Announce Type: new  Abstract: Hyperspectral imagery provides rich spectral detail but poses unique challenges because of its high dimensionality in both spatial and spectral domains. We propose \\textit{HyperspectralMAE}, a Transformer-based foundation model for hyperspectral data that employs a \\textit{dual masking} strategy: during pre-training we randomly occlude 50\\% of spatial patches and 50\\% of spectral bands. This forces the model to learn representations capable of reconstructing missing information across both dimensions. To encode spectral order, we introduce learnable harmonic Fourier positional embeddings based on wavelength. The reconstruction objective combines mean-squared error (MSE) with the spectral angle mapper (SAM) to balance pixel-level accuracy and spectral-shape fidelity.   The resulting model contains about $1.8\\times10^{8}$ parameters and produces 768-dimensional embeddings, giving it sufficient capacity for transfer learning. We pre-trained HyperspectralMAE on two large hyperspectral corpora -- NASA EO-1 Hyperion ($\\sim$1\\,600 scenes, $\\sim$$3\\times10^{11}$ pixel spectra) and DLR EnMAP Level-0 ($\\sim$1\\,300 scenes, $\\sim$$3\\times10^{11}$ pixel spectra) -- and fine-tuned it for land-cover classification on the Indian Pines benchmark. HyperspectralMAE achieves state-of-the-art transfer-learning accuracy on Indian Pines, confirming that masked dual-dimensional pre-training yields robust spectral-spatial representations. These results demonstrate that dual masking and wavelength-aware embeddings advance hyperspectral image reconstruction and downstream analysis.",
        "arxiv_id": "2505.05710",
        "ARXIVID": "2505.05710",
        "COMMENT": "Does not match any specific criteria but is related to vision foundation models for hyperspectral imagery.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.05853": {
        "authors": [
            "Tongda Xu",
            "Jiahao Li",
            "Bin Li",
            "Yan Wang",
            "Ya-Qin Zhang",
            "Yan Lu"
        ],
        "title": "PICD: Versatile Perceptual Image Compression with Diffusion Rendering",
        "abstract": "arXiv:2505.05853v1 Announce Type: new  Abstract: Recently, perceptual image compression has achieved significant advancements, delivering high visual quality at low bitrates for natural images. However, for screen content, existing methods often produce noticeable artifacts when compressing text. To tackle this challenge, we propose versatile perceptual screen image compression with diffusion rendering (PICD), a codec that works well for both screen and natural images. More specifically, we propose a compression framework that encodes the text and image separately, and renders them into one image using diffusion model. For this diffusion rendering, we integrate conditional information into diffusion models at three distinct levels: 1). Domain level: We fine-tune the base diffusion model using text content prompts with screen content. 2). Adaptor level: We develop an efficient adaptor to control the diffusion model using compressed image and text as input. 3). Instance level: We apply instance-wise guidance to further enhance the decoding process. Empirically, our PICD surpasses existing perceptual codecs in terms of both text accuracy and perceptual quality. Additionally, without text conditions, our approach serves effectively as a perceptual codec for natural images.",
        "arxiv_id": "2505.05853",
        "ARXIVID": "2505.05853",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling and image compression.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.06117": {
        "authors": [
            "Dongying Li",
            "Binyi Su",
            "Hua Zhang",
            "Yong Li",
            "Haiyong Chen"
        ],
        "title": "Photovoltaic Defect Image Generator with Boundary Alignment Smoothing Constraint for Domain Shift Mitigation",
        "abstract": "arXiv:2505.06117v1 Announce Type: new  Abstract: Accurate defect detection of photovoltaic (PV) cells is critical for ensuring quality and efficiency in intelligent PV manufacturing systems. However, the scarcity of rich defect data poses substantial challenges for effective model training. While existing methods have explored generative models to augment datasets, they often suffer from instability, limited diversity, and domain shifts. To address these issues, we propose PDIG, a Photovoltaic Defect Image Generator based on Stable Diffusion (SD). PDIG leverages the strong priors learned from large-scale datasets to enhance generation quality under limited data. Specifically, we introduce a Semantic Concept Embedding (SCE) module that incorporates text-conditioned priors to capture the relational concepts between defect types and their appearances. To further enrich the domain distribution, we design a Lightweight Industrial Style Adaptor (LISA), which injects industrial defect characteristics into the SD model through cross-disentangled attention. At inference, we propose a Text-Image Dual-Space Constraints (TIDSC) module, enforcing the quality of generated images via positional consistency and spatial smoothing alignment. Extensive experiments demonstrate that PDIG achieves superior realism and diversity compared to state-of-the-art methods. Specifically, our approach improves Frechet Inception Distance (FID) by 19.16 points over the second-best method and significantly enhances the performance of downstream defect detection tasks.",
        "arxiv_id": "2505.06117",
        "ARXIVID": "2505.06117",
        "COMMENT": "Does not match any specific criteria. Focuses on photovoltaic defect image generation for domain shift mitigation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.05901": {
        "authors": [
            "Hanzhe Liang",
            "Aoran Wang",
            "Jie Zhou",
            "Xin Jin",
            "Can Gao",
            "Jinbao Wang"
        ],
        "title": "Examining the Source of Defects from a Mechanical Perspective for 3D Anomaly Detection",
        "abstract": "arXiv:2505.05901v1 Announce Type: new  Abstract: In this paper, we go beyond identifying anomalies only in structural terms and think about better anomaly detection motivated by anomaly causes. Most anomalies are regarded as the result of unpredictable defective forces from internal and external sources, and their opposite forces are sought to correct the anomalies. We introduced a Mechanics Complementary framework for 3D anomaly detection (MC4AD) to generate internal and external Corrective forces for each point. A Diverse Anomaly-Generation (DA-Gen) module is first proposed to simulate various anomalies. Then, we present a Corrective Force Prediction Network (CFP-Net) with complementary representations for point-level representation to simulate the different contributions of internal and external corrective forces. A combined loss was proposed, including a new symmetric loss and an overall loss, to constrain the corrective forces properly. As a highlight, we consider 3D anomaly detection in industry more comprehensively, creating a hierarchical quality control strategy based on a three-way decision and contributing a dataset named Anomaly-IntraVariance with intraclass variance to evaluate the model. On the proposed and existing five datasets, we obtained nine state-of-the-art performers with the minimum parameters and the fastest inference speed. The source is available at https://github.com/hzzzzzhappy/MC4AD",
        "arxiv_id": "2505.05901",
        "ARXIVID": "2505.05901",
        "COMMENT": "Does not match any specific criteria. Focuses on 3D anomaly detection with a mechanical perspective.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.05573": {
        "authors": [
            "Mikhail Chaichuk",
            "Sushant Gautam",
            "Steven Hicks",
            "Elena Tutubalina"
        ],
        "title": "Prompt to Polyp: Clinically-Aware Medical Image Synthesis with Diffusion Models",
        "abstract": "arXiv:2505.05573v1 Announce Type: new  Abstract: The generation of realistic medical images from text descriptions has significant potential to address data scarcity challenges in healthcare AI while preserving patient privacy. This paper presents a comprehensive study of text-to-image synthesis in the medical domain, comparing two distinct approaches: (1) fine-tuning large pre-trained latent diffusion models and (2) training small, domain-specific models. We introduce a novel model named MSDM, an optimized architecture based on Stable Diffusion that integrates a clinical text encoder, variational autoencoder, and cross-attention mechanisms to better align medical text prompts with generated images. Our study compares two approaches: fine-tuning large pre-trained models (FLUX, Kandinsky) versus training compact domain-specific models (MSDM). Evaluation across colonoscopy (MedVQA-GI) and radiology (ROCOv2) datasets reveals that while large models achieve higher fidelity, our optimized MSDM delivers comparable quality with lower computational costs. Quantitative metrics and qualitative evaluations by medical experts reveal strengths and limitations of each approach.",
        "arxiv_id": "2505.05573",
        "ARXIVID": "2505.05573",
        "COMMENT": "Does not match any specific criteria. Focuses on medical image synthesis using diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.05589": {
        "authors": [
            "Jingzhong Lin",
            "Yuanyuan Qi",
            "Xinru Li",
            "Wenxuan Huang",
            "Xiangfeng Xu",
            "Bangyan Li",
            "Xuejiao Wang",
            "Gaoqi He"
        ],
        "title": "ReactDance: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation",
        "abstract": "arXiv:2505.05589v1 Announce Type: new  Abstract: Reactive dance generation (RDG) produces follower movements conditioned on guiding dancer and music while ensuring spatial coordination and temporal coherence. However, existing methods overemphasize global constraints and optimization, overlooking local information, such as fine-grained spatial interactions and localized temporal context. Therefore, we present ReactDance, a novel diffusion-based framework for high-fidelity RDG with long-term coherence and multi-scale controllability. Unlike existing methods that struggle with interaction fidelity, synchronization, and temporal consistency in duet synthesis, our approach introduces two key innovations: 1)Group Residual Finite Scalar Quantization (GRFSQ), a multi-scale disentangled motion representation that captures interaction semantics from coarse body rhythms to fine-grained joint dynamics, and 2)Blockwise Local Context (BLC), a sampling strategy eliminating error accumulation in long sequence generation via local block causal masking and periodic positional encoding. Built on the decoupled multi-scale GRFSQ representation, we implement a diffusion model withLayer-Decoupled Classifier-free Guidance (LDCFG), allowing granular control over motion semantics across scales. Extensive experiments on standard benchmarks demonstrate that ReactDance surpasses existing methods, achieving state-of-the-art performance.",
        "arxiv_id": "2505.05589",
        "ARXIVID": "2505.05589",
        "COMMENT": "Does not match any specific criteria. Focuses on reactive dance generation using a diffusion-based framework.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.05684": {
        "authors": [
            "Han Wu",
            "Jie Yin"
        ],
        "title": "Prompted Meta-Learning for Few-shot Knowledge Graph Completion",
        "abstract": "arXiv:2505.05684v1 Announce Type: new  Abstract: Few-shot knowledge graph completion (KGC) has obtained significant attention due to its practical applications in real-world scenarios, where new knowledge often emerges with limited available data. While most existing methods for few-shot KGC have predominantly focused on leveraging relational information, rich semantics inherent in KGs have been largely overlooked. To address this gap, we propose a novel prompted meta-learning (PromptMeta) framework that seamlessly integrates meta-semantics with relational information for few-shot KGC. PrompMeta has two key innovations: (1) a meta-semantic prompt pool that captures and consolidates high-level meta-semantics, enabling effective knowledge transfer and adaptation to rare and newly emerging relations. (2) a learnable fusion prompt that dynamically combines meta-semantic information with task-specific relational information tailored to different few-shot tasks. Both components are optimized together with model parameters within a meta-learning framework. Extensive experiments on two benchmark datasets demonstrate the effectiveness of our approach.",
        "arxiv_id": "2505.05684",
        "ARXIVID": "2505.05684",
        "COMMENT": "Does not match any specific criteria. Focuses on few-shot knowledge graph completion using meta-learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.06068": {
        "authors": [
            "Kunpeng Qiu",
            "Zhiqiang Gao",
            "Zhiying Zhou",
            "Mingjie Sun",
            "Yongxin Guo"
        ],
        "title": "Noise-Consistent Siamese-Diffusion for Medical Image Synthesis and Segmentation",
        "abstract": "arXiv:2505.06068v1 Announce Type: new  Abstract: Deep learning has revolutionized medical image segmentation, yet its full potential remains constrained by the paucity of annotated datasets. While diffusion models have emerged as a promising approach for generating synthetic image-mask pairs to augment these datasets, they paradoxically suffer from the same data scarcity challenges they aim to mitigate. Traditional mask-only models frequently yield low-fidelity images due to their inability to adequately capture morphological intricacies, which can critically compromise the robustness and reliability of segmentation models. To alleviate this limitation, we introduce Siamese-Diffusion, a novel dual-component model comprising Mask-Diffusion and Image-Diffusion. During training, a Noise Consistency Loss is introduced between these components to enhance the morphological fidelity of Mask-Diffusion in the parameter space. During sampling, only Mask-Diffusion is used, ensuring diversity and scalability. Comprehensive experiments demonstrate the superiority of our method. Siamese-Diffusion boosts SANet's mDice and mIoU by 3.6% and 4.4% on the Polyps, while UNet improves by 1.52% and 1.64% on the ISIC2018. Code is available at GitHub.",
        "arxiv_id": "2505.06068",
        "ARXIVID": "2505.06068",
        "COMMENT": "Does not match any specific criteria. Focuses on medical image synthesis and segmentation using diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.05711": {
        "authors": [
            "Ho-Joong Kim",
            "Yearang Lee",
            "Jung-Ho Hong",
            "Seong-Whan Lee"
        ],
        "title": "DiGIT: Multi-Dilated Gated Encoder and Central-Adjacent Region Integrated Decoder for Temporal Action Detection Transformer",
        "abstract": "arXiv:2505.05711v1 Announce Type: new  Abstract: In this paper, we examine a key limitation in query-based detectors for temporal action detection (TAD), which arises from their direct adaptation of originally designed architectures for object detection. Despite the effectiveness of the existing models, they struggle to fully address the unique challenges of TAD, such as the redundancy in multi-scale features and the limited ability to capture sufficient temporal context. To address these issues, we propose a multi-dilated gated encoder and central-adjacent region integrated decoder for temporal action detection transformer (DiGIT). Our approach replaces the existing encoder that consists of multi-scale deformable attention and feedforward network with our multi-dilated gated encoder. Our proposed encoder reduces the redundant information caused by multi-level features while maintaining the ability to capture fine-grained and long-range temporal information. Furthermore, we introduce a central-adjacent region integrated decoder that leverages a more comprehensive sampling strategy for deformable cross-attention to capture the essential information. Extensive experiments demonstrate that DiGIT achieves state-of-the-art performance on THUMOS14, ActivityNet v1.3, and HACS-Segment. Code is available at: https://github.com/Dotori-HJ/DiGIT",
        "arxiv_id": "2505.05711",
        "ARXIVID": "2505.05711",
        "COMMENT": "Does not match any specific criteria but is related to temporal action detection in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.05880": {
        "authors": [
            "Bettina Fazzinga",
            "Sergio Flesca",
            "Filippo Furfaro",
            "Luigi Pontieri",
            "Francesco Scala"
        ],
        "title": "Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams",
        "abstract": "arXiv:2505.05880v1 Announce Type: new  Abstract: Monitoring and analyzing process traces is a critical task for modern companies and organizations. In scenarios where there is a gap between trace events and reference business activities, this entails an interpretation problem, amounting to translating each event of any ongoing trace into the corresponding step of the activity instance. Building on a recent approach that frames the interpretation problem as an acceptance problem within an Abstract Argumentation Framework (AAF), one can elegantly analyze plausible event interpretations (possibly in an aggregated form), as well as offer explanations for those that conflict with prior process knowledge. Since, in settings where event-to-activity mapping is highly uncertain (or simply under-specified) this reasoning-based approach may yield lowly-informative results and heavy computation, one can think of discovering a sequencetagging model, trained to suggest highly-probable candidate event interpretations in a context-aware way. However, training such a model optimally may require using a large amount of manually-annotated example traces. Considering the urgent need of developing Green AI solutions enabling environmental and societal sustainability (with reduced labor/computational costs and carbon footprint), we propose a data/computation-efficient neuro-symbolic approach to the problem, where the candidate interpretations returned by the example-driven sequence tagger is refined by the AAF-based reasoner. This allows us to also leverage prior knowledge to compensate for the scarcity of example data, as confirmed by experimental results; clearly, this property is particularly useful in settings where data annotation and model optimization costs are subject to stringent constraints.",
        "arxiv_id": "2505.05880",
        "ARXIVID": "2505.05880",
        "COMMENT": "Does not match any specific criteria but is related to machine learning and neuro-symbolic approaches.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.05672": {
        "authors": [
            "Gengyan Li",
            "Paulo Gotardo",
            "Timo Bolkart",
            "Stephan Garbin",
            "Kripasindhu Sarkar",
            "Abhimitra Meka",
            "Alexandros Lattas",
            "Thabo Beeler"
        ],
        "title": "TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling",
        "abstract": "arXiv:2505.05672v1 Announce Type: new  Abstract: Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have recently enabled animatable 3D head avatars that are rendered under arbitrary viewpoints with impressive photorealism. Today, such photoreal avatars are seen as a key component in emerging applications in telepresence, extended reality, and entertainment. Building a photoreal avatar requires estimating the complex non-rigid motion of different facial components as seen in input video images; due to inaccurate motion estimation, animatable models typically present a loss of fidelity and detail when compared to their non-animatable counterparts, built from an individual facial expression. Also, recent state-of-the-art models are often affected by memory limitations that reduce the number of 3D Gaussians used for modeling, leading to lower detail and quality. To address these problems, we present a new high-detail 3D head avatar model that improves upon the state of the art, largely increasing the number of 3D Gaussians and modeling quality for rendering at 4K resolution. Our high-quality model is reconstructed from multiview input video and builds on top of a mesh-based 3D morphable model, which provides a coarse deformation layer for the head. Photoreal appearance is modelled by 3D Gaussians embedded within the continuous UVD tangent space of this mesh, allowing for more effective densification where most needed. Additionally, these Gaussians are warped by a novel UVD deformation field to capture subtle, localized motion. Our key contribution is the novel deformable Gaussian encoding and overall fitting procedure that allows our head model to preserve appearance detail, while capturing facial motion and other transient high-frequency features such as skin wrinkling.",
        "arxiv_id": "2505.05672",
        "ARXIVID": "2505.05672",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.05834": {
        "authors": [
            "Chunlai Dong",
            "Haochao Ying",
            "Qibo Qiu",
            "Jinhong Wang",
            "Danny Chen",
            "Jian Wu"
        ],
        "title": "Dual-level Fuzzy Learning with Patch Guidance for Image Ordinal Regression",
        "abstract": "arXiv:2505.05834v1 Announce Type: new  Abstract: Ordinal regression bridges regression and classification by assigning objects to ordered classes. While human experts rely on discriminative patch-level features for decisions, current approaches are limited by the availability of only image-level ordinal labels, overlooking fine-grained patch-level characteristics. In this paper, we propose a Dual-level Fuzzy Learning with Patch Guidance framework, named DFPG that learns precise feature-based grading boundaries from ambiguous ordinal labels, with patch-level supervision. Specifically, we propose patch-labeling and filtering strategies to enable the model to focus on patch-level features exclusively with only image-level ordinal labels available. We further design a dual-level fuzzy learning module, which leverages fuzzy logic to quantitatively capture and handle label ambiguity from both patch-wise and channel-wise perspectives. Extensive experiments on various image ordinal regression datasets demonstrate the superiority of our proposed method, further confirming its ability in distinguishing samples from difficult-to-classify categories. The code is available at https://github.com/ZJUMAI/DFPG-ord.",
        "arxiv_id": "2505.05834",
        "ARXIVID": "2505.05834",
        "COMMENT": "Does not match any specific criteria. Focuses on image ordinal regression, unrelated to spatial intelligence, VLLMs, MLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.05748": {
        "authors": [
            "Huan Yan",
            "Junjie Hu"
        ],
        "title": "kFuse: A novel density based agglomerative clustering",
        "abstract": "arXiv:2505.05748v1 Announce Type: new  Abstract: Agglomerative clustering has emerged as a vital tool in data analysis due to its intuitive and flexible characteristics. However, existing agglomerative clustering methods often involve additional parameters for sub-cluster partitioning and inter-cluster similarity assessment. This necessitates different parameter settings across various datasets, which is undoubtedly challenging in the absence of prior knowledge. Moreover, existing agglomerative clustering techniques are constrained by the calculation method of connection distance, leading to unstable clustering results. To address these issues, this paper introduces a novel density-based agglomerative clustering method, termed kFuse. kFuse comprises four key components: (1) sub-cluster partitioning based on natural neighbors; (2) determination of boundary connectivity between sub-clusters through the computation of adjacent samples and shortest distances; (3) assessment of density similarity between sub-clusters via the calculation of mean density and variance; and (4) establishment of merging rules between sub-clusters based on boundary connectivity and density similarity. kFuse requires the specification of the number of clusters only at the final merging stage. Additionally, by comprehensively considering adjacent samples, distances, and densities among different sub-clusters, kFuse significantly enhances accuracy during the merging phase, thereby greatly improving its identification capability. Experimental results on both synthetic and real-world datasets validate the effectiveness of kFuse.",
        "arxiv_id": "2505.05748",
        "ARXIVID": "2505.05748",
        "COMMENT": "Does not match any specific criteria. Focuses on a novel clustering method, unrelated to spatial intelligence, VLLMs, MLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.05587": {
        "authors": [
            "Peihao Wang",
            "Yuehao Wang",
            "Dilin Wang",
            "Sreyas Mohan",
            "Zhiwen Fan",
            "Lemeng Wu",
            "Ruisi Cai",
            "Yu-Ying Yeh",
            "Zhangyang Wang",
            "Qiang Liu",
            "Rakesh Ranjan"
        ],
        "title": "Steepest Descent Density Control for Compact 3D Gaussian Splatting",
        "abstract": "arXiv:2505.05587v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time, high-resolution novel view synthesis. By representing scenes as a mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for efficient rendering and reconstruction. To optimize scene coverage and capture fine details, 3DGS employs a densification algorithm to generate additional points. However, this process often leads to redundant point clouds, resulting in excessive memory usage, slower performance, and substantial storage demands - posing significant challenges for deployment on resource-constrained devices. To address this limitation, we propose a theoretical framework that demystifies and improves density control in 3DGS. Our analysis reveals that splitting is crucial for escaping saddle points. Through an optimization-theoretic approach, we establish the necessary conditions for densification, determine the minimal number of offspring Gaussians, identify the optimal parameter update direction, and provide an analytical solution for normalizing off-spring opacity. Building on these insights, we introduce SteepGS, incorporating steepest density control, a principled strategy that minimizes loss while maintaining a compact point cloud. SteepGS achieves a ~50% reduction in Gaussian points without compromising rendering quality, significantly enhancing both efficiency and scalability.",
        "arxiv_id": "2505.05587",
        "ARXIVID": "2505.05587",
        "COMMENT": "Does not match any specific criteria. Focuses on 3D Gaussian splatting for novel view synthesis, unrelated to spatial intelligence, VLLMs, MLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.05806": {
        "authors": [
            "Kaili Qi",
            "Wenli Yang",
            "Ye Li",
            "Zhongyi Huang"
        ],
        "title": "Image Segmentation via Variational Model Based Tailored UNet: A Deep Variational Framework",
        "abstract": "arXiv:2505.05806v1 Announce Type: new  Abstract: Traditional image segmentation methods, such as variational models based on partial differential equations (PDEs), offer strong mathematical interpretability and precise boundary modeling, but often suffer from sensitivity to parameter settings and high computational costs. In contrast, deep learning models such as UNet, which are relatively lightweight in parameters, excel in automatic feature extraction but lack theoretical interpretability and require extensive labeled data. To harness the complementary strengths of both paradigms, we propose Variational Model Based Tailored UNet (VM_TUNet), a novel hybrid framework that integrates the fourth-order modified Cahn-Hilliard equation with the deep learning backbone of UNet, which combines the interpretability and edge-preserving properties of variational methods with the adaptive feature learning of neural networks. Specifically, a data-driven operator is introduced to replace manual parameter tuning, and we incorporate the tailored finite point method (TFPM) to enforce high-precision boundary preservation. Experimental results on benchmark datasets demonstrate that VM_TUNet achieves superior segmentation performance compared to existing approaches, especially for fine boundary delineation.",
        "arxiv_id": "2505.05806",
        "ARXIVID": "2505.05806",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and segmentation methods.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.05488": {
        "authors": [
            "Yunfan Lu",
            "Xiaogang Xu",
            "Pengteng Li",
            "Yusheng Wang",
            "Yi Cui",
            "Huizai Yao",
            "Hui Xiong"
        ],
        "title": "From Events to Enhancement: A Survey on Event-Based Imaging Technologies",
        "abstract": "arXiv:2505.05488v1 Announce Type: new  Abstract: Event cameras offering high dynamic range and low latency have emerged as disruptive technologies in imaging. Despite growing research on leveraging these benefits for different imaging tasks, a comprehensive study of recently advances and challenges are still lacking. This limits the broader understanding of how to utilize events in universal imaging applications. In this survey, we first introduce a physical model and the characteristics of different event sensors as the foundation. Following this, we highlight the advancement and interaction of image/video enhancement tasks with events. Additionally, we explore advanced tasks, which capture richer light information with events, \\eg~light field estimation, multi-view generation, and photometric. Finally, we discuss new challenges and open questions offering a perspective for this rapidly evolving field. More continuously updated resources are at this link: https://github.com/yunfanLu/Awesome-Event-Imaging",
        "arxiv_id": "2505.05488",
        "ARXIVID": "2505.05488",
        "COMMENT": "Does not match any specific criteria but is a survey on event-based imaging technologies, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2505.06133": {
        "authors": [
            "Hongming Wang",
            "Yifeng Wu",
            "Huimin Huang",
            "Hongtao Wu",
            "Jia-Xuan Jiang",
            "Xiaodong Zhang",
            "Hao Zheng",
            "Xian Wu",
            "Yefeng Zheng",
            "Jinping Xu",
            "Jing Cheng"
        ],
        "title": "BrainSegDMlF: A Dynamic Fusion-enhanced SAM for Brain Lesion Segmentation",
        "abstract": "arXiv:2505.06133v1 Announce Type: new  Abstract: The segmentation of substantial brain lesions is a significant and challenging task in the field of medical image segmentation. Substantial brain lesions in brain imaging exhibit high heterogeneity, with indistinct boundaries between lesion regions and normal brain tissue. Small lesions in single slices are difficult to identify, making the accurate and reproducible segmentation of abnormal regions, as well as their feature description, highly complex. Existing methods have the following limitations: 1) They rely solely on single-modal information for learning, neglecting the multi-modal information commonly used in diagnosis. This hampers the ability to comprehensively acquire brain lesion information from multiple perspectives and prevents the effective integration and utilization of multi-modal data inputs, thereby limiting a holistic understanding of lesions. 2) They are constrained by the amount of data available, leading to low sensitivity to small lesions and difficulty in detecting subtle pathological changes. 3) Current SAM-based models rely on external prompts, which cannot achieve automatic segmentation and, to some extent, affect diagnostic efficiency.To address these issues, we have developed a large-scale fully automated segmentation model specifically designed for brain lesion segmentation, named BrainSegDMLF. This model has the following features: 1) Dynamic Modal Interactive Fusion (DMIF) module that processes and integrates multi-modal data during the encoding process, providing the SAM encoder with more comprehensive modal information. 2) Layer-by-Layer Upsampling Decoder, enabling the model to extract rich low-level and high-level features even with limited data, thereby detecting the presence of small lesions. 3) Automatic segmentation masks, allowing the model to generate lesion masks automatically without requiring manual prompts.",
        "arxiv_id": "2505.06133",
        "ARXIVID": "2505.06133",
        "COMMENT": "Does not match any specific criteria. Focuses on medical image segmentation, not spatial intelligence, VLLMs, MLLMs, or embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}