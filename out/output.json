{
    "2512.07062": {
        "authors": [
            "Changliang Xia",
            "Chengyou Jia",
            "Minnan Luo",
            "Zhuohang Dang",
            "Xin Shen",
            "Bowen Ping"
        ],
        "title": "$\\mathrm{D}^{\\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction",
        "abstract": "arXiv:2512.07062v1 Announce Type: new  Abstract: Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\\mathrm{D}^{\\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\\mathrm{D}^{\\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\\mathrm{D}^{\\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.",
        "arxiv_id": "2512.07062",
        "ARXIVID": "2512.07062",
        "COMMENT": "The paper introduces D3-Predictor, a noise-free deterministic framework for dense prediction using a pretrained diffusion model. It aligns with the criterion of unified diffusion models for multiple vision tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2512.07568": {
        "authors": [
            "Xuecheng Li",
            "Weikuan Jia",
            "Alisher Kurbonaliev",
            "Qurbonaliev Alisher",
            "Khudzhamkulov Rustam",
            "Ismoilov Shuhratjon",
            "Eshmatov Javhariddin",
            "Yuanjie Zheng"
        ],
        "title": "Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation",
        "abstract": "arXiv:2512.07568v1 Announce Type: new  Abstract: Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while na\\\"ive fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.",
        "arxiv_id": "2512.07568",
        "ARXIVID": "2512.07568",
        "COMMENT": "The paper proposes DSRSD-Net for cross-modal representation learning, focusing on disentangling modality-specific and shared information. It does not address joint generation and segmentation or unified diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.06251": {
        "authors": [
            "Fangzhou Lin",
            "Yuping Wang",
            "Yuliang Guo",
            "Zixun Huang",
            "Xinyu Huang",
            "Haichong Zhang",
            "Kazunori Yamada",
            "Zhengzhong Tu",
            "Liu Ren",
            "Ziming Zhang"
        ],
        "title": "NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks",
        "abstract": "arXiv:2512.06251v1 Announce Type: new  Abstract: Partially Supervised Multi-Task Learning (PS-MTL) aims to leverage knowledge across tasks when annotations are incomplete. Existing approaches, however, have largely focused on the simpler setting of homogeneous, dense prediction tasks, leaving the more realistic challenge of learning from structurally diverse tasks unexplored. To this end, we introduce NexusFlow, a novel, lightweight, and plug-and-play framework effective in both settings. NexusFlow introduces a set of surrogate networks with invertible coupling layers to align the latent feature distributions of tasks, creating a unified representation that enables effective knowledge transfer. The coupling layers are bijective, preserving information while mapping features into a shared canonical space. This invertibility avoids representational collapse and enables alignment across structurally different tasks without reducing expressive capacity. We first evaluate NexusFlow on the core challenge of domain-partitioned autonomous driving, where dense map reconstruction and sparse multi-object tracking are supervised in different geographic regions, creating both structural disparity and a strong domain gap. NexusFlow sets a new state-of-the-art result on nuScenes, outperforming strong partially supervised baselines. To demonstrate generality, we further test NexusFlow on NYUv2 using three homogeneous dense prediction tasks, segmentation, depth, and surface normals, as a representative N-task PS-MTL scenario. NexusFlow yields consistent gains across all tasks, confirming its broad applicability.",
        "arxiv_id": "2512.06251",
        "ARXIVID": "2512.06251",
        "COMMENT": "The paper introduces a novel framework, NexusFlow, for partially supervised multi-task learning, which aligns latent feature distributions across tasks using invertible flow networks. However, it does not specifically address joint image/video generation and segmentation or unified diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2512.06802": {
        "authors": [
            "Yutong Wang",
            "Haiyu Zhang",
            "Tianfan Xue",
            "Yu Qiao",
            "Yaohui Wang",
            "Chang Xu",
            "Xinyuan Chen"
        ],
        "title": "VDOT: Efficient Unified Video Creation via Optimal Transport Distillation",
        "abstract": "arXiv:2512.06802v1 Announce Type: new  Abstract: The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.",
        "arxiv_id": "2512.06802",
        "ARXIVID": "2512.06802",
        "COMMENT": "The paper proposes VDOT, a unified video creation model using optimal transport distillation. It focuses on video generation efficiency and quality but does not address joint generation and segmentation or multi-task diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}