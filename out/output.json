{
    "2503.20853": {
        "authors": [
            "Alexander Swerdlow",
            "Mihir Prabhudesai",
            "Siddharth Gandhi",
            "Deepak Pathak",
            "Katerina Fragkiadaki"
        ],
        "title": "Unified Multimodal Discrete Diffusion",
        "abstract": "arXiv:2503.20853v1 Announce Type: new  Abstract: Multimodal generative models that can understand and generate across multiple modalities are dominated by autoregressive (AR) approaches, which process tokens sequentially from left to right, or top to bottom. These models jointly handle images, text, video, and audio for various tasks such as image captioning, question answering, and image generation. In this work, we explore discrete diffusion models as a unified generative formulation in the joint text and image domain, building upon their recent success in text generation. Discrete diffusion models offer several advantages over AR models, including improved control over quality versus diversity of generated samples, the ability to perform joint multimodal inpainting (across both text and image domains), and greater controllability in generation through guidance. Leveraging these benefits, we present the first Unified Multimodal Discrete Diffusion (UniDisc) model which is capable of jointly understanding and generating text and images for a variety of downstream tasks. We compare UniDisc to multimodal AR models, performing a scaling analysis and demonstrating that UniDisc outperforms them in terms of both performance and inference-time compute, enhanced controllability, editability, inpainting, and flexible trade-off between inference time and generation quality. Code and additional visualizations are available at https://unidisc.github.io.",
        "arxiv_id": "2503.20853",
        "ARXIVID": "2503.20853",
        "COMMENT": "Matches criterion 2 as it introduces a unified multimodal discrete diffusion model for text and image generation, which is a novel VLLM/MLLM approach.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.21268": {
        "authors": [
            "Ming Yan",
            "Xincheng Lin",
            "Yuhua Luo",
            "Shuqi Fan",
            "Yudi Dai",
            "Qixin Zhong",
            "Lincai Zhong",
            "Yuexin Ma",
            "Lan Xu",
            "Chenglu Wen",
            "Siqi Shen",
            "Cheng Wang"
        ],
        "title": "ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World Coordinate",
        "abstract": "arXiv:2503.21268v1 Announce Type: new  Abstract: Human Motion Recovery (HMR) research mainly focuses on ground-based motions such as running. The study on capturing climbing motion, an off-ground motion, is sparse. This is partly due to the limited availability of climbing motion datasets, especially large-scale and challenging 3D labeled datasets. To address the insufficiency of climbing motion datasets, we collect AscendMotion, a large-scale well-annotated, and challenging climbing motion dataset. It consists of 412k RGB, LiDAR frames, and IMU measurements, including the challenging climbing motions of 22 skilled climbing coaches across 12 different rock walls. Capturing the climbing motions is challenging as it requires precise recovery of not only the complex pose but also the global position of climbers. Although multiple global HMR methods have been proposed, they cannot faithfully capture climbing motions. To address the limitations of HMR methods for climbing, we propose ClimbingCap, a motion recovery method that reconstructs continuous 3D human climbing motion in a global coordinate system. One key insight is to use the RGB and LiDAR modalities to separately reconstruct motions in camera coordinates and global coordinates and to optimize them jointly. We demonstrate the quality of the AscendMotion dataset and present promising results from ClimbingCap. The AscendMotion dataset and source code release publicly at \\href{this link}{http://www.lidarhumanmotion.net/climbingcap/}",
        "arxiv_id": "2503.21268",
        "ARXIVID": "2503.21268",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset (AscendMotion) and a novel method (ClimbingCap) for embodied AI focusing on climbing motion, which is a novel angle.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.21055": {
        "authors": [
            "Chi-Hsi Kung",
            "Frangil Ramirez",
            "Juhyung Ha",
            "Yi-Ting Chen",
            "David Crandall",
            "Yi-Hsuan Tsai"
        ],
        "title": "What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning",
        "abstract": "arXiv:2503.21055v1 Announce Type: new  Abstract: Understanding a procedural activity requires modeling both how action steps transform the scene, and how evolving scene transformations can influence the sequence of action steps, even those that are accidental or erroneous. Existing work has studied procedure-aware video representations by proposing novel approaches such as modeling the temporal order of actions and has not explicitly learned the state changes (scene transformations). In this work, we study procedure-aware video representation learning by incorporating state-change descriptions generated by Large Language Models (LLMs) as supervision signals for video encoders. Moreover, we generate state-change counterfactuals that simulate hypothesized failure outcomes, allowing models to learn by imagining the unseen ``What if'' scenarios. This counterfactual reasoning facilitates the model's ability to understand the cause and effect of each step in an activity. To verify the procedure awareness of our model, we conduct extensive experiments on procedure-aware tasks, including temporal action segmentation and error detection. Our results demonstrate the effectiveness of the proposed state-change descriptions and their counterfactuals and achieve significant improvements on multiple tasks. We will make our source code and data publicly available soon.",
        "arxiv_id": "2503.21055",
        "ARXIVID": "2503.21055",
        "COMMENT": "Matches criterion 3 as it focuses on embodied AI with a novel approach to procedure-aware video representation learning using state-change counterfactuals.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2503.21757": {
        "authors": [
            "Adrian Bulat",
            "Yassine Ouali",
            "Georgios Tzimiropoulos"
        ],
        "title": "Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck",
        "abstract": "arXiv:2503.21757v1 Announce Type: new  Abstract: In this work, we aim to compress the vision tokens of a Large Vision Language Model (LVLM) into a representation that is simultaneously suitable for (a) generative and (b) discriminative tasks, (c) is nearly lossless, and (d) is storage-efficient. We propose a novel compression approach, called Fwd2Bot, that uses the LVLM itself to compress the visual information in a task-agnostic manner. At the core of Fwd2bot there exists a \"double-forward pass\" training strategy, whereby, during the first forward pass, the LLM (of the LVLM) creates a bottleneck by condensing the visual information into a small number of summary tokens. Then, using the same LLM, the second forward pass processes the language instruction(s) alongside the summary tokens, used as a direct replacement for the image ones. The training signal is provided by two losses: an autoregressive one applied after the second pass that provides a direct optimization objective for compression, and a contrastive loss, applied after the first pass, that further boosts the representation strength, especially for discriminative tasks. The training is further enhanced by stage-specific adapters. We accompany the proposed method by an in-depth ablation study. Overall, Fwd2Bot results in highly-informative compressed representations suitable for both generative and discriminative tasks. For generative tasks, we offer a 2x higher compression rate without compromising the generative capabilities, setting a new state-of-the-art result. For discriminative tasks, we set a new state-of-the-art on image retrieval and compositionality.",
        "arxiv_id": "2503.21757",
        "ARXIVID": "2503.21757",
        "COMMENT": "Matches criterion 2 as it proposes a novel compression approach for vision tokens in Large Vision Language Models (LVLMs).",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2503.21780": {
        "authors": [
            "Reza Qorbani",
            "Gianluca Villani",
            "Theodoros Panagiotakopoulos",
            "Marc Botet Colomer",
            "Linus H\\\"arenstam-Nielsen",
            "Mattia Segu",
            "Pier Luigi Dovesi",
            "Jussi Karlgren",
            "Daniel Cremers",
            "Federico Tombari",
            "Matteo Poggi"
        ],
        "title": "Semantic Library Adaptation: LoRA Retrieval and Fusion for Open-Vocabulary Semantic Segmentation",
        "abstract": "arXiv:2503.21780v1 Announce Type: new  Abstract: Open-vocabulary semantic segmentation models associate vision and text to label pixels from an undefined set of classes using textual queries, providing versatile performance on novel datasets. However, large shifts between training and test domains degrade their performance, requiring fine-tuning for effective real-world applications. We introduce Semantic Library Adaptation (SemLA), a novel framework for training-free, test-time domain adaptation. SemLA leverages a library of LoRA-based adapters indexed with CLIP embeddings, dynamically merging the most relevant adapters based on proximity to the target domain in the embedding space. This approach constructs an ad-hoc model tailored to each specific input without additional training. Our method scales efficiently, enhances explainability by tracking adapter contributions, and inherently protects data privacy, making it ideal for sensitive applications. Comprehensive experiments on a 20-domain benchmark built over 10 standard datasets demonstrate SemLA's superior adaptability and performance across diverse settings, establishing a new standard in domain adaptation for open-vocabulary semantic segmentation.",
        "arxiv_id": "2503.21780",
        "ARXIVID": "2503.21780",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models and their application to open-vocabulary semantic segmentation with a novel domain adaptation framework.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.21104": {
        "authors": [
            "Yuyin Chen",
            "Yida Wang",
            "Xueyang Zhang",
            "Kun Zhan",
            "Peng Jia",
            "Yifei Zhan",
            "Xianpeng Lang"
        ],
        "title": "StyledStreets: Multi-style Street Simulator with Spatial and Temporal Consistency",
        "abstract": "arXiv:2503.21104v1 Announce Type: new  Abstract: Urban scene reconstruction requires modeling both static infrastructure and dynamic elements while supporting diverse environmental conditions. We present \\textbf{StyledStreets}, a multi-style street simulator that achieves instruction-driven scene editing with guaranteed spatial and temporal consistency. Building on a state-of-the-art Gaussian Splatting framework for street scenarios enhanced by our proposed pose optimization and multi-view training, our method enables photorealistic style transfers across seasons, weather conditions, and camera setups through three key innovations: First, a hybrid embedding scheme disentangles persistent scene geometry from transient style attributes, allowing realistic environmental edits while preserving structural integrity. Second, uncertainty-aware rendering mitigates supervision noise from diffusion priors, enabling robust training across extreme style variations. Third, a unified parametric model prevents geometric drift through regularized updates, maintaining multi-view consistency across seven vehicle-mounted cameras.   Our framework preserves the original scene's motion patterns and geometric relationships. Qualitative results demonstrate plausible transitions between diverse conditions (snow, sandstorm, night), while quantitative evaluations show state-of-the-art geometric accuracy under style transfers. The approach establishes new capabilities for urban simulation, with applications in autonomous vehicle testing and augmented reality systems requiring reliable environmental consistency. Codes will be publicly available upon publication.",
        "arxiv_id": "2503.21104",
        "ARXIVID": "2503.21104",
        "COMMENT": "Matches criterion 3 as it presents a new urban simulation benchmark with novel methods for spatial and temporal consistency in multi-style street scenarios.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.21099": {
        "authors": [
            "Yun Zhu",
            "Le Hui",
            "Hang Yang",
            "Jianjun Qian",
            "Jin Xie",
            "Jian Yang"
        ],
        "title": "Learning Class Prototypes for Unified Sparse Supervised 3D Object Detection",
        "abstract": "arXiv:2503.21099v1 Announce Type: new  Abstract: Both indoor and outdoor scene perceptions are essential for embodied intelligence. However, current sparse supervised 3D object detection methods focus solely on outdoor scenes without considering indoor settings. To this end, we propose a unified sparse supervised 3D object detection method for both indoor and outdoor scenes through learning class prototypes to effectively utilize unlabeled objects. Specifically, we first propose a prototype-based object mining module that converts the unlabeled object mining into a matching problem between class prototypes and unlabeled features. By using optimal transport matching results, we assign prototype labels to high-confidence features, thereby achieving the mining of unlabeled objects. We then present a multi-label cooperative refinement module to effectively recover missed detections through pseudo label quality control and prototype label cooperation. Experiments show that our method achieves state-of-the-art performance under the one object per scene sparse supervised setting across indoor and outdoor datasets. With only one labeled object per scene, our method achieves about 78%, 90%, and 96% performance compared to the fully supervised detector on ScanNet V2, SUN RGB-D, and KITTI, respectively, highlighting the scalability of our method. Code is available at https://github.com/zyrant/CPDet3D.",
        "arxiv_id": "2503.21099",
        "ARXIVID": "2503.21099",
        "COMMENT": "Matches criterion 3 as it proposes a unified sparse supervised 3D object detection method for both indoor and outdoor scenes, addressing overlooked aspects like unlabeled object mining.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.21778": {
        "authors": [
            "Ziren Gong",
            "Fabio Tosi",
            "Youmin Zhang",
            "Stefano Mattoccia",
            "Matteo Poggi"
        ],
        "title": "HS-SLAM: Hybrid Representation with Structural Supervision for Improved Dense SLAM",
        "abstract": "arXiv:2503.21778v1 Announce Type: new  Abstract: NeRF-based SLAM has recently achieved promising results in tracking and reconstruction. However, existing methods face challenges in providing sufficient scene representation, capturing structural information, and maintaining global consistency in scenes emerging significant movement or being forgotten. To this end, we present HS-SLAM to tackle these problems. To enhance scene representation capacity, we propose a hybrid encoding network that combines the complementary strengths of hash-grid, tri-planes, and one-blob, improving the completeness and smoothness of reconstruction. Additionally, we introduce structural supervision by sampling patches of non-local pixels rather than individual rays to better capture the scene structure. To ensure global consistency, we implement an active global bundle adjustment (BA) to eliminate camera drifts and mitigate accumulative errors. Experimental results demonstrate that HS-SLAM outperforms the baselines in tracking and reconstruction accuracy while maintaining the efficiency required for robotics.",
        "arxiv_id": "2503.21778",
        "ARXIVID": "2503.21778",
        "COMMENT": "Matches criterion 3 as it introduces a new SLAM method with hybrid representation and structural supervision, focusing on novel aspects like global consistency.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.21770": {
        "authors": [
            "Anand Bhattad",
            "Konpat Preechakul",
            "Alexei A. Efros"
        ],
        "title": "Visual Jenga: Discovering Object Dependencies via Counterfactual Inpainting",
        "abstract": "arXiv:2503.21770v1 Announce Type: new  Abstract: This paper proposes a novel scene understanding task called Visual Jenga. Drawing inspiration from the game Jenga, the proposed task involves progressively removing objects from a single image until only the background remains. Just as Jenga players must understand structural dependencies to maintain tower stability, our task reveals the intrinsic relationships between scene elements by systematically exploring which objects can be removed while preserving scene coherence in both physical and geometric sense. As a starting point for tackling the Visual Jenga task, we propose a simple, data-driven, training-free approach that is surprisingly effective on a range of real-world images. The principle behind our approach is to utilize the asymmetry in the pairwise relationships between objects within a scene and employ a large inpainting model to generate a set of counterfactuals to quantify the asymmetry.",
        "arxiv_id": "2503.21770",
        "ARXIVID": "2503.21770",
        "COMMENT": "Matches criterion 1 as it introduces a novel task (Visual Jenga) for scene understanding, which involves spatial intelligence and dependencies.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.20871": {
        "authors": [
            "Silin Gao",
            "Sheryl Mathew",
            "Li Mi",
            "Sepideh Mamooler",
            "Mengjie Zhao",
            "Hiromi Wakaki",
            "Yuki Mitsufuji",
            "Syrielle Montariol",
            "Antoine Bosselut"
        ],
        "title": "VinaBench: Benchmark for Faithful and Consistent Visual Narratives",
        "abstract": "arXiv:2503.20871v1 Announce Type: new  Abstract: Visual narrative generation transforms textual narratives into sequences of images illustrating the content of the text. However, generating visual narratives that are faithful to the input text and self-consistent across generated images remains an open challenge, due to the lack of knowledge constraints used for planning the stories. In this work, we propose a new benchmark, VinaBench, to address this challenge. Our benchmark annotates the underlying commonsense and discourse constraints in visual narrative samples, offering systematic scaffolds for learning the implicit strategies of visual storytelling. Based on the incorporated narrative constraints, we further propose novel metrics to closely evaluate the consistency of generated narrative images and the alignment of generations with the input textual narrative. Our results across three generative vision models demonstrate that learning with VinaBench's knowledge constraints effectively improves the faithfulness and cohesion of generated visual narratives.",
        "arxiv_id": "2503.20871",
        "ARXIVID": "2503.20871",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (VinaBench) for visual narrative generation, focusing on novel evaluation metrics for consistency and faithfulness.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.21775": {
        "authors": [
            "Ziyu Guo",
            "Young Yoon Lee",
            "Joseph Liu",
            "Yizhak Ben-Shabat",
            "Victor Zordan",
            "Mubbasir Kapadia"
        ],
        "title": "StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion",
        "abstract": "arXiv:2503.21775v1 Announce Type: new  Abstract: We present StyleMotif, a novel Stylized Motion Latent Diffusion model, generating motion conditioned on both content and style from multiple modalities. Unlike existing approaches that either focus on generating diverse motion content or transferring style from sequences, StyleMotif seamlessly synthesizes motion across a wide range of content while incorporating stylistic cues from multi-modal inputs, including motion, text, image, video, and audio. To achieve this, we introduce a style-content cross fusion mechanism and align a style encoder with a pre-trained multi-modal model, ensuring that the generated motion accurately captures the reference style while preserving realism. Extensive experiments demonstrate that our framework surpasses existing methods in stylized motion generation and exhibits emergent capabilities for multi-modal motion stylization, enabling more nuanced motion synthesis. Source code and pre-trained models will be released upon acceptance. Project Page: https://stylemotif.github.io",
        "arxiv_id": "2503.21775",
        "ARXIVID": "2503.21775",
        "COMMENT": "Matches criterion 2 as it introduces a multi-modal motion stylization model using diffusion, which is relevant to generative modeling in multi-modal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.21747": {
        "authors": [
            "Aniket Didolkar",
            "Andrii Zadaianchuk",
            "Rabiul Awal",
            "Maximilian Seitzer",
            "Efstratios Gavves",
            "Aishwarya Agrawal"
        ],
        "title": "CTRL-O: Language-Controllable Object-Centric Visual Representation Learning",
        "abstract": "arXiv:2503.21747v1 Announce Type: new  Abstract: Object-centric representation learning aims to decompose visual scenes into fixed-size vectors called \"slots\" or \"object files\", where each slot captures a distinct object. Current state-of-the-art object-centric models have shown remarkable success in object discovery in diverse domains, including complex real-world scenes. However, these models suffer from a key limitation: they lack controllability. Specifically, current object-centric models learn representations based on their preconceived understanding of objects, without allowing user input to guide which objects are represented. Introducing controllability into object-centric models could unlock a range of useful capabilities, such as the ability to extract instance-specific representations from a scene. In this work, we propose a novel approach for user-directed control over slot representations by conditioning slots on language descriptions. The proposed ConTRoLlable Object-centric representation learning approach, which we term CTRL-O, achieves targeted object-language binding in complex real-world scenes without requiring mask supervision. Next, we apply these controllable slot representations on two downstream vision language tasks: text-to-image generation and visual question answering. The proposed approach enables instance-specific text-to-image generation and also achieves strong performance on visual question answering.",
        "arxiv_id": "2503.21747",
        "ARXIVID": "2503.21747",
        "COMMENT": "Matches criterion 2 as it introduces a novel approach for language-controllable object-centric visual representation learning and applies it to vision-language tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.21277": {
        "authors": [
            "Hiroya Makino",
            "Takahiro Yamaguchi",
            "Hiroyuki Sakai"
        ],
        "title": "Zero-Shot Visual Concept Blending Without Text Guidance",
        "abstract": "arXiv:2503.21277v1 Announce Type: new  Abstract: We propose a novel, zero-shot image generation technique called \"Visual Concept Blending\" that provides fine-grained control over which features from multiple reference images are transferred to a source image. If only a single reference image is available, it is difficult to isolate which specific elements should be transferred. However, using multiple reference images, the proposed approach distinguishes between common and unique features by selectively incorporating them into a generated output. By operating within a partially disentangled Contrastive Language-Image Pre-training (CLIP) embedding space (from IP-Adapter), our method enables the flexible transfer of texture, shape, motion, style, and more abstract conceptual transformations without requiring additional training or text prompts. We demonstrate its effectiveness across a diverse range of tasks, including style transfer, form metamorphosis, and conceptual transformations, showing how subtle or abstract attributes (e.g., brushstroke style, aerodynamic lines, and dynamism) can be seamlessly combined into a new image. In a user study, participants accurately recognized which features were intended to be transferred. Its simplicity, flexibility, and high-level control make Visual Concept Blending valuable for creative fields such as art, design, and content creation, where combining specific visual qualities from multiple inspirations is crucial.",
        "arxiv_id": "2503.21277",
        "ARXIVID": "2503.21277",
        "COMMENT": "Matches criterion 4 as it introduces a novel zero-shot image generation technique leveraging vision foundation models for visual concept blending.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.21210": {
        "authors": [
            "Yueying Gao",
            "Dongliang Chang",
            "Bingyao Yu",
            "Haotian Qin",
            "Lei Chen",
            "Kongming Liang",
            "Zhanyu Ma"
        ],
        "title": "FakeReasoning: Towards Generalizable Forgery Detection and Reasoning",
        "abstract": "arXiv:2503.21210v1 Announce Type: new  Abstract: Accurate and interpretable detection of AI-generated images is essential for mitigating risks associated with AI misuse. However, the substantial domain gap among generative models makes it challenging to develop a generalizable forgery detection model. Moreover, since every pixel in an AI-generated image is synthesized, traditional saliency-based forgery explanation methods are not well suited for this task. To address these challenges, we propose modeling AI-generated image detection and explanation as a Forgery Detection and Reasoning task (FDR-Task), leveraging vision-language models (VLMs) to provide accurate detection through structured and reliable reasoning over forgery attributes. To facilitate this task, we introduce the Multi-Modal Forgery Reasoning dataset (MMFR-Dataset), a large-scale dataset containing 100K images across 10 generative models, with 10 types of forgery reasoning annotations, enabling comprehensive evaluation of FDR-Task. Additionally, we propose FakeReasoning, a forgery detection and reasoning framework with two key components. First, Forgery-Aligned Contrastive Learning enhances VLMs' understanding of forgery-related semantics through both cross-modal and intra-modal contrastive learning between images and forgery attribute reasoning. Second, a Classification Probability Mapper bridges the optimization gap between forgery detection and language modeling by mapping the output logits of VLMs to calibrated binary classification probabilities. Experiments across multiple generative models demonstrate that FakeReasoning not only achieves robust generalization but also outperforms state-of-the-art methods on both detection and reasoning tasks.",
        "arxiv_id": "2503.21210",
        "ARXIVID": "2503.21210",
        "COMMENT": "Matches criterion 2 as it leverages vision-language models for forgery detection and reasoning, focusing on generalization and interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.21745": {
        "authors": [
            "Yuhan Zhang",
            "Mengchen Zhang",
            "Tong Wu",
            "Tengfei Wang",
            "Gordon Wetzstein",
            "Dahua Lin",
            "Ziwei Liu"
        ],
        "title": "3DGen-Bench: Comprehensive Benchmark Suite for 3D Generative Models",
        "abstract": "arXiv:2503.21745v1 Announce Type: new  Abstract: 3D generation is experiencing rapid advancements, while the development of 3D evaluation has not kept pace. How to keep automatic evaluation equitably aligned with human perception has become a well-recognized challenge. Recent advances in the field of language and image generation have explored human preferences and showcased respectable fitting ability. However, the 3D domain still lacks such a comprehensive preference dataset over generative models. To mitigate this absence, we develop 3DGen-Arena, an integrated platform in a battle manner. Then, we carefully design diverse text and image prompts and leverage the arena platform to gather human preferences from both public users and expert annotators, resulting in a large-scale multi-dimension human preference dataset 3DGen-Bench. Using this dataset, we further train a CLIP-based scoring model, 3DGen-Score, and a MLLM-based automatic evaluator, 3DGen-Eval. These two models innovatively unify the quality evaluation of text-to-3D and image-to-3D generation, and jointly form our automated evaluation system with their respective strengths. Extensive experiments demonstrate the efficacy of our scoring model in predicting human preferences, exhibiting a superior correlation with human ranks compared to existing metrics. We believe that our 3DGen-Bench dataset and automated evaluation system will foster a more equitable evaluation in the field of 3D generation, further promoting the development of 3D generative models and their downstream applications.",
        "arxiv_id": "2503.21745",
        "ARXIVID": "2503.21745",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (3DGen-Bench) for 3D generative models, focusing on evaluation and human preferences, which is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.21581": {
        "authors": [
            "Liuyue Xie",
            "Jiancong Guo",
            "Ozan Cakmakci",
            "Andre Araujo",
            "Laszlo A. Jeni",
            "Zhiheng Jia"
        ],
        "title": "AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion",
        "abstract": "arXiv:2503.21581v1 Announce Type: new  Abstract: Accurate camera calibration is a fundamental task for 3D perception, especially when dealing with real-world, in-the-wild environments where complex optical distortions are common. Existing methods often rely on pre-rectified images or calibration patterns, which limits their applicability and flexibility. In this work, we introduce a novel framework that addresses these challenges by jointly modeling camera intrinsic and extrinsic parameters using a generic ray camera model. Unlike previous approaches, AlignDiff shifts focus from semantic to geometric features, enabling more accurate modeling of local distortions. We propose AlignDiff, a diffusion model conditioned on geometric priors, enabling the simultaneous estimation of camera distortions and scene geometry. To enhance distortion prediction, we incorporate edge-aware attention, focusing the model on geometric features around image edges, rather than semantic content. Furthermore, to enhance generalizability to real-world captures, we incorporate a large database of ray-traced lenses containing over three thousand samples. This database characterizes the distortion inherent in a diverse variety of lens forms. Our experiments demonstrate that the proposed method significantly reduces the angular error of estimated ray bundles by ~8.2 degrees and overall calibration accuracy, outperforming existing approaches on challenging, real-world datasets.",
        "arxiv_id": "2503.21581",
        "ARXIVID": "2503.21581",
        "COMMENT": "Matches criterion 1 as it introduces a novel framework for camera alignment using diffusion models, focusing on spatial understanding and geometric features.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.21190": {
        "authors": [
            "Erika Mori",
            "Yue Qiu",
            "Hirokatsu Kataoka",
            "Yoshimitsu Aoki"
        ],
        "title": "Leveraging LLMs with Iterative Loop Structure for Enhanced Social Intelligence in Video Question Answering",
        "abstract": "arXiv:2503.21190v1 Announce Type: new  Abstract: Social intelligence, the ability to interpret emotions, intentions, and behaviors, is essential for effective communication and adaptive responses. As robots and AI systems become more prevalent in caregiving, healthcare, and education, the demand for AI that can interact naturally with humans grows. However, creating AI that seamlessly integrates multiple modalities, such as vision and speech, remains a challenge. Current video-based methods for social intelligence rely on general video recognition or emotion recognition techniques, often overlook the unique elements inherent in human interactions. To address this, we propose the Looped Video Debating (LVD) framework, which integrates Large Language Models (LLMs) with visual information, such as facial expressions and body movements, to enhance the transparency and reliability of question-answering tasks involving human interaction videos. Our results on the Social-IQ 2.0 benchmark show that LVD achieves state-of-the-art performance without fine-tuning. Furthermore, supplementary human annotations on existing datasets provide insights into the model's accuracy, guiding future improvements in AI-driven social intelligence.",
        "arxiv_id": "2503.21190",
        "ARXIVID": "2503.21190",
        "COMMENT": "Matches criterion 2 as it integrates LLMs with visual information for enhanced social intelligence in video question answering, aligning with multi-modal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2503.21782": {
        "authors": [
            "Abdelrahman Shaker",
            "Muhammad Maaz",
            "Chenhui Gou",
            "Hamid Rezatofighi",
            "Salman Khan",
            "Fahad Shahbaz Khan"
        ],
        "title": "Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model",
        "abstract": "arXiv:2503.21782v1 Announce Type: new  Abstract: Video understanding models often struggle with high computational requirements, extensive parameter counts, and slow inference speed, making them inefficient for practical use. To tackle these challenges, we propose Mobile-VideoGPT, an efficient multimodal framework designed to operate with fewer than a billion parameters. Unlike traditional video large multimodal models (LMMs), Mobile-VideoGPT consists of lightweight dual visual encoders, efficient projectors, and a small language model (SLM), enabling real-time throughput. To further improve efficiency, we present an Attention-Based Frame Scoring mechanism to select the key-frames, along with an efficient token projector that prunes redundant visual tokens and preserves essential contextual cues. We evaluate our model across well-established six video understanding benchmarks (e.g., MVBench, EgoSchema, NextQA, and PercepTest). Our results show that Mobile-VideoGPT-0.5B can generate up to 46 tokens per second while outperforming existing state-of-the-art 0.5B-parameter models by 6 points on average with 40% fewer parameters and more than 2x higher throughput. Our code and models are publicly available at: https://github.com/Amshaker/Mobile-VideoGPT.",
        "arxiv_id": "2503.21782",
        "ARXIVID": "2503.21782",
        "COMMENT": "Matches criterion 2 as it introduces a new efficient multimodal video understanding model, which is a type of MLLM.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2503.21459": {
        "authors": [
            "Chirag Parikh",
            "Deepti Rawat",
            "Rakshitha R. T.",
            "Tathagata Ghosh",
            "Ravi Kiran Sarvadevabhatla"
        ],
        "title": "RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event Understanding from Social Video Narratives",
        "abstract": "arXiv:2503.21459v1 Announce Type: new  Abstract: We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored for generic road event understanding from social media narratives. Unlike existing datasets limited by regional bias, viewpoint bias and expert-driven annotations, RoadSocial captures the global complexity of road events with varied geographies, camera viewpoints (CCTV, handheld, drones) and rich social discourse. Our scalable semi-automatic annotation framework leverages Text LLMs and Video LLMs to generate comprehensive question-answer pairs across 12 challenging QA tasks, pushing the boundaries of road event understanding. RoadSocial is derived from social media videos spanning 14M frames and 414K social comments, resulting in a dataset with 13.2K videos, 674 tags and 260K high-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary, driving-specific and general-purpose) on our road event understanding benchmark. We also demonstrate RoadSocial's utility in improving road event understanding capabilities of general-purpose Video LLMs.",
        "arxiv_id": "2503.21459",
        "ARXIVID": "2503.21459",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for video question answering with a focus on road event understanding, which is a novel angle.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.21751": {
        "authors": [
            "Yan Xia",
            "Xiaowei Zhou",
            "Etienne Vouga",
            "Qixing Huang",
            "Georgios Pavlakos"
        ],
        "title": "Reconstructing Humans with a Biomechanically Accurate Skeleton",
        "abstract": "arXiv:2503.21751v1 Announce Type: new  Abstract: In this paper, we introduce a method for reconstructing 3D humans from a single image using a biomechanically accurate skeleton model. To achieve this, we train a transformer that takes an image as input and estimates the parameters of the model. Due to the lack of training data for this task, we build a pipeline to produce pseudo ground truth model parameters for single images and implement a training procedure that iteratively refines these pseudo labels. Compared to state-of-the-art methods for 3D human mesh recovery, our model achieves competitive performance on standard benchmarks, while it significantly outperforms them in settings with extreme 3D poses and viewpoints. Additionally, we show that previous reconstruction methods frequently violate joint angle limits, leading to unnatural rotations. In contrast, our approach leverages the biomechanically plausible degrees of freedom making more realistic joint rotation estimates. We validate our approach across multiple human pose estimation benchmarks. We make the code, models and data available at: https://isshikihugh.github.io/HSMR/",
        "arxiv_id": "2503.21751",
        "ARXIVID": "2503.21751",
        "COMMENT": "Matches criterion 1 as it introduces a new method for spatial understanding using a biomechanically accurate skeleton model for 3D human reconstruction.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.21408": {
        "authors": [
            "Marshall Thomas",
            "Edward Fish",
            "Richard Bowden"
        ],
        "title": "VALLR: Visual ASR Language Model for Lip Reading",
        "abstract": "arXiv:2503.21408v1 Announce Type: new  Abstract: Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex task requiring the interpretation of spoken language exclusively from visual cues, primarily lip movements and facial expressions. This task is especially challenging due to the absence of auditory information and the inherent ambiguity when visually distinguishing phonemes that have overlapping visemes where different phonemes appear identical on the lips. Current methods typically attempt to predict words or characters directly from these visual cues, but this approach frequently encounters high error rates due to coarticulation effects and viseme ambiguity. We propose a novel two-stage, phoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that addresses these longstanding challenges. First, our model predicts a compact sequence of phonemes from visual inputs using a Video Transformer with a CTC head, thereby reducing the task complexity and achieving robust speaker invariance. This phoneme output then serves as the input to a fine-tuned Large Language Model (LLM), which reconstructs coherent words and sentences by leveraging broader linguistic context. Unlike existing methods that either predict words directly-often faltering on visually similar phonemes-or rely on large-scale multimodal pre-training, our approach explicitly encodes intermediate linguistic structure while remaining highly data efficient. We demonstrate state-of-the-art performance on two challenging datasets, LRS2 and LRS3, where our method achieves significant reductions in Word Error Rate (WER) achieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data than the next best approach.",
        "arxiv_id": "2503.21408",
        "ARXIVID": "2503.21408",
        "COMMENT": "Matches criterion 2 as it proposes a novel visual ASR language model leveraging a two-stage framework with a fine-tuned LLM.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.21364": {
        "authors": [
            "Zhenxiang Ma",
            "Zhenyu Yang",
            "Miao Tao",
            "Yuanzhen Zhou",
            "Zeyu He",
            "Yuchang Zhang",
            "Rong Fu",
            "Hengjie Li"
        ],
        "title": "LandMarkSystem Technical Report",
        "abstract": "arXiv:2503.21364v1 Announce Type: new  Abstract: 3D reconstruction is vital for applications in autonomous driving, virtual reality, augmented reality, and the metaverse. Recent advancements such as Neural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformed the field, yet traditional deep learning frameworks struggle to meet the increasing demands for scene quality and scale. This paper introduces LandMarkSystem, a novel computing framework designed to enhance multi-scale scene reconstruction and rendering. By leveraging a componentized model adaptation layer, LandMarkSystem supports various NeRF and 3DGS structures while optimizing computational efficiency through distributed parallel computing and model parameter offloading. Our system addresses the limitations of existing frameworks, providing dedicated operators for complex 3D sparse computations, thus facilitating efficient training and rapid inference over extensive scenes. Key contributions include a modular architecture, a dynamic loading strategy for limited resources, and proven capabilities across multiple representative algorithms.This comprehensive solution aims to advance the efficiency and effectiveness of 3D reconstruction tasks.To facilitate further research and collaboration, the source code and documentation for the LandMarkSystem project are publicly available in an open-source repository, accessing the repository at: https://github.com/InternLandMark/LandMarkSystem.",
        "arxiv_id": "2503.21364",
        "ARXIVID": "2503.21364",
        "COMMENT": "Matches criterion 3 as it introduces a new framework for 3D reconstruction with novel computational strategies and modular architecture.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.21562": {
        "authors": [
            "Jonathan Lee",
            "Bolivar Solarte",
            "Chin-Hsuan Wu",
            "Jin-Cheng Jhang",
            "Fu-En Wang",
            "Yi-Hsuan Tsai",
            "Min Sun"
        ],
        "title": "uLayout: Unified Room Layout Estimation for Perspective and Panoramic Images",
        "abstract": "arXiv:2503.21562v1 Announce Type: new  Abstract: We present uLayout, a unified model for estimating room layout geometries from both perspective and panoramic images, whereas traditional solutions require different model designs for each image type. The key idea of our solution is to unify both domains into the equirectangular projection, particularly, allocating perspective images into the most suitable latitude coordinate to effectively exploit both domains seamlessly. To address the Field-of-View (FoV) difference between the input domains, we design uLayout with a shared feature extractor with an extra 1D-Convolution layer to condition each domain input differently. This conditioning allows us to efficiently formulate a column-wise feature regression problem regardless of the FoV input. This simple yet effective approach achieves competitive performance with current state-of-the-art solutions and shows for the first time a single end-to-end model for both domains. Extensive experiments in the real-world datasets, LSUN, Matterport3D, PanoContext, and Stanford 2D-3D evidence the contribution of our approach. Code is available at https://github.com/JonathanLee112/uLayout.",
        "arxiv_id": "2503.21562",
        "ARXIVID": "2503.21562",
        "COMMENT": "Matches criterion 1 as it proposes a unified model for room layout estimation, which is a methodological improvement in spatial understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.21777": {
        "authors": [
            "Jiahao Xie",
            "Alessio Tonioni",
            "Nathalie Rauschmayr",
            "Federico Tombari",
            "Bernt Schiele"
        ],
        "title": "Test-Time Visual In-Context Tuning",
        "abstract": "arXiv:2503.21777v1 Announce Type: new  Abstract: Visual in-context learning (VICL), as a new paradigm in computer vision, allows the model to rapidly adapt to various tasks with only a handful of prompts and examples. While effective, the existing VICL paradigm exhibits poor generalizability under distribution shifts. In this work, we propose test-time Visual In-Context Tuning (VICT), a method that can adapt VICL models on the fly with a single test sample. Specifically, we flip the role between the task prompts and the test sample and use a cycle consistency loss to reconstruct the original task prompt output. Our key insight is that a model should be aware of a new test distribution if it can successfully recover the original task prompts. Extensive experiments on six representative vision tasks ranging from high-level visual understanding to low-level image processing, with 15 common corruptions, demonstrate that our VICT can improve the generalizability of VICL to unseen new domains. In addition, we show the potential of applying VICT for unseen tasks at test time. Code: https://github.com/Jiahao000/VICT.",
        "arxiv_id": "2503.21777",
        "ARXIVID": "2503.21777",
        "COMMENT": "Matches criterion 4 as it proposes a novel method to improve generalizability in visual in-context learning, which is related to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.21124": {
        "authors": [
            "Shuaiyu Zhang",
            "Xun Lin",
            "Rongxiang Zhang",
            "Yu Bai",
            "Yong Xu",
            "Tao Tan",
            "Xunbin Zheng",
            "Zitong Yu"
        ],
        "title": "AdaMHF: Adaptive Multimodal Hierarchical Fusion for Survival Prediction",
        "abstract": "arXiv:2503.21124v1 Announce Type: new  Abstract: The integration of pathologic images and genomic data for survival analysis has gained increasing attention with advances in multimodal learning. However, current methods often ignore biological characteristics, such as heterogeneity and sparsity, both within and across modalities, ultimately limiting their adaptability to clinical practice. To address these challenges, we propose AdaMHF: Adaptive Multimodal Hierarchical Fusion, a framework designed for efficient, comprehensive, and tailored feature extraction and fusion. AdaMHF is specifically adapted to the uniqueness of medical data, enabling accurate predictions with minimal resource consumption, even under challenging scenarios with missing modalities. Initially, AdaMHF employs an experts expansion and residual structure to activate specialized experts for extracting heterogeneous and sparse features. Extracted tokens undergo refinement via selection and aggregation, reducing the weight of non-dominant features while preserving comprehensive information. Subsequently, the encoded features are hierarchically fused, allowing multi-grained interactions across modalities to be captured. Furthermore, we introduce a survival prediction benchmark designed to resolve scenarios with missing modalities, mirroring real-world clinical conditions. Extensive experiments on TCGA datasets demonstrate that AdaMHF surpasses current state-of-the-art (SOTA) methods, showcasing exceptional performance in both complete and incomplete modality settings.",
        "arxiv_id": "2503.21124",
        "ARXIVID": "2503.21124",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for survival prediction with missing modalities, focusing on real-world clinical conditions.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.21695": {
        "authors": [
            "Jiahe Qian",
            "Yaoyu Fang",
            "Jinkui Hao",
            "Bo Zhou"
        ],
        "title": "AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model for High-Fidelity Histology Nuclei Segmentation",
        "abstract": "arXiv:2503.21695v1 Announce Type: new  Abstract: Accurate segmentation of cell nuclei in histopathology images is essential for numerous biomedical research and clinical applications. However, existing cell nucleus segmentation methods only consider a single dataset (i.e., primary domain), while neglecting to leverage supplementary data from diverse sources (i.e., auxiliary domains) to reduce overfitting and enhance the performance. Although incorporating multiple datasets could alleviate overfitting, it often exacerbates performance drops caused by domain shifts. In this work, we introduce Adversarial Multi-domain Alignment of Segment Anything Model (AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these obstacles through two key innovations. First, we propose a Conditional Gradient Reversal Layer (CGRL), a multi-domain alignment module that harmonizes features from diverse domains to promote domain-invariant representation learning while preserving crucial discriminative features for the primary dataset. Second, we address SAM's inherent low-resolution output by designing a High-Resolution Decoder (HR-Decoder), which directly produces fine-grained segmentation maps in order to capture intricate nuclei boundaries in high-resolution histology images. To the best of our knowledge, this is the first attempt to adapt SAM for multi-dataset learning with application to histology nuclei segmentation. We validate our method on several publicly available datasets, demonstrating consistent and significant improvements over state-of-the-art approaches.",
        "arxiv_id": "2503.21695",
        "ARXIVID": "2503.21695",
        "COMMENT": "Matches criterion 4 as it extends the Segment Anything Model (SAM) for multi-domain learning and high-resolution segmentation, which is related to vision foundation models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.21781": {
        "authors": [
            "Chi-Pin Huang",
            "Yen-Siang Wu",
            "Hung-Kai Chung",
            "Kai-Po Chang",
            "Fu-En Yang",
            "Yu-Chiang Frank Wang"
        ],
        "title": "VideoMage: Multi-Subject and Motion Customization of Text-to-Video Diffusion Models",
        "abstract": "arXiv:2503.21781v1 Announce Type: new  Abstract: Customized text-to-video generation aims to produce high-quality videos that incorporate user-specified subject identities or motion patterns. However, existing methods mainly focus on personalizing a single concept, either subject identity or motion pattern, limiting their effectiveness for multiple subjects with the desired motion patterns. To tackle this challenge, we propose a unified framework VideoMage for video customization over both multiple subjects and their interactive motions. VideoMage employs subject and motion LoRAs to capture personalized content from user-provided images and videos, along with an appearance-agnostic motion learning approach to disentangle motion patterns from visual appearance. Furthermore, we develop a spatial-temporal composition scheme to guide interactions among subjects within the desired motion patterns. Extensive experiments demonstrate that VideoMage outperforms existing methods, generating coherent, user-controlled videos with consistent subject identities and interactions.",
        "arxiv_id": "2503.21781",
        "ARXIVID": "2503.21781",
        "COMMENT": "Matches criterion 2 as it introduces VideoMage, a framework for multi-subject and motion customization in text-to-video diffusion models.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2503.21776": {
        "authors": [
            "Kaituo Feng",
            "Kaixiong Gong",
            "Bohao Li",
            "Zonghao Guo",
            "Yibing Wang",
            "Tianshuo Peng",
            "Benyou Wang",
            "Xiangyu Yue"
        ],
        "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
        "abstract": "arXiv:2503.21776v1 Announce Type: new  Abstract: Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for eliciting video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-COT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All codes, models, data are released.",
        "arxiv_id": "2503.21776",
        "ARXIVID": "2503.21776",
        "COMMENT": "Matches criterion 2 as it introduces Video-R1, a novel approach for video reasoning in MLLMs using temporal modeling and reinforcement learning.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2503.21721": {
        "authors": [
            "Jaywon Koo",
            "Jefferson Hernandez",
            "Moayed Haji-Ali",
            "Ziyan Yang",
            "Vicente Ordonez"
        ],
        "title": "Evaluating Text-to-Image Synthesis with a Conditional Fr\\'{e}chet Distance",
        "abstract": "arXiv:2503.21721v1 Announce Type: new  Abstract: Evaluating text-to-image synthesis is challenging due to misalignment between established metrics and human preferences. We propose cFreD, a metric based on the notion of Conditional Fr\\'echet Distance that explicitly accounts for both visual fidelity and text-prompt alignment. Existing metrics such as Inception Score (IS), Fr\\'echet Inception Distance (FID) and CLIPScore assess either image quality or image-text alignment but not both which limits their correlation with human preferences. Scoring models explicitly trained to replicate human preferences require constant updates and may not generalize to novel generation techniques or out-of-domain inputs. Through extensive experiments across multiple recently proposed text-to-image models and diverse prompt datasets, we demonstrate that cFreD exhibits a higher correlation with human judgments compared to statistical metrics, including metrics trained with human preferences. Our findings validate cFreD as a robust, future-proof metric for the systematic evaluation of text-to-image models, standardizing benchmarking in this rapidly evolving field. We release our evaluation toolkit and benchmark in the appendix.",
        "arxiv_id": "2503.21721",
        "ARXIVID": "2503.21721",
        "COMMENT": "Matches criterion 4 as it proposes a new metric for evaluating text-to-image synthesis, which is relevant to vision foundation models and their evaluation.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2503.21449": {
        "authors": [
            "Lucas Nunes",
            "Rodrigo Marcuzzi",
            "Jens Behley",
            "Cyrill Stachniss"
        ],
        "title": "Towards Generating Realistic 3D Semantic Training Data for Autonomous Driving",
        "abstract": "arXiv:2503.21449v1 Announce Type: new  Abstract: Semantic scene understanding is crucial for robotics and computer vision applications. In autonomous driving, 3D semantic segmentation plays an important role for enabling safe navigation. Despite significant advances in the field, the complexity of collecting and annotating 3D data is a bottleneck in this developments. To overcome that data annotation limitation, synthetic simulated data has been used to generate annotated data on demand. There is still however a domain gap between real and simulated data. More recently, diffusion models have been in the spotlight, enabling close-to-real data synthesis. Those generative models have been recently applied to the 3D data domain for generating scene-scale data with semantic annotations. Still, those methods either rely on image projection or decoupled models trained with different resolutions in a coarse-to-fine manner. Such intermediary representations impact the generated data quality due to errors added in those transformations. In this work, we propose a novel approach able to generate 3D semantic scene-scale data without relying on any projection or decoupled trained multi-resolution models, achieving more realistic semantic scene data generation compared to previous state-of-the-art methods. Besides improving 3D semantic scene-scale data synthesis, we thoroughly evaluate the use of the synthetic scene samples as labeled data to train a semantic segmentation network. In our experiments, we show that using the synthetic annotated data generated by our method as training data together with the real semantic segmentation labels, leads to an improvement in the semantic segmentation model performance. Our results show the potential of generated scene-scale point clouds to generate more training data to extend existing datasets, reducing the data annotation effort. Our code is available at https://github.com/PRBonn/3DiSS.",
        "arxiv_id": "2503.21449",
        "ARXIVID": "2503.21449",
        "COMMENT": "Matches criterion 4 as it explores generative models for realistic 3D semantic training data in autonomous driving.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.21755": {
        "authors": [
            "Dian Zheng",
            "Ziqi Huang",
            "Hongbo Liu",
            "Kai Zou",
            "Yinan He",
            "Fan Zhang",
            "Yuanhan Zhang",
            "Jingwen He",
            "Wei-Shi Zheng",
            "Yu Qiao",
            "Ziwei Liu"
        ],
        "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness",
        "abstract": "arXiv:2503.21755v1 Announce Type: new  Abstract: Video generation has advanced significantly, evolving from producing unrealistic outputs to generating videos that appear visually convincing and temporally coherent. To evaluate these video generative models, benchmarks such as VBench have been developed to assess their faithfulness, measuring factors like per-frame aesthetics, temporal consistency, and basic prompt adherence. However, these aspects mainly represent superficial faithfulness, which focus on whether the video appears visually convincing rather than whether it adheres to real-world principles. While recent models perform increasingly well on these metrics, they still struggle to generate videos that are not just visually plausible but fundamentally realistic. To achieve real \"world models\" through video generation, the next frontier lies in intrinsic faithfulness to ensure that generated videos adhere to physical laws, commonsense reasoning, anatomical correctness, and compositional integrity. Achieving this level of realism is essential for applications such as AI-assisted filmmaking and simulated world modeling. To bridge this gap, we introduce VBench-2.0, a next-generation benchmark designed to automatically evaluate video generative models for their intrinsic faithfulness. VBench-2.0 assesses five key dimensions: Human Fidelity, Controllability, Creativity, Physics, and Commonsense, each further broken down into fine-grained capabilities. Tailored for individual dimensions, our evaluation framework integrates generalists such as state-of-the-art VLMs and LLMs, and specialists, including anomaly detection methods proposed for video generation. We conduct extensive annotations to ensure alignment with human judgment. By pushing beyond superficial faithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new standard for the next generation of video generative models in pursuit of intrinsic faithfulness.",
        "arxiv_id": "2503.21755",
        "ARXIVID": "2503.21755",
        "COMMENT": "Matches criterion 3 as it introduces VBench-2.0, a new benchmark for evaluating intrinsic faithfulness in video generation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.21457": {
        "authors": [
            "Xiaoqin Wang",
            "Xusen Ma",
            "Xianxu Hou",
            "Meidan Ding",
            "Yudong Li",
            "Junliang Chen",
            "Wenting Chen",
            "Xiaoyang Peng",
            "Linlin Shen"
        ],
        "title": "FaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for Benchmarking Face Perception MLLMs",
        "abstract": "arXiv:2503.21457v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in various tasks. However, effectively evaluating these MLLMs on face perception remains largely unexplored. To address this gap, we introduce FaceBench, a dataset featuring hierarchical multi-view and multi-level attributes specifically designed to assess the comprehensive face perception abilities of MLLMs. Initially, we construct a hierarchical facial attribute structure, which encompasses five views with up to three levels of attributes, totaling over 210 attributes and 700 attribute values. Based on the structure, the proposed FaceBench consists of 49,919 visual question-answering (VQA) pairs for evaluation and 23,841 pairs for fine-tuning. Moreover, we further develop a robust face perception MLLM baseline, Face-LLaVA, by training with our proposed face VQA data. Extensive experiments on various mainstream MLLMs and Face-LLaVA are conducted to test their face perception ability, with results also compared against human performance. The results reveal that, the existing MLLMs are far from satisfactory in understanding the fine-grained facial attributes, while our Face-LLaVA significantly outperforms existing open-source models with a small amount of training data and is comparable to commercial ones like GPT-4o and Gemini. The dataset will be released at https://github.com/CVI-SZU/FaceBench.",
        "arxiv_id": "2503.21457",
        "ARXIVID": "2503.21457",
        "COMMENT": "Matches criterion 3 as it introduces FaceBench, a new benchmark dataset for evaluating face perception in MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.21620": {
        "authors": [
            "Zhengxi Lu",
            "Yuxiang Chai",
            "Yaxuan Guo",
            "Xi Yin",
            "Liang Liu",
            "Hao Wang",
            "Guanjing Xiong",
            "Hongsheng Li"
        ],
        "title": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning",
        "abstract": "arXiv:2503.21620v1 Announce Type: new  Abstract: The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Building on this idea, we are the first to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for graphic user interface (GUI) action prediction tasks. To this end, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. We also introduce a unified rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). Experimental results demonstrate that our proposed data-efficient model, UI-R1-3B, achieves substantial improvements on both in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID benchmark AndroidControl, the action type accuracy improves by 15%, while grounding accuracy increases by 10.3%, compared with the base model (i.e. Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model surpasses the base model by 6.0% and achieves competitive performance with larger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning (SFT) on 76K data. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain.",
        "arxiv_id": "2503.21620",
        "ARXIVID": "2503.21620",
        "COMMENT": "Matches criterion 2 as it explores reinforcement learning to enhance reasoning capabilities in multimodal large language models (MLLMs) for GUI tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.21246": {
        "authors": [
            "Haoyu Zhao",
            "Zhongang Qi",
            "Cong Wang",
            "Qingping Zheng",
            "Guansong Lu",
            "Fei Chen",
            "Hang Xu",
            "Zuxuan Wu"
        ],
        "title": "DynamiCtrl: Rethinking the Basic Structure and the Role of Text for High-quality Human Image Animation",
        "abstract": "arXiv:2503.21246v1 Announce Type: new  Abstract: Human image animation has recently gained significant attention due to advancements in generative models. However, existing methods still face two major challenges: (1) architectural limitations, most models rely on U-Net, which underperforms compared to the MM-DiT; and (2) the neglect of textual information, which can enhance controllability. In this work, we introduce DynamiCtrl, a novel framework that not only explores different pose-guided control structures in MM-DiT, but also reemphasizes the crucial role of text in this task. Specifically, we employ a Shared VAE encoder for both reference images and driving pose videos, eliminating the need for an additional pose encoder and simplifying the overall framework. To incorporate pose features into the full attention blocks, we propose Pose-adaptive Layer Norm (PadaLN), which utilizes adaptive layer normalization to encode sparse pose features. The encoded features are directly added to the visual input, preserving the spatiotemporal consistency of the backbone while effectively introducing pose control into MM-DiT. Furthermore, within the full attention mechanism, we align textual and visual features to enhance controllability. By leveraging text, we not only enable fine-grained control over the generated content, but also, for the first time, achieve simultaneous control over both background and motion. Experimental results verify the superiority of DynamiCtrl on benchmark datasets, demonstrating its strong identity preservation, heterogeneous character driving, background controllability, and high-quality synthesis. The project page is available at https://gulucaptain.github.io/DynamiCtrl/.",
        "arxiv_id": "2503.21246",
        "ARXIVID": "2503.21246",
        "COMMENT": "Matches criterion 2 as it introduces a novel framework DynamiCtrl for human image animation, leveraging text and visual alignment in MM-DiT.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.21419": {
        "authors": [
            "Yupei Li",
            "Manuel Milling",
            "Bj\\\"orn W. Schuller"
        ],
        "title": "Neuroplasticity in Artificial Intelligence -- An Overview and Inspirations on Drop In \\& Out Learning",
        "abstract": "arXiv:2503.21419v1 Announce Type: new  Abstract: Artificial Intelligence (AI) has achieved new levels of performance and spread in public usage with the rise of deep neural networks (DNNs). Initially inspired by human neurons and their connections, NNs have become the foundation of AI models for many advanced architectures. However, some of the most integral processes in the human brain, particularly neurogenesis and neuroplasticity in addition to the more spread neuroapoptosis have largely been ignored in DNN architecture design. Instead, contemporary AI development predominantly focuses on constructing advanced frameworks, such as large language models, which retain a static structure of neural connections during training and inference. In this light, we explore how neurogenesis, neuroapoptosis, and neuroplasticity can inspire future AI advances. Specifically, we examine analogous activities in artificial NNs, introducing the concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and structural pruning for neuroapoptosis. We additionally suggest neuroplasticity combining the two for future large NNs in ``life-long learning'' settings following the biological inspiration. We conclude by advocating for greater research efforts in this interdisciplinary domain and identifying promising directions for future exploration.",
        "arxiv_id": "2503.21419",
        "ARXIVID": "2503.21419",
        "COMMENT": "Does not match any specific criterion but discusses neuroplasticity-inspired AI, which is tangentially relevant to your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.21164": {
        "authors": [
            "Samra Irshad",
            "Seungkyu Lee",
            "Nassir Navab",
            "Hong Joo Lee",
            "Seong Tae Kim"
        ],
        "title": "Adversarial Wear and Tear: Exploiting Natural Damage for Generating Physical-World Adversarial Examples",
        "abstract": "arXiv:2503.21164v1 Announce Type: new  Abstract: The presence of adversarial examples in the physical world poses significant challenges to the deployment of Deep Neural Networks in safety-critical applications such as autonomous driving. Most existing methods for crafting physical-world adversarial examples are ad-hoc, relying on temporary modifications like shadows, laser beams, or stickers that are tailored to specific scenarios. In this paper, we introduce a new class of physical-world adversarial examples, AdvWT, which draws inspiration from the naturally occurring phenomenon of `wear and tear', an inherent property of physical objects. Unlike manually crafted perturbations, `wear and tear' emerges organically over time due to environmental degradation, as seen in the gradual deterioration of outdoor signboards. To achieve this, AdvWT follows a two-step approach. First, a GAN-based, unsupervised image-to-image translation network is employed to model these naturally occurring damages, particularly in the context of outdoor signboards. The translation network encodes the characteristics of damaged signs into a latent `damage style code'. In the second step, we introduce adversarial perturbations into the style code, strategically optimizing its transformation process. This manipulation subtly alters the damage style representation, guiding the network to generate adversarial images where the appearance of damages remains perceptually realistic, while simultaneously ensuring their effectiveness in misleading neural networks. Through comprehensive experiments on two traffic sign datasets, we show that AdvWT effectively misleads DNNs in both digital and physical domains. AdvWT achieves an effective attack success rate, greater robustness, and a more natural appearance compared to existing physical-world adversarial examples. Additionally, integrating AdvWT into training enhances a model's generalizability to real-world damaged signs.",
        "arxiv_id": "2503.21164",
        "ARXIVID": "2503.21164",
        "COMMENT": "Does not match any specific criterion but focuses on adversarial examples in the physical world, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.21732": {
        "authors": [
            "Xianglong He",
            "Zi-Xin Zou",
            "Chia-Hao Chen",
            "Yuan-Chen Guo",
            "Ding Liang",
            "Chun Yuan",
            "Wanli Ouyang",
            "Yan-Pei Cao",
            "Yangguang Li"
        ],
        "title": "SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling",
        "abstract": "arXiv:2503.21732v1 Announce Type: new  Abstract: Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains a significant challenge. Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions. This paper introduces SparseFlex, a novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to $1024^3$ directly from rendering losses. SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces. Crucially, we introduce a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training. This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision. Building upon this, we demonstrate a complete shape modeling pipeline by training a variational autoencoder (VAE) and a rectified flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art reconstruction accuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling.",
        "arxiv_id": "2503.21732",
        "ARXIVID": "2503.21732",
        "COMMENT": "Does not match any specific criterion but focuses on 3D shape modeling, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.21765": {
        "authors": [
            "Minghui Lin",
            "Xiang Wang",
            "Yishan Wang",
            "Shu Wang",
            "Fengqi Dai",
            "Pengxiang Ding",
            "Cunxiang Wang",
            "Zhengrong Zuo",
            "Nong Sang",
            "Siteng Huang",
            "Donglin Wang"
        ],
        "title": "Exploring the Evolution of Physics Cognition in Video Generation: A Survey",
        "abstract": "arXiv:2503.21765v1 Announce Type: new  Abstract: Recent advancements in video generation have witnessed significant progress, especially with the rapid advancement of diffusion models. Despite this, their deficiencies in physical cognition have gradually received widespread attention - generated content often violates the fundamental laws of physics, falling into the dilemma of ''visual realism but physical absurdity\". Researchers began to increasingly recognize the importance of physical fidelity in video generation and attempted to integrate heuristic physical cognition such as motion representations and physical knowledge into generative systems to simulate real-world dynamic scenarios. Considering the lack of a systematic overview in this field, this survey aims to provide a comprehensive summary of architecture designs and their applications to fill this gap. Specifically, we discuss and organize the evolutionary process of physical cognition in video generation from a cognitive science perspective, while proposing a three-tier taxonomy: 1) basic schema perception for generation, 2) passive cognition of physical knowledge for generation, and 3) active cognition for world simulation, encompassing state-of-the-art methods, classical paradigms, and benchmarks. Subsequently, we emphasize the inherent key challenges in this domain and delineate potential pathways for future research, contributing to advancing the frontiers of discussion in both academia and industry. Through structured review and interdisciplinary analysis, this survey aims to provide directional guidance for developing interpretable, controllable, and physically consistent video generation paradigms, thereby propelling generative models from the stage of ''visual mimicry'' towards a new phase of ''human-like physical comprehension''.",
        "arxiv_id": "2503.21765",
        "ARXIVID": "2503.21765",
        "COMMENT": "Does not match any specific criteria but is tangentially related to generative modeling and video generation, which aligns with the general interest area.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.21072": {
        "authors": [
            "Judy X Yang",
            "Jing Wang",
            "Zhuanfeng",
            "Li",
            "Chenhong Sui Zekun Long",
            "Jun Zhou"
        ],
        "title": "HSLiNets: Evaluating Band Ordering Strategies in Hyperspectral and LiDAR Fusion",
        "abstract": "arXiv:2503.21072v1 Announce Type: new  Abstract: The integration of hyperspectral imaging (HSI) and Light Detection and Ranging (LiDAR) data provides complementary spectral and spatial information for remote sensing applications. While previous studies have explored the role of band selection and grouping in HSI classification, little attention has been given to how the spectral sequence or band order affects classification outcomes when fused with LiDAR. In this work, we systematically investigate the influence of band order on HSI-LiDAR fusion performance. Through extensive experiments, we demonstrate that band order significantly impacts classification accuracy, revealing a previously overlooked factor in fusion-based models. Motivated by this observation, we propose a novel fusion architecture that not only integrates HSI and LiDAR data but also learns from multiple band order configurations. The proposed method enhances feature representation by adaptively fusing different spectral sequences, leading to improved classification accuracy. Experimental results on the Houston 2013 and Trento datasets show that our approach outperforms state-of-the-art fusion models. Data and code are available at https://github.com/Judyxyang/HSLiNets.",
        "arxiv_id": "2503.21072",
        "ARXIVID": "2503.21072",
        "COMMENT": "Does not match any specific criteria but investigates hyperspectral and LiDAR fusion, which is tangentially relevant to spatial understanding.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.21076": {
        "authors": [
            "Yusong Hu",
            "Zichen Liang",
            "Fei Yang",
            "Qibin Hou",
            "Xialei Liu",
            "Ming-Ming Cheng"
        ],
        "title": "KAC: Kolmogorov-Arnold Classifier for Continual Learning",
        "abstract": "arXiv:2503.21076v1 Announce Type: new  Abstract: Continual learning requires models to train continuously across consecutive tasks without forgetting. Most existing methods utilize linear classifiers, which struggle to maintain a stable classification space while learning new tasks. Inspired by the success of Kolmogorov-Arnold Networks (KAN) in preserving learning stability during simple continual regression tasks, we set out to explore their potential in more complex continual learning scenarios. In this paper, we introduce the Kolmogorov-Arnold Classifier (KAC), a novel classifier developed for continual learning based on the KAN structure. We delve into the impact of KAN's spline functions and introduce Radial Basis Functions (RBF) for improved compatibility with continual learning. We replace linear classifiers with KAC in several recent approaches and conduct experiments across various continual learning benchmarks, all of which demonstrate performance improvements, highlighting the effectiveness and robustness of KAC in continual learning. The code is available at https://github.com/Ethanhuhuhu/KAC.",
        "arxiv_id": "2503.21076",
        "ARXIVID": "2503.21076",
        "COMMENT": "Does not match any specific criteria but is related to continual learning, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.21771": {
        "authors": [
            "Hongkai Lin",
            "Dingkang Liang",
            "Zhenghao Qi",
            "Xiang Bai"
        ],
        "title": "A Unified Image-Dense Annotation Generation Model for Underwater Scenes",
        "abstract": "arXiv:2503.21771v1 Announce Type: new  Abstract: Underwater dense prediction, especially depth estimation and semantic segmentation, is crucial for gaining a comprehensive understanding of underwater scenes. Nevertheless, high-quality and large-scale underwater datasets with dense annotations remain scarce because of the complex environment and the exorbitant data collection costs. This paper proposes a unified Text-to-Image and DEnse annotation generation method (TIDE) for underwater scenes. It relies solely on text as input to simultaneously generate realistic underwater images and multiple highly consistent dense annotations. Specifically, we unify the generation of text-to-image and text-to-dense annotations within a single model. The Implicit Layout Sharing mechanism (ILS) and cross-modal interaction method called Time Adaptive Normalization (TAN) are introduced to jointly optimize the consistency between image and dense annotations. We synthesize a large-scale underwater dataset using TIDE to validate the effectiveness of our method in underwater dense prediction tasks. The results demonstrate that our method effectively improves the performance of existing underwater dense prediction models and mitigates the scarcity of underwater data with dense annotations. We hope our method can offer new perspectives on alleviating data scarcity issues in other fields. The code is available at https: //github.com/HongkLin/TIDE.",
        "arxiv_id": "2503.21771",
        "ARXIVID": "2503.21771",
        "COMMENT": "Does not match any specific criterion but is related to dense prediction and dataset generation, which is tangentially relevant to your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.21525": {
        "authors": [
            "Yuxi Hu",
            "Jun Zhang",
            "Zhe Zhang",
            "Rafael Weilharter",
            "Yuchen Rao",
            "Kuangyi Chen",
            "Runze Yuan",
            "Friedrich Fraundorfer"
        ],
        "title": "ICG-MVSNet: Learning Intra-view and Cross-view Relationships for Guidance in Multi-View Stereo",
        "abstract": "arXiv:2503.21525v1 Announce Type: new  Abstract: Multi-view Stereo (MVS) aims to estimate depth and reconstruct 3D point clouds from a series of overlapping images. Recent learning-based MVS frameworks overlook the geometric information embedded in features and correlations, leading to weak cost matching. In this paper, we propose ICG-MVSNet, which explicitly integrates intra-view and cross-view relationships for depth estimation. Specifically, we develop an intra-view feature fusion module that leverages the feature coordinate correlations within a single image to enhance robust cost matching. Additionally, we introduce a lightweight cross-view aggregation module that efficiently utilizes the contextual information from volume correlations to guide regularization. Our method is evaluated on the DTU dataset and Tanks and Temples benchmark, consistently achieving competitive performance against state-of-the-art works, while requiring lower computational resources.",
        "arxiv_id": "2503.21525",
        "ARXIVID": "2503.21525",
        "COMMENT": "Does not match any specific criterion but is related to computer vision and 3D reconstruction, which is tangentially relevant to your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.20925": {
        "authors": [
            "Venkat Adithya Amula",
            "Sunayana Samavedam",
            "Saurabh Saini",
            "Avani Gupta",
            "Narayanan P J"
        ],
        "title": "Prototype Guided Backdoor Defense",
        "abstract": "arXiv:2503.20925v1 Announce Type: new  Abstract: Deep learning models are susceptible to {\\em backdoor attacks} involving malicious attackers perturbing a small subset of training data with a {\\em trigger} to causes misclassifications. Various triggers have been used, including semantic triggers that are easily realizable without requiring the attacker to manipulate the image. The emergence of generative AI has eased the generation of varied poisoned samples. Robustness across types of triggers is crucial to effective defense. We propose Prototype Guided Backdoor Defense (PGBD), a robust post-hoc defense that scales across different trigger types, including previously unsolved semantic triggers. PGBD exploits displacements in the geometric spaces of activations to penalize movements toward the trigger. This is done using a novel sanitization loss of a post-hoc fine-tuning step. The geometric approach scales easily to all types of attacks. PGBD achieves better performance across all settings. We also present the first defense against a new semantic attack on celebrity face images. Project page: \\hyperlink{https://venkatadithya9.github.io/pgbd.github.io/}{this https URL}.",
        "arxiv_id": "2503.20925",
        "ARXIVID": "2503.20925",
        "COMMENT": "Does not match any specific criterion but is related to robustness in deep learning, which is tangentially relevant to your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.21036": {
        "authors": [
            "Yunnan Wu",
            "Paul Chen",
            "Deshank Baranwal",
            "Jinlong Zhou",
            "Jian Yuan"
        ],
        "title": "The Art of Tool Interface Design",
        "abstract": "arXiv:2503.21036v1 Announce Type: new  Abstract: We present an agentic framework, Thinker, which achieves state of art performance in challenging reasoning tasks for realistic customer service scenarios that involve complex business logic and human interactions via long horizons. On the $\\tau$-bench retail dataset, Thinker achieves 82.6\\% success rate with GPT-4o (version 2024-06-01) (baseline: 68.3\\%), and 81.9\\% success rate with Llama-3.1 405B (baseline: 49.6\\%), without any fine-tuning. Thinker effectively closes the gap in reasoning capabilities between the base models by introducing proper structure.   The key features of the Thinker framework are: (1) State-Machine Augmented Generation (SMAG), which represents business logic as state machines and the LLM uses state machines as tools. (2) Delegation of tasks from the main reasoning loop to LLM-powered tools. (3) Adaptive context management.   Our prompting-only solution achieves signficant gains, while still maintaining a standard agentic architecture with a ReAct style reasoning loop. The key is to innovate on the tool interface design, as exemplified by SMAG and the LLM-powered tools.",
        "arxiv_id": "2503.21036",
        "ARXIVID": "2503.21036",
        "COMMENT": "Does not match any specific criteria but is related to reasoning frameworks and tool interface design, which is outside the specified focus areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.20967": {
        "authors": [
            "David Wong",
            "Bin Wang",
            "Gorkem Durak",
            "Marouane Tliba",
            "Akshay Chaudhari",
            "Aladine Chetouani",
            "Ahmet Enis Cetin",
            "Cagdas Topel",
            "Nicolo Gennaro",
            "Camila Lopes Vendrami",
            "Tugce Agirlar Trabzonlu",
            "Amir Ali Rahsepar",
            "Laetitia Perronne",
            "Matthew Antalek",
            "Onural Ozturk",
            "Gokcan Okur",
            "Andrew C. Gordon",
            "Ayis Pyrros",
            "Frank H. Miller",
            "Amir Borhani",
            "Hatice Savas",
            "Eric Hart",
            "Drew Torigian",
            "Jayaram K. Udupa",
            "Elizabeth Krupinski",
            "Ulas Bagci"
        ],
        "title": "Eyes Tell the Truth: GazeVal Highlights Shortcomings of Generative AI in Medical Imaging",
        "abstract": "arXiv:2503.20967v1 Announce Type: new  Abstract: The demand for high-quality synthetic data for model training and augmentation has never been greater in medical imaging. However, current evaluations predominantly rely on computational metrics that fail to align with human expert recognition. This leads to synthetic images that may appear realistic numerically but lack clinical authenticity, posing significant challenges in ensuring the reliability and effectiveness of AI-driven medical tools. To address this gap, we introduce GazeVal, a practical framework that synergizes expert eye-tracking data with direct radiological evaluations to assess the quality of synthetic medical images. GazeVal leverages gaze patterns of radiologists as they provide a deeper understanding of how experts perceive and interact with synthetic data in different tasks (i.e., diagnostic or Turing tests). Experiments with sixteen radiologists revealed that 96.6% of the generated images (by the most recent state-of-the-art AI algorithm) were identified as fake, demonstrating the limitations of generative AI in producing clinically accurate images.",
        "arxiv_id": "2503.20967",
        "ARXIVID": "2503.20967",
        "COMMENT": "Does not match any specific criteria but is related to medical imaging and generative AI evaluation, which is outside the specified focus areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.21125": {
        "authors": [
            "Jiajie Quan",
            "Ao Tong",
            "Yuxuan Cai",
            "Xinwei He",
            "Yulong Wang",
            "Yang Zhou"
        ],
        "title": "Omni-AD: Learning to Reconstruct Global and Local Features for Multi-class Anomaly Detection",
        "abstract": "arXiv:2503.21125v1 Announce Type: new  Abstract: In multi-class unsupervised anomaly detection(MUAD), reconstruction-based methods learn to map input images to normal patterns to identify anomalous pixels. However, this strategy easily falls into the well-known \"learning shortcut\" issue when decoders fail to capture normal patterns and reconstruct both normal and abnormal samples naively. To address that, we propose to learn the input features in global and local manners, forcing the network to memorize the normal patterns more comprehensively. Specifically, we design a two-branch decoder block, named Omni-block. One branch corresponds to global feature learning, where we serialize two self-attention blocks but replace the query and (key, value) with learnable tokens, respectively, thus capturing global features of normal patterns concisely and thoroughly. The local branch comprises depth-separable convolutions, whose locality enables effective and efficient learning of local features for normal patterns. By stacking Omni-blocks, we build a framework, Omni-AD, to learn normal patterns of different granularity and reconstruct them progressively. Comprehensive experiments on public anomaly detection benchmarks show that our method outperforms state-of-the-art approaches in MUAD. Code is available at https://github.com/easyoo/Omni-AD.git.",
        "arxiv_id": "2503.21125",
        "ARXIVID": "2503.21125",
        "COMMENT": "Does not match any specific criteria but is related to anomaly detection, which is outside the specified focus areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.21061": {
        "authors": [
            "Mehraveh Javan Roshtkhari",
            "Matthew Toews",
            "Marco Pedersoli"
        ],
        "title": "Neural Architecture Search by Learning a Hierarchical Search Space",
        "abstract": "arXiv:2503.21061v1 Announce Type: new  Abstract: Monte-Carlo Tree Search (MCTS) is a powerful tool for many non-differentiable search related problems such as adversarial games. However, the performance of such approach highly depends on the order of the nodes that are considered at each branching of the tree. If the first branches cannot distinguish between promising and deceiving configurations for the final task, the efficiency of the search is exponentially reduced. In Neural Architecture Search (NAS), as only the final architecture matters, the visiting order of the branching can be optimized to improve learning. In this paper, we study the application of MCTS to NAS for image classification. We analyze several sampling methods and branching alternatives for MCTS and propose to learn the branching by hierarchical clustering of architectures based on their similarity. The similarity is measured by the pairwise distance of output vectors of architectures. Extensive experiments on two challenging benchmarks on CIFAR10 and ImageNet show that MCTS, if provided with a good branching hierarchy, can yield promising solutions more efficiently than other approaches for NAS problems.",
        "arxiv_id": "2503.21061",
        "ARXIVID": "2503.21061",
        "COMMENT": "Does not match any specific criteria but is related to neural architecture search, which is outside the specified focus areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.21259": {
        "authors": [
            "Wencheng Han",
            "Dongqian Guo",
            "Xiao Chen",
            "Pang Lyu",
            "Yi Jin",
            "Jianbing Shen"
        ],
        "title": "Reducing CT Metal Artifacts by Learning Latent Space Alignment with Gemstone Spectral Imaging Data",
        "abstract": "arXiv:2503.21259v1 Announce Type: new  Abstract: Metal artifacts in CT slices have long posed challenges in medical diagnostics. These artifacts degrade image quality, resulting in suboptimal visualization and complicating the accurate interpretation of tissues adjacent to metal implants. To address these issues, we introduce the Latent Gemstone Spectral Imaging (GSI) Alignment Framework, which effectively reduces metal artifacts while avoiding the introduction of noise information. Our work is based on a key finding that even artifact-affected ordinary CT sequences contain sufficient information to discern detailed structures. The challenge lies in the inability to clearly represent this information. To address this issue, we developed an Alignment Framework that adjusts the representation of ordinary CT images to match GSI CT sequences. GSI is an advanced imaging technique using multiple energy levels to mitigate artifacts caused by metal implants. By aligning the representation to GSI data, we can effectively suppress metal artifacts while clearly revealing detailed structure, without introducing extraneous information into CT sequences. To facilitate the application, we propose a new dataset, Artifacts-GSI, captured from real patients with metal implants, and establish a new benchmark based on this dataset. Experimental results show that our method significantly reduces metal artifacts and greatly enhances the readability of CT slices. All our code and data are available at: https://um-lab.github.io/GSI-MAR/",
        "arxiv_id": "2503.21259",
        "ARXIVID": "2503.21259",
        "COMMENT": "Does not match any specific criteria but is related to medical imaging and artifact reduction, which is outside the specified focus areas.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.21749": {
        "authors": [
            "Shitian Zhao",
            "Qilong Wu",
            "Xinyue Li",
            "Bo Zhang",
            "Ming Li",
            "Qi Qin",
            "Dongyang Liu",
            "Kaipeng Zhang",
            "Hongsheng Li",
            "Yu Qiao",
            "Peng Gao",
            "Bin Fu",
            "Zhen Li"
        ],
        "title": "LeX-Art: Rethinking Text Generation via Scalable High-Quality Data Synthesis",
        "abstract": "arXiv:2503.21749v1 Announce Type: new  Abstract: We introduce LeX-Art, a comprehensive suite for high-quality text-image synthesis that systematically bridges the gap between prompt expressiveness and text rendering fidelity. Our approach follows a data-centric paradigm, constructing a high-quality data synthesis pipeline based on Deepseek-R1 to curate LeX-10K, a dataset of 10K high-resolution, aesthetically refined 1024$\\times$1024 images. Beyond dataset construction, we develop LeX-Enhancer, a robust prompt enrichment model, and train two text-to-image models, LeX-FLUX and LeX-Lumina, achieving state-of-the-art text rendering performance. To systematically evaluate visual text generation, we introduce LeX-Bench, a benchmark that assesses fidelity, aesthetics, and alignment, complemented by Pairwise Normalized Edit Distance (PNED), a novel metric for robust text accuracy evaluation. Experiments demonstrate significant improvements, with LeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX outperforming baselines in color (+3.18%), positional (+4.45%), and font accuracy (+3.81%). Our codes, models, datasets, and demo are publicly available.",
        "arxiv_id": "2503.21749",
        "ARXIVID": "2503.21749",
        "COMMENT": "Does not match any specific criterion but focuses on text-to-image synthesis and evaluation, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.21772": {
        "authors": [
            "Zilin Xiao",
            "Pavel Suma",
            "Ayush Sachdeva",
            "Hao-Jen Wang",
            "Giorgos Kordopatis-Zilos",
            "Giorgos Tolias",
            "Vicente Ordonez"
        ],
        "title": "LOCORE: Image Re-ranking with Long-Context Sequence Modeling",
        "abstract": "arXiv:2503.21772v1 Announce Type: new  Abstract: We introduce LOCORE, Long-Context Re-ranker, a model that takes as input local descriptors corresponding to an image query and a list of gallery images and outputs similarity scores between the query and each gallery image. This model is used for image retrieval, where typically a first ranking is performed with an efficient similarity measure, and then a shortlist of top-ranked images is re-ranked based on a more fine-grained similarity measure. Compared to existing methods that perform pair-wise similarity estimation with local descriptors or list-wise re-ranking with global descriptors, LOCORE is the first method to perform list-wise re-ranking with local descriptors. To achieve this, we leverage efficient long-context sequence models to effectively capture the dependencies between query and gallery images at the local-descriptor level. During testing, we process long shortlists with a sliding window strategy that is tailored to overcome the context size limitations of sequence models. Our approach achieves superior performance compared with other re-rankers on established image retrieval benchmarks of landmarks (ROxf and RPar), products (SOP), fashion items (In-Shop), and bird species (CUB-200) while having comparable latency to the pair-wise local descriptor re-rankers.",
        "arxiv_id": "2503.21772",
        "ARXIVID": "2503.21772",
        "COMMENT": "Does not match any specific criterion but focuses on image retrieval and re-ranking, which is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.21616": {
        "authors": [
            "Jiahui Chen",
            "Yang Huan",
            "Runhua Shi",
            "Chanfan Ding",
            "Xiaoqi Mo",
            "Siyu Xiong",
            "Yinong He"
        ],
        "title": "Audio-driven Gesture Generation via Deviation Feature in the Latent Space",
        "abstract": "arXiv:2503.21616v1 Announce Type: new  Abstract: Gestures are essential for enhancing co-speech communication, offering visual emphasis and complementing verbal interactions. While prior work has concentrated on point-level motion or fully supervised data-driven methods, we focus on co-speech gestures, advocating for weakly supervised learning and pixel-level motion deviations. We introduce a weakly supervised framework that learns latent representation deviations, tailored for co-speech gesture video generation. Our approach employs a diffusion model to integrate latent motion features, enabling more precise and nuanced gesture representation. By leveraging weakly supervised deviations in latent space, we effectively generate hand gestures and mouth movements, crucial for realistic video production. Experiments show our method significantly improves video quality, surpassing current state-of-the-art techniques.",
        "arxiv_id": "2503.21616",
        "ARXIVID": "2503.21616",
        "COMMENT": "Does not match any specific criterion but focuses on gesture generation using diffusion models, which is tangentially related to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.21352": {
        "authors": [
            "Tianhang Zhang",
            "Shengnan Fu",
            "David M. Schultz",
            "Zhonghua Zheng"
        ],
        "title": "Using large language models to produce literature reviews: Usages and systematic biases of microphysics parametrizations in 2699 publications",
        "abstract": "arXiv:2503.21352v1 Announce Type: new  Abstract: Large language models afford opportunities for using computers for intensive tasks, realizing research opportunities that have not been considered before. One such opportunity could be a systematic interrogation of the scientific literature. Here, we show how a large language model can be used to construct a literature review of 2699 publications associated with microphysics parametrizations in the Weather and Research Forecasting (WRF) model, with the goal of learning how they were used and their systematic biases, when simulating precipitation. The database was constructed of publications identified from Web of Science and Scopus searches. The large language model GPT-4 Turbo was used to extract information about model configurations and performance from the text of 2699 publications. Our results reveal the landscape of how nine of the most popular microphysics parameterizations have been used around the world: Lin, Ferrier, WRF Single-Moment, Goddard Cumulus Ensemble, Morrison, Thompson, and WRF Double-Moment. More studies used one-moment parameterizations before 2020 and two-moment parameterizations after 2020. Seven out of nine parameterizations tended to overestimate precipitation. However, systematic biases of parameterizations differed in various regions. Except simulations using the Lin, Ferrier, and Goddard parameterizations that tended to underestimate precipitation over almost all locations, the remaining six parameterizations tended to overestimate, particularly over China, southeast Asia, western United States, and central Africa. This method could be used by other researchers to help understand how the increasingly massive body of scientific literature can be harnessed through the power of artificial intelligence to solve their research problems.",
        "arxiv_id": "2503.21352",
        "ARXIVID": "2503.21352",
        "COMMENT": "Does not match any specific criterion but is relevant to using LLMs for literature reviews and systematic analysis.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.21557": {
        "authors": [
            "Xingdi Yuan",
            "Morgane M Moss",
            "Charbel El Feghali",
            "Chinmay Singh",
            "Darya Moldavskaya",
            "Drew MacPhee",
            "Lucas Caccia",
            "Matheus Pereira",
            "Minseon Kim",
            "Alessandro Sordoni",
            "Marc-Alexandre C\\^ot\\'e"
        ],
        "title": "debug-gym: A Text-Based Environment for Interactive Debugging",
        "abstract": "arXiv:2503.21557v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly relied upon for coding tasks, yet in most scenarios it is assumed that all relevant information can be either accessed in context or matches their training data. We posit that LLMs can benefit from the ability to interactively explore a codebase to gather the information relevant to their task. To achieve this, we present a textual environment, namely debug-gym, for developing LLM-based agents in an interactive coding setting. Our environment is lightweight and provides a preset of useful tools, such as a Python debugger (pdb), designed to facilitate an LLM-based agent's interactive debugging. Beyond coding and debugging tasks, this approach can be generalized to other tasks that would benefit from information-seeking behavior by an LLM agent.",
        "arxiv_id": "2503.21557",
        "ARXIVID": "2503.21557",
        "COMMENT": "Does not match any specific criterion but is relevant to interactive debugging environments for LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.21766": {
        "authors": [
            "Haolin Liu",
            "Xiaohang Zhan",
            "Zizheng Yan",
            "Zhongjin Luo",
            "Yuxin Wen",
            "Xiaoguang Han"
        ],
        "title": "Stable-SCore: A Stable Registration-based Framework for 3D Shape Correspondence",
        "abstract": "arXiv:2503.21766v1 Announce Type: new  Abstract: Establishing character shape correspondence is a critical and fundamental task in computer vision and graphics, with diverse applications including re-topology, attribute transfer, and shape interpolation. Current dominant functional map methods, while effective in controlled scenarios, struggle in real situations with more complex challenges such as non-isometric shape discrepancies. In response, we revisit registration-for-correspondence methods and tap their potential for more stable shape correspondence estimation. To overcome their common issues including unstable deformations and the necessity for careful pre-alignment or high-quality initial 3D correspondences, we introduce Stable-SCore: A Stable Registration-based Framework for 3D Shape Correspondence. We first re-purpose a foundation model for 2D character correspondence that ensures reliable and stable 2D mappings. Crucially, we propose a novel Semantic Flow Guided Registration approach that leverages 2D correspondence to guide mesh deformations. Our framework significantly surpasses existing methods in challenging scenarios, and brings possibilities for a wide array of real applications, as demonstrated in our results.",
        "arxiv_id": "2503.21766",
        "ARXIVID": "2503.21766",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and foundational 3D shape correspondence methods.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.21646": {
        "authors": [
            "Thomas Monks",
            "Alison Harper",
            "Amy Heather"
        ],
        "title": "Unlocking the Potential of Past Research: Using Generative AI to Reconstruct Healthcare Simulation Models",
        "abstract": "arXiv:2503.21646v1 Announce Type: new  Abstract: Discrete-event simulation (DES) is widely used in healthcare Operations Research, but the models themselves are rarely shared. This limits their potential for reuse and long-term impact in the modelling and healthcare communities. This study explores the feasibility of using generative artificial intelligence (AI) to recreate published models using Free and Open Source Software (FOSS), based on the descriptions provided in an academic journal. Using a structured methodology, we successfully generated, tested and internally reproduced two DES models, including user interfaces. The reported results were replicated for one model, but not the other, likely due to missing information on distributions. These models are substantially more complex than AI-generated DES models published to date. Given the challenges we faced in prompt engineering, code generation, and model testing, we conclude that our iterative approach to model development, systematic comparison and testing, and the expertise of our team were necessary to the success of our recreated simulation models.",
        "arxiv_id": "2503.21646",
        "ARXIVID": "2503.21646",
        "COMMENT": "Does not match any specific criteria but explores generative AI applications in healthcare simulation, which is tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}