{
    "2510.08431": {
        "authors": [
            "Kaiwen Zheng",
            "Yuji Wang",
            "Qianli Ma",
            "Huayu Chen",
            "Jintao Zhang",
            "Yogesh Balaji",
            "Jianfei Chen",
            "Ming-Yu Liu",
            "Jun Zhu",
            "Qinsheng Zhang"
        ],
        "title": "Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency",
        "abstract": "arXiv:2510.08431v1 Announce Type: new  Abstract: This work represents the first effort to scale up continuous-time consistency distillation to general application-level image and video diffusion models. Although continuous-time consistency model (sCM) is theoretically principled and empirically powerful for accelerating academic-scale diffusion, its applicability to large-scale text-to-image and video tasks remains unclear due to infrastructure challenges in Jacobian-vector product (JVP) computation and the limitations of standard evaluation benchmarks. We first develop a parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on models with over 10 billion parameters and high-dimensional video tasks. Our investigation reveals fundamental quality limitations of sCM in fine-detail generation, which we attribute to error accumulation and the \"mode-covering\" nature of its forward-divergence objective. To remedy this, we propose the score-regularized continuous-time consistency model (rCM), which incorporates score distillation as a long-skip regularizer. This integration complements sCM with the \"mode-seeking\" reverse divergence, effectively improving visual quality while maintaining high generation diversity. Validated on large-scale models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM matches or surpasses the state-of-the-art distillation method DMD2 on quality metrics while offering notable advantages in diversity, all without GAN tuning or extensive hyperparameter searches. The distilled models generate high-fidelity samples in only $1\\sim4$ steps, accelerating diffusion sampling by $15\\times\\sim50\\times$. These results position rCM as a practical and theoretically grounded framework for advancing large-scale diffusion distillation.",
        "arxiv_id": "2510.08431",
        "ARXIVID": "2510.08431",
        "COMMENT": "Matches criteria 2 for Unified Diffusion Models",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2510.07944": {
        "authors": [
            "Tianrui Zhang",
            "Yichen Liu",
            "Zilin Guo",
            "Yuxin Guo",
            "Jingcheng Ni",
            "Chenjing Ding",
            "Dan Xu",
            "Lewei Lu",
            "Zehuan Wu"
        ],
        "title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving",
        "abstract": "arXiv:2510.07944v1 Announce Type: new  Abstract: Generative models have been widely applied to world modeling for environment simulation and future state prediction. With advancements in autonomous driving, there is a growing demand not only for high-fidelity video generation under various controls, but also for producing diverse and meaningful information such as depth estimation. To address this, we propose CVD-STORM, a cross-view video diffusion model utilizing a spatial-temporal reconstruction Variational Autoencoder (VAE) that generates long-term, multi-view videos with 4D reconstruction capabilities under various control inputs. Our approach first fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its ability to encode 3D structures and temporal dynamics. Subsequently, we integrate this VAE into the video diffusion process to significantly improve generation quality. Experimental results demonstrate that our model achieves substantial improvements in both FID and FVD metrics. Additionally, the jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic scenes, providing valuable geometric information for comprehensive scene understanding.",
        "arxiv_id": "2510.07944",
        "ARXIVID": "2510.07944",
        "COMMENT": "Matches criterion 2: Unified Diffusion Models for multiple vision tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.07721": {
        "authors": [
            "Zipeng Guo",
            "Lichen Ma",
            "Xiaolong Fu",
            "Gaojing Zhou",
            "Lan Yang",
            "Yuchen Zhou",
            "Linkai Liu",
            "Yu He",
            "Ximan Liu",
            "Shiping Dong",
            "Jingling Fu",
            "Zhen Chen",
            "Yu Shi",
            "Junshi Huang",
            "Jason Li",
            "Chao Gou"
        ],
        "title": "RePainter: Empowering E-commerce Object Removal via Spatial-matting Reinforcement Learning",
        "abstract": "arXiv:2510.07721v1 Announce Type: new  Abstract: In web data, product images are central to boosting user engagement and advertising efficacy on e-commerce platforms, yet the intrusive elements such as watermarks and promotional text remain major obstacles to delivering clear and appealing product visuals. Although diffusion-based inpainting methods have advanced, they still face challenges in commercial settings due to unreliable object removal and limited domain-specific adaptation. To tackle these challenges, we propose Repainter, a reinforcement learning framework that integrates spatial-matting trajectory refinement with Group Relative Policy Optimization (GRPO). Our approach modulates attention mechanisms to emphasize background context, generating higher-reward samples and reducing unwanted object insertion. We also introduce a composite reward mechanism that balances global, local, and semantic constraints, effectively reducing visual artifacts and reward hacking. Additionally, we contribute EcomPaint-100K, a high-quality, large-scale e-commerce inpainting dataset, and a standardized benchmark EcomPaint-Bench for fair evaluation. Extensive experiments demonstrate that Repainter significantly outperforms state-of-the-art methods, especially in challenging scenes with intricate compositions. We will release our code and weights upon acceptance.",
        "arxiv_id": "2510.07721",
        "ARXIVID": "2510.07721",
        "COMMENT": "Matches criteria 3 for Image Matting",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2510.07670": {
        "authors": [
            "Haoyi Duan",
            "Yunzhi Zhang",
            "Yilun Du",
            "Jiajun Wu"
        ],
        "title": "Controllable Video Synthesis via Variational Inference",
        "abstract": "arXiv:2510.07670v1 Announce Type: new  Abstract: Many video workflows benefit from a mixture of user controls with varying granularity, from exact 4D object trajectories and camera paths to coarse text prompts, while existing video generative models are typically trained for fixed input formats. We develop a video synthesis method that addresses this need and generates samples with high controllability for specified elements while maintaining diversity for under-specified ones. We cast the task as variational inference to approximate a composed distribution, leveraging multiple video generation backbones to account for all task constraints collectively. To address the optimization challenge, we break down the problem into step-wise KL divergence minimization over an annealed sequence of distributions, and further propose a context-conditioned factorization technique that reduces modes in the solution space to circumvent local optima. Experiments suggest that our method produces samples with improved controllability, diversity, and 3D consistency compared to prior works.",
        "arxiv_id": "2510.07670",
        "ARXIVID": "2510.07670",
        "COMMENT": "The paper does not match any specific criteria closely. It discusses controllable video synthesis via variational inference, but does not mention joint generation and segmentation or a unified diffusion model for multiple tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 7
    },
    "2510.08131": {
        "authors": [
            "Kesen Zhao",
            "Jiaxin Shi",
            "Beier Zhu",
            "Junbao Zhou",
            "Xiaolong Shen",
            "Yuan Zhou",
            "Qianru Sun",
            "Hanwang Zhang"
        ],
        "title": "Real-Time Motion-Controllable Autoregressive Video Diffusion",
        "abstract": "arXiv:2510.08131v1 Announce Type: new  Abstract: Real-time motion-controllable video generation remains challenging due to the inherent latency of bidirectional diffusion models and the lack of effective autoregressive (AR) approaches. Existing AR video diffusion models are limited to simple control signals or text-to-video generation, and often suffer from quality degradation and motion artifacts in few-step generation. To address these challenges, we propose AR-Drag, the first RL-enhanced few-step AR video diffusion model for real-time image-to-video generation with diverse motion control. We first fine-tune a base I2V model to support basic motion control, then further improve it via reinforcement learning with a trajectory-based reward model. Our design preserves the Markov property through a Self-Rollout mechanism and accelerates training by selectively introducing stochasticity in denoising steps. Extensive experiments demonstrate that AR-Drag achieves high visual fidelity and precise motion alignment, significantly reducing latency compared with state-of-the-art motion-controllable VDMs, while using only 1.3B parameters. Additional visualizations can be found on our project page: https://kesenzhao.github.io/AR-Drag.github.io/.",
        "arxiv_id": "2510.08131",
        "ARXIVID": "2510.08131",
        "COMMENT": "The paper does not match any specific criteria closely. It focuses on real-time motion-controllable video generation using a diffusion model, but does not mention joint generation and segmentation or a unified diffusion model for multiple tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2510.08485": {
        "authors": [
            "Chong Mou",
            "Qichao Sun",
            "Yanze Wu",
            "Pengze Zhang",
            "Xinghui Li",
            "Fulong Ye",
            "Songtao Zhao",
            "Qian He"
        ],
        "title": "InstructX: Towards Unified Visual Editing with MLLM Guidance",
        "abstract": "arXiv:2510.08485v1 Announce Type: new  Abstract: With recent advances in Multimodal Large Language Models (MLLMs) showing strong visual understanding and reasoning, interest is growing in using them to improve the editing performance of diffusion models. Despite rapid progress, most studies lack an in-depth analysis of MLLM design choices. Moreover, the integration of MLLMs and diffusion models remains an open challenge in some difficult tasks, such as video editing. In this paper, we present InstructX, a unified framework for image and video editing. Specifically, we conduct a comprehensive study on integrating MLLMs and diffusion models for instruction-driven editing across diverse tasks. Building on this study, we analyze the cooperation and distinction between images and videos in unified modeling. (1) We show that training on image data can lead to emergent video editing capabilities without explicit supervision, thereby alleviating the constraints imposed by scarce video training data. (2) By incorporating modality-specific MLLM features, our approach effectively unifies image and video editing tasks within a single model. Extensive experiments demonstrate that our method can handle a broad range of image and video editing tasks and achieves state-of-the-art performance.",
        "arxiv_id": "2510.08485",
        "ARXIVID": "2510.08485",
        "COMMENT": "No criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.07546": {
        "authors": [
            "Soroush Mehraban",
            "Vida Adeli",
            "Jacob Rommann",
            "Babak Taati",
            "Kyryl Truskovskyi"
        ],
        "title": "PickStyle: Video-to-Video Style Transfer with Context-Style Adapters",
        "abstract": "arXiv:2510.07546v1 Announce Type: new  Abstract: We address the task of video style transfer with diffusion models, where the goal is to preserve the context of an input video while rendering it in a target style specified by a text prompt. A major challenge is the lack of paired video data for supervision. We propose PickStyle, a video-to-video style transfer framework that augments pretrained video diffusion backbones with style adapters and benefits from paired still image data with source-style correspondences for training. PickStyle inserts low-rank adapters into the self-attention layers of conditioning modules, enabling efficient specialization for motion-style transfer while maintaining strong alignment between video content and style. To bridge the gap between static image supervision and dynamic video, we construct synthetic training clips from paired images by applying shared augmentations that simulate camera motion, ensuring temporal priors are preserved. In addition, we introduce Context-Style Classifier-Free Guidance (CS-CFG), a novel factorization of classifier-free guidance into independent text (style) and video (context) directions. CS-CFG ensures that context is preserved in generated video while the style is effectively transferred. Experiments across benchmarks show that our approach achieves temporally coherent, style-faithful, and content-preserving video translations, outperforming existing baselines both qualitatively and quantitatively.",
        "arxiv_id": "2510.07546",
        "ARXIVID": "2510.07546",
        "COMMENT": "No criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2510.07723": {
        "authors": [
            "Wenyue Chen",
            "Peng Li",
            "Wangguandong Zheng",
            "Chengfeng Zhao",
            "Mengfei Li",
            "Yaolong Zhu",
            "Zhiyang Dou",
            "Ronggang Wang",
            "Yuan Liu"
        ],
        "title": "SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view Human Reconstruction",
        "abstract": "arXiv:2510.07723v1 Announce Type: new  Abstract: Photorealistic 3D full-body human reconstruction from a single image is a critical yet challenging task for applications in films and video games due to inherent ambiguities and severe self-occlusions. While recent approaches leverage SMPL estimation and SMPL-conditioned image generative models to hallucinate novel views, they suffer from inaccurate 3D priors estimated from SMPL meshes and have difficulty in handling difficult human poses and reconstructing fine details. In this paper, we propose SyncHuman, a novel framework that combines 2D multiview generative model and 3D native generative model for the first time, enabling high-quality clothed human mesh reconstruction from single-view images even under challenging human poses. Multiview generative model excels at capturing fine 2D details but struggles with structural consistency, whereas 3D native generative model generates coarse yet structurally consistent 3D shapes. By integrating the complementary strengths of these two approaches, we develop a more effective generation framework. Specifically, we first jointly fine-tune the multiview generative model and the 3D native generative model with proposed pixel-aligned 2D-3D synchronization attention to produce geometrically aligned 3D shapes and 2D multiview images. To further improve details, we introduce a feature injection mechanism that lifts fine details from 2D multiview images onto the aligned 3D shapes, enabling accurate and high-fidelity reconstruction. Extensive experiments demonstrate that SyncHuman achieves robust and photo-realistic 3D human reconstruction, even for images with challenging poses. Our method outperforms baseline methods in geometric accuracy and visual fidelity, demonstrating a promising direction for future 3D generation models.",
        "arxiv_id": "2510.07723",
        "ARXIVID": "2510.07723",
        "COMMENT": "No criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}