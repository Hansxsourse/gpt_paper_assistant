{
    "2505.21079": {
        "authors": [
            "Yue Zhang",
            "Yingzhao Jian",
            "Hehe Fan",
            "Yi Yang",
            "Roger Zimmermann"
        ],
        "title": "Uni3D-MoE: Scalable Multimodal 3D Scene Understanding via Mixture of Experts",
        "abstract": "arXiv:2505.21079v1 Announce Type: new  Abstract: Recent advancements in multimodal large language models (MLLMs) have demonstrated considerable potential for comprehensive 3D scene understanding. However, existing approaches typically utilize only one or a limited subset of 3D modalities, resulting in incomplete representations of 3D scenes and reduced interpretive accuracy. Furthermore, different types of queries inherently depend on distinct modalities, indicating that uniform processing of all modality tokens may fail to effectively capture query-specific context. To address these challenges, we propose Uni3D-MoE, a sparse Mixture-of-Experts (MoE)-based 3D MLLM designed to enable adaptive 3D multimodal fusion. Specifically, Uni3D-MoE integrates a comprehensive set of 3D modalities, including multi-view RGB and depth images, bird's-eye-view (BEV) maps, point clouds, and voxel representations. At its core, our framework employs a learnable routing mechanism within the sparse MoE-based large language model, dynamically selecting appropriate experts at the token level. Each expert specializes in processing multimodal tokens based on learned modality preferences, thus facilitating flexible collaboration tailored to diverse task-specific requirements. Extensive evaluations on standard 3D scene understanding benchmarks and specialized datasets demonstrate the efficacy of Uni3D-MoE.",
        "arxiv_id": "2505.21079",
        "ARXIVID": "2505.21079",
        "COMMENT": "Matches criterion 2 and 4 as it proposes a scalable multimodal 3D scene understanding framework using a mixture of experts.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2505.21457": {
        "authors": [
            "Muzhi Zhu",
            "Hao Zhong",
            "Canyu Zhao",
            "Zongze Du",
            "Zheng Huang",
            "Mingyu Liu",
            "Hao Chen",
            "Cheng Zou",
            "Jingdong Chen",
            "Ming Yang",
            "Chunhua Shen"
        ],
        "title": "Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO",
        "abstract": "arXiv:2505.21457v1 Announce Type: new  Abstract: Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimodal Large Language Models (MLLMs) as central planning and decision-making modules in robotic systems has gained extensive attention. However, despite the importance of active perception in embodied intelligence, there is little to no exploration of how MLLMs can be equipped with or learn active perception capabilities. In this paper, we first provide a systematic definition of MLLM-based active perception tasks. We point out that the recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a special case of active perception; however, it still suffers from low search efficiency and inaccurate region selection. To address these issues, we propose ACTIVE-O3, a purely reinforcement learning based training framework built on top of GRPO, designed to equip MLLMs with active perception capabilities. We further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across both general open-world tasks, such as small-object and dense object grounding, and domain-specific scenarios, including small object detection in remote sensing and autonomous driving, as well as fine-grained interactive segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot reasoning abilities on the V* Benchmark, without relying on any explicit reasoning data. We hope that our work can provide a simple codebase and evaluation protocol to facilitate future research on active perception in MLLMs.",
        "arxiv_id": "2505.21457",
        "ARXIVID": "2505.21457",
        "COMMENT": "Matches criterion 2 and 3 as it introduces a new MLLM framework with active perception capabilities and establishes a benchmark for evaluation.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2505.21076": {
        "authors": [
            "Weihao Xuan",
            "Junjue Wang",
            "Heli Qi",
            "Zihang Chen",
            "Zhuo Zheng",
            "Yanfei Zhong",
            "Junshi Xia",
            "Naoto Yokoya"
        ],
        "title": "DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic City Understanding",
        "abstract": "arXiv:2505.21076v1 Announce Type: new  Abstract: Multimodal large language models have demonstrated remarkable capabilities in visual understanding, but their application to long-term Earth observation analysis remains limited, primarily focusing on single-temporal or bi-temporal imagery. To address this gap, we introduce DVL-Suite, a comprehensive framework for analyzing long-term urban dynamics through remote sensing imagery. Our suite comprises 15,063 high-resolution (1.0m) multi-temporal images spanning 42 megacities in the U.S. from 2005 to 2023, organized into two components: DVL-Bench and DVL-Instruct. The DVL-Bench includes seven urban understanding tasks, from fundamental change detection (pixel-level) to quantitative analyses (regional-level) and comprehensive urban narratives (scene-level), capturing diverse urban dynamics including expansion/transformation patterns, disaster assessment, and environmental challenges. We evaluate 17 state-of-the-art multimodal large language models and reveal their limitations in long-term temporal understanding and quantitative analysis. These challenges motivate the creation of DVL-Instruct, a specialized instruction-tuning dataset designed to enhance models' capabilities in multi-temporal Earth observation. Building upon this dataset, we develop DVLChat, a baseline model capable of both image-level question-answering and pixel-level segmentation, facilitating a comprehensive understanding of city dynamics through language interactions.",
        "arxiv_id": "2505.21076",
        "ARXIVID": "2505.21076",
        "COMMENT": "Matches criterion 3 and 4 as it introduces a new benchmark (DVL-Suite) for multimodal large language models in dynamic city understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2505.20759": {
        "authors": [
            "Ansel Blume",
            "Jeonghwan Kim",
            "Hyeonjeong Ha",
            "Elen Chatikyan",
            "Xiaomeng Jin",
            "Khanh Duy Nguyen",
            "Nanyun Peng",
            "Kai-Wei Chang",
            "Derek Hoiem",
            "Heng Ji"
        ],
        "title": "PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding",
        "abstract": "arXiv:2505.20759v1 Announce Type: new  Abstract: Real-world objects are composed of distinctive, object-specific parts. Identifying these parts is key to performing fine-grained, compositional reasoning-yet, large multimodal models (LMMs) struggle to perform this seemingly straightforward task. In this work, we introduce PARTONOMY, an LMM benchmark designed for pixel-level part grounding. We construct PARTONOMY from existing part datasets and our own rigorously annotated set of images, encompassing 862 part labels and 534 object labels for evaluation. Unlike existing datasets that simply ask models to identify generic parts, PARTONOMY uses specialized concepts (e.g., agricultural airplane), and challenges models to compare objects' parts, consider part-whole relationships, and justify textual predictions with visual segmentations. Our experiments demonstrate significant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only 5.9% gIoU), highlighting a critical gap in their part grounding abilities. We note that existing segmentation-enabled LMMs (segmenting LMMs) have two key architectural shortcomings: they use special [SEG] tokens not seen during pretraining which induce distribution shift, and they discard predicted segmentations instead of using past predictions to guide future ones. To address these deficiencies, we train several part-centric LMMs and propose PLUM, a novel segmenting LMM that uses span tagging instead of segmentation tokens and that conditions on prior predictions in a feedback loop. We find that pretrained PLUM outperforms existing segmenting LMMs on reasoning segmentation, VQA, and visual hallucination benchmarks. In addition, PLUM finetuned on our proposed Explanatory Part Segmentation task is competitive with segmenting LMMs trained on significantly more segmentation data. Our work opens up new avenues towards enabling fine-grained, grounded visual understanding in LMMs.",
        "arxiv_id": "2505.20759",
        "ARXIVID": "2505.20759",
        "COMMENT": "Matches criterion 2 as it introduces a new benchmark (PARTONOMY) and proposes a novel LMM (PLUM) for part-level visual understanding.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2505.20640": {
        "authors": [
            "Yifan Li",
            "Yuhang Chen",
            "Anh Dao",
            "Lichi Li",
            "Zhongyi Cai",
            "Zhen Tan",
            "Tianlong Chen",
            "Yu Kong"
        ],
        "title": "IndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios",
        "abstract": "arXiv:2505.20640v1 Announce Type: new  Abstract: Existing Embodied Question Answering (EQA) benchmarks primarily focus on household environments, often overlooking safety-critical aspects and reasoning processes pertinent to industrial settings. This drawback limits the evaluation of agent readiness for real-world industrial applications. To bridge this, we introduce IndustryEQA, the first benchmark dedicated to evaluating embodied agent capabilities within safety-critical warehouse scenarios. Built upon the NVIDIA Isaac Sim platform, IndustryEQA provides high-fidelity episodic memory videos featuring diverse industrial assets, dynamic human agents, and carefully designed hazardous situations inspired by real-world safety guidelines. The benchmark includes rich annotations covering six categories: equipment safety, human safety, object recognition, attribute recognition, temporal understanding, and spatial understanding. Besides, it also provides extra reasoning evaluation based on these categories. Specifically, it comprises 971 question-answer pairs generated from small warehouse and 373 pairs from large ones, incorporating scenarios with and without human. We further propose a comprehensive evaluation framework, including various baseline models, to assess their general perception and reasoning abilities in industrial environments. IndustryEQA aims to steer EQA research towards developing more robust, safety-aware, and practically applicable embodied agents for complex industrial environments. Benchmark and codes are available.",
        "arxiv_id": "2505.20640",
        "ARXIVID": "2505.20640",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for embodied question answering in industrial scenarios, focusing on safety-critical and spatial reasoning aspects.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2505.21375": {
        "authors": [
            "Fengxiang Wang",
            "Mingshuo Chen",
            "Yueying Li",
            "Di Wang",
            "Haotian Wang",
            "Zonghao Guo",
            "Zefan Wang",
            "Boqi Shan",
            "Long Lan",
            "Yulin Wang",
            "Hongzhen Wang",
            "Wenjing Yang",
            "Bo Du",
            "Jing Zhang"
        ],
        "title": "GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution",
        "abstract": "arXiv:2505.21375v1 Announce Type: new  Abstract: Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data for Earth observation but pose challenges for existing multimodal foundation models due to two key bottlenecks: (1) limited availability of UHR training data, and (2) token explosion caused by the large image size. To address data scarcity, we introduce SuperRS-VQA (avg. 8,376$\\times$8,376) and HighRS-VQA (avg. 2,000$\\times$1,912), the highest-resolution vision-language datasets in RS to date, covering 22 real-world dialogue tasks. To mitigate token explosion, our pilot studies reveal significant redundancy in RS images: crucial information is concentrated in a small subset of object-centric tokens, while pruning background tokens (e.g., ocean or forest) can even improve performance. Motivated by these findings, we propose two strategies: Background Token Pruning and Anchored Token Selection, to reduce the memory footprint while preserving key semantics.Integrating these techniques, we introduce GeoLLaVA-8K, the first RS-focused multimodal large language model capable of handling inputs up to 8K$\\times$8K resolution, built on the LLaVA framework. Trained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art on the XLRS-Bench.",
        "arxiv_id": "2505.21375",
        "ARXIVID": "2505.21375",
        "COMMENT": "Matches criterion 2 as it introduces GeoLLaVA-8K, a multimodal large language model for ultra-high-resolution remote sensing, addressing token explosion and data scarcity.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2505.20753": {
        "authors": [
            "Yufei Zhan",
            "Hongyin Zhao",
            "Yousong Zhu",
            "Shurong Zheng",
            "Fan Yang",
            "Ming Tang",
            "Jinqiao Wang"
        ],
        "title": "Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models",
        "abstract": "arXiv:2505.20753v1 Announce Type: new  Abstract: Large Multimodal Models (LMMs) have recently demonstrated remarkable visual understanding performance on both vision-language and vision-centric tasks. However, they often fall short in integrating advanced, task-specific capabilities for compositional reasoning, which hinders their progress toward truly competent general vision models. To address this, we present a unified visual reasoning mechanism that enables LMMs to solve complicated compositional problems by leveraging their intrinsic capabilities (e.g. grounding and visual understanding capabilities). Different from the previous shortcut learning mechanism, our approach introduces a human-like understanding-thinking-answering process, allowing the model to complete all steps in a single pass forwarding without the need for multiple inferences or external tools. This design bridges the gap between foundational visual capabilities and general question answering, encouraging LMMs to generate faithful and traceable responses for complex visual reasoning. Meanwhile, we curate 334K visual instruction samples covering both general scenes and text-rich scenes and involving multiple foundational visual capabilities. Our trained model, Griffon-R, has the ability of end-to-end automatic understanding, self-thinking, and reasoning answers. Comprehensive experiments show that Griffon-R not only achieves advancing performance on complex visual reasoning benchmarks including VSR and CLEVR, but also enhances multimodal capabilities across various benchmarks like MMBench and ScienceQA. Data, models, and codes will be release at https://github.com/jefferyZhan/Griffon/tree/master/Griffon-R soon.",
        "arxiv_id": "2505.20753",
        "ARXIVID": "2505.20753",
        "COMMENT": "Matches criterion 2 as it introduces a new LMM (Griffon-R) with advanced visual reasoning capabilities.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2505.20414": {
        "authors": [
            "Royden Wagner",
            "Omer Sahin Tas",
            "Felix Hauser",
            "Marlon Steiner",
            "Dominik Strutz",
            "Abhishek Vivekanandan",
            "Carlos Fernandez",
            "Christoph Stiller"
        ],
        "title": "RetroMotion: Retrocausal Motion Forecasting Models are Instructable",
        "abstract": "arXiv:2505.20414v1 Announce Type: new  Abstract: Motion forecasts of road users (i.e., agents) vary in complexity as a function of scene constraints and interactive behavior. We address this with a multi-task learning method for motion forecasting that includes a retrocausal flow of information. The corresponding tasks are to forecast (1) marginal trajectory distributions for all modeled agents and (2) joint trajectory distributions for interacting agents. Using a transformer model, we generate the joint distributions by re-encoding marginal distributions followed by pairwise modeling. This incorporates a retrocausal flow of information from later points in marginal trajectories to earlier points in joint trajectories. Per trajectory point, we model positional uncertainty using compressed exponential power distributions. Notably, our method achieves state-of-the-art results in the Waymo Interaction Prediction dataset and generalizes well to the Argoverse 2 dataset. Additionally, our method provides an interface for issuing instructions through trajectory modifications. Our experiments show that regular training of motion forecasting leads to the ability to follow goal-based instructions and to adapt basic directional instructions to the scene context. Code: https://github.com/kit-mrt/future-motion",
        "arxiv_id": "2505.20414",
        "ARXIVID": "2505.20414",
        "COMMENT": "Matches criterion 3 as it introduces a novel multi-task learning method for motion forecasting with retrocausal flow, which is relevant to embodied AI benchmarks and methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.21200": {
        "authors": [
            "Xudong Tan",
            "Yaoxin Yang",
            "Peng Ye",
            "Jialin Zheng",
            "Bizhe Bai",
            "Xinyi Wang",
            "Jia Hao",
            "Tao Chen"
        ],
        "title": "Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models",
        "abstract": "arXiv:2505.21200v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models have emerged as a powerful paradigm for general-purpose robot control through natural language instructions. However, their high inference cost-stemming from large-scale token computation and autoregressive decoding-poses significant challenges for real-time deployment and edge applications. While prior work has primarily focused on architectural optimization, we take a different perspective by identifying a dual form of redundancy in VLA models: (i) high similarity across consecutive action steps, and (ii) substantial redundancy in visual tokens. Motivated by these observations, we propose FlashVLA, the first training-free and plug-and-play acceleration framework that enables action reuse in VLA models. FlashVLA improves inference efficiency through a token-aware action reuse mechanism that avoids redundant decoding across stable action steps, and an information-guided visual token selection strategy that prunes low-contribution tokens. Extensive experiments on the LIBERO benchmark show that FlashVLA reduces FLOPs by 55.7% and latency by 36.0%, with only a 0.7% drop in task success rate. These results demonstrate the effectiveness of FlashVLA in enabling lightweight, low-latency VLA inference without retraining.",
        "arxiv_id": "2505.21200",
        "ARXIVID": "2505.21200",
        "COMMENT": "Matches criterion 1 and 3 as it introduces a novel method for efficient inference in Vision-Language-Action models and evaluates it on a benchmark (LIBERO).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.20897": {
        "authors": [
            "Pingrui Zhang",
            "Yifei Su",
            "Pengyuan Wu",
            "Dong An",
            "Li Zhang",
            "Zhigang Wang",
            "Dong Wang",
            "Yan Ding",
            "Bin Zhao",
            "Xuelong Li"
        ],
        "title": "Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation",
        "abstract": "arXiv:2505.20897v1 Announce Type: new  Abstract: Vision-and-Language Navigation (VLN) requires the agent to navigate by following natural instructions under partial observability, making it difficult to align perception with language. Recent methods mitigate this by imagining future scenes, yet they rely on vision-based synthesis, leading to high computational cost and redundant details. To this end, we propose to adaptively imagine key environmental semantics via \\textit{language} form, enabling a more reliable and efficient strategy. Specifically, we introduce a novel Adaptive Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a large language model (LLM). ATD is designed with a human-like left-right brain architecture, where the left brain focuses on logical integration, and the right brain is responsible for imaginative prediction of future scenes. To achieve this, we fine-tune only the Q-former within both brains to efficiently activate domain-specific knowledge in the LLM, enabling dynamic updates of logical reasoning and imagination during navigation. Furthermore, we introduce a cross-interaction mechanism to regularize the imagined outputs and inject them into a navigation expert module, allowing ATD to jointly exploit both the reasoning capacity of the LLM and the expertise of the navigation model. We conduct extensive experiments on the R2R benchmark, where ATD achieves state-of-the-art performance with fewer parameters. The code is \\href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}.",
        "arxiv_id": "2505.20897",
        "ARXIVID": "2505.20897",
        "COMMENT": "Matches criterion 1 and 3 as it introduces a novel method for spatial understanding in Vision-and-Language Navigation using a dual-branch imagination policy.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.21050": {
        "authors": [
            "Xin Yang",
            "Jiantao Lin",
            "Yingjie Xu",
            "Haodong Li",
            "Yingcong Chen"
        ],
        "title": "Advancing high-fidelity 3D and Texture Generation with 2.5D latents",
        "abstract": "arXiv:2505.21050v1 Announce Type: new  Abstract: Despite the availability of large-scale 3D datasets and advancements in 3D generative models, the complexity and uneven quality of 3D geometry and texture data continue to hinder the performance of 3D generation techniques. In most existing approaches, 3D geometry and texture are generated in separate stages using different models and non-unified representations, frequently leading to unsatisfactory coherence between geometry and texture. To address these challenges, we propose a novel framework for joint generation of 3D geometry and texture. Specifically, we focus in generate a versatile 2.5D representations that can be seamlessly transformed between 2D and 3D. Our approach begins by integrating multiview RGB, normal, and coordinate images into a unified representation, termed as 2.5D latents. Next, we adapt pre-trained 2D foundation models for high-fidelity 2.5D generation, utilizing both text and image conditions. Finally, we introduce a lightweight 2.5D-to-3D refiner-decoder framework that efficiently generates detailed 3D representations from 2.5D images. Extensive experiments demonstrate that our model not only excels in generating high-quality 3D objects with coherent structure and color from text and image inputs but also significantly outperforms existing methods in geometry-conditioned texture generation.",
        "arxiv_id": "2505.21050",
        "ARXIVID": "2505.21050",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models and their application to 3D generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.21233": {
        "authors": [
            "Jiawei Guo",
            "Feifei Zhai",
            "Pu Jian",
            "Qianrun Wei",
            "Yu Zhou"
        ],
        "title": "CROP: Contextual Region-Oriented Visual Token Pruning",
        "abstract": "arXiv:2505.21233v1 Announce Type: new  Abstract: Current VLM-based VQA methods often process entire images, leading to excessive visual tokens that include redundant information irrelevant to the posed question. This abundance of unnecessary image details creates numerous visual tokens, drastically increasing memory and computational requirements in VLMs. To address this, we propose Contextual Region-Oriented Visual Token Pruning (CROP), a novel framework to compress visual tokens through a two-step process: Localization and Pruning. Specifically, CROP first employs an efficient model to identify the contextual region relevant to the input query. Subsequently, two distinct strategies are introduced for pruning: (1) Pre-LLM Compression (PLC), which adaptively compresses different image regions with varying ratios, and (2) Inner-LLM Pruning (ILP), a training-free method that prunes tokens within early LLM layers guided by the identified contextual region. Extensive experiments on a wide range of VQA tasks demonstrate that CROP significantly outperforms existing visual token pruning methods and achieves state-of-the-art performance. Our code and datasets will be made available.",
        "arxiv_id": "2505.21233",
        "ARXIVID": "2505.21233",
        "COMMENT": "Matches criterion 2 as it proposes a novel visual token pruning framework for VLM-based VQA tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.21089": {
        "authors": [
            "Junjue Wang",
            "Weihao Xuan",
            "Heli Qi",
            "Zhihao Liu",
            "Kunyi Liu",
            "Yuhan Wu",
            "Hongruixuan Chen",
            "Jian Song",
            "Junshi Xia",
            "Zhuo Zheng",
            "Naoto Yokoya"
        ],
        "title": "DisasterM3: A Remote Sensing Vision-Language Dataset for Disaster Damage Assessment and Response",
        "abstract": "arXiv:2505.21089v1 Announce Type: new  Abstract: Large vision-language models (VLMs) have made great achievements in Earth vision. However, complex disaster scenes with diverse disaster types, geographic regions, and satellite sensors have posed new challenges for VLM applications. To fill this gap, we curate a remote sensing vision-language dataset (DisasterM3) for global-scale disaster assessment and response. DisasterM3 includes 26,988 bi-temporal satellite images and 123k instruction pairs across 5 continents, with three characteristics: 1) Multi-hazard: DisasterM3 involves 36 historical disaster events with significant impacts, which are categorized into 10 common natural and man-made disasters. 2)Multi-sensor: Extreme weather during disasters often hinders optical sensor imaging, making it necessary to combine Synthetic Aperture Radar (SAR) imagery for post-disaster scenes. 3) Multi-task: Based on real-world scenarios, DisasterM3 includes 9 disaster-related visual perception and reasoning tasks, harnessing the full potential of VLM's reasoning ability with progressing from disaster-bearing body recognition to structural damage assessment and object relational reasoning, culminating in the generation of long-form disaster reports. We extensively evaluated 14 generic and remote sensing VLMs on our benchmark, revealing that state-of-the-art models struggle with the disaster tasks, largely due to the lack of a disaster-specific corpus, cross-sensor gap, and damage object counting insensitivity. Focusing on these issues, we fine-tune four VLMs using our dataset and achieve stable improvements across all tasks, with robust cross-sensor and cross-disaster generalization capabilities.",
        "arxiv_id": "2505.21089",
        "ARXIVID": "2505.21089",
        "COMMENT": "Matches criterion 3 as it introduces DisasterM3, a remote sensing vision-language dataset for disaster assessment, focusing on novel multi-hazard, multi-sensor, and multi-task challenges.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.20680": {
        "authors": [
            "Haodong Lu",
            "Xinyu Zhang",
            "Kristen Moore",
            "Jason Xue",
            "Lina Yao",
            "Anton van den Hengel",
            "Dong Gong"
        ],
        "title": "Continual Learning on CLIP via Incremental Prompt Tuning with Intrinsic Textual Anchors",
        "abstract": "arXiv:2505.20680v1 Announce Type: new  Abstract: Continual learning (CL) enables deep networks to acquire new knowledge while avoiding catastrophic forgetting. The powerful generalization ability of pre-trained models (PTMs), such as the Contrastive Language-Image Pre-training (CLIP) model, has inspired a range of CL methods targeting new and specialized tasks, providing rich multi-modal embeddings that support lightweight, incremental prompt tuning. Existing methods often rely on complex designs built upon specific assumptions, such as intricate regularization schemes for prompt pools, specialized routing mechanisms, or multi-stage incrementations, that introduce additional-and possibly unnecessary-complexity, underutilizing CLIP's intrinsic capabilities. In this paper, we propose a concise CL approach for CLIP based on incremental prompt tuning that fully exploits its multi-modal structure and the stability of textual representations. Our method, Textual Prototype-guided Prompt Tuning (TPPT), introduces textual prototypes not merely as static classifiers, as in existing methods, but as stable anchors to guide the learning of visual prompts, thereby shaping the embedding space (i.e., TPPT-V). We show that our bidirectional supervision strategy enables more effective learning of new knowledge while reducing forgetting. To further close the vision-language gap during CL, we jointly optimizes visual and textual prompts (i.e., TPPT-VT). We also introduce a relational diversity regularization on the textual anchors to prevent embedding space collapse and mitigate correlated forgetting. Extensive experiments and analyses demonstrate the effectiveness of our proposed approach, highlighting the benefits of leveraging CLIP's intrinsic guidance for continual adaptation.",
        "arxiv_id": "2505.20680",
        "ARXIVID": "2505.20680",
        "COMMENT": "Matches criterion 4 as it focuses on continual learning for vision foundation models (CLIP) and introduces novel prompt tuning methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.21488": {
        "authors": [
            "Omer Dahary",
            "Yehonathan Cohen",
            "Or Patashnik",
            "Kfir Aberman",
            "Daniel Cohen-Or"
        ],
        "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation",
        "abstract": "arXiv:2505.21488v1 Announce Type: new  Abstract: Generating multiple distinct subjects remains a challenge for existing text-to-image diffusion models. Complex prompts often lead to subject leakage, causing inaccuracies in quantities, attributes, and visual features. Preventing leakage among subjects necessitates knowledge of each subject's spatial location. Recent methods provide these spatial locations via an external layout control. However, enforcing such a prescribed layout often conflicts with the innate layout dictated by the sampled initial noise, leading to misalignment with the model's prior. In this work, we introduce a new approach that predicts a spatial layout aligned with the prompt, derived from the initial noise, and refines it throughout the denoising process. By relying on this noise-induced layout, we avoid conflicts with externally imposed layouts and better preserve the model's prior. Our method employs a small neural network to predict and refine the evolving noise-induced layout at each denoising step, ensuring clear boundaries between subjects while maintaining consistency. Experimental results show that this noise-aligned strategy achieves improved text-image alignment and more stable multi-subject generation compared to existing layout-guided techniques, while preserving the rich diversity of the model's original distribution.",
        "arxiv_id": "2505.21488",
        "ARXIVID": "2505.21488",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for spatial understanding in text-to-image diffusion models by predicting and refining noise-induced layouts.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.20718": {
        "authors": [
            "Kui Wu",
            "Shuhang Xu",
            "Hao Chen",
            "Churan Wang",
            "Zhoujun Li",
            "Yizhou Wang",
            "Fangwei Zhong"
        ],
        "title": "VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with Self-Improving Visual-Language Models",
        "abstract": "arXiv:2505.20718v1 Announce Type: new  Abstract: We introduce a novel self-improving framework that enhances Embodied Visual Tracking (EVT) with Visual-Language Models (VLMs) to address the limitations of current active visual tracking systems in recovering from tracking failure. Our approach combines the off-the-shelf active tracking methods with VLMs' reasoning capabilities, deploying a fast visual policy for normal tracking and activating VLM reasoning only upon failure detection. The framework features a memory-augmented self-reflection mechanism that enables the VLM to progressively improve by learning from past experiences, effectively addressing VLMs' limitations in 3D spatial reasoning. Experimental results demonstrate significant performance improvements, with our framework boosting success rates by $72\\%$ with state-of-the-art RL-based approaches and $220\\%$ with PID-based methods in challenging environments. This work represents the first integration of VLM-based reasoning to assist EVT agents in proactive failure recovery, offering substantial advances for real-world robotic applications that require continuous target monitoring in dynamic, unstructured environments. Project website: https://sites.google.com/view/evt-recovery-assistant.",
        "arxiv_id": "2505.20718",
        "ARXIVID": "2505.20718",
        "COMMENT": "Matches criterion 3 as it introduces a novel framework for embodied visual tracking using VLMs for failure recovery.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.20460": {
        "authors": [
            "Ruqi Wu",
            "Xinjie Wang",
            "Liu Liu",
            "Chunle Guo",
            "Jiaxiong Qiu",
            "Chongyi Li",
            "Lichao Huang",
            "Zhizhong Su",
            "Ming-Ming Cheng"
        ],
        "title": "DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data",
        "abstract": "arXiv:2505.20460v1 Announce Type: new  Abstract: We present DIPO, a novel framework for the controllable generation of articulated 3D objects from a pair of images: one depicting the object in a resting state and the other in an articulated state. Compared to the single-image approach, our dual-image input imposes only a modest overhead for data collection, but at the same time provides important motion information, which is a reliable guide for predicting kinematic relationships between parts. Specifically, we propose a dual-image diffusion model that captures relationships between the image pair to generate part layouts and joint parameters. In addition, we introduce a Chain-of-Thought (CoT) based graph reasoner that explicitly infers part connectivity relationships. To further improve robustness and generalization on complex articulated objects, we develop a fully automated dataset expansion pipeline, name LEGO-Art, that enriches the diversity and complexity of PartNet-Mobility dataset. We propose PM-X, a large-scale dataset of complex articulated 3D objects, accompanied by rendered images, URDF annotations, and textual descriptions. Extensive experiments demonstrate that DIPO significantly outperforms existing baselines in both the resting state and the articulated state, while the proposed PM-X dataset further enhances generalization to diverse and structurally complex articulated objects. Our code and dataset will be released to the community upon publication.",
        "arxiv_id": "2505.20460",
        "ARXIVID": "2505.20460",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework (DIPO) for articulated object generation and introduces a new dataset (PM-X).",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.20967": {
        "authors": [
            "Jiarui Zhang",
            "Zhihao Li",
            "Chong Wang",
            "Bihan Wen"
        ],
        "title": "RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes",
        "abstract": "arXiv:2505.20967v1 Announce Type: new  Abstract: Neural fields (NFs) have demonstrated remarkable performance in scene reconstruction, powering various tasks such as novel view synthesis. However, existing NF methods relying on RGB or LiDAR inputs often exhibit severe fragility to adverse weather, particularly when applied in outdoor scenarios like autonomous driving. In contrast, millimeter-wave radar is inherently robust to environmental changes, while unfortunately, its integration with NFs remains largely underexplored. Besides, as outdoor driving scenarios frequently involve moving objects, making spatiotemporal modeling essential for temporally consistent novel view synthesis. To this end, we introduce RF4D, a radar-based neural field framework specifically designed for novel view synthesis in outdoor dynamic scenes. RF4D explicitly incorporates temporal information into its representation, significantly enhancing its capability to model moving objects. We further introduce a feature-level flow module that predicts latent temporal offsets between adjacent frames, enforcing temporal coherence in dynamic scene modeling. Moreover, we propose a radar-specific power rendering formulation closely aligned with radar sensing physics, improving synthesis accuracy and interoperability. Extensive experiments on public radar datasets demonstrate the superior performance of RF4D in terms of radar measurement synthesis quality and occupancy estimation accuracy, achieving especially pronounced improvements in dynamic outdoor scenarios.",
        "arxiv_id": "2505.20967",
        "ARXIVID": "2505.20967",
        "COMMENT": "Matches criterion 1 as it introduces a novel radar-based neural field framework for spatial understanding in dynamic outdoor scenes.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2505.20426": {
        "authors": [
            "Yunlong Tang",
            "Pinxin Liu",
            "Mingqian Feng",
            "Zhangyun Tan",
            "Rui Mao",
            "Chao Huang",
            "Jing Bi",
            "Yunzhong Xiao",
            "Susan Liang",
            "Hang Hua",
            "Ali Vosoughi",
            "Luchuan Song",
            "Zeliang Zhang",
            "Chenliang Xu"
        ],
        "title": "MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness",
        "abstract": "arXiv:2505.20426v1 Announce Type: new  Abstract: Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs' understanding of perspective through 10 carefully crafted tasks across three complementary dimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark comprises 2,711 real-world and synthetic image instances with 5,083 question-answer pairs that probe key capabilities, such as vanishing point perception and counting, perspective type reasoning, line relationship understanding in 3D space, invariance to perspective-preserving transformations, etc. Through a comprehensive evaluation of 43 state-of-the-art MLLMs, we uncover significant limitations: while models demonstrate competence on surface-level perceptual tasks, they struggle with compositional reasoning and maintaining spatial consistency under perturbations. Our analysis further reveals intriguing patterns between model architecture, scale, and perspective capabilities, highlighting both robustness bottlenecks and the benefits of chain-of-thought prompting. MMPerspective establishes a valuable testbed for diagnosing and advancing spatial understanding in vision-language systems. Resources available at: https://yunlong10.github.io/MMPerspective/",
        "arxiv_id": "2505.20426",
        "ARXIVID": "2505.20426",
        "COMMENT": "Matches criterion 2 as it evaluates multimodal large language models (MLLMs) on perspective understanding, which is a novel angle for vision-language models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2505.20941": {
        "authors": [
            "Yaohua Zha",
            "Yanzi Wang",
            "Hang Guo",
            "Jinpeng Wang",
            "Tao Dai",
            "Bin Chen",
            "Zhihao Ouyang",
            "Xue Yuerong",
            "Ke Chen",
            "Shu-Tao Xia"
        ],
        "title": "PMA: Towards Parameter-Efficient Point Cloud Understanding via Point Mamba Adapter",
        "abstract": "arXiv:2505.20941v1 Announce Type: new  Abstract: Applying pre-trained models to assist point cloud understanding has recently become a mainstream paradigm in 3D perception. However, existing application strategies are straightforward, utilizing only the final output of the pre-trained model for various task heads. It neglects the rich complementary information in the intermediate layer, thereby failing to fully unlock the potential of pre-trained models. To overcome this limitation, we propose an orthogonal solution: Point Mamba Adapter (PMA), which constructs an ordered feature sequence from all layers of the pre-trained model and leverages Mamba to fuse all complementary semantics, thereby promoting comprehensive point cloud understanding. Constructing this ordered sequence is non-trivial due to the inherent isotropy of 3D space. Therefore, we further propose a geometry-constrained gate prompt generator (G2PG) shared across different layers, which applies shared geometric constraints to the output gates of the Mamba and dynamically optimizes the spatial order, thus enabling more effective integration of multi-layer information. Extensive experiments conducted on challenging point cloud datasets across various tasks demonstrate that our PMA elevates the capability for point cloud understanding to a new level by fusing diverse complementary intermediate features. Code is available at https://github.com/zyh16143998882/PMA.",
        "arxiv_id": "2505.20941",
        "ARXIVID": "2505.20941",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for spatial understanding in 3D point cloud perception using pre-trained models.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2505.21238": {
        "authors": [
            "Jieyu Yuan",
            "Yujun Li",
            "Yuanlin Zhang",
            "Chunle Guo",
            "Xiongxin Tang",
            "Ruixing Wang",
            "Chongyi Li"
        ],
        "title": "3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics-Based Appearance-Medium Decouplin",
        "abstract": "arXiv:2505.21238v1 Announce Type: new  Abstract: Novel view synthesis for underwater scene reconstruction presents unique challenges due to complex light-media interactions. Optical scattering and absorption in water body bring inhomogeneous medium attenuation interference that disrupts conventional volume rendering assumptions of uniform propagation medium. While 3D Gaussian Splatting (3DGS) offers real-time rendering capabilities, it struggles with underwater inhomogeneous environments where scattering media introduce artifacts and inconsistent appearance. In this study, we propose a physics-based framework that disentangles object appearance from water medium effects through tailored Gaussian modeling. Our approach introduces appearance embeddings, which are explicit medium representations for backscatter and attenuation, enhancing scene consistency. In addition, we propose a distance-guided optimization strategy that leverages pseudo-depth maps as supervision with depth regularization and scale penalty terms to improve geometric fidelity. By integrating the proposed appearance and medium modeling components via an underwater imaging model, our approach achieves both high-quality novel view synthesis and physically accurate scene restoration. Experiments demonstrate our significant improvements in rendering quality and restoration accuracy over existing methods. The project page is available at \\href{https://bilityniu.github.io/3D-UIR}{https://bilityniu.github.io/3D-UIR",
        "arxiv_id": "2505.21238",
        "ARXIVID": "2505.21238",
        "COMMENT": "Matches criterion 3 as it focuses on a novel framework for underwater 3D scene reconstruction, which involves spatial intelligence and new methods for handling complex environments.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2505.20498": {
        "authors": [
            "Dongyu Luo",
            "Kelin Yu",
            "Amir-Hossein Shahidzadeh",
            "Cornelia Ferm\\\"uller",
            "Yiannis Aloimonos"
        ],
        "title": "ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image",
        "abstract": "arXiv:2505.20498v1 Announce Type: new  Abstract: Vision-based tactile sensing has been widely used in perception, reconstruction, and robotic manipulation. However, collecting large-scale tactile data remains costly due to the localized nature of sensor-object interactions and inconsistencies across sensor instances. Existing approaches to scaling tactile data, such as simulation and free-form tactile generation, often suffer from unrealistic output and poor transferability to downstream tasks.To address this, we propose ControlTac, a two-stage controllable framework that generates realistic tactile images conditioned on a single reference tactile image, contact force, and contact position. With those physical priors as control input, ControlTac generates physically plausible and varied tactile images that can be used for effective data augmentation. Through experiments on three downstream tasks, we demonstrate that ControlTac can effectively augment tactile datasets and lead to consistent gains. Our three real-world experiments further validate the practical utility of our approach. Project page: https://dongyuluo.github.io/controltac.",
        "arxiv_id": "2505.20498",
        "ARXIVID": "2505.20498",
        "COMMENT": "Matches criterion 3 as it proposes a novel tactile data augmentation framework for embodied AI applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.21061": {
        "authors": [
            "Fatemeh Pesaran Zadeh",
            "Yoojin Oh",
            "Gunhee Kim"
        ],
        "title": "LPOI: Listwise Preference Optimization for Vision Language Models",
        "abstract": "arXiv:2505.21061v1 Announce Type: new  Abstract: Aligning large VLMs with human preferences is a challenging task, as methods like RLHF and DPO often overfit to textual information or exacerbate hallucinations. Although augmenting negative image samples partially addresses these pitfalls, no prior work has employed listwise preference optimization for VLMs, due to the complexity and cost of constructing listwise image samples. In this work, we propose LPOI, the first object-aware listwise preference optimization developed for reducing hallucinations in VLMs. LPOI identifies and masks a critical object in the image, and then interpolates the masked region between the positive and negative images to form a sequence of incrementally more complete images. The model is trained to rank these images in ascending order of object visibility, effectively reducing hallucinations while retaining visual fidelity. LPOI requires no extra annotations beyond standard pairwise preference data, as it automatically constructs the ranked lists through object masking and interpolation. Comprehensive experiments on MMHalBench, AMBER, and Object HalBench confirm that LPOI outperforms existing preference optimization methods in reducing hallucinations and enhancing VLM performance. We make the code available at https://github.com/fatemehpesaran310/lpoi.",
        "arxiv_id": "2505.21061",
        "ARXIVID": "2505.21061",
        "COMMENT": "Matches criterion 2 as it proposes a novel optimization method for vision-language models to reduce hallucinations.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.20793": {
        "authors": [
            "Juan A. Rodriguez",
            "Haotian Zhang",
            "Abhay Puri",
            "Aarash Feizi",
            "Rishav Pramanik",
            "Pascal Wichmann",
            "Arnab Mondal",
            "Mohammad Reza Samsami",
            "Rabiul Awal",
            "Perouz Taslakian",
            "Spandana Gella",
            "Sai Rajeswar",
            "David Vazquez",
            "Christopher Pal",
            "Marco Pedersoli"
        ],
        "title": "Rendering-Aware Reinforcement Learning for Vector Graphics Generation",
        "abstract": "arXiv:2505.20793v1 Announce Type: new  Abstract: Scalable Vector Graphics (SVG) offer a powerful format for representing visual designs as interpretable code. Recent advances in vision-language models (VLMs) have enabled high-quality SVG generation by framing the problem as a code generation task and leveraging large-scale pretraining. VLMs are particularly suitable for this task as they capture both global semantics and fine-grained visual patterns, while transferring knowledge across vision, natural language, and code domains. However, existing VLM approaches often struggle to produce faithful and efficient SVGs because they never observe the rendered images during training. Although differentiable rendering for autoregressive SVG code generation remains unavailable, rendered outputs can still be compared to original inputs, enabling evaluative feedback suitable for reinforcement learning (RL). We introduce RLRF(Reinforcement Learning from Rendering Feedback), an RL method that enhances SVG generation in autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an input image, the model generates SVG roll-outs that are rendered and compared to the original image to compute a reward. This visual fidelity feedback guides the model toward producing more accurate, efficient, and semantically coherent SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common failure modes and enabling precise, high-quality SVG generation with strong structural understanding and generalization.",
        "arxiv_id": "2505.20793",
        "ARXIVID": "2505.20793",
        "COMMENT": "Matches criterion 2 as it enhances SVG generation using reinforcement learning and vision-language models, which aligns with multi-modal large language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.21357": {
        "authors": [
            "Wenyuan Li",
            "Shunlin Liang",
            "Keyan Chen",
            "Yongzhe Chen",
            "Han Ma",
            "Jianglei Xu",
            "Yichuan Ma",
            "Shikang Guan",
            "Husheng Fang",
            "Zhenwei Shi"
        ],
        "title": "AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping",
        "abstract": "arXiv:2505.21357v1 Announce Type: new  Abstract: Accurate crop mapping fundamentally relies on modeling multi-scale spatiotemporal patterns, where spatial scales range from individual field textures to landscape-level context, and temporal scales capture both short-term phenological transitions and full growing-season dynamics. Transformer-based remote sensing foundation models (RSFMs) offer promising potential for crop mapping due to their innate ability for unified spatiotemporal processing. However, current RSFMs remain suboptimal for crop mapping: they either employ fixed spatiotemporal windows that ignore the multi-scale nature of crop systems or completely disregard temporal information by focusing solely on spatial patterns. To bridge these gaps, we present AgriFM, a multi-source remote sensing foundation model specifically designed for agricultural crop mapping. Our approach begins by establishing the necessity of simultaneous hierarchical spatiotemporal feature extraction, leading to the development of a modified Video Swin Transformer architecture where temporal down-sampling is synchronized with spatial scaling operations. This modified backbone enables efficient unified processing of long time-series satellite inputs. AgriFM leverages temporally rich data streams from three satellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is pre-trained on a global representative dataset comprising over 25 million image samples supervised by land cover products. The resulting framework incorporates a versatile decoder architecture that dynamically fuses these learned spatiotemporal representations, supporting diverse downstream tasks. Comprehensive evaluations demonstrate AgriFM's superior performance over conventional deep learning approaches and state-of-the-art general-purpose RSFMs across all downstream tasks. Codes will be available at urlhttps://github.com/flyakon/AgriFM.",
        "arxiv_id": "2505.21357",
        "ARXIVID": "2505.21357",
        "COMMENT": "Matches criterion 4 as it introduces AgriFM, a vision foundation model for spatiotemporal crop mapping, with novel hierarchical spatiotemporal feature extraction.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.21327": {
        "authors": [
            "Jiakang Yuan",
            "Tianshuo Peng",
            "Yilei Jiang",
            "Yiting Lu",
            "Renrui Zhang",
            "Kaituo Feng",
            "Chaoyou Fu",
            "Tao Chen",
            "Lei Bai",
            "Bo Zhang",
            "Xiangyu Yue"
        ],
        "title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs",
        "abstract": "arXiv:2505.21327v1 Announce Type: new  Abstract: Logical reasoning is a fundamental aspect of human intelligence and an essential capability for multimodal large language models (MLLMs). Despite the significant advancement in multimodal reasoning, existing benchmarks fail to comprehensively evaluate their reasoning abilities due to the lack of explicit categorization for logical reasoning types and an unclear understanding of reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning (i.e., inductive, deductive, and abductive) in its questions. We carefully curate the data to ensure that each question effectively evaluates reasoning ability rather than perceptual skills or knowledge breadth, and extend the evaluation protocols to cover the evaluation of diverse questions. Our evaluation reveals substantial limitations of state-of-the-art MLLMs when subjected to holistic assessments of logical reasoning capabilities. Even the most advanced MLLMs show limited performance in comprehensive logical reasoning, with notable performance imbalances across reasoning types. In addition, we conducted an in-depth analysis of approaches such as ``thinking mode'' and Rule-based RL, which are commonly believed to enhance reasoning abilities. These findings highlight the critical limitations and performance imbalances of current MLLMs in diverse logical reasoning scenarios, providing comprehensive and systematic insights into the understanding and evaluation of reasoning capabilities.",
        "arxiv_id": "2505.21327",
        "ARXIVID": "2505.21327",
        "COMMENT": "Matches criterion 3 as it introduces a comprehensive benchmark (MME-Reasoning) for logical reasoning in MLLMs, focusing on reasoning types often ignored in prior work.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.20740": {
        "authors": [
            "Xiangyu Zhao",
            "Wanghan Xu",
            "Bo Liu",
            "Yuhao Zhou",
            "Fenghua Ling",
            "Ben Fei",
            "Xiaoyu Yue",
            "Lei Bai",
            "Wenlong Zhang",
            "Xiao-Ming Wu"
        ],
        "title": "MSEarth: A Benchmark for Multimodal Scientific Comprehension of Earth Science",
        "abstract": "arXiv:2505.20740v1 Announce Type: new  Abstract: The rapid advancement of multimodal large language models (MLLMs) has unlocked new opportunities to tackle complex scientific challenges. Despite this progress, their application in addressing earth science problems, especially at the graduate level, remains underexplored. A significant barrier is the absence of benchmarks that capture the depth and contextual complexity of geoscientific reasoning. Current benchmarks often rely on synthetic datasets or simplistic figure-caption pairs, which do not adequately reflect the intricate reasoning and domain-specific insights required for real-world scientific applications. To address these gaps, we introduce MSEarth, a multimodal scientific benchmark curated from high-quality, open-access scientific publications. MSEarth encompasses the five major spheres of Earth science: atmosphere, cryosphere, hydrosphere, lithosphere, and biosphere, featuring over 7K figures with refined captions. These captions are crafted from the original figure captions and enriched with discussions and reasoning from the papers, ensuring the benchmark captures the nuanced reasoning and knowledge-intensive content essential for advanced scientific tasks. MSEarth supports a variety of tasks, including scientific figure captioning, multiple choice questions, and open-ended reasoning challenges. By bridging the gap in graduate-level benchmarks, MSEarth provides a scalable and high-fidelity resource to enhance the development and evaluation of MLLMs in scientific reasoning. The benchmark is publicly available to foster further research and innovation in this field. Resources related to this benchmark can be found at https://huggingface.co/MSEarth and https://github.com/xiangyu-mm/MSEarth.",
        "arxiv_id": "2505.20740",
        "ARXIVID": "2505.20740",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (MSEarth) for multimodal scientific reasoning in Earth science, which is a novel angle for MLLMs.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.21501": {
        "authors": [
            "Yinjie Chen",
            "Zipeng Yan",
            "Chong Zhou",
            "Bo Dai",
            "Andrew F. Luo"
        ],
        "title": "Vision Transformers with Self-Distilled Registers",
        "abstract": "arXiv:2505.21501v1 Announce Type: new  Abstract: Vision Transformers (ViTs) have emerged as the dominant architecture for visual processing tasks, demonstrating excellent scalability with increased training data and model size. However, recent work has identified the emergence of artifact tokens in ViTs that are incongruous with the local semantics. These anomalous tokens degrade ViT performance in tasks that require fine-grained localization or structural coherence. An effective mitigation of this issue is to the addition of register tokens to ViTs, which implicitly \"absorb\" the artifact term during training. Given the availability of various large-scale pre-trained ViTs, in this paper we aim at equipping them with such register tokens without the need of re-training them from scratch, which is infeasible considering their size. Specifically, we propose Post Hoc Registers (PH-Reg), an efficient self-distillation method that integrates registers into an existing ViT without requiring additional labeled data and full retraining. PH-Reg initializes both teacher and student networks from the same pre-trained ViT. The teacher remains frozen and unmodified, while the student is augmented with randomly initialized register tokens. By applying test-time augmentation to the teacher's inputs, we generate denoised dense embeddings free of artifacts, which are then used to optimize only a small subset of unlocked student weights. We show that our approach can effectively reduce the number of artifact tokens, improving the segmentation and depth prediction of the student ViT under zero-shot and linear probing.",
        "arxiv_id": "2505.21501",
        "ARXIVID": "2505.21501",
        "COMMENT": "Matches criterion 4 as it proposes a novel method (PH-Reg) to enhance vision transformers, which are foundational models in vision.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2505.20471": {
        "authors": [
            "Chenghao Qian",
            "Wenjing Li",
            "Yuhu Guo",
            "Gustav Markkula"
        ],
        "title": "WeatherEdit: Controllable Weather Editing with 4D Gaussian Field",
        "abstract": "arXiv:2505.20471v1 Announce Type: new  Abstract: In this work, we present WeatherEdit, a novel weather editing pipeline for generating realistic weather effects with controllable types and severity in 3D scenes. Our approach is structured into two key components: weather background editing and weather particle construction. For weather background editing, we introduce an all-in-one adapter that integrates multiple weather styles into a single pretrained diffusion model, enabling the generation of diverse weather effects in 2D image backgrounds. During inference, we design a Temporal-View (TV-) attention mechanism that follows a specific order to aggregate temporal and spatial information, ensuring consistent editing across multi-frame and multi-view images. To construct the weather particles, we first reconstruct a 3D scene using the edited images and then introduce a dynamic 4D Gaussian field to generate snowflakes, raindrops and fog in the scene. The attributes and dynamics of these particles are precisely controlled through physical-based modelling and simulation, ensuring realistic weather representation and flexible severity adjustments. Finally, we integrate the 4D Gaussian field with the 3D scene to render consistent and highly realistic weather effects. Experiments on multiple driving datasets demonstrate that WeatherEdit can generate diverse weather effects with controllable condition severity, highlighting its potential for autonomous driving simulation in adverse weather. See project page: https://jumponthemoon.github.io/w-edit",
        "arxiv_id": "2505.20471",
        "ARXIVID": "2505.20471",
        "COMMENT": "Matches criterion 4 as it introduces a novel method for weather editing in 3D scenes, which is a vision-related application with potential for simulation tasks.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2505.21494": {
        "authors": [
            "Xiaojun Jia",
            "Sensen Gao",
            "Simeng Qin",
            "Tianyu Pang",
            "Chao Du",
            "Yihao Huang",
            "Xinfeng Li",
            "Yiming Li",
            "Bo Li",
            "Yang Liu"
        ],
        "title": "Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment",
        "abstract": "arXiv:2505.21494v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP's [CLS] token-between adversarial and target samples, they often overlook the rich local information encoded in patch tokens. This leads to suboptimal alignment and limited transferability, particularly for closed-source models. To address this limitation, we propose a targeted transferable adversarial attack method based on feature optimal alignment, called FOA-Attack, to improve adversarial transfer capability. Specifically, at the global level, we introduce a global feature loss based on cosine similarity to align the coarse-grained features of adversarial samples with those of target samples. At the local level, given the rich local representations within Transformers, we leverage clustering techniques to extract compact local patterns to alleviate redundant local features. We then formulate local feature alignment between adversarial and target samples as an optimal transport (OT) problem and propose a local clustering optimal transport loss to refine fine-grained feature alignment. Additionally, we propose a dynamic ensemble model weighting strategy to adaptively balance the influence of multiple models during adversarial example generation, thereby further improving transferability. Extensive experiments across various models demonstrate the superiority of the proposed method, outperforming state-of-the-art methods, especially in transferring to closed-source MLLMs. The code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack.",
        "arxiv_id": "2505.21494",
        "ARXIVID": "2505.21494",
        "COMMENT": "Matches criterion 2 as it focuses on adversarial attacks against multi-modal large language models, which is relevant to vision-language models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2505.20764": {
        "authors": [
            "Eric Xing",
            "Pranavi Kolouju",
            "Robert Pless",
            "Abby Stylianou",
            "Nathan Jacobs"
        ],
        "title": "ConText-CIR: Learning from Concepts in Text for Composed Image Retrieval",
        "abstract": "arXiv:2505.20764v1 Announce Type: new  Abstract: Composed image retrieval (CIR) is the task of retrieving a target image specified by a query image and a relative text that describes a semantic modification to the query image. Existing methods in CIR struggle to accurately represent the image and the text modification, resulting in subpar performance. To address this limitation, we introduce a CIR framework, ConText-CIR, trained with a Text Concept-Consistency loss that encourages the representations of noun phrases in the text modification to better attend to the relevant parts of the query image. To support training with this loss function, we also propose a synthetic data generation pipeline that creates training data from existing CIR datasets or unlabeled images. We show that these components together enable stronger performance on CIR tasks, setting a new state-of-the-art in composed image retrieval in both the supervised and zero-shot settings on multiple benchmark datasets, including CIRR and CIRCO. Source code, model checkpoints, and our new datasets are available at https://github.com/mvrl/ConText-CIR.",
        "arxiv_id": "2505.20764",
        "ARXIVID": "2505.20764",
        "COMMENT": "Matches criterion 4 as it focuses on composed image retrieval, which is a vision-related task, and introduces a novel framework with state-of-the-art performance.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2505.20417": {
        "authors": [
            "Meng Cao",
            "Shuyuan Zhang",
            "Xiao-Wen Chang",
            "Doina Precup"
        ],
        "title": "SCAR: Shapley Credit Assignment for More Efficient RLHF",
        "abstract": "arXiv:2505.20417v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) is a widely used technique for aligning Large Language Models (LLMs) with human preferences, yet it often suffers from sparse reward signals, making effective credit assignment challenging. In typical setups, the reward model provides a single scalar score for an entire generated sequence, offering little insight into which token or span-level decisions were responsible for the outcome. To address this, we propose Shapley Credit Assignment Rewards (SCAR), a novel method that leverages Shapley values in cooperative game theory. SCAR distributes the total sequence-level reward among constituent tokens or text spans based on their principled marginal contributions. This creates dense reward signals, crucially, without necessitating the training of auxiliary critique models or recourse to fine-grained human annotations at intermediate generation stages. Unlike prior dense reward methods, SCAR offers a game-theoretic foundation for fair credit attribution. Theoretically, we demonstrate that SCAR preserves the original optimal policy, and empirically, across diverse tasks including sentiment control, text summarization, and instruction tuning, we show that SCAR converges significantly faster and achieves higher final reward scores compared to standard RLHF and attention-based dense reward baselines. Our findings suggest that SCAR provides a more effective and theoretically sound method for credit assignment in RLHF, leading to more efficient alignment of LLMs.",
        "arxiv_id": "2505.20417",
        "ARXIVID": "2505.20417",
        "COMMENT": "Does not match any specific criterion but introduces a novel method for reinforcement learning from human feedback, which is tangentially related to your friend's interest in clever statistical tricks.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.21117": {
        "authors": [
            "Adeela Islam",
            "Stefano Fiorini",
            "Stuart James",
            "Pietro Morerio",
            "Alessio Del Bue"
        ],
        "title": "ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco Reconstruction",
        "abstract": "arXiv:2505.21117v1 Announce Type: new  Abstract: The task of reassembly is a significant challenge across multiple domains, including archaeology, genomics, and molecular docking, requiring the precise placement and orientation of elements to reconstruct an original structure. In this work, we address key limitations in state-of-the-art Deep Learning methods for reassembly, namely i) scalability; ii) multimodality; and iii) real-world applicability: beyond square or simple geometric shapes, realistic and complex erosion, or other real-world problems. We propose ReassembleNet, a method that reduces complexity by representing each input piece as a set of contour keypoints and learning to select the most informative ones by Graph Neural Networks pooling inspired techniques. ReassembleNet effectively lowers computational complexity while enabling the integration of features from multiple modalities, including both geometric and texture data. Further enhanced through pretraining on a semi-synthetic dataset. We then apply diffusion-based pose estimation to recover the original structure. We improve on prior methods by 55% and 86% for RMSE Rotation and Translation, respectively.",
        "arxiv_id": "2505.21117",
        "ARXIVID": "2505.21117",
        "COMMENT": "Does not match any specific criteria but focuses on reassembly tasks using keypoints and diffusion.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.20704": {
        "authors": [
            "Zixuan Hu",
            "Yichun Hu",
            "Xiaotong Li",
            "Shixiang Tang",
            "Ling-Yu Duan"
        ],
        "title": "Beyond Entropy: Region Confidence Proxy for Wild Test-Time Adaptation",
        "abstract": "arXiv:2505.20704v1 Announce Type: new  Abstract: Wild Test-Time Adaptation (WTTA) is proposed to adapt a source model to unseen domains under extreme data scarcity and multiple shifts. Previous approaches mainly focused on sample selection strategies, while overlooking the fundamental problem on underlying optimization. Initially, we critically analyze the widely-adopted entropy minimization framework in WTTA and uncover its significant limitations in noisy optimization dynamics that substantially hinder adaptation efficiency. Through our analysis, we identify region confidence as a superior alternative to traditional entropy, however, its direct optimization remains computationally prohibitive for real-time applications. In this paper, we introduce a novel region-integrated method ReCAP that bypasses the lengthy process. Specifically, we propose a probabilistic region modeling scheme that flexibly captures semantic changes in embedding space. Subsequently, we develop a finite-to-infinite asymptotic approximation that transforms the intractable region confidence into a tractable and upper-bounded proxy. These innovations significantly unlock the overlooked potential dynamics in local region in a concise solution. Our extensive experiments demonstrate the consistent superiority of ReCAP over existing methods across various datasets and wild scenarios.",
        "arxiv_id": "2505.20704",
        "ARXIVID": "2505.20704",
        "COMMENT": "Does not match any specific criteria but focuses on test-time adaptation in wild scenarios.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.20611": {
        "authors": [
            "Zenghao Zheng",
            "Lianping Yang",
            "Jinshan Pan",
            "Hegui Zhu"
        ],
        "title": "Mamba-Driven Topology Fusion for Monocular 3-D Human Pose Estimation",
        "abstract": "arXiv:2505.20611v1 Announce Type: new  Abstract: Transformer-based methods for 3-D human pose estimation face significant computational challenges due to the quadratic growth of self-attention mechanism complexity with sequence length. Recently, the Mamba model has substantially reduced computational overhead and demonstrated outstanding performance in modeling long sequences by leveraging state space model (SSM). However, the ability of SSM to process sequential data is not suitable for 3-D joint sequences with topological structures, and the causal convolution structure in Mamba also lacks insight into local joint relationships. To address these issues, we propose the Mamba-Driven Topology Fusion framework in this paper. Specifically, the proposed Bone Aware Module infers the direction and length of bone vectors in the spherical coordinate system, providing effective topological guidance for the Mamba model in processing joint sequences. Furthermore, we enhance the convolutional structure within the Mamba model by integrating forward and backward graph convolutional network, enabling it to better capture local joint dependencies. Finally, we design a Spatiotemporal Refinement Module to model both temporal and spatial relationships within the sequence. Through the incorporation of skeletal topology, our approach effectively alleviates Mamba's limitations in capturing human structural relationships. We conduct extensive experiments on the Human3.6M and MPI-INF-3DHP datasets for testing and comparison, and the results show that the proposed method greatly reduces computational cost while achieving higher accuracy. Ablation studies further demonstrate the effectiveness of each proposed module. The code and models will be released.",
        "arxiv_id": "2505.20611",
        "ARXIVID": "2505.20611",
        "COMMENT": "Does not match any specific criteria but focuses on 3D human pose estimation with computational improvements.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.21060": {
        "authors": [
            "Peng Wang",
            "Xiang Liu",
            "Peidong Liu"
        ],
        "title": "Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and Styles",
        "abstract": "arXiv:2505.21060v1 Announce Type: new  Abstract: Stylizing 3D scenes instantly while maintaining multi-view consistency and faithfully resembling a style image remains a significant challenge. Current state-of-the-art 3D stylization methods typically involve computationally intensive test-time optimization to transfer artistic features into a pretrained 3D representation, often requiring dense posed input images. In contrast, leveraging recent advances in feed-forward reconstruction models, we demonstrate a novel approach to achieve direct 3D stylization in less than a second using unposed sparse-view scene images and an arbitrary style image. To address the inherent decoupling between reconstruction and stylization, we introduce a branched architecture that separates structure modeling and appearance shading, effectively preventing stylistic transfer from distorting the underlying 3D scene structure. Furthermore, we adapt an identity loss to facilitate pre-training our stylization model through the novel view synthesis task. This strategy also allows our model to retain its original reconstruction capabilities while being fine-tuned for stylization. Comprehensive evaluations, using both in-domain and out-of-domain datasets, demonstrate that our approach produces high-quality stylized 3D content that achieve a superior blend of style and scene appearance, while also outperforming existing methods in terms of multi-view consistency and efficiency.",
        "arxiv_id": "2505.21060",
        "ARXIVID": "2505.21060",
        "COMMENT": "Does not match any specific criteria but is related to 3D scene stylization and reconstruction.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.21258": {
        "authors": [
            "Changguanng Wu",
            "Jiangxin Dong",
            "Chengjian Li",
            "Jinhui Tang"
        ],
        "title": "Plenodium: UnderWater 3D Scene Reconstruction with Plenoptic Medium Representation",
        "abstract": "arXiv:2505.21258v1 Announce Type: new  Abstract: We present Plenodium (plenoptic medium), an effective and efficient 3D representation framework capable of jointly modeling both objects and participating media. In contrast to existing medium representations that rely solely on view-dependent modeling, our novel plenoptic medium representation incorporates both directional and positional information through spherical harmonics encoding, enabling highly accurate underwater scene reconstruction. To address the initialization challenge in degraded underwater environments, we propose the pseudo-depth Gaussian complementation to augment COLMAP-derived point clouds with robust depth priors. In addition, a depth ranking regularized loss is developed to optimize the geometry of the scene and improve the ordinal consistency of the depth maps. Extensive experiments on real-world underwater datasets demonstrate that our method achieves significant improvements in 3D reconstruction. Furthermore, we conduct a simulated dataset with ground truth and the controllable scattering medium to demonstrate the restoration capability of our method in underwater scenarios. Our code and dataset are available at https://plenodium.github.io/.",
        "arxiv_id": "2505.21258",
        "ARXIVID": "2505.21258",
        "COMMENT": "This paper focuses on underwater 3D scene reconstruction and does not match any of the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.21473": {
        "authors": [
            "Yiheng Liu",
            "Liao Qu",
            "Huichao Zhang",
            "Xu Wang",
            "Yi Jiang",
            "Yiming Gao",
            "Hu Ye",
            "Xian Li",
            "Shuai Wang",
            "Daniel K. Du",
            "Shu Cheng",
            "Zehuan Yuan",
            "Xinglong Wu"
        ],
        "title": "DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via Next-Detail Prediction",
        "abstract": "arXiv:2505.21473v1 Announce Type: new  Abstract: This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image generation method that models images through a novel next-detail prediction strategy. By learning a resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process to start from the global structure and incrementally refine details. This coarse-to-fine 1D token sequence aligns well with the autoregressive inference mechanism, providing a more natural and efficient way for the AR model to generate complex visual content. Our compact 1D AR model achieves high-quality image synthesis with significantly fewer tokens than previous approaches, i.e. VAR/VQGAN. We further propose a parallel inference mechanism with self-correction that accelerates generation speed by approximately 8x while reducing accumulation sampling error inherent in teacher-forcing supervision. On the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128 tokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require 680 tokens in their AR models. Moreover, due to the significantly reduced token count and parallel inference mechanism, our method runs nearly 2x faster inference speed compared to VAR and FlexVAR. Extensive experimental results demonstrate DetailFlow's superior generation quality and efficiency compared to existing state-of-the-art methods.",
        "arxiv_id": "2505.21473",
        "ARXIVID": "2505.21473",
        "COMMENT": "This paper focuses on autoregressive image generation and does not match any of the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.20904": {
        "authors": [
            "Guanghu Xie",
            "Yonglong Zhang",
            "Zhiduo Jiang",
            "Yang Liu",
            "Zongwu Xie",
            "Baoshi Cao",
            "Hong Liu"
        ],
        "title": "HTMNet: A Hybrid Network with Transformer-Mamba Bottleneck Multimodal Fusion for Transparent and Reflective Objects Depth Completion",
        "abstract": "arXiv:2505.20904v1 Announce Type: new  Abstract: Transparent and reflective objects pose significant challenges for depth sensors, resulting in incomplete depth information that adversely affects downstream robotic perception and manipulation tasks. To address this issue, we propose HTMNet, a novel hybrid model integrating Transformer, CNN, and Mamba architectures. The encoder is constructed based on a dual-branch Transformer-CNN framework, while the multi-scale fusion module leverages a Transformer-Mamba architecture, which also serves as the foundation for the decoder design. We introduce a novel multimodal fusion module grounded in self-attention mechanisms and state space models, marking the first application of the Mamba architecture in the field of transparent object depth completion and revealing its promising potential. Additionally, we design an innovative multi-scale fusion module for the decoder that combines channel attention, spatial attention, and multi-scale feature extraction techniques to effectively integrate multi-scale features through a down-fusion strategy. Extensive evaluations on multiple public datasets demonstrate that our model achieves state-of-the-art(SOTA) performance, validating the effectiveness of our approach.",
        "arxiv_id": "2505.20904",
        "ARXIVID": "2505.20904",
        "COMMENT": "This paper introduces a novel multimodal fusion method for depth completion, which does not match any of the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.21114": {
        "authors": [
            "Shuai Wang",
            "Zexian Li",
            "Qipeng zhang",
            "Tianhui Song",
            "Xubin Li",
            "Tiezheng Ge",
            "Bo Zheng",
            "Limin Wang"
        ],
        "title": "Differentiable Solver Search for Fast Diffusion Sampling",
        "abstract": "arXiv:2505.21114v1 Announce Type: new  Abstract: Diffusion models have demonstrated remarkable generation quality but at the cost of numerous function evaluations. Recently, advanced ODE-based solvers have been developed to mitigate the substantial computational demands of reverse-diffusion solving under limited sampling steps. However, these solvers, heavily inspired by Adams-like multistep methods, rely solely on t-related Lagrange interpolation. We show that t-related Lagrange interpolation is suboptimal for diffusion model and reveal a compact search space comprised of time steps and solver coefficients. Building on our analysis, we propose a novel differentiable solver search algorithm to identify more optimal solver. Equipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and FlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256 with only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of 2.33 with only 10 steps. Notably, our searched solver outperforms traditional solvers by a significant margin. Moreover, our searched solver demonstrates generality across various model architectures, resolutions, and model sizes.",
        "arxiv_id": "2505.21114",
        "ARXIVID": "2505.21114",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling through diffusion sampling optimization.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2505.21010": {
        "authors": [
            "Sabbir Ahmed",
            "Mamshad Nayeem Rizve",
            "Abdullah Al Arafat",
            "Jacqueline Liu",
            "Rahim Hossain",
            "Mohaiminul Al Nahian",
            "Adnan Siraj Rakin"
        ],
        "title": "Unified Alignment Protocol: Making Sense of the Unlabeled Data in New Domains",
        "abstract": "arXiv:2505.21010v1 Announce Type: new  Abstract: Semi-Supervised Federated Learning (SSFL) is gaining popularity over conventional Federated Learning in many real-world applications. Due to the practical limitation of limited labeled data on the client side, SSFL considers that participating clients train with unlabeled data, and only the central server has the necessary resources to access limited labeled data, making it an ideal fit for real-world applications (e.g., healthcare). However, traditional SSFL assumes that the data distributions in the training phase and testing phase are the same. In practice, however, domain shifts frequently occur, making it essential for SSFL to incorporate generalization capabilities and enhance their practicality. The core challenge is improving model generalization to new, unseen domains while the client participate in SSFL. However, the decentralized setup of SSFL and unsupervised client training necessitates innovation to achieve improved generalization across domains. To achieve this, we propose a novel framework called the Unified Alignment Protocol (UAP), which consists of an alternating two-stage training process. The first stage involves training the server model to learn and align the features with a parametric distribution, which is subsequently communicated to clients without additional communication overhead. The second stage proposes a novel training algorithm that utilizes the server feature distribution to align client features accordingly. Our extensive experiments on standard domain generalization benchmark datasets across multiple model architectures reveal that proposed UAP successfully achieves SOTA generalization performance in SSFL setting.",
        "arxiv_id": "2505.21010",
        "ARXIVID": "2505.21010",
        "COMMENT": "Does not match any specific criteria but is generally relevant to machine learning and federated learning, which might be of tangential interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.21420": {
        "authors": [
            "Jinbao Wang",
            "Hanzhe Liang",
            "Can Gao",
            "Chenxi Hu",
            "Jie Zhou",
            "Yunkang Cao",
            "Linlin Shen",
            "Weiming Shen"
        ],
        "title": "Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via Multi-modality Mentor Learning",
        "abstract": "arXiv:2505.21420v1 Announce Type: new  Abstract: Multimodal feature reconstruction is a promising approach for 3D anomaly detection, leveraging the complementary information from dual modalities. We further advance this paradigm by utilizing multi-modal mentor learning, which fuses intermediate features to further distinguish normal from feature differences. To address these challenges, we propose a novel method called Mentor3AD, which utilizes multi-modal mentor learning. By leveraging the shared features of different modalities, Mentor3AD can extract more effective features and guide feature reconstruction, ultimately improving detection performance. Specifically, Mentor3AD includes a Mentor of Fusion Module (MFM) that merges features extracted from RGB and 3D modalities to create a mentor feature. Additionally, we have designed a Mentor of Guidance Module (MGM) to facilitate cross-modal reconstruction, supported by the mentor feature. Lastly, we introduce a Voting Module (VM) to more accurately generate the final anomaly score. Extensive comparative and ablation studies on MVTec 3D-AD and Eyecandies have verified the effectiveness of the proposed method.",
        "arxiv_id": "2505.21420",
        "ARXIVID": "2505.21420",
        "COMMENT": "Does not match any specific criterion but discusses multi-modal mentor learning for 3D anomaly detection, which is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.21067": {
        "authors": [
            "Xiao Hu",
            "Xingyu Lu",
            "Liyuan Mao",
            "YiFan Zhang",
            "Tianke Zhang",
            "Bin Wen",
            "Fan Yang",
            "Tingting Gao",
            "Guorui Zhou"
        ],
        "title": "Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning",
        "abstract": "arXiv:2505.21067v1 Announce Type: new  Abstract: Reinforcement learning (RL) has played an important role in improving the reasoning ability of large language models (LLMs). Some studies apply RL directly to \\textit{smaller} base models (known as zero-RL) and also achieve notable progress. However, in this paper, we show that using only 920 examples, a simple distillation method based on the base model can clearly outperform zero-RL, which typically requires much more data and computational cost. By analyzing the token frequency in model outputs, we find that the distilled model shows more flexible reasoning. It uses anthropomorphic tokens and logical connectors much more often than the zero-RL model. Further analysis reveals that distillation enhances the presence of two advanced cognitive behaviors: Multi-Perspective Thinking or Attempting and Metacognitive Awareness. Frequent occurrences of these two advanced cognitive behaviors give rise to flexible reasoning, which is essential for solving complex reasoning problems, while zero-RL fails to significantly boost the frequency of these behaviors.",
        "arxiv_id": "2505.21067",
        "ARXIVID": "2505.21067",
        "COMMENT": "Does not match any specific criterion but discusses reasoning in language models, which is tangentially related to your friend's interest in multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.21187": {
        "authors": [
            "Hesam Araghi",
            "Jan van Gemert",
            "Nergis Tomen"
        ],
        "title": "Making Every Event Count: Balancing Data Efficiency and Accuracy in Event Camera Subsampling",
        "abstract": "arXiv:2505.21187v1 Announce Type: new  Abstract: Event cameras offer high temporal resolution and power efficiency, making them well-suited for edge AI applications. However, their high event rates present challenges for data transmission and processing. Subsampling methods provide a practical solution, but their effect on downstream visual tasks remains underexplored. In this work, we systematically evaluate six hardware-friendly subsampling methods using convolutional neural networks for event video classification on various benchmark datasets. We hypothesize that events from high-density regions carry more task-relevant information and are therefore better suited for subsampling. To test this, we introduce a simple causal density-based subsampling method, demonstrating improved classification accuracy in sparse regimes. Our analysis further highlights key factors affecting subsampling performance, including sensitivity to hyperparameters and failure cases in scenarios with large event count variance. These findings provide insights for utilization of hardware-efficient subsampling strategies that balance data efficiency and task accuracy. The code for this paper will be released at: https://github.com/hesamaraghi/event-camera-subsampling-methods.",
        "arxiv_id": "2505.21187",
        "ARXIVID": "2505.21187",
        "COMMENT": "This paper focuses on event camera subsampling and does not match any of the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.20582": {
        "authors": [
            "Yizhou Zhao",
            "Chunjiang Liu",
            "Haoyu Chen",
            "Bhiksha Raj",
            "Min Xu",
            "Tadas Baltrusaitis",
            "Mitch Rundle",
            "HsiangTao Wu",
            "Kamran Ghasedi"
        ],
        "title": "Total-Editing: Head Avatar with Editable Appearance, Motion, and Lighting",
        "abstract": "arXiv:2505.20582v1 Announce Type: new  Abstract: Face reenactment and portrait relighting are essential tasks in portrait editing, yet they are typically addressed independently, without much synergy. Most face reenactment methods prioritize motion control and multiview consistency, while portrait relighting focuses on adjusting shading effects. To take advantage of both geometric consistency and illumination awareness, we introduce Total-Editing, a unified portrait editing framework that enables precise control over appearance, motion, and lighting. Specifically, we design a neural radiance field decoder with intrinsic decomposition capabilities. This allows seamless integration of lighting information from portrait images or HDR environment maps into synthesized portraits. We also incorporate a moving least squares based deformation field to enhance the spatiotemporal coherence of avatar motion and shading effects. With these innovations, our unified framework significantly improves the quality and realism of portrait editing results. Further, the multi-source nature of Total-Editing supports more flexible applications, such as illumination transfer from one portrait to another, or portrait animation with customized backgrounds.",
        "arxiv_id": "2505.20582",
        "ARXIVID": "2505.20582",
        "COMMENT": "This paper focuses on portrait editing and neural radiance fields, which does not match any of the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.21099": {
        "authors": [
            "Tianhao Peng",
            "Ho Man Kwan",
            "Yuxuan Jiang",
            "Ge Gao",
            "Fan Zhang",
            "Xiaozhong Xu",
            "Shan Liu",
            "David Bull"
        ],
        "title": "Instance Data Condensation for Image Super-Resolution",
        "abstract": "arXiv:2505.21099v1 Announce Type: new  Abstract: Deep learning based image Super-Resolution (ISR) relies on large training datasets to optimize model generalization; this requires substantial computational and storage resources during training. While dataset condensation has shown potential in improving data efficiency and privacy for high-level computer vision tasks, it has not yet been fully exploited for ISR. In this paper, we propose a novel Instance Data Condensation (IDC) framework specifically for ISR, which achieves instance-level data condensation through Random Local Fourier Feature Extraction and Multi-level Feature Distribution Matching. This aims to optimize feature distributions at both global and local levels and obtain high-quality synthesized training content with fine detail. This framework has been utilized to condense the most commonly used training dataset for ISR, DIV2K, with a 10% condensation rate. The resulting synthetic dataset offers comparable or (in certain cases) even better performance compared to the original full dataset and excellent training stability when used to train various popular ISR models. To the best of our knowledge, this is the first time that a condensed/synthetic dataset (with a 10% data volume) has demonstrated such performance. The source code and the synthetic dataset have been made available at https://github.com/.",
        "arxiv_id": "2505.21099",
        "ARXIVID": "2505.21099",
        "COMMENT": "This paper does not match any of the specific criteria but is related to computer vision and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.21478": {
        "authors": [
            "Uri Gadot",
            "Rinon Gal",
            "Yftah Ziser",
            "Gal Chechik",
            "Shie Mannor"
        ],
        "title": "Policy Optimized Text-to-Image Pipeline Design",
        "abstract": "arXiv:2505.21478v1 Announce Type: new  Abstract: Text-to-image generation has evolved beyond single monolithic models to complex multi-component pipelines. These combine fine-tuned generators, adapters, upscaling blocks and even editing steps, leading to significant improvements in image quality. However, their effective design requires substantial expertise. Recent approaches have shown promise in automating this process through large language models (LLMs), but they suffer from two critical limitations: extensive computational requirements from generating images with hundreds of predefined pipelines, and poor generalization beyond memorized training examples. We introduce a novel reinforcement learning-based framework that addresses these inefficiencies. Our approach first trains an ensemble of reward models capable of predicting image quality scores directly from prompt-workflow combinations, eliminating the need for costly image generation during training. We then implement a two-phase training strategy: initial workflow vocabulary training followed by GRPO-based optimization that guides the model toward higher-performing regions of the workflow space. Additionally, we incorporate a classifier-free guidance based enhancement technique that extrapolates along the path between the initial and GRPO-tuned models, further improving output quality. We validate our approach through a set of comparisons, showing that it can successfully create new flows with greater diversity and lead to superior image quality compared to existing baselines.",
        "arxiv_id": "2505.21478",
        "ARXIVID": "2505.21478",
        "COMMENT": "Does not match any specific criterion but is relevant to your friend's general interest in generative modeling and optimization in text-to-image pipelines.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.20525": {
        "authors": [
            "Aniket Roy",
            "Maitreya Suin",
            "Ketul Shah",
            "Rama Chellappa"
        ],
        "title": "MultLFG: Training-free Multi-LoRA composition using Frequency-domain Guidance",
        "abstract": "arXiv:2505.20525v1 Announce Type: new  Abstract: Low-Rank Adaptation (LoRA) has gained prominence as a computationally efficient method for fine-tuning generative models, enabling distinct visual concept synthesis with minimal overhead. However, current methods struggle to effectively merge multiple LoRA adapters without training, particularly in complex compositions involving diverse visual elements. We introduce MultLFG, a novel framework for training-free multi-LoRA composition that utilizes frequency-domain guidance to achieve adaptive fusion of multiple LoRAs. Unlike existing methods that uniformly aggregate concept-specific LoRAs, MultLFG employs a timestep and frequency subband adaptive fusion strategy, selectively activating relevant LoRAs based on content relevance at specific timesteps and frequency bands. This frequency-sensitive guidance not only improves spatial coherence but also provides finer control over multi-LoRA composition, leading to more accurate and consistent results. Experimental evaluations on the ComposLoRA benchmark reveal that MultLFG substantially enhances compositional fidelity and image quality across various styles and concept sets, outperforming state-of-the-art baselines in multi-concept generation tasks. Code will be released.",
        "arxiv_id": "2505.20525",
        "ARXIVID": "2505.20525",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling through multi-LoRA composition.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.20914": {
        "authors": [
            "Jianman Lin",
            "Haojie Li",
            "Chunmei Qing",
            "Zhijing Yang",
            "Liang Lin",
            "Tianshui Chen"
        ],
        "title": "Geometry-Editable and Appearance-Preserving Object Compositon",
        "abstract": "arXiv:2505.20914v1 Announce Type: new  Abstract: General object composition (GOC) aims to seamlessly integrate a target object into a background scene with desired geometric properties, while simultaneously preserving its fine-grained appearance details. Recent approaches derive semantic embeddings and integrate them into advanced diffusion models to enable geometry-editable generation. However, these highly compact embeddings encode only high-level semantic cues and inevitably discard fine-grained appearance details. We introduce a Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) model that first leverages semantic embeddings to implicitly capture the desired geometric transformations and then employs a cross-attention retrieval mechanism to align fine-grained appearance features with the geometry-edited representation, facilitating both precise geometry editing and faithful appearance preservation in object composition. Specifically, DGAD builds on CLIP/DINO-derived and reference networks to extract semantic embeddings and appearance-preserving representations, which are then seamlessly integrated into the encoding and decoding pipelines in a disentangled manner. We first integrate the semantic embeddings into pre-trained diffusion models that exhibit strong spatial reasoning capabilities to implicitly capture object geometry, thereby facilitating flexible object manipulation and ensuring effective editability. Then, we design a dense cross-attention mechanism that leverages the implicitly learned object geometry to retrieve and spatially align appearance features with their corresponding regions, ensuring faithful appearance consistency. Extensive experiments on public benchmarks demonstrate the effectiveness of the proposed DGAD framework.",
        "arxiv_id": "2505.20914",
        "ARXIVID": "2505.20914",
        "COMMENT": "Does not match any specific criterion but is related to computer vision through object composition and geometry-editable models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.20876": {
        "authors": [
            "Tatsuya Sasayama",
            "Shintaro Ito",
            "Koichi Ito",
            "Takafumi Aoki"
        ],
        "title": "Stereo Radargrammetry Using Deep Learning from Airborne SAR Images",
        "abstract": "arXiv:2505.20876v1 Announce Type: new  Abstract: In this paper, we propose a stereo radargrammetry method using deep learning from airborne Synthetic Aperture Radar (SAR) images.Deep learning-based methods are considered to suffer less from geometric image modulation, while there is no public SAR image dataset used to train such methods.We create a SAR image dataset and perform fine-tuning of a deep learning-based image correspondence method.The proposed method suppresses the degradation of image quality by pixel interpolation without ground projection of the SAR image and divides the SAR image into patches for processing, which makes it possible to apply deep learning.Through a set of experiments, we demonstrate that the proposed method exhibits a wider range and more accurate elevation measurements compared to conventional methods.",
        "arxiv_id": "2505.20876",
        "ARXIVID": "2505.20876",
        "COMMENT": "Does not match any specific criterion but is related to spatial intelligence and computer vision through stereo radargrammetry.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2505.20644": {
        "authors": [
            "Haoyu Zhang",
            "Yisen Feng",
            "Qiaohui Chu",
            "Meng Liu",
            "Weili Guan",
            "Yaowei Wang",
            "Liqiang Nie"
        ],
        "title": "HCQA-1.5 @ Ego4D EgoSchema Challenge 2025",
        "abstract": "arXiv:2505.20644v1 Announce Type: new  Abstract: In this report, we present the method that achieves third place for Ego4D EgoSchema Challenge in CVPR 2025. To improve the reliability of answer prediction in egocentric video question answering, we propose an effective extension to the previously proposed HCQA framework. Our approach introduces a multi-source aggregation strategy to generate diverse predictions, followed by a confidence-based filtering mechanism that selects high-confidence answers directly. For low-confidence cases, we incorporate a fine-grained reasoning module that performs additional visual and contextual analysis to refine the predictions. Evaluated on the EgoSchema blind test set, our method achieves 77% accuracy on over 5,000 human-curated multiple-choice questions, outperforming last year's winning solution and the majority of participating teams. Our code will be added at https://github.com/Hyu-Zhang/HCQA.",
        "arxiv_id": "2505.20644",
        "ARXIVID": "2505.20644",
        "COMMENT": "This paper focuses on egocentric video question answering and does not match any of the specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}