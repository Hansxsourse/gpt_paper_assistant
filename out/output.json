{
    "2602.11942": {
        "authors": [
            "Soufiane Ben Haddou",
            "Laura Alvarez-Florez",
            "Erik J. Bekkers",
            "Fleur V. Y. Tjong",
            "Ahmad S. Amin",
            "Connie R. Bezzina",
            "Ivana I\\v{s}gum"
        ],
        "title": "Synthesis of Late Gadolinium Enhancement Images via Implicit Neural Representations for Cardiac Scar Segmentation",
        "abstract": "arXiv:2602.11942v1 Announce Type: new  Abstract: Late gadolinium enhancement (LGE) imaging is the clinical standard for myocardial scar assessment, but limited annotated datasets hinder the development of automated segmentation methods. We propose a novel framework that synthesises both LGE images and their corresponding segmentation masks using implicit neural representations (INRs) combined with denoising diffusion models. Our approach first trains INRs to capture continuous spatial representations of LGE data and associated myocardium and fibrosis masks. These INRs are then compressed into compact latent embeddings, preserving essential anatomical information. A diffusion model operates on this latent space to generate new representations, which are decoded into synthetic LGE images with anatomically consistent segmentation masks. Experiments on 133 cardiac MRI scans suggest that augmenting training data with 200 synthetic volumes contributes to improved fibrosis segmentation performance, with the Dice score showing an increase from 0.509 to 0.524. Our approach provides an annotation-free method to help mitigate data scarcity.The code for this research is publicly available.",
        "arxiv_id": "2602.11942",
        "ARXIVID": "2602.11942",
        "COMMENT": "Matches criterion 1: Unified Image/Video Generation and Segmentation",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.11401": {
        "authors": [
            "Alan Baade",
            "Eric Ryan Chan",
            "Kyle Sargent",
            "Changan Chen",
            "Justin Johnson",
            "Ehsan Adeli",
            "Li Fei-Fei"
        ],
        "title": "Latent Forcing: Reordering the Diffusion Trajectory for Pixel-Space Image Generation",
        "abstract": "arXiv:2602.11401v1 Announce Type: new  Abstract: Latent diffusion models excel at generating high-quality images but lose the benefits of end-to-end modeling. They discard information during image encoding, require a separately trained decoder, and model an auxiliary distribution to the raw data. In this paper, we propose Latent Forcing, a simple modification to existing architectures that achieves the efficiency of latent diffusion while operating on raw natural images. Our approach orders the denoising trajectory by jointly processing latents and pixels with separately tuned noise schedules. This allows the latents to act as a scratchpad for intermediate computation before high-frequency pixel features are generated. We find that the order of conditioning signals is critical, and we analyze this to explain differences between REPA distillation in the tokenizer and the diffusion model, conditional versus unconditional generation, and how tokenizer reconstruction quality relates to diffusability. Applied to ImageNet, Latent Forcing achieves a new state-of-the-art for diffusion transformer-based pixel generation at our compute scale.",
        "arxiv_id": "2602.11401",
        "ARXIVID": "2602.11401",
        "COMMENT": "Matches criterion 2: Unified Diffusion Models for multiple vision tasks",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2602.11703": {
        "authors": [
            "Qiwen Xu",
            "David R\\\"ugamer",
            "Holger Wenz",
            "Johann Fontana",
            "Nora Meggyeshazi",
            "Andreas Bender",
            "M\\'at\\'e E. Maros"
        ],
        "title": "Semantically Conditioned Diffusion Models for Cerebral DSA Synthesis",
        "abstract": "arXiv:2602.11703v1 Announce Type: new  Abstract: Digital subtraction angiography (DSA) plays a central role in the diagnosis and treatment of cerebrovascular disease, yet its invasive nature and high acquisition cost severely limit large-scale data collection and public data sharing. Therefore, we developed a semantically conditioned latent diffusion model (LDM) that synthesizes arterial-phase cerebral DSA frames under explicit control of anatomical circulation (anterior vs.\\ posterior) and canonical C-arm positions. We curated a large single-centre DSA dataset of 99,349 frames and trained a conditional LDM using text embeddings that encoded anatomy and acquisition geometry. To assess clinical realism, four medical experts, including two neuroradiologists, one neurosurgeon, and one internal medicine expert, systematically rated 400 synthetic DSA images using a 5-grade Likert scale for evaluating proximal large, medium, and small peripheral vessels. The generated images achieved image-wise overall Likert scores ranging from 3.1 to 3.3, with high inter-rater reliability (ICC(2,k) = 0.80--0.87). Distributional similarity to real DSA frames was supported by a low median Fr\\'echet inception distance (FID) of 15.27. Our results indicate that semantically controlled LDMs can produce realistic synthetic DSAs suitable for downstream algorithm development, research, and training.",
        "arxiv_id": "2602.11703",
        "ARXIVID": "2602.11703",
        "COMMENT": "Matches criterion 2: Unified Diffusion Models for multiple vision tasks",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2602.12221": {
        "authors": [
            "Onkar Susladkar",
            "Tushar Prakash",
            "Gayatri Deshmukh",
            "Kiet A. Nguyen",
            "Jiaxun Zhang",
            "Adheesh Juvekar",
            "Tianshu Bao",
            "Lin Chai",
            "Sparsh Mittal",
            "Inderjit S Dhillon",
            "Ismini Lourentzou"
        ],
        "title": "Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Matching",
        "abstract": "arXiv:2602.12221v1 Announce Type: new  Abstract: We propose UniDFlow, a unified discrete flow-matching framework for multimodal understanding, generation, and editing. It decouples understanding and generation via task-specific low-rank adapters, avoiding objective interference and representation entanglement, while a novel reference-based multimodal preference alignment optimizes relative outcomes under identical conditioning, improving faithfulness and controllability without large-scale retraining. UniDFlpw achieves SOTA performance across eight benchmarks and exhibits strong zero-shot generalization to tasks including inpainting, in-context image generation, reference-based editing, and compositional generation, despite no explicit task-specific training.",
        "arxiv_id": "2602.12221",
        "ARXIVID": "2602.12221",
        "COMMENT": "Does not match any specific criterion",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2602.12205": {
        "authors": [
            "Dianyi Wang",
            "Ruihang Li",
            "Feng Han",
            "Chaofan Ma",
            "Wei Song",
            "Siyuan Wang",
            "Yibin Wang",
            "Yi Xin",
            "Hongjian Liu",
            "Zhixiong Zhang",
            "Shengyuan Ding",
            "Tianhang Wang",
            "Zhenglin Cheng",
            "Tao Lin",
            "Cheng Jin",
            "Kaicheng Yu",
            "Jingjing Chen",
            "Wenjie Wang",
            "Zhongyu Wei",
            "Jiaqi Wang"
        ],
        "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing",
        "abstract": "arXiv:2602.12205v1 Announce Type: new  Abstract: Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.",
        "arxiv_id": "2602.12205",
        "ARXIVID": "2602.12205",
        "COMMENT": "Does not match any specific criterion",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.11845": {
        "authors": [
            "Qisen Wang",
            "Yifan Zhao",
            "Jia Li"
        ],
        "title": "WorldTree: Towards 4D Dynamic Worlds from Monocular Video using Tree-Chains",
        "abstract": "arXiv:2602.11845v1 Announce Type: new  Abstract: Dynamic reconstruction has achieved remarkable progress, but there remain challenges in monocular input for more practical applications. The prevailing works attempt to construct efficient motion representations, but lack a unified spatiotemporal decomposition framework, suffering from either holistic temporal optimization or coupled hierarchical spatial composition. To this end, we propose WorldTree, a unified framework comprising Temporal Partition Tree (TPT) that enables coarse-to-fine optimization based on the inheritance-based partition tree structure for hierarchical temporal decomposition, and Spatial Ancestral Chains (SAC) that recursively query ancestral hierarchical structure to provide complementary spatial dynamics while specializing motion representations across ancestral nodes. Experimental results on different datasets indicate that our proposed method achieves 8.26% improvement of LPIPS on NVIDIA-LS and 9.09% improvement of mLPIPS on DyCheck compared to the second-best method. Code: https://github.com/iCVTEAM/WorldTree.",
        "arxiv_id": "2602.11845",
        "ARXIVID": "2602.11845",
        "COMMENT": "Does not match any specific criterion",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2602.12160": {
        "authors": [
            "Xu Guo",
            "Fulong Ye",
            "Qichao Sun",
            "Liyang Chen",
            "Bingchuan Li",
            "Pengze Zhang",
            "Jiawei Liu",
            "Songtao Zhao",
            "Qian He",
            "Xiangwang Hou"
        ],
        "title": "DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation",
        "abstract": "arXiv:2602.12160v1 Announce Type: new  Abstract: Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.",
        "arxiv_id": "2602.12160",
        "ARXIVID": "2602.12160",
        "COMMENT": "Does not match any specific criterion",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}