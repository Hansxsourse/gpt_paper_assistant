{
    "2508.16239": {
        "authors": [
            "Nan wang",
            "Zhiyi Xia",
            "Yiming Li",
            "Shi Tang",
            "Zuxin Fan",
            "Xi Fang",
            "Haoyi Tao",
            "Xiaochen Cai",
            "Guolin Ke",
            "Linfeng Zhang",
            "Yanhui Hong"
        ],
        "title": "UniEM-3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation",
        "abstract": "arXiv:2508.16239v1 Announce Type: new  Abstract: Quantitative microstructural characterization is fundamental to materials science, where electron micrograph (EM) provides indispensable high-resolution insights. However, progress in deep learning-based EM characterization has been hampered by the scarcity of large-scale, diverse, and expert-annotated datasets, due to acquisition costs, privacy concerns, and annotation complexity. To address this issue, we introduce UniEM-3M, the first large-scale and multimodal EM dataset for instance-level understanding. It comprises 5,091 high-resolution EMs, about 3 million instance segmentation labels, and image-level attribute-disentangled textual descriptions, a subset of which will be made publicly available. Furthermore, we are also releasing a text-to-image diffusion model trained on the entire collection to serve as both a powerful data augmentation tool and a proxy for the complete data distribution. To establish a rigorous benchmark, we evaluate various representative instance segmentation methods on the complete UniEM-3M and present UniEM-Net as a strong baseline model. Quantitative experiments demonstrate that this flow-based model outperforms other advanced methods on this challenging benchmark. Our multifaceted release of a partial dataset, a generative model, and a comprehensive benchmark -- available at huggingface -- will significantly accelerate progress in automated materials analysis.",
        "arxiv_id": "2508.16239",
        "ARXIVID": "2508.16239",
        "COMMENT": "Criteria 1",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2508.16211": {
        "authors": [
            "Shikang Zheng",
            "Liang Feng",
            "Xinyu Wang",
            "Qinming Zhou",
            "Peiliang Cai",
            "Chang Zou",
            "Jiacheng Liu",
            "Yuqi Lin",
            "Junjie Chen",
            "Yue Ma",
            "Linfeng Zhang"
        ],
        "title": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers",
        "abstract": "arXiv:2508.16211v1 Announce Type: new  Abstract: Diffusion Transformers (DiTs) have demonstrated exceptional performance in high-fidelity image and video generation. To reduce their substantial computational costs, feature caching techniques have been proposed to accelerate inference by reusing hidden representations from previous timesteps. However, current methods often struggle to maintain generation quality at high acceleration ratios, where prediction errors increase sharply due to the inherent instability of long-step forecasting. In this work, we adopt an ordinary differential equation (ODE) perspective on the hidden-feature sequence, modeling layer representations along the trajectory as a feature-ODE. We attribute the degradation of existing caching strategies to their inability to robustly integrate historical features under large skipping intervals. To address this, we propose FoCa (Forecast-then-Calibrate), which treats feature caching as a feature-ODE solving problem. Extensive experiments on image synthesis, video generation, and super-resolution tasks demonstrate the effectiveness of FoCa, especially under aggressive acceleration. Without additional training, FoCa achieves near-lossless speedups of 5.50 times on FLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high quality with a 4.53 times speedup on DiT.",
        "arxiv_id": "2508.16211",
        "ARXIVID": "2508.16211",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}