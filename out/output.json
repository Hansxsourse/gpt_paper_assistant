{
    "2511.22242": {
        "authors": [
            "Qingtao Yu",
            "Changlin Song",
            "Minghao Sun",
            "Zhengyang Yu",
            "Vinay Kumar Verma",
            "Soumya Roy",
            "Sumit Negi",
            "Hongdong Li",
            "Dylan Campbell"
        ],
        "title": "TTSnap: Test-Time Scaling of Diffusion Models via Noise-Aware Pruning",
        "abstract": "arXiv:2511.22242v1 Announce Type: new  Abstract: A prominent approach to test-time scaling for text-to-image diffusion models formulates the problem as a search over multiple noise seeds, selecting the one that maximizes a certain image-reward function. The effectiveness of this strategy heavily depends on the number and diversity of noise seeds explored. However, verifying each candidate is computationally expensive, because each must be fully denoised before a reward can be computed. This severely limits the number of samples that can be explored under a fixed budget. We propose test-time scaling with noise-aware pruning (TTSnap), a framework that prunes low-quality candidates without fully denoising them. The key challenge is that reward models are learned in the clean image domain, and the ranking of rewards predicted for intermediate estimates are often inconsistent with those predicted for clean images. To overcome this, we train noise-aware reward models via self-distillation to align the reward for intermediate estimates with that of the final clean images. To stabilize learning across different noise levels, we adopt a curriculum training strategy that progressively shifts the data domain from clean images to noise images. In addition, we introduce a new metric that measures reward alignment and computational budget utilization. Experiments demonstrate that our approach improves performance by over 16\\% compared with existing methods, enabling more efficient and effective test-time scaling. It also provides orthogonal gains when combined with post-training techniques and local test-time optimization. Code: https://github.com/TerrysLearning/TTSnap/.",
        "arxiv_id": "2511.22242",
        "ARXIVID": "2511.22242",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2511.22170": {
        "authors": [
            "Delong Zhao",
            "Qiang Huang",
            "Di Yan",
            "Yiqun Sun",
            "Jun Yu"
        ],
        "title": "Partially Shared Concept Bottleneck Models",
        "abstract": "arXiv:2511.22170v1 Announce Type: new  Abstract: Concept Bottleneck Models (CBMs) enhance interpretability by introducing a layer of human-understandable concepts between inputs and predictions. While recent methods automate concept generation using Large Language Models (LLMs) and Vision-Language Models (VLMs), they still face three fundamental challenges: poor visual grounding, concept redundancy, and the absence of principled metrics to balance predictive accuracy and concept compactness. We introduce PS-CBM, a Partially Shared CBM framework that addresses these limitations through three core components: (1) a multimodal concept generator that integrates LLM-derived semantics with exemplar-based visual cues; (2) a Partially Shared Concept Strategy that merges concepts based on activation patterns to balance specificity and compactness; and (3) Concept-Efficient Accuracy (CEA), a post-hoc metric that jointly captures both predictive accuracy and concept compactness. Extensive experiments on eleven diverse datasets show that PS-CBM consistently outperforms state-of-the-art CBMs, improving classification accuracy by 1.0%-7.4% and CEA by 2.0%-9.5%, while requiring significantly fewer concepts. These results underscore PS-CBM's effectiveness in achieving both high accuracy and strong interpretability.",
        "arxiv_id": "2511.22170",
        "ARXIVID": "2511.22170",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2511.22699": {
        "authors": [
            "Image Team",
            "Huanqia Cai",
            "Sihan Cao",
            "Ruoyi Du",
            "Peng Gao",
            "Steven Hoi",
            "Shijie Huang",
            "Zhaohui Hou",
            "Dengyang Jiang",
            "Xin Jin",
            "Liangchen Li",
            "Zhen Li",
            "Zhong-Yu Li",
            "David Liu",
            "Dongyang Liu",
            "Junhan Shi",
            "Qilong Wu",
            "Feng Yu",
            "Chi Zhang",
            "Shifeng Zhang",
            "Shilin Zhou"
        ],
        "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer",
        "abstract": "arXiv:2511.22699v1 Announce Type: new  Abstract: The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.",
        "arxiv_id": "2511.22699",
        "ARXIVID": "2511.22699",
        "COMMENT": "",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}