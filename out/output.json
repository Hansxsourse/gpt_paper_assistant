{
    "2502.00426": {
        "authors": [
            "Rui Yan",
            "Jin Wang",
            "Hongyu Qu",
            "Xiaoyu Du",
            "Dong Zhang",
            "Jinhui Tang",
            "Tieniu Tan"
        ],
        "title": "TeST-V: TEst-time Support-set Tuning for Zero-shot Video Classification",
        "abstract": "arXiv:2502.00426v1 Announce Type: new  Abstract: Recently, adapting Vision Language Models (VLMs) to zero-shot visual classification by tuning class embedding with a few prompts (Test-time Prompt Tuning, TPT) or replacing class names with generated visual samples (support-set) has shown promising results. However, TPT cannot avoid the semantic gap between modalities while the support-set cannot be tuned. To this end, we draw on each other's strengths and propose a novel framework namely TEst-time Support-set Tuning for zero-shot Video Classification (TEST-V). It first dilates the support-set with multiple prompts (Multi-prompting Support-set Dilation, MSD) and then erodes the support-set via learnable weights to mine key cues dynamically (Temporal-aware Support-set Erosion, TSE). Specifically, i) MSD expands the support samples for each class based on multiple prompts enquired from LLMs to enrich the diversity of the support-set. ii) TSE tunes the support-set with factorized learnable weights according to the temporal prediction consistency in a self-supervised manner to dig pivotal supporting cues for each class. $\\textbf{TEST-V}$ achieves state-of-the-art results across four benchmarks and has good interpretability for the support-set dilation and erosion.",
        "arxiv_id": "2502.00426",
        "ARXIVID": "2502.00426",
        "COMMENT": "Matches criterion 2 as it proposes a novel framework for adapting Vision Language Models (VLMs) to zero-shot video classification, which aligns with multi-modal large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2502.00858": {
        "authors": [
            "Manjie Xu",
            "Xinyi Yang",
            "Wei Liang",
            "Chi Zhang",
            "Yixin Zhu"
        ],
        "title": "Learning to Plan with Personalized Preferences",
        "abstract": "arXiv:2502.00858v1 Announce Type: new  Abstract: Effective integration of AI agents into daily life requires them to understand and adapt to individual human preferences, particularly in collaborative roles. Although recent studies on embodied intelligence have advanced significantly, they typically adopt generalized approaches that overlook personal preferences in planning. We address this limitation by developing agents that not only learn preferences from few demonstrations but also learn to adapt their planning strategies based on these preferences. Our research leverages the observation that preferences, though implicitly expressed through minimal demonstrations, can generalize across diverse planning scenarios. To systematically evaluate this hypothesis, we introduce Preference-based Planning (PbP) benchmark, an embodied benchmark featuring hundreds of diverse preferences spanning from atomic actions to complex sequences. Our evaluation of SOTA methods reveals that while symbol-based approaches show promise in scalability, significant challenges remain in learning to generate and execute plans that satisfy personalized preferences. We further demonstrate that incorporating learned preferences as intermediate representations in planning significantly improves the agent's ability to construct personalized plans. These findings establish preferences as a valuable abstraction layer for adaptive planning, opening new directions for research in preference-guided plan generation and execution.",
        "arxiv_id": "2502.00858",
        "ARXIVID": "2502.00858",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (PbP) for embodied AI focusing on personalized planning, which is a novel angle.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.00372": {
        "authors": [
            "Zhixi Cai",
            "Fucai Ke",
            "Simindokht Jahangard",
            "Maria Garcia de la Banda",
            "Reza Haffari",
            "Peter J. Stuckey",
            "Hamid Rezatofighi"
        ],
        "title": "NAVER: A Neuro-Symbolic Compositional Automaton for Visual Grounding with Explicit Logic Reasoning",
        "abstract": "arXiv:2502.00372v1 Announce Type: new  Abstract: Visual Grounding (VG) tasks, such as referring expression detection and segmentation tasks are important for linking visual entities to context, especially in complex reasoning tasks that require detailed query interpretation. This paper explores VG beyond basic perception, highlighting challenges for methods that require reasoning like human cognition. Recent advances in large language methods (LLMs) and Vision-Language methods (VLMs) have improved abilities for visual comprehension, contextual understanding, and reasoning. These methods are mainly split into end-to-end and compositional methods, with the latter offering more flexibility. Compositional approaches that integrate LLMs and foundation models show promising performance but still struggle with complex reasoning with language-based logical representations. To address these limitations, we propose NAVER, a compositional visual grounding method that integrates explicit probabilistic logic reasoning within a finite-state automaton, equipped with a self-correcting mechanism. This design improves robustness and interpretability in inference through explicit logic reasoning. Our results show that NAVER achieves SoTA performance comparing to recent end-to-end and compositional baselines. The code is available at https://github.com/ControlNet/NAVER .",
        "arxiv_id": "2502.00372",
        "ARXIVID": "2502.00372",
        "COMMENT": "Matches criterion 1 and 3. Proposes a neuro-symbolic compositional automaton for visual grounding with explicit logic reasoning, which is relevant to spatial understanding and embodied AI methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.01401": {
        "authors": [
            "Boyu Mi",
            "Hanqing Wang",
            "Tai Wang",
            "Yilun Chen",
            "Jiangmiao Pang"
        ],
        "title": "Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection",
        "abstract": "arXiv:2502.01401v1 Announce Type: new  Abstract: 3D visual grounding (3DVG) is challenging because of the requirement of understanding on visual information, language and spatial relationships. While supervised approaches have achieved superior performance, they are constrained by the scarcity and high cost of 3D vision-language datasets. On the other hand, LLM/VLM based agents are proposed for 3DVG, eliminating the need for training data. However, these methods incur prohibitive time and token costs during inference. To address the challenges, we introduce a novel training-free symbolic framework for 3D visual grounding, namely Evolvable Symbolic Visual Grounder, that offers significantly reduced inference costs compared to previous agent-based methods while maintaining comparable performance. EaSe uses LLM generated codes to compute on spatial relationships. EaSe also implements an automatic pipeline to evaluate and optimize the quality of these codes and integrate VLMs to assist in the grounding process. Experimental results demonstrate that EaSe achieves 52.9% accuracy on Nr3D dataset and 49.2% Acc@0.25 on ScanRefer, which is top-tier among training-free methods. Moreover, it substantially reduces the inference time and cost, offering a balanced trade-off between performance and efficiency. Codes are available at https://github.com/OpenRobotLab/EaSe.",
        "arxiv_id": "2502.01401",
        "ARXIVID": "2502.01401",
        "COMMENT": "Matches criterion 1 and 3. Proposes a novel symbolic framework for 3D visual grounding with reduced inference costs, which is relevant to spatial understanding and embodied AI benchmarks.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.00954": {
        "authors": [
            "Ye Mao",
            "Weixun Luo",
            "Junpeng Jing",
            "Anlan Qiu",
            "Krystian Mikolajczyk"
        ],
        "title": "Hypo3D: Exploring Hypothetical Reasoning in 3D",
        "abstract": "arXiv:2502.00954v1 Announce Type: new  Abstract: The rise of vision-language foundation models marks an advancement in bridging the gap between human and machine capabilities in 3D scene reasoning. Existing 3D reasoning benchmarks assume real-time scene accessibility, which is impractical due to the high cost of frequent scene updates. To this end, we introduce Hypothetical 3D Reasoning, namely Hypo3D, a benchmark designed to evaluate models' ability to reason without access to real-time scene data. Models need to imagine the scene state based on a provided change description before reasoning. Hypo3D is formulated as a 3D Visual Question Answering (VQA) benchmark, comprising 7,727 context changes across 700 indoor scenes, resulting in 14,885 question-answer pairs. An anchor-based world frame is established for all scenes, ensuring consistent reference to a global frame for directional terms in context changes and QAs. Extensive experiments show that state-of-the-art foundation models struggle to reason in hypothetically changed scenes. This reveals a substantial performance gap compared to humans, particularly in scenarios involving movement changes and directional reasoning. Even when the context change is irrelevant to the question, models often incorrectly adjust their answers.",
        "arxiv_id": "2502.00954",
        "ARXIVID": "2502.00954",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (Hypo3D) for hypothetical reasoning in 3D, focusing on scenarios ignored by previous work.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2502.01216": {
        "authors": [
            "Tongkun Liu",
            "Bing Li",
            "Xiao Jin",
            "Yupeng Shi",
            "Qiuying Li",
            "Xiang Wei"
        ],
        "title": "Exploring Few-Shot Defect Segmentation in General Industrial Scenarios with Metric Learning and Vision Foundation Models",
        "abstract": "arXiv:2502.01216v1 Announce Type: new  Abstract: Industrial defect segmentation is critical for manufacturing quality control. Due to the scarcity of training defect samples, few-shot semantic segmentation (FSS) holds significant value in this field. However, existing studies mostly apply FSS to tackle defects on simple textures, without considering more diverse scenarios. This paper aims to address this gap by exploring FSS in broader industrial products with various defect types. To this end, we contribute a new real-world dataset and reorganize some existing datasets to build a more comprehensive few-shot defect segmentation (FDS) benchmark. On this benchmark, we thoroughly investigate metric learning-based FSS methods, including those based on meta-learning and those based on Vision Foundation Models (VFMs). We observe that existing meta-learning-based methods are generally not well-suited for this task, while VFMs hold great potential. We further systematically study the applicability of various VFMs in this task, involving two paradigms: feature matching and the use of Segment Anything (SAM) models. We propose a novel efficient FDS method based on feature matching. Meanwhile, we find that SAM2 is particularly effective for addressing FDS through its video track mode. The contributed dataset and code will be available at: https://github.com/liutongkun/GFDS.",
        "arxiv_id": "2502.01216",
        "ARXIVID": "2502.01216",
        "COMMENT": "Matches criterion 4 as it explores Vision Foundation Models (VFMs) and their application to few-shot defect segmentation in industrial scenarios.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2502.01045": {
        "authors": [
            "Zilong Wang",
            "Zhiyang Dou",
            "Yuan Liu",
            "Cheng Lin",
            "Xiao Dong",
            "Yunhui Guo",
            "Chenxu Zhang",
            "Xin Li",
            "Wenping Wang",
            "Xiaohu Guo"
        ],
        "title": "WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction",
        "abstract": "arXiv:2502.01045v1 Announce Type: new  Abstract: In this paper, we present WonderHuman to reconstruct dynamic human avatars from a monocular video for high-fidelity novel view synthesis. Previous dynamic human avatar reconstruction methods typically require the input video to have full coverage of the observed human body. However, in daily practice, one typically has access to limited viewpoints, such as monocular front-view videos, making it a cumbersome task for previous methods to reconstruct the unseen parts of the human avatar. To tackle the issue, we present WonderHuman, which leverages 2D generative diffusion model priors to achieve high-quality, photorealistic reconstructions of dynamic human avatars from monocular videos, including accurate rendering of unseen body parts. Our approach introduces a Dual-Space Optimization technique, applying Score Distillation Sampling (SDS) in both canonical and observation spaces to ensure visual consistency and enhance realism in dynamic human reconstruction. Additionally, we present a View Selection strategy and Pose Feature Injection to enforce the consistency between SDS predictions and observed data, ensuring pose-dependent effects and higher fidelity in the reconstructed avatar. In the experiments, our method achieves SOTA performance in producing photorealistic renderings from the given monocular video, particularly for those challenging unseen parts. The project page and source code can be found at https://wyiguanw.github.io/WonderHuman/.",
        "arxiv_id": "2502.01045",
        "ARXIVID": "2502.01045",
        "COMMENT": "This paper matches criterion 4 as it leverages 2D generative diffusion model priors for dynamic 3D human reconstruction, which aligns with vision foundation models and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 8
    },
    "2502.00965": {
        "authors": [
            "Xinze Wang",
            "Chen Chen",
            "Yinfei Yang",
            "Hong-You Chen",
            "Bowen Zhang",
            "Aditya Pal",
            "Xiangxin Zhu",
            "Xianzhi Du"
        ],
        "title": "CLIP-UP: A Simple and Efficient Mixture-of-Experts CLIP Training Recipe with Sparse Upcycling",
        "abstract": "arXiv:2502.00965v1 Announce Type: new  Abstract: Mixture-of-Experts (MoE) models are crucial for scaling model capacity while controlling inference costs. While integrating MoE into multimodal models like CLIP improves performance, training these models is notoriously challenging and expensive. We propose CLIP-Upcycling (CLIP-UP), an efficient alternative training strategy that converts a pre-trained dense CLIP model into a sparse MoE architecture. Through extensive experimentation with various settings and auxiliary losses, we demonstrate that CLIP-UP significantly reduces training complexity and cost. Remarkably, our sparse CLIP B/16 model, trained with CLIP-UP, outperforms its dense counterpart by 7.2% and 6.6% on COCO and Flickr30k text-to-image Recall@1 benchmarks respectively. It even surpasses the larger CLIP L/14 model on this task while using only 30% of the inference FLOPs. We further demonstrate the generalizability of our training recipe across different scales, establishing sparse upcycling as a practical and scalable approach for building efficient, high-performance CLIP models.",
        "arxiv_id": "2502.00965",
        "ARXIVID": "2502.00965",
        "COMMENT": "Matches criterion 4 as it focuses on improving vision foundation models (CLIP) with a novel sparse upcycling approach.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2502.01312": {
        "authors": [
            "Xiao Lin",
            "Yun Peng",
            "Liuyi Wang",
            "Xianyou Zhong",
            "Minghao Zhu",
            "Jingwei Yang",
            "Chengju Liu",
            "Qijun Chen"
        ],
        "title": "CleanPose: Category-Level Object Pose Estimation via Causal Learning and Knowledge Distillation",
        "abstract": "arXiv:2502.01312v1 Announce Type: new  Abstract: Category-level object pose estimation aims to recover the rotation, translation and size of unseen instances within predefined categories. In this task, deep neural network-based methods have demonstrated remarkable performance. However, previous studies show they suffer from spurious correlations raised by \"unclean\" confounders in models, hindering their performance on novel instances with significant variations. To address this issue, we propose CleanPose, a novel approach integrating causal learning and knowledge distillation to enhance category-level pose estimation. To mitigate the negative effect of unobserved confounders, we develop a causal inference module based on front-door adjustment, which promotes unbiased estimation by reducing potential spurious correlations. Additionally, to further improve generalization ability, we devise a residual-based knowledge distillation method that has proven effective in providing comprehensive category information guidance. Extensive experiments across multiple benchmarks (REAL275, CAMERA25 and HouseCat6D) hightlight the superiority of proposed CleanPose over state-of-the-art methods. Code will be released.",
        "arxiv_id": "2502.01312",
        "ARXIVID": "2502.01312",
        "COMMENT": "This paper matches criterion 1 as it focuses on category-level object pose estimation, which involves spatial understanding and causal learning techniques.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2502.00094": {
        "authors": [
            "Ahmed Heakl",
            "Sara Ghaboura",
            "Omkar Thawkar",
            "Fahad Shahbaz Khan",
            "Hisham Cholakkal",
            "Rao Muhammad Anwer",
            "Salman Khan"
        ],
        "title": "AIN: The Arabic INclusive Large Multimodal Model",
        "abstract": "arXiv:2502.00094v1 Announce Type: new  Abstract: Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narrowly focusing on a few specific aspects of the language and visual understanding. To bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal Model-designed to excel across diverse domains. AIN is an English-Arabic bilingual LMM designed to excel in English and Arabic, leveraging carefully constructed 3.6 million high-quality Arabic-English multimodal data samples. AIN demonstrates state-of-the-art Arabic performance, while also possessing strong English-language visual capabilities. On the recent CAMEL-Bench benchmark comprising 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding, our AIN demonstrates strong performance with the 7B model outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains and 38 sub-domains. AIN's superior capabilities position it as a significant step toward empowering Arabic speakers with advanced multimodal generative AI tools across diverse applications.",
        "arxiv_id": "2502.00094",
        "ARXIVID": "2502.00094",
        "COMMENT": "Matches criterion 2 as it introduces a new multimodal large language model (LMM) for Arabic-English tasks.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.00711": {
        "authors": [
            "Chunbai Zhang",
            "Chao Wang",
            "Yang Zhou",
            "Yan Peng"
        ],
        "title": "VIKSER: Visual Knowledge-Driven Self-Reinforcing Reasoning Framework",
        "abstract": "arXiv:2502.00711v1 Announce Type: new  Abstract: Visual reasoning refers to the task of solving questions about visual information. Current visual reasoning methods typically employ pre-trained vision-language model (VLM) strategies or deep neural network approaches. However, existing efforts are constrained by limited reasoning interpretability, while hindering by the phenomenon of underspecification in the question text. Additionally, the absence of fine-grained visual knowledge limits the precise understanding of subject behavior in visual reasoning tasks. To address these issues, we propose VIKSER (Visual Knowledge-Driven Self-Reinforcing Reasoning Framework). Specifically, VIKSER, trained using knowledge distilled from large language models, extracts fine-grained visual knowledge with the assistance of visual relationship detection techniques. Subsequently, VIKSER utilizes fine-grained visual knowledge to paraphrase the question with underspecification. Additionally, we design a novel prompting method called Chain-of-Evidence (CoE), which leverages the power of ``evidence for reasoning'' to endow VIKSER with interpretable reasoning capabilities. Meanwhile, the integration of self-reflection technology empowers VIKSER with the ability to learn and improve from its mistakes. Experiments conducted on widely used datasets demonstrate that VIKSER achieves new state-of-the-art (SOTA) results in relevant tasks.",
        "arxiv_id": "2502.00711",
        "ARXIVID": "2502.00711",
        "COMMENT": "Matches criterion 2 as it introduces a visual reasoning framework leveraging vision-language models and fine-grained visual knowledge.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.01507": {
        "authors": [
            "Yeruru Asrar Ahmed",
            "Anurag Mittal"
        ],
        "title": "End-to-end Training for Text-to-Image Synthesis using Dual-Text Embeddings",
        "abstract": "arXiv:2502.01507v1 Announce Type: new  Abstract: Text-to-Image (T2I) synthesis is a challenging task that requires modeling complex interactions between two modalities ( i.e., text and image). A common framework adopted in recent state-of-the-art approaches to achieving such multimodal interactions is to bootstrap the learning process with pre-trained image-aligned text embeddings trained using contrastive loss. Furthermore, these embeddings are typically trained generically and reused across various synthesis models. In contrast, we explore an approach to learning text embeddings specifically tailored to the T2I synthesis network, trained in an end-to-end fashion. Further, we combine generative and contrastive training and use two embeddings, one optimized to enhance the photo-realism of the generated images, and the other seeking to capture text-to-image alignment. A comprehensive set of experiments on three text-to-image benchmark datasets (Oxford-102, Caltech-UCSD, and MS-COCO) reveal that having two separate embeddings gives better results than using a shared one and that such an approach performs favourably in comparison with methods that use text representations from a pre-trained text encoder trained using a discriminative approach. Finally, we demonstrate that such learned embeddings can be used in other contexts as well, such as text-to-image manipulation.",
        "arxiv_id": "2502.01507",
        "ARXIVID": "2502.01507",
        "COMMENT": "Matches criterion 2 as it proposes a novel approach for text-to-image synthesis using dual-text embeddings, relevant to multimodal large language models.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.01639": {
        "authors": [
            "Rohit Gandikota",
            "Zongze Wu",
            "Richard Zhang",
            "David Bau",
            "Eli Shechtman",
            "Nick Kolkin"
        ],
        "title": "SliderSpace: Decomposing the Visual Capabilities of Diffusion Models",
        "abstract": "arXiv:2502.01639v1 Announce Type: new  Abstract: We present SliderSpace, a framework for automatically decomposing the visual capabilities of diffusion models into controllable and human-understandable directions. Unlike existing control methods that require a user to specify attributes for each edit direction individually, SliderSpace discovers multiple interpretable and diverse directions simultaneously from a single text prompt. Each direction is trained as a low-rank adaptor, enabling compositional control and the discovery of surprising possibilities in the model's latent space. Through extensive experiments on state-of-the-art diffusion models, we demonstrate SliderSpace's effectiveness across three applications: concept decomposition, artistic style exploration, and diversity enhancement. Our quantitative evaluation shows that SliderSpace-discovered directions decompose the visual structure of model's knowledge effectively, offering insights into the latent capabilities encoded within diffusion models. User studies further validate that our method produces more diverse and useful variations compared to baselines. Our code, data and trained weights are available at https://sliderspace.baulab.info",
        "arxiv_id": "2502.01639",
        "ARXIVID": "2502.01639",
        "COMMENT": "Matches criterion 4 as it explores the latent capabilities of diffusion models, which are foundational in vision applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.01419": {
        "authors": [
            "Mingi Jung",
            "Saehuyng Lee",
            "Eunji Kim",
            "Sungroh Yoon"
        ],
        "title": "Visual Attention Never Fades: Selective Progressive Attention ReCalibration for Detailed Image Captioning in Multimodal Large Language Models",
        "abstract": "arXiv:2502.01419v1 Announce Type: new  Abstract: Detailed image captioning is essential for tasks like data generation and aiding visually impaired individuals. High-quality captions require a balance between precision and recall, which remains challenging for current multimodal large language models (MLLMs). In this work, we hypothesize that this limitation stems from weakening and increasingly noisy visual attention as responses lengthen. To address this issue, we propose SPARC (Selective Progressive Attention ReCalibration), a training-free method that enhances the contribution of visual tokens during decoding. SPARC is founded on three key observations: (1) increasing the influence of all visual tokens reduces recall; thus, SPARC selectively amplifies visual tokens; (2) as captions lengthen, visual attention becomes noisier, so SPARC identifies critical visual tokens by leveraging attention differences across time steps; (3) as visual attention gradually weakens, SPARC reinforces it to preserve its influence. Our experiments, incorporating both automated and human evaluations, demonstrate that existing methods improve the precision of MLLMs at the cost of recall. In contrast, our proposed method enhances both precision and recall with minimal computational overhead.",
        "arxiv_id": "2502.01419",
        "ARXIVID": "2502.01419",
        "COMMENT": "Matches criterion 2 as it proposes a novel method (SPARC) to improve multimodal large language models (MLLMs) for detailed image captioning.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2502.00510": {
        "authors": [
            "Yingxuan Yang",
            "Bo Huang",
            "Siyuan Qi",
            "Chao Feng",
            "Haoyi Hu",
            "Yuxuan Zhu",
            "Jinbo Hu",
            "Haoran Zhao",
            "Ziyi He",
            "Xiao Liu",
            "Zongyu Wang",
            "Lin Qiu",
            "Xuezhi Cao",
            "Xunliang Cai",
            "Yong Yu",
            "Weinan Zhang"
        ],
        "title": "Who's the MVP? A Game-Theoretic Evaluation Benchmark for Modular Attribution in LLM Agents",
        "abstract": "arXiv:2502.00510v1 Announce Type: new  Abstract: Large Language Model (LLM) agents frameworks often employ modular architectures, incorporating components such as planning, reasoning, action execution, and reflection to tackle complex tasks. However, quantifying the contribution of each module to overall system performance remains a significant challenge, impeding optimization and interpretability. To address this, we introduce CapaBench (Capability-level Assessment Benchmark), an evaluation framework grounded in cooperative game theory's Shapley Value, which systematically measures the marginal impact of individual modules and their interactions within an agent's architecture. By replacing default modules with test variants across all possible combinations, CapaBench provides a principle method for attributing performance contributions. Key contributions include: (1) We are the first to propose a Shapley Value-based methodology for quantifying the contributions of capabilities in LLM agents; (2) Modules with high Shapley Values consistently lead to predictable performance gains when combined, enabling targeted optimization; and (3) We build a multi-round dataset of over 1,000 entries spanning diverse domains and practical task scenarios, enabling comprehensive evaluation of agent capabilities. CapaBench bridges the gap between component-level evaluation and holistic system assessment, providing actionable insights for optimizing modular LLM agents and advancing their deployment in complex, real-world scenarios.",
        "arxiv_id": "2502.00510",
        "ARXIVID": "2502.00510",
        "COMMENT": "This paper matches criterion 3 as it introduces a new benchmark for modular attribution in LLM agents, which could be relevant for embodied AI evaluation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2502.00074": {
        "authors": [
            "Dong-Hee Paek",
            "Seung-Hyun Kong"
        ],
        "title": "SpikingRTNH: Spiking Neural Network for 4D Radar Object Detection",
        "abstract": "arXiv:2502.00074v1 Announce Type: new  Abstract: Recently, 4D Radar has emerged as a crucial sensor for 3D object detection in autonomous vehicles, offering both stable perception in adverse weather and high-density point clouds for object shape recognition. However, processing such high-density data demands substantial computational resources and energy consumption. We propose SpikingRTNH, the first spiking neural network (SNN) for 3D object detection using 4D Radar data. By replacing conventional ReLU activation functions with leaky integrate-and-fire (LIF) spiking neurons, SpikingRTNH achieves significant energy efficiency gains. Furthermore, inspired by human cognitive processes, we introduce biological top-down inference (BTI), which processes point clouds sequentially from higher to lower densities. This approach effectively utilizes points with lower noise and higher importance for detection. Experiments on K-Radar dataset demonstrate that SpikingRTNH with BTI significantly reduces energy consumption by 78% while achieving comparable detection performance to its ANN counterpart (51.1% AP 3D, 57.0% AP BEV). These results establish the viability of SNNs for energy-efficient 4D Radar-based object detection in autonomous driving systems. All codes are available at https://github.com/kaist-avelab/k-radar.",
        "arxiv_id": "2502.00074",
        "ARXIVID": "2502.00074",
        "COMMENT": "This paper matches criterion 3 as it introduces a novel method for 4D Radar-based object detection in autonomous vehicles, focusing on energy-efficient spiking neural networks and biological inference.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2502.00498": {
        "authors": [
            "Yuxuan Chen",
            "Xu Zhu",
            "Hua Zhou",
            "Zhuyin Ren"
        ],
        "title": "MetaOpenFOAM 2.0: Large Language Model Driven Chain of Thought for Automating CFD Simulation and Post-Processing",
        "abstract": "arXiv:2502.00498v1 Announce Type: new  Abstract: Computational Fluid Dynamics (CFD) is widely used in aerospace, energy, and biology to model fluid flow, heat transfer, and chemical reactions. While Large Language Models (LLMs) have transformed various domains, their application in CFD remains limited, particularly for complex tasks like post-processing. To bridge this gap, we introduce MetaOpenFOAM 2.0, which leverages Chain of Thought (COT) decomposition and iterative verification to enhance accessibility for non-expert users through natural language inputs. Tested on a new benchmark covering simulation (fluid flow, heat transfer, combustion) and post-processing (extraction, visualization), MetaOpenFOAM 2.0 achieved an Executability score of 6.3/7 and a pass rate of 86.9%, significantly outperforming MetaOpenFOAM 1.0 (2.1/7, 0%). Additionally, it proved cost-efficient, averaging $0.15 per case. An ablation study confirmed that COT-driven decomposition and iterative refinement substantially improved task performance. Furthermore, scaling laws showed that increasing COT steps enhanced accuracy while raising token usage, aligning with LLM post-training scaling trends. These results highlight the transformative potential of LLMs in automating CFD workflows for industrial and research applications. Code is available at https://github.com/Terry-cyx/MetaOpenFOAM",
        "arxiv_id": "2502.00498",
        "ARXIVID": "2502.00498",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark and method for automating CFD simulation and post-processing using large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2502.01576": {
        "authors": [
            "Hashmat Shadab Malik",
            "Fahad Shamshad",
            "Muzammal Naseer",
            "Karthik Nandakumar",
            "Fahad Khan",
            "Salman Khan"
        ],
        "title": "Robust-LLaVA: On the Effectiveness of Large-Scale Robust Image Encoders for Multi-modal Large Language Models",
        "abstract": "arXiv:2502.01576v1 Announce Type: new  Abstract: Multi-modal Large Language Models (MLLMs) excel in vision-language tasks but remain vulnerable to visual adversarial perturbations that can induce hallucinations, manipulate responses, or bypass safety mechanisms. Existing methods seek to mitigate these risks by applying constrained adversarial fine-tuning to CLIP vision encoders on ImageNet-scale data, ensuring their generalization ability is preserved. However, this limited adversarial training restricts robustness and broader generalization. In this work, we explore an alternative approach of leveraging existing vision classification models that have been adversarially pre-trained on large-scale data. Our analysis reveals two principal contributions: (1) the extensive scale and diversity of adversarial pre-training enables these models to demonstrate superior robustness against diverse adversarial threats, ranging from imperceptible perturbations to advanced jailbreaking attempts, without requiring additional adversarial training, and (2) end-to-end MLLM integration with these robust models facilitates enhanced adaptation of language components to robust visual features, outperforming existing plug-and-play methodologies on complex reasoning tasks. Through systematic evaluation across visual question-answering, image captioning, and jail-break attacks, we demonstrate that MLLMs trained with these robust models achieve superior adversarial robustness while maintaining favorable clean performance. Our framework achieves 2x and 1.5x average robustness gains in captioning and VQA tasks, respectively, and delivers over 10% improvement against jailbreak attacks. Code and pretrained models will be available at https://github.com/HashmatShadab/Robust-LLaVA.",
        "arxiv_id": "2502.01576",
        "ARXIVID": "2502.01576",
        "COMMENT": "Matches criterion 2 as it explores robust vision encoders for multi-modal large language models (MLLMs).",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2502.00688": {
        "authors": [
            "Bo Chen",
            "Chengyue Gong",
            "Xiaoyu Li",
            "Yingyu Liang",
            "Zhizhou Sha",
            "Zhenmei Shi",
            "Zhao Song",
            "Mingda Wan"
        ],
        "title": "High-Order Matching for One-Step Shortcut Diffusion Models",
        "abstract": "arXiv:2502.00688v1 Announce Type: new  Abstract: One-step shortcut diffusion models [Frans, Hafner, Levine and Abbeel, ICLR 2025] have shown potential in vision generation, but their reliance on first-order trajectory supervision is fundamentally limited. The Shortcut model's simplistic velocity-only approach fails to capture intrinsic manifold geometry, leading to erratic trajectories, poor geometric alignment, and instability-especially in high-curvature regions. These shortcomings stem from its inability to model mid-horizon dependencies or complex distributional features, leaving it ill-equipped for robust generative modeling. In this work, we introduce HOMO (High-Order Matching for One-Step Shortcut Diffusion), a game-changing framework that leverages high-order supervision to revolutionize distribution transportation. By incorporating acceleration, jerk, and beyond, HOMO not only fixes the flaws of the Shortcut model but also achieves unprecedented smoothness, stability, and geometric precision. Theoretically, we prove that HOMO's high-order supervision ensures superior approximation accuracy, outperforming first-order methods. Empirically, HOMO dominates in complex settings, particularly in high-curvature regions where the Shortcut model struggles. Our experiments show that HOMO delivers smoother trajectories and better distributional alignment, setting a new standard for one-step generative models.",
        "arxiv_id": "2502.00688",
        "ARXIVID": "2502.00688",
        "COMMENT": "Matches criterion 4 as it introduces a novel high-order diffusion model for vision generation, which is relevant to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2502.00896": {
        "authors": [
            "Can Jin",
            "Ying Li",
            "Mingyu Zhao",
            "Shiyu Zhao",
            "Zhenting Wang",
            "Xiaoxiao He",
            "Ligong Han",
            "Tong Che",
            "Dimitris N. Metaxas"
        ],
        "title": "LoR-VP: Low-Rank Visual Prompting for Efficient Vision Model Adaptation",
        "abstract": "arXiv:2502.00896v1 Announce Type: new  Abstract: Visual prompting has gained popularity as a method for adapting pre-trained models to specific tasks, particularly in the realm of parameter-efficient tuning. However, existing visual prompting techniques often pad the prompt parameters around the image, limiting the interaction between the visual prompts and the original image to a small set of patches while neglecting the inductive bias present in shared information across different patches. In this study, we conduct a thorough preliminary investigation to identify and address these limitations. We propose a novel visual prompt design, introducing Low-Rank matrix multiplication for Visual Prompting (LoR-VP), which enables shared and patch-specific information across rows and columns of image pixels. Extensive experiments across seven network architectures and four datasets demonstrate significant improvements in both performance and efficiency compared to state-of-the-art visual prompting methods, achieving up to 6 times faster training times, utilizing 18 times fewer visual prompt parameters, and delivering a 3.1% improvement in performance. The code is available as https://github.com/jincan333/LoR-VP.",
        "arxiv_id": "2502.00896",
        "ARXIVID": "2502.00896",
        "COMMENT": "Matches criterion 4 as it introduces a novel visual prompting method for efficient vision model adaptation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.01467": {
        "authors": [
            "Haowen Bai",
            "Zixiang Zhao",
            "Jiangshe Zhang",
            "Baisong Jiang",
            "Lilun Deng",
            "Yukun Cui",
            "Shuang Xu",
            "Chunxia Zhang"
        ],
        "title": "Deep Unfolding Multi-modal Image Fusion Network via Attribution Analysis",
        "abstract": "arXiv:2502.01467v1 Announce Type: new  Abstract: Multi-modal image fusion synthesizes information from multiple sources into a single image, facilitating downstream tasks such as semantic segmentation. Current approaches primarily focus on acquiring informative fusion images at the visual display stratum through intricate mappings. Although some approaches attempt to jointly optimize image fusion and downstream tasks, these efforts often lack direct guidance or interaction, serving only to assist with a predefined fusion loss. To address this, we propose an ``Unfolding Attribution Analysis Fusion network'' (UAAFusion), using attribution analysis to tailor fused images more effectively for semantic segmentation, enhancing the interaction between the fusion and segmentation. Specifically, we utilize attribution analysis techniques to explore the contributions of semantic regions in the source images to task discrimination. At the same time, our fusion algorithm incorporates more beneficial features from the source images, thereby allowing the segmentation to guide the fusion process. Our method constructs a model-driven unfolding network that uses optimization objectives derived from attribution analysis, with an attribution fusion loss calculated from the current state of the segmentation network. We also develop a new pathway function for attribution analysis, specifically tailored to the fusion tasks in our unfolding network. An attribution attention mechanism is integrated at each network stage, allowing the fusion network to prioritize areas and pixels crucial for high-level recognition tasks. Additionally, to mitigate the information loss in traditional unfolding networks, a memory augmentation module is incorporated into our network to improve the information flow across various network layers. Extensive experiments demonstrate our method's superiority in image fusion and applicability to semantic segmentation.",
        "arxiv_id": "2502.01467",
        "ARXIVID": "2502.01467",
        "COMMENT": "Matches criterion 4 as it proposes a novel multi-modal image fusion network with applications in semantic segmentation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.00801": {
        "authors": [
            "Zhiwei Huang",
            "Jiaqi Li",
            "Ping Zhong",
            "Rui Fan"
        ],
        "title": "Environment-Driven Online LiDAR-Camera Extrinsic Calibration",
        "abstract": "arXiv:2502.00801v1 Announce Type: new  Abstract: LiDAR-camera extrinsic calibration (LCEC) is the core for data fusion in computer vision. Existing methods typically rely on customized calibration targets or fixed scene types, lacking the flexibility to handle variations in sensor data and environmental contexts. This paper introduces EdO-LCEC, the first environment-driven, online calibration approach that achieves human-like adaptability. Inspired by the human perceptual system, EdO-LCEC incorporates a generalizable scene discriminator to actively interpret environmental conditions, creating multiple virtual cameras that capture detailed spatial and textural information. To overcome cross-modal feature matching challenges between LiDAR and camera, we propose dual-path correspondence matching (DPCM), which leverages both structural and textural consistency to achieve reliable 3D-2D correspondences. Our approach formulates the calibration process as a spatial-temporal joint optimization problem, utilizing global constraints from multiple views and scenes to improve accuracy, particularly in sparse or partially overlapping sensor views. Extensive experiments on real-world datasets demonstrate that EdO-LCEC achieves state-of-the-art performance, providing reliable and precise calibration across diverse, challenging environments.",
        "arxiv_id": "2502.00801",
        "ARXIVID": "2502.00801",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for spatial understanding in LiDAR-camera calibration.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.00315": {
        "authors": [
            "Jihyeok Kim",
            "Seongwoo Moon",
            "Sungwon Nah",
            "David Hyunchul Shim"
        ],
        "title": "MonoDINO-DETR: Depth-Enhanced Monocular 3D Object Detection Using a Vision Foundation Model",
        "abstract": "arXiv:2502.00315v1 Announce Type: new  Abstract: This paper proposes novel methods to enhance the performance of monocular 3D object detection models by leveraging the generalized feature extraction capabilities of a vision foundation model. Unlike traditional CNN-based approaches, which often suffer from inaccurate depth estimation and rely on multi-stage object detection pipelines, this study employs a Vision Transformer (ViT)-based foundation model as the backbone, which excels at capturing global features for depth estimation. It integrates a detection transformer (DETR) architecture to improve both depth estimation and object detection performance in a one-stage manner. Specifically, a hierarchical feature fusion block is introduced to extract richer visual features from the foundation model, further enhancing feature extraction capabilities. Depth estimation accuracy is further improved by incorporating a relative depth estimation model trained on large-scale data and fine-tuning it through transfer learning. Additionally, the use of queries in the transformer's decoder, which consider reference points and the dimensions of 2D bounding boxes, enhances recognition performance. The proposed model outperforms recent state-of-the-art methods, as demonstrated through quantitative and qualitative evaluations on the KITTI 3D benchmark and a custom dataset collected from high-elevation racing environments. Code is available at https://github.com/JihyeokKim/MonoDINO-DETR.",
        "arxiv_id": "2502.00315",
        "ARXIVID": "2502.00315",
        "COMMENT": "Matches criterion 4 as it leverages a vision foundation model for monocular 3D object detection.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.00662": {
        "authors": [
            "Yimu Wang",
            "Evelien Riddell",
            "Adrian Chow",
            "Sean Sedwards",
            "Krzysztof Czarnecki"
        ],
        "title": "Mitigating the Modality Gap: Few-Shot Out-of-Distribution Detection with Multi-modal Prototypes and Image Bias Estimation",
        "abstract": "arXiv:2502.00662v1 Announce Type: new  Abstract: Existing vision-language model (VLM)-based methods for out-of-distribution (OOD) detection typically rely on similarity scores between input images and in-distribution (ID) text prototypes. However, the modality gap between image and text often results in high false positive rates, as OOD samples can exhibit high similarity to ID text prototypes. To mitigate the impact of this modality gap, we propose incorporating ID image prototypes along with ID text prototypes. We present theoretical analysis and empirical evidence indicating that this approach enhances VLM-based OOD detection performance without any additional training. To further reduce the gap between image and text, we introduce a novel few-shot tuning framework, SUPREME, comprising biased prompts generation (BPG) and image-text consistency (ITC) modules. BPG enhances image-text fusion and improves generalization by conditioning ID text prototypes on the Gaussian-based estimated image domain bias; ITC reduces the modality gap by minimizing intra- and inter-modal distances. Moreover, inspired by our theoretical and empirical findings, we introduce a novel OOD score $S_{\\textit{GMP}}$, leveraging uni- and cross-modal similarities. Finally, we present extensive experiments to demonstrate that SUPREME consistently outperforms existing VLM-based OOD detection methods.",
        "arxiv_id": "2502.00662",
        "ARXIVID": "2502.00662",
        "COMMENT": "Matches criterion 2 as it proposes a novel few-shot tuning framework for vision-language models, addressing out-of-distribution detection.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.00528": {
        "authors": [
            "Zachary Huemann",
            "Samuel Church",
            "Joshua D. Warner",
            "Daniel Tran",
            "Xin Tie",
            "Alan B McMillan",
            "Junjie Hu",
            "Steve Y. Cho",
            "Meghan Lubner",
            "Tyler J. Bradshaw"
        ],
        "title": "Vision-Language Modeling in PET/CT for Visual Grounding of Positive Findings",
        "abstract": "arXiv:2502.00528v1 Announce Type: new  Abstract: Vision-language models can connect the text description of an object to its specific location in an image through visual grounding. This has potential applications in enhanced radiology reporting. However, these models require large annotated image-text datasets, which are lacking for PET/CT. We developed an automated pipeline to generate weak labels linking PET/CT report descriptions to their image locations and used it to train a 3D vision-language visual grounding model. Our pipeline finds positive findings in PET/CT reports by identifying mentions of SUVmax and axial slice numbers. From 25,578 PET/CT exams, we extracted 11,356 sentence-label pairs. Using this data, we trained ConTEXTual Net 3D, which integrates text embeddings from a large language model with a 3D nnU-Net via token-level cross-attention. The model's performance was compared against LLMSeg, a 2.5D version of ConTEXTual Net, and two nuclear medicine physicians. The weak-labeling pipeline accurately identified lesion locations in 98% of cases (246/251), with 7.5% requiring boundary adjustments. ConTEXTual Net 3D achieved an F1 score of 0.80, outperforming LLMSeg (F1=0.22) and the 2.5D model (F1=0.53), though it underperformed both physicians (F1=0.94 and 0.91). The model achieved better performance on FDG (F1=0.78) and DCFPyL (F1=0.75) exams, while performance dropped on DOTATE (F1=0.58) and Fluciclovine (F1=0.66). The model performed consistently across lesion sizes but showed reduced accuracy on lesions with low uptake. Our novel weak labeling pipeline accurately produced an annotated dataset of PET/CT image-text pairs, facilitating the development of 3D visual grounding models. ConTEXTual Net 3D significantly outperformed other models but fell short of the performance of nuclear medicine physicians. Our study suggests that even larger datasets may be needed to close this performance gap.",
        "arxiv_id": "2502.00528",
        "ARXIVID": "2502.00528",
        "COMMENT": "Matches criterion 2 as it develops a vision-language model for PET/CT visual grounding, which is a multi-modal large language model application.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.00960": {
        "authors": [
            "Mingyu Yang",
            "Jitong Lu",
            "Hun-Seok Kim"
        ],
        "title": "SAM-guided Pseudo Label Enhancement for Multi-modal 3D Semantic Segmentation",
        "abstract": "arXiv:2502.00960v1 Announce Type: new  Abstract: Multi-modal 3D semantic segmentation is vital for applications such as autonomous driving and virtual reality (VR). To effectively deploy these models in real-world scenarios, it is essential to employ cross-domain adaptation techniques that bridge the gap between training data and real-world data. Recently, self-training with pseudo-labels has emerged as a predominant method for cross-domain adaptation in multi-modal 3D semantic segmentation. However, generating reliable pseudo-labels necessitates stringent constraints, which often result in sparse pseudo-labels after pruning. This sparsity can potentially hinder performance improvement during the adaptation process. We propose an image-guided pseudo-label enhancement approach that leverages the complementary 2D prior knowledge from the Segment Anything Model (SAM) to introduce more reliable pseudo-labels, thereby boosting domain adaptation performance. Specifically, given a 3D point cloud and the SAM masks from its paired image data, we collect all 3D points covered by each SAM mask that potentially belong to the same object. Then our method refines the pseudo-labels within each SAM mask in two steps. First, we determine the class label for each mask using majority voting and employ various constraints to filter out unreliable mask labels. Next, we introduce Geometry-Aware Progressive Propagation (GAPP) which propagates the mask label to all 3D points within the SAM mask while avoiding outliers caused by 2D-3D misalignment. Experiments conducted across multiple datasets and domain adaptation scenarios demonstrate that our proposed method significantly increases the quantity of high-quality pseudo-labels and enhances the adaptation performance over baseline methods.",
        "arxiv_id": "2502.00960",
        "ARXIVID": "2502.00960",
        "COMMENT": "Matches criterion 4 as it uses SAM for enhancing pseudo-labels in multi-modal 3D semantic segmentation, which is a vision foundation model application.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.00630": {
        "authors": [
            "Bin Xie",
            "Hao Tang",
            "Dawen Cai",
            "Yan Yan",
            "Gady Agam"
        ],
        "title": "Self-Prompt SAM: Medical Image Segmentation via Automatic Prompt SAM Adaptation",
        "abstract": "arXiv:2502.00630v1 Announce Type: new  Abstract: Segment Anything Model (SAM) has demonstrated impressive zero-shot performance and brought a range of unexplored capabilities to natural image segmentation tasks. However, as a very important branch of image segmentation, the performance of SAM remains uncertain when applied to medical image segmentation due to the significant differences between natural images and medical images. Meanwhile, it is harsh to meet the SAM's requirements of extra prompts provided, such as points or boxes to specify medical regions. In this paper, we propose a novel self-prompt SAM adaptation framework for medical image segmentation, named Self-Prompt-SAM. We design a multi-scale prompt generator combined with the image encoder in SAM to generate auxiliary masks. Then, we use the auxiliary masks to generate bounding boxes as box prompts and use Distance Transform to select the most central points as point prompts. Meanwhile, we design a 3D depth-fused adapter (DfusedAdapter) and inject the DFusedAdapter into each transformer in the image encoder and mask decoder to enable pre-trained 2D SAM models to extract 3D information and adapt to 3D medical images. Extensive experiments demonstrate that our method achieves state-of-the-art performance and outperforms nnUNet by 2.3% on AMOS2022, 1.6% on ACDCand 0.5% on Synapse datasets.",
        "arxiv_id": "2502.00630",
        "ARXIVID": "2502.00630",
        "COMMENT": "Matches criterion 4 as it adapts the Segment Anything Model (SAM) for medical image segmentation, which is a vision foundation model application.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.01000": {
        "authors": [
            "Jingyun Yang",
            "Guoqing Zhang",
            "Jingge Wang",
            "Yang Li"
        ],
        "title": "Adapting Foundation Models for Few-Shot Medical Image Segmentation: Actively and Sequentially",
        "abstract": "arXiv:2502.01000v1 Announce Type: new  Abstract: Recent advances in foundation models have brought promising results in computer vision, including medical image segmentation. Fine-tuning foundation models on specific low-resource medical tasks has become a standard practice. However, ensuring reliable and robust model adaptation when the target task has a large domain gap and few annotated samples remains a challenge. Previous few-shot domain adaptation (FSDA) methods seek to bridge the distribution gap between source and target domains by utilizing auxiliary data. The selection and scheduling of auxiliaries are often based on heuristics, which can easily cause negative transfer. In this work, we propose an Active and Sequential domain AdaPtation (ASAP) framework for dynamic auxiliary dataset selection in FSDA. We formulate FSDA as a multi-armed bandit problem and derive an efficient reward function to prioritize training on auxiliary datasets that align closely with the target task, through a single-round fine-tuning. Empirical validation on diverse medical segmentation datasets demonstrates that our method achieves favorable segmentation performance, significantly outperforming the state-of-the-art FSDA methods, achieving an average gain of 27.75% on MRI and 7.52% on CT datasets in Dice score. Code is available at the git repository: https://github.com/techicoco/ASAP.",
        "arxiv_id": "2502.01000",
        "ARXIVID": "2502.01000",
        "COMMENT": "Matches criterion 4 as it discusses adapting foundation models for medical image segmentation, which is a vision foundation model application.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.01522": {
        "authors": [
            "Junhao Cheng",
            "Wei-Ting Chen",
            "Xi Lu",
            "Ming-Hsuan Yang"
        ],
        "title": "BD-Diff: Generative Diffusion Model for Image Deblurring on Unknown Domains with Blur-Decoupled Learning",
        "abstract": "arXiv:2502.01522v1 Announce Type: new  Abstract: Generative diffusion models trained on large-scale datasets have achieved remarkable progress in image synthesis. In favor of their ability to supplement missing details and generate aesthetically pleasing contents, recent works have applied them to image deblurring tasks via training an adapter on blurry-sharp image pairs to provide structural conditions for restoration. However, acquiring substantial amounts of realistic paired data is challenging and costly in real-world scenarios. On the other hand, relying solely on synthetic data often results in overfitting, leading to unsatisfactory performance when confronted with unseen blur patterns. To tackle this issue, we propose BD-Diff, a generative-diffusion-based model designed to enhance deblurring performance on unknown domains by decoupling structural features and blur patterns through joint training on three specially designed tasks. We employ two Q-Formers as structural representations and blur patterns extractors separately. The features extracted by them will be used for the supervised deblurring task on synthetic data and the unsupervised blur-transfer task by leveraging unpaired blurred images from the target domain simultaneously. Furthermore, we introduce a reconstruction task to make the structural features and blur patterns complementary. This blur-decoupled learning process enhances the generalization capabilities of BD-Diff when encountering unknown domain blur patterns. Experiments on real-world datasets demonstrate that BD-Diff outperforms existing state-of-the-art methods in blur removal and structural preservation in various challenging scenarios. The codes will be released in https://github.com/donahowe/BD-Diff",
        "arxiv_id": "2502.01522",
        "ARXIVID": "2502.01522",
        "COMMENT": "Matches criterion 4 as it involves generative diffusion models and their application to image deblurring, which is a vision foundation model application.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.01100": {
        "authors": [
            "Bill Yuchen Lin",
            "Ronan Le Bras",
            "Kyle Richardson",
            "Ashish Sabharwal",
            "Radha Poovendran",
            "Peter Clark",
            "Yejin Choi"
        ],
        "title": "ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning",
        "abstract": "arXiv:2502.01100v1 Announce Type: new  Abstract: We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with controllable and quantifiable complexity, facilitating a systematic study of the scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By encompassing a broad range of search space complexities and diverse logical constraints, ZebraLogic provides a structured environment to evaluate reasoning under increasing difficulty.   Our results reveal a significant decline in accuracy as problem complexity grows -- a phenomenon we term the curse of complexity. This limitation persists even with larger models and increased inference-time computation, suggesting inherent constraints in current LLM reasoning capabilities. Additionally, we explore strategies to enhance logical reasoning, including Best-of-N sampling, backtracking mechanisms, and self-verification prompts. Our findings offer critical insights into the scalability of LLM reasoning, highlight fundamental limitations, and outline potential directions for improvement.",
        "arxiv_id": "2502.01100",
        "ARXIVID": "2502.01100",
        "COMMENT": "Matches criterion 2 as it evaluates LLM reasoning capabilities and introduces a new evaluation framework for logical reasoning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.01411": {
        "authors": [
            "Jue Gong",
            "Jingkai Wang",
            "Zheng Chen",
            "Xing Liu",
            "Hong Gu",
            "Yulun Zhang",
            "Xiaokang Yang"
        ],
        "title": "Human Body Restoration with One-Step Diffusion Model and A New Benchmark",
        "abstract": "arXiv:2502.01411v1 Announce Type: new  Abstract: Human body restoration, as a specific application of image restoration, is widely applied in practice and plays a vital role across diverse fields. However, thorough research remains difficult, particularly due to the lack of benchmark datasets. In this study, we propose a high-quality dataset automated cropping and filtering (HQ-ACF) pipeline. This pipeline leverages existing object detection datasets and other unlabeled images to automatically crop and filter high-quality human images. Using this pipeline, we constructed a person-based restoration with sophisticated objects and natural activities (\\emph{PERSONA}) dataset, which includes training, validation, and test sets. The dataset significantly surpasses other human-related datasets in both quality and content richness. Finally, we propose \\emph{OSDHuman}, a novel one-step diffusion model for human body restoration. Specifically, we propose a high-fidelity image embedder (HFIE) as the prompt generator to better guide the model with low-quality human image information, effectively avoiding misleading prompts. Experimental results show that OSDHuman outperforms existing methods in both visual quality and quantitative metrics. The dataset and code will at https://github.com/gobunu/OSDHuman.",
        "arxiv_id": "2502.01411",
        "ARXIVID": "2502.01411",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset and a novel diffusion model for human body restoration.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.00640": {
        "authors": [
            "Shirley Wu",
            "Michel Galley",
            "Baolin Peng",
            "Hao Cheng",
            "Gavin Li",
            "Yao Dou",
            "Weixin Cai",
            "James Zou",
            "Jure Leskovec",
            "Jianfeng Gao"
        ],
        "title": "CollabLLM: From Passive Responders to Active Collaborators",
        "abstract": "arXiv:2502.00640v1 Announce Type: new  Abstract: Large Language Models are typically trained with next-turn rewards, limiting their ability to optimize for long-term interaction. As a result, they often respond passively to ambiguous or open-ended user requests, failing to help users reach their ultimate intents and leading to inefficient conversations. To address these limitations, we introduce CollabLLM, a novel and general training framework that enhances multiturn human-LLM collaboration. Its key innovation is a collaborative simulation that estimates the long-term contribution of responses using Multiturn-aware Rewards. By reinforcement fine-tuning these rewards, CollabLLM goes beyond responding to user requests, and actively uncovers user intent and offers insightful suggestions-a key step towards more human-centered AI. We also devise a multiturn interaction benchmark with three challenging tasks such as document creation. CollabLLM significantly outperforms our baselines with averages of 18.5% higher task performance and 46.3% improved interactivity by LLM judges. Finally, we conduct a large user study with 201 judges, where CollabLLM increases user satisfaction by 17.6% and reduces user spent time by 10.4%.",
        "arxiv_id": "2502.00640",
        "ARXIVID": "2502.00640",
        "COMMENT": "Matches criterion 2 as it introduces a novel framework for improving multi-turn interactions in LLMs, which could be relevant to VLLMs or MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.00843": {
        "authors": [
            "Yuxin Lin",
            "Mengshi Qi",
            "Liang Liu",
            "Huadong Ma"
        ],
        "title": "VLM-Assisted Continual learning for Visual Question Answering in Self-Driving",
        "abstract": "arXiv:2502.00843v1 Announce Type: new  Abstract: In this paper, we propose a novel approach for solving the Visual Question Answering (VQA) task in autonomous driving by integrating Vision-Language Models (VLMs) with continual learning. In autonomous driving, VQA plays a vital role in enabling the system to understand and reason about its surroundings. However, traditional models often struggle with catastrophic forgetting when sequentially exposed to new driving tasks, such as perception, prediction, and planning, each requiring different forms of knowledge. To address this challenge, we present a novel continual learning framework that combines VLMs with selective memory replay and knowledge distillation, reinforced by task-specific projection layer regularization. The knowledge distillation allows a previously trained model to act as a \"teacher\" to guide the model through subsequent tasks, minimizing forgetting. Meanwhile, task-specific projection layers calculate the loss based on the divergence of feature representations, ensuring continuity in learning and reducing the shift between tasks. Evaluated on the DriveLM dataset, our framework shows substantial performance improvements, with gains ranging from 21.40% to 32.28% across various metrics. These results highlight the effectiveness of combining continual learning with VLMs in enhancing the resilience and reliability of VQA systems in autonomous driving. We will release our source code.",
        "arxiv_id": "2502.00843",
        "ARXIVID": "2502.00843",
        "COMMENT": "Matches criterion 2 as it integrates vision-language models (VLMs) with continual learning for visual question answering in autonomous driving.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.00717": {
        "authors": [
            "Chao Wang",
            "Jianming Yang",
            "Yang Zhou"
        ],
        "title": "MINT: Mitigating Hallucinations in Large Vision-Language Models via Token Reduction",
        "abstract": "arXiv:2502.00717v1 Announce Type: new  Abstract: Hallucination has been a long-standing and inevitable problem that hinders the application of Large Vision-Language Models (LVLMs) in domains that require high reliability. Various methods focus on improvement depending on data annotations or training strategies, yet place less emphasis on LLM's inherent problems. To fill this gap, we delve into the attention mechanism of the decoding process in the LVLM. Intriguingly, our investigation uncovers the prevalent attention redundancy within the hierarchical architecture of the LVLM, manifesting as overextended image processing in deep layers and an overabundance of non-essential image tokens. Stemming from the observation, we thus propose MINT, a novel training-free decoding strategy, MItigating hallucinations via tokeN reducTion. Specifically, we dynamically intensify the LVLM's local perception capability by masking its attention to irrelevant image tokens. In addition, we use contrastive decoding that pushes the model to focus more on those key image regions. Our full method aims to guide the model in concentrating more on key visual elements during generation. Extensive experimental results on several popular public benchmarks show that our approach achieves a 4% improvement in mitigating hallucinations caused by distracted perception compared to original models. Meanwhile, our approach is demonstrated to make the model perceive 5% more visual points even though we reduce a suite of image tokens.",
        "arxiv_id": "2502.00717",
        "ARXIVID": "2502.00717",
        "COMMENT": "Matches criterion 2 as it proposes a novel decoding strategy to mitigate hallucinations in large vision-language models (LVLMs).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.01056": {
        "authors": [
            "Chao Wang",
            "Xuancheng Zhou",
            "Weiwei Fu",
            "Yang Zhou"
        ],
        "title": "Mitigating Hallucinations in Large Vision-Language Models with Internal Fact-based Contrastive Decoding",
        "abstract": "arXiv:2502.01056v1 Announce Type: new  Abstract: Large Visual Language Models (LVLMs) integrate visual and linguistic modalities, exhibiting exceptional performance across various multimodal tasks. Nevertheless, LVLMs remain vulnerable to the issue of object hallucinations. Previous efforts to mitigate this issue focus on supervised fine-tuning (SFT) or incorporating external knowledge, both of which entail significant costs related to training and the acquisition of external data. To address these challenges, we propose a novel model-agnostic approach termed Internal Fact-based Contrastive Decoding (IFCD), designed to mitigate and suppress hallucinations during the inference process of LVLMs by exploiting the LVLMs' own hallucinations. IFCD is grounded in experimental observations that alterations to the LVLMs' internal representations tend to amplify hallucinations caused by language bias. By contrasting disturbed distribution, IFCD calibrates the LVLMs' output and effectively removes the hallucinatory logits from the final predictions. Experimental results validate that IFCD significantly alleviates both object-level and attribute-level hallucinations while achieving an average 9% accuracy improvement on POPE and 8% accuracy improvement on MME object hallucinations subset compared with direct decoding, respectively.",
        "arxiv_id": "2502.01056",
        "ARXIVID": "2502.01056",
        "COMMENT": "Matches criterion 2 as it proposes a novel decoding method to mitigate hallucinations in large vision-language models (LVLMs).",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.00392": {
        "authors": [
            "Zhichao Sun",
            "Yepeng Liu",
            "Huachao Zhu",
            "Yuliang Gu",
            "Yuda Zou",
            "Zelong Liu",
            "Gui-Song Xia",
            "Bo Du",
            "Yongchao Xu"
        ],
        "title": "RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes",
        "abstract": "arXiv:2502.00392v1 Announce Type: new  Abstract: Drones have become prevalent robotic platforms with diverse applications, showing significant potential in Embodied Artificial Intelligence (Embodied AI). Referring Expression Comprehension (REC) enables drones to locate objects based on natural language expressions, a crucial capability for Embodied AI. Despite advances in REC for ground-level scenes, aerial views introduce unique challenges including varying viewpoints, occlusions and scale variations. To address this gap, we introduce RefDrone, a REC benchmark for drone scenes. RefDrone reveals three key challenges in REC: 1) multi-scale and small-scale target detection; 2) multi-target and no-target samples; 3) complex environment with rich contextual expressions. To efficiently construct this dataset, we develop RDAgent (referring drone annotation framework with multi-agent system), a semi-automated annotation tool for REC tasks. RDAgent ensures high-quality contextual expressions and reduces annotation cost. Furthermore, we propose Number GroundingDINO (NGDINO), a novel method designed to handle multi-target and no-target cases. NGDINO explicitly learns and utilizes the number of objects referred to in the expression. Comprehensive experiments with state-of-the-art REC methods demonstrate that NGDINO achieves superior performance on both the proposed RefDrone and the existing gRefCOCO datasets. The dataset and code will be publicly at https://github.com/sunzc-sunny/refdrone.",
        "arxiv_id": "2502.00392",
        "ARXIVID": "2502.00392",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (RefDrone) for referring expression comprehension in drone scenes, focusing on unique challenges in aerial views.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.01630": {
        "authors": [
            "Yubin Ge",
            "Salvatore Romeo",
            "Jason Cai",
            "Raphael Shu",
            "Monica Sunkara",
            "Yassine Benajiba",
            "Yi Zhang"
        ],
        "title": "TReMu: Towards Neuro-Symbolic Temporal Reasoning for LLM-Agents with Memory in Multi-Session Dialogues",
        "abstract": "arXiv:2502.01630v1 Announce Type: new  Abstract: Temporal reasoning in multi-session dialogues presents a significant challenge which has been under-studied in previous temporal reasoning benchmarks. To bridge this gap, we propose a new evaluation task for temporal reasoning in multi-session dialogues and introduce an approach to construct a new benchmark by augmenting dialogues from LoCoMo and creating multi-choice QAs. Furthermore, we present TReMu, a new framework aimed at enhancing the temporal reasoning capabilities of LLM-agents in this context. Specifically, the framework employs \\textit{time-aware memorization} through timeline summarization, generating retrievable memory by summarizing events in each dialogue session with their inferred dates. Additionally, we integrate \\textit{neuro-symbolic temporal reasoning}, where LLMs generate Python code to perform temporal calculations and select answers. Experimental evaluations on popular LLMs demonstrate that our benchmark is challenging, and the proposed framework significantly improves temporal reasoning performance compared to baseline methods, raising from 29.83 on GPT-4o via standard prompting to 77.67 via our approach and highlighting its effectiveness in addressing temporal reasoning in multi-session dialogues.",
        "arxiv_id": "2502.01630",
        "ARXIVID": "2502.01630",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark for temporal reasoning in multi-session dialogues and proposes a novel framework for LLM-agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.01081": {
        "authors": [
            "Vernon Y. H. Toh",
            "Yew Ken Chia",
            "Deepanway Ghosal",
            "Soujanya Poria"
        ],
        "title": "The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles",
        "abstract": "arXiv:2502.01081v1 Announce Type: new  Abstract: The releases of OpenAI's o1 and o3 mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, o3 outperformed humans in novel problem-solving and skill acquisition on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data. Thus, there is an urgent need to investigate advanced reasoning capabilities in multimodal tasks. To this end, we track the evolution of the GPT-[n] and o-[n] series models on challenging multimodal puzzles, requiring fine-grained visual perception with abstract or algorithmic reasoning. The superior performance of o1 comes at nearly 750 times the computational cost of GPT-4o, raising concerns about its efficiency. Our results reveal a clear upward trend in reasoning capabilities across model iterations, with notable performance jumps across GPT-series models and subsequently to o1. Nonetheless, we observe that the o1 model still struggles with simple multimodal puzzles requiring abstract reasoning. Furthermore, its performance in algorithmic puzzles remains poor. We plan to continuously track new models in the series and update our results in this paper accordingly. All resources used in this evaluation are openly available https://github.com/declare-lab/LLM-PuzzleTest.",
        "arxiv_id": "2502.01081",
        "ARXIVID": "2502.01081",
        "COMMENT": "Matches criterion 2 as it evaluates reasoning performance in multimodal tasks for GPT and o-series models, focusing on their evolution and limitations.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2502.00639": {
        "authors": [
            "Tao Ren",
            "Zishi Zhang",
            "Zehao Li",
            "Jingyang Jiang",
            "Shentao Qin",
            "Guanghao Li",
            "Yan Li",
            "Yi Zheng",
            "Xinping Li",
            "Min Zhan",
            "Yijie Peng"
        ],
        "title": "Zeroth-order Informed Fine-Tuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer",
        "abstract": "arXiv:2502.00639v1 Announce Type: new  Abstract: The probabilistic diffusion model (DM), generating content by inferencing through a recursive chain structure, has emerged as a powerful framework for visual generation. After pre-training on enormous unlabeled data, the model needs to be properly aligned to meet requirements for downstream applications. How to efficiently align the foundation DM is a crucial task. Contemporary methods are either based on Reinforcement Learning (RL) or truncated Backpropagation (BP). However, RL and truncated BP suffer from low sample efficiency and biased gradient estimation respectively, resulting in limited improvement or, even worse, complete training failure. To overcome the challenges, we propose the Recursive Likelihood Ratio (RLR) optimizer, a zeroth-order informed fine-tuning paradigm for DM. The zeroth-order gradient estimator enables the computation graph rearrangement within the recursive diffusive chain, making the RLR's gradient estimator an unbiased one with the lower variance than other methods. We provide theoretical guarantees for the performance of the RLR. Extensive experiments are conducted on image and video generation tasks to validate the superiority of the RLR. Furthermore, we propose a novel prompt technique that is natural for the RLR to achieve a synergistic effect.",
        "arxiv_id": "2502.00639",
        "ARXIVID": "2502.00639",
        "COMMENT": "Does not match any specific criterion but focuses on fine-tuning diffusion models, which is tangentially relevant to generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.00500": {
        "authors": [
            "Yang Cao",
            "Zhao Song",
            "Chiwun Yang"
        ],
        "title": "Video Latent Flow Matching: Optimal Polynomial Projections for Video Interpolation and Extrapolation",
        "abstract": "arXiv:2502.00500v1 Announce Type: new  Abstract: This paper considers an efficient video modeling process called Video Latent Flow Matching (VLFM). Unlike prior works, which randomly sampled latent patches for video generation, our method relies on current strong pre-trained image generation models, modeling a certain caption-guided flow of latent patches that can be decoded to time-dependent video frames. We first speculate multiple images of a video are differentiable with respect to time in some latent space. Based on this conjecture, we introduce the HiPPO framework to approximate the optimal projection for polynomials to generate the probability path. Our approach gains the theoretical benefits of the bounded universal approximation error and timescale robustness. Moreover, VLFM processes the interpolation and extrapolation abilities for video generation with arbitrary frame rates. We conduct experiments on several text-to-video datasets to showcase the effectiveness of our method.",
        "arxiv_id": "2502.00500",
        "ARXIVID": "2502.00500",
        "COMMENT": "This paper does not match any of the specified criteria. It focuses on video interpolation and extrapolation using latent flow matching, which is not directly related to the requested topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.01191": {
        "authors": [
            "Yuxuan Cai",
            "Xiyu Wang",
            "Satoshi Tsutsui",
            "Winnie Pang",
            "Bihan Wen"
        ],
        "title": "Towards Robust and Reliable Concept Representations: Reliability-Enhanced Concept Embedding Model",
        "abstract": "arXiv:2502.01191v1 Announce Type: new  Abstract: Concept Bottleneck Models (CBMs) aim to enhance interpretability by predicting human-understandable concepts as intermediates for decision-making. However, these models often face challenges in ensuring reliable concept representations, which can propagate to downstream tasks and undermine robustness, especially under distribution shifts. Two inherent issues contribute to concept unreliability: sensitivity to concept-irrelevant features (e.g., background variations) and lack of semantic consistency for the same concept across different samples. To address these limitations, we propose the Reliability-Enhanced Concept Embedding Model (RECEM), which introduces a two-fold strategy: Concept-Level Disentanglement to separate irrelevant features from concept-relevant information and a Concept Mixup mechanism to ensure semantic alignment across samples. These mechanisms work together to improve concept reliability, enabling the model to focus on meaningful object attributes and generate faithful concept representations. Experimental results demonstrate that RECEM consistently outperforms existing baselines across multiple datasets, showing superior performance under background and domain shifts. These findings highlight the effectiveness of disentanglement and alignment strategies in enhancing both reliability and robustness in CBMs.",
        "arxiv_id": "2502.01191",
        "ARXIVID": "2502.01191",
        "COMMENT": "This paper does not match any of the specified criteria. It focuses on concept bottleneck models and enhancing interpretability, which is not directly related to the requested topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.00700": {
        "authors": [
            "Yunuo Chen",
            "Qian Li",
            "Bing He",
            "Donghui Feng",
            "Ronghua Wu",
            "Qi Wang",
            "Li Song",
            "Guo Lu",
            "Wenjun Zhang"
        ],
        "title": "S2CFormer: Reorienting Learned Image Compression from Spatial Interaction to Channel Aggregation",
        "abstract": "arXiv:2502.00700v1 Announce Type: new  Abstract: Transformers have achieved significant success in learned image compression (LIC), with Swin Transformers emerging as the mainstream choice for nonlinear transforms. A common belief is that their sophisticated spatial operations contribute most to their efficacy. However, the crucial role of the feed-forward network (FFN) based Channel Aggregation module within the transformer architecture has been largely overlooked, and the over-design of spatial operations leads to a suboptimal trade-off between decoding latency and R-D performance. In this paper, we reevaluate the key factors behind the competence of transformers in LIC. By replacing spatial operations with identity mapping, we are surprised to find that channel operations alone can approach the R-D performance of the leading methods. This solid lower bound of performance emphasizes that the presence of channel aggregation is more essential for the LIC model to achieve competitive performance, while the previously complex spatial interactions are partly redundant. Based on this insight, we initiate the \"S2CFormer\" paradigm, a general architecture that reorients the focus of LIC from Spatial Interaction to Channel Aggregation. We present two instantiations of the S2CFormer: S2C-Conv, and S2C-Attention. Each one incorporates a simple operator for spatial interaction and serves as nonlinear transform blocks for our LIC models. Both models demonstrate state-of-the-art (SOTA) R-D performance and significantly faster decoding speed. These results also motivate further exploration of advanced FFN structures to enhance the R-D performance while maintaining model efficiency. With these foundations, we introduce S2C-Hybrid, an enhanced LIC model that combines the strengths of different S2CFormer instantiations. This model outperforms all the existing methods on several datasets, setting a new benchmark for efficient and high-performance LIC.",
        "arxiv_id": "2502.00700",
        "ARXIVID": "2502.00700",
        "COMMENT": "This paper does not match any of the specified criteria. It focuses on learned image compression and channel aggregation, which is not directly related to spatial understanding, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.00654": {
        "authors": [
            "Junuk Cha",
            "Seongro Yoon",
            "Valeriya Strizhkova",
            "Francois Bremond",
            "Seungryul Baek"
        ],
        "title": "EmoTalkingGaussian: Continuous Emotion-conditioned Talking Head Synthesis",
        "abstract": "arXiv:2502.00654v1 Announce Type: new  Abstract: 3D Gaussian splatting-based talking head synthesis has recently gained attention for its ability to render high-fidelity images with real-time inference speed. However, since it is typically trained on only a short video that lacks the diversity in facial emotions, the resultant talking heads struggle to represent a wide range of emotions. To address this issue, we propose a lip-aligned emotional face generator and leverage it to train our EmoTalkingGaussian model. It is able to manipulate facial emotions conditioned on continuous emotion values (i.e., valence and arousal); while retaining synchronization of lip movements with input audio. Additionally, to achieve the accurate lip synchronization for in-the-wild audio, we introduce a self-supervised learning method that leverages a text-to-speech network and a visual-audio synchronization network. We experiment our EmoTalkingGaussian on publicly available videos and have obtained better results than state-of-the-arts in terms of image quality (measured in PSNR, SSIM, LPIPS), emotion expression (measured in V-RMSE, A-RMSE, V-SA, A-SA, Emotion Accuracy), and lip synchronization (measured in LMD, Sync-E, Sync-C), respectively.",
        "arxiv_id": "2502.00654",
        "ARXIVID": "2502.00654",
        "COMMENT": "This paper does not match any of the specified criteria. It focuses on talking head synthesis and emotion-conditioned generation, which is not directly related to the requested topics.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.00873": {
        "authors": [
            "Subhash Kantamneni",
            "Max Tegmark"
        ],
        "title": "Language Models Use Trigonometry to Do Addition",
        "abstract": "arXiv:2502.00873v1 Announce Type: new  Abstract: Mathematical reasoning is an increasingly important indicator of large language model (LLM) capabilities, yet we lack understanding of how LLMs process even simple mathematical tasks. To address this, we reverse engineer how three mid-sized LLMs compute addition. We first discover that numbers are represented in these LLMs as a generalized helix, which is strongly causally implicated for the tasks of addition and subtraction, and is also causally relevant for integer division, multiplication, and modular arithmetic. We then propose that LLMs compute addition by manipulating this generalized helix using the \"Clock\" algorithm: to solve $a+b$, the helices for $a$ and $b$ are manipulated to produce the $a+b$ answer helix which is then read out to model logits. We model influential MLP outputs, attention head outputs, and even individual neuron preactivations with these helices and verify our understanding with causal interventions. By demonstrating that LLMs represent numbers on a helix and manipulate this helix to perform addition, we present the first representation-level explanation of an LLM's mathematical capability.",
        "arxiv_id": "2502.00873",
        "ARXIVID": "2502.00873",
        "COMMENT": "Does not match any specific criteria but provides an interesting empirical insight into how LLMs perform mathematical reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.00360": {
        "authors": [
            "Liangchen Li",
            "Caoliwen Wang",
            "Yuqi Zhou",
            "Bailin Deng",
            "Juyong Zhang"
        ],
        "title": "Shape from Semantics: 3D Shape Generation from Multi-View Semantics",
        "abstract": "arXiv:2502.00360v1 Announce Type: new  Abstract: We propose ``Shape from Semantics'', which is able to create 3D models whose geometry and appearance match given semantics when observed from different views. Traditional ``Shape from X'' tasks usually use visual input (e.g., RGB images or depth maps) to reconstruct geometry, imposing strict constraints that limit creative explorations. As applications, works like Shadow Art and Wire Art often struggle to grasp the embedded semantics of their design through direct observation and rely heavily on specific setups for proper display. To address these limitations, our framework uses semantics as input, greatly expanding the design space to create objects that integrate multiple semantic elements and are easily discernible by observers. Considering that this task requires a rich imagination, we adopt various generative models and structure-to-detail pipelines. Specifically, we adopt multi-semantics Score Distillation Sampling (SDS) to distill 3D geometry and appearance from 2D diffusion models, ensuring that the initial shape is consistent with the semantic input. We then use image restoration and video generation models to add more details as supervision. Finally, we introduce neural signed distance field (SDF) representation to achieve detailed shape reconstruction. Our framework generates meshes with complex details, well-structured geometry, coherent textures, and smooth transitions, resulting in visually appealing and eye-catching designs. Project page: https://shapefromsemantics.github.io",
        "arxiv_id": "2502.00360",
        "ARXIVID": "2502.00360",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and 3D shape generation, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.01262": {
        "authors": [
            "Eun-Sol Park",
            "MiSo Park",
            "Seung Park",
            "Yong-Goo Shin"
        ],
        "title": "FSPGD: Rethinking Black-box Attacks on Semantic Segmentation",
        "abstract": "arXiv:2502.01262v1 Announce Type: new  Abstract: Transferability, the ability of adversarial examples crafted for one model to deceive other models, is crucial for black-box attacks. Despite advancements in attack methods for semantic segmentation, transferability remains limited, reducing their effectiveness in real-world applications. To address this, we introduce the Feature Similarity Projected Gradient Descent (FSPGD) attack, a novel black-box approach that enhances both attack performance and transferability. Unlike conventional segmentation attacks that rely on output predictions for gradient calculation, FSPGD computes gradients from intermediate layer features. Specifically, our method introduces a loss function that targets local information by comparing features between clean images and adversarial examples, while also disrupting contextual information by accounting for spatial relationships between objects. Experiments on Pascal VOC 2012 and Cityscapes datasets demonstrate that FSPGD achieves superior transferability and attack performance, establishing a new state-of-the-art benchmark. Code is available at https://github.com/KU-AIVS/FSPGD.",
        "arxiv_id": "2502.01262",
        "ARXIVID": "2502.01262",
        "COMMENT": "Does not match any specific criteria but is generally relevant to computer vision and machine learning due to its focus on black-box attacks for semantic segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.00792": {
        "authors": [
            "Leng Cai",
            "Junxuan He",
            "Yikai Li",
            "Junjie Liang",
            "Yuanping Lin",
            "Ziming Quan",
            "Yawen Zeng",
            "Jin Xu"
        ],
        "title": "RTBAgent: A LLM-based Agent System for Real-Time Bidding",
        "abstract": "arXiv:2502.00792v1 Announce Type: new  Abstract: Real-Time Bidding (RTB) enables advertisers to place competitive bids on impression opportunities instantaneously, striving for cost-effectiveness in a highly competitive landscape. Although RTB has widely benefited from the utilization of technologies such as deep learning and reinforcement learning, the reliability of related methods often encounters challenges due to the discrepancies between online and offline environments and the rapid fluctuations of online bidding. To handle these challenges, RTBAgent is proposed as the first RTB agent system based on large language models (LLMs), which synchronizes real competitive advertising bidding environments and obtains bidding prices through an integrated decision-making process. Specifically, obtaining reasoning ability through LLMs, RTBAgent is further tailored to be more professional for RTB via involved auxiliary modules, i.e., click-through rate estimation model, expert strategy knowledge, and daily reflection. In addition, we propose a two-step decision-making process and multi-memory retrieval mechanism, which enables RTBAgent to review historical decisions and transaction records and subsequently make decisions more adaptive to market changes in real-time bidding. Empirical testing with real advertising datasets demonstrates that RTBAgent significantly enhances profitability. The RTBAgent code will be publicly accessible at: https://github.com/CaiLeng/RTBAgent.",
        "arxiv_id": "2502.00792",
        "ARXIVID": "2502.00792",
        "COMMENT": "Does not match any specific criterion but discusses LLM-based systems, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.01116": {
        "authors": [
            "Guanlin Li",
            "Kangjie Chen",
            "Shangwei Guo",
            "Jie Zhang",
            "Han Qiu",
            "Chao Zhang",
            "Guoyin Wang",
            "Tianwei Zhang",
            "Jiwei Li"
        ],
        "title": "Picky LLMs and Unreliable RMs: An Empirical Study on Safety Alignment after Instruction Tuning",
        "abstract": "arXiv:2502.01116v1 Announce Type: new  Abstract: Large language models (LLMs) have emerged as powerful tools for addressing a wide range of general inquiries and tasks. Despite this, fine-tuning aligned LLMs on smaller, domain-specific datasets, critical to adapting them to specialized tasks, can inadvertently degrade their safety alignment, even when the datasets are benign. This phenomenon makes models more susceptible to providing inappropriate responses. In this study, we systematically examine the factors contributing to safety alignment degradation in benign fine-tuning scenarios. Our analysis identifies three critical factors affecting aligned LLMs: answer structure, identity calibration, and role-play. Additionally, we evaluate the reliability of state-of-the-art reward models (RMs), which are often used to guide alignment processes. Our findings reveal that these RMs frequently fail to accurately reflect human preferences regarding safety, underscoring their limitations in practical applications. By uncovering these challenges, our work highlights the complexities of maintaining safety alignment during fine-tuning and offers guidance to help developers balance utility and safety in LLMs. Datasets and fine-tuning code used in our experiments can be found in https://github.com/GuanlinLee/llm_instruction_tuning.",
        "arxiv_id": "2502.01116",
        "ARXIVID": "2502.01116",
        "COMMENT": "Does not match any specific criterion but discusses safety alignment in LLMs, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.01550": {
        "authors": [
            "Dimitrios Michail",
            "Charalampos Davalas",
            "Lefki-Ioanna Panagiotou",
            "Ioannis Prapas",
            "Spyros Kondylatos",
            "Nikolaos Ioannis Bountos",
            "Ioannis Papoutsis"
        ],
        "title": "FireCastNet: Earth-as-a-Graph for Seasonal Fire Prediction",
        "abstract": "arXiv:2502.01550v1 Announce Type: new  Abstract: With climate change expected to exacerbate fire weather conditions, the accurate and timely anticipation of wildfires becomes increasingly crucial for disaster mitigation. In this study, we utilize SeasFire, a comprehensive global wildfire dataset with climate, vegetation, oceanic indices, and human-related variables, to enable seasonal wildfire forecasting with machine learning. For the predictive analysis, we present FireCastNet, a novel architecture which combines a 3D convolutional encoder with GraphCast, originally developed for global short-term weather forecasting using graph neural networks. FireCastNet is trained to capture the context leading to wildfires, at different spatial and temporal scales. Our investigation focuses on assessing the effectiveness of our model in predicting the presence of burned areas at varying forecasting time horizons globally, extending up to six months into the future, and on how different spatial or/and temporal context affects the performance. Our findings demonstrate the potential of deep learning models in seasonal fire forecasting; longer input time-series leads to more robust predictions, while integrating spatial information to capture wildfire spatio-temporal dynamics boosts performance. Finally, our results hint that in order to enhance performance at longer forecasting horizons, a larger receptive field spatially needs to be considered.",
        "arxiv_id": "2502.01550",
        "ARXIVID": "2502.01550",
        "COMMENT": "Does not match any specific criterion but involves spatio-temporal modeling, which is tangentially relevant to spatial intelligence.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.01187": {
        "authors": [
            "Hao Li",
            "Di Huang",
            "Ziyu Wang",
            "Amir M. Rahmani"
        ],
        "title": "Skewed Memorization in Large Language Models: Quantification and Decomposition",
        "abstract": "arXiv:2502.01187v1 Announce Type: new  Abstract: Memorization in Large Language Models (LLMs) poses privacy and security risks, as models may unintentionally reproduce sensitive or copyrighted data. Existing analyses focus on average-case scenarios, often neglecting the highly skewed distribution of memorization. This paper examines memorization in LLM supervised fine-tuning (SFT), exploring its relationships with training duration, dataset size, and inter-sample similarity. By analyzing memorization probabilities over sequence lengths, we link this skewness to the token generation process, offering insights for estimating memorization and comparing it to established metrics. Through theoretical analysis and empirical evaluation, we provide a comprehensive understanding of memorization behaviors and propose strategies to detect and mitigate risks, contributing to more privacy-preserving LLMs.",
        "arxiv_id": "2502.01187",
        "ARXIVID": "2502.01187",
        "COMMENT": "Does not match any specific criterion but is related to LLMs and privacy, which is tangentially relevant to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.01490": {
        "authors": [
            "Yuto Matsuo",
            "Ryo Hayamizu",
            "Hirokatsu Kataoka",
            "Akio Nakamura"
        ],
        "title": "MoireDB: Formula-generated Interference-fringe Image Dataset",
        "abstract": "arXiv:2502.01490v1 Announce Type: new  Abstract: Image recognition models have struggled to treat recognition robustness to real-world degradations. In this context, data augmentation methods like PixMix improve robustness but rely on generative arts and feature visualizations (FVis), which have copyright, drawing cost, and scalability issues. We propose MoireDB, a formula-generated interference-fringe image dataset for image augmentation enhancing robustness. MoireDB eliminates copyright concerns, reduces dataset assembly costs, and enhances robustness by leveraging illusory patterns. Experiments show that MoireDB augmented images outperforms traditional Fractal arts and FVis-based augmentations, making it a scalable and effective solution for improving model robustness against real-world degradations.",
        "arxiv_id": "2502.01490",
        "ARXIVID": "2502.01490",
        "COMMENT": "This paper does not match any of the specified criteria. It focuses on a formula-generated dataset for image augmentation, which is not directly related to spatial understanding, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.00307": {
        "authors": [
            "Mengfei Xia",
            "Yu Zhou",
            "Ran Yi",
            "Yong-Jin Liu",
            "Wenping Wang"
        ],
        "title": "A Diffusion Model Translator for Efficient Image-to-Image Translation",
        "abstract": "arXiv:2502.00307v1 Announce Type: new  Abstract: Applying diffusion models to image-to-image translation (I2I) has recently received increasing attention due to its practical applications. Previous attempts inject information from the source image into each denoising step for an iterative refinement, thus resulting in a time-consuming implementation. We propose an efficient method that equips a diffusion model with a lightweight translator, dubbed a Diffusion Model Translator (DMT), to accomplish I2I. Specifically, we first offer theoretical justification that in employing the pioneering DDPM work for the I2I task, it is both feasible and sufficient to transfer the distribution from one domain to another only at some intermediate step. We further observe that the translation performance highly depends on the chosen timestep for domain transfer, and therefore propose a practical strategy to automatically select an appropriate timestep for a given task. We evaluate our approach on a range of I2I applications, including image stylization, image colorization, segmentation to image, and sketch to image, to validate its efficacy and general utility. The comparisons show that our DMT surpasses existing methods in both quality and efficiency. Code will be made publicly available.",
        "arxiv_id": "2502.00307",
        "ARXIVID": "2502.00307",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.00404": {
        "authors": [
            "Rongchang Lu",
            "Changyu Li",
            "Donghang Li",
            "Guojing Zhang",
            "Jianqiang Huang",
            "Xilai Li"
        ],
        "title": "Exploring Linear Attention Alternative for Single Image Super-Resolution",
        "abstract": "arXiv:2502.00404v1 Announce Type: new  Abstract: Deep learning-based single-image super-resolution (SISR) technology focuses on enhancing low-resolution (LR) images into high-resolution (HR) ones. Although significant progress has been made, challenges remain in computational complexity and quality, particularly in remote sensing image processing. To address these issues, we propose our Omni-Scale RWKV Super-Resolution (OmniRWKVSR) model which presents a novel approach that combines the Receptance Weighted Key Value (RWKV) architecture with feature extraction techniques such as Visual RWKV Spatial Mixing (VRSM) and Visual RWKV Channel Mixing (VRCM), aiming to overcome the limitations of existing methods and achieve superior SISR performance. This work has proved able to provide effective solutions for high-quality image reconstruction. Under the 4x Super-Resolution tasks, compared to the MambaIR model, we achieved an average improvement of 0.26% in PSNR and 0.16% in SSIM.",
        "arxiv_id": "2502.00404",
        "ARXIVID": "2502.00404",
        "COMMENT": "Does not match any specific criteria. Focuses on single-image super-resolution using a novel architecture, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.01201": {
        "authors": [
            "Yiyue Li",
            "Shaoting Zhang",
            "Kang Li",
            "Qicheng Lao"
        ],
        "title": "One-to-Normal: Anomaly Personalization for Few-shot Anomaly Detection",
        "abstract": "arXiv:2502.01201v1 Announce Type: new  Abstract: Traditional Anomaly Detection (AD) methods have predominantly relied on unsupervised learning from extensive normal data. Recent AD methods have evolved with the advent of large pre-trained vision-language models, enhancing few-shot anomaly detection capabilities. However, these latest AD methods still exhibit limitations in accuracy improvement. One contributing factor is their direct comparison of a query image's features with those of few-shot normal images. This direct comparison often leads to a loss of precision and complicates the extension of these techniques to more complex domains--an area that remains underexplored in a more refined and comprehensive manner. To address these limitations, we introduce the anomaly personalization method, which performs a personalized one-to-normal transformation of query images using an anomaly-free customized generation model, ensuring close alignment with the normal manifold. Moreover, to further enhance the stability and robustness of prediction results, we propose a triplet contrastive anomaly inference strategy, which incorporates a comprehensive comparison between the query and generated anomaly-free data pool and prompt information. Extensive evaluations across eleven datasets in three domains demonstrate our model's effectiveness compared to the latest AD methods. Additionally, our method has been proven to transfer flexibly to other AD methods, with the generated image data effectively improving the performance of other AD methods.",
        "arxiv_id": "2502.01201",
        "ARXIVID": "2502.01201",
        "COMMENT": "Does not match any specific criteria. Focuses on anomaly detection and personalization, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.01335": {
        "authors": [
            "Costin F. Ciusdel",
            "Alex Serban",
            "Tiziano Passerini"
        ],
        "title": "ConceptVAE: Self-Supervised Fine-Grained Concept Disentanglement from 2D Echocardiographies",
        "abstract": "arXiv:2502.01335v1 Announce Type: new  Abstract: While traditional self-supervised learning methods improve performance and robustness across various medical tasks, they rely on single-vector embeddings that may not capture fine-grained concepts such as anatomical structures or organs. The ability to identify such concepts and their characteristics without supervision has the potential to improve pre-training methods, and enable novel applications such as fine-grained image retrieval and concept-based outlier detection. In this paper, we introduce ConceptVAE, a novel pre-training framework that detects and disentangles fine-grained concepts from their style characteristics in a self-supervised manner. We present a suite of loss terms and model architecture primitives designed to discretise input data into a preset number of concepts along with their local style. We validate ConceptVAE both qualitatively and quantitatively, demonstrating its ability to detect fine-grained anatomical structures such as blood pools and septum walls from 2D cardiac echocardiographies. Quantitatively, ConceptVAE outperforms traditional self-supervised methods in tasks such as region-based instance retrieval, semantic segmentation, out-of-distribution detection, and object detection. Additionally, we explore the generation of in-distribution synthetic data that maintains the same concepts as the training data but with distinct styles, highlighting its potential for more calibrated data generation. Overall, our study introduces and validates a promising new pre-training technique based on concept-style disentanglement, opening multiple avenues for developing models for medical image analysis that are more interpretable and explainable than black-box approaches.",
        "arxiv_id": "2502.01335",
        "ARXIVID": "2502.01335",
        "COMMENT": "Does not match any specific criteria. Focuses on medical image analysis and self-supervised learning, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.00665": {
        "authors": [
            "Fuxi Ling",
            "Hongye Liu",
            "Guoqiang Huang",
            "Jing Li",
            "Hong Wu",
            "Zhihao Tang"
        ],
        "title": "Cross-Modal Synergies: Unveiling the Potential of Motion-Aware Fusion Networks in Handling Dynamic and Static ReID Scenarios",
        "abstract": "arXiv:2502.00665v1 Announce Type: new  Abstract: Navigating the complexities of person re-identification (ReID) in varied surveillance scenarios, particularly when occlusions occur, poses significant challenges. We introduce an innovative Motion-Aware Fusion (MOTAR-FUSE) network that utilizes motion cues derived from static imagery to significantly enhance ReID capabilities. This network incorporates a dual-input visual adapter capable of processing both images and videos, thereby facilitating more effective feature extraction. A unique aspect of our approach is the integration of a motion consistency task, which empowers the motion-aware transformer to adeptly capture the dynamics of human motion. This technique substantially improves the recognition of features in scenarios where occlusions are prevalent, thereby advancing the ReID process. Our comprehensive evaluations across multiple ReID benchmarks, including holistic, occluded, and video-based scenarios, demonstrate that our MOTAR-FUSE network achieves superior performance compared to existing approaches.",
        "arxiv_id": "2502.00665",
        "ARXIVID": "2502.00665",
        "COMMENT": "Focuses on person re-identification using motion-aware fusion, which is not directly related to the criteria but involves multi-modal aspects.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.01232": {
        "authors": [
            "Andrew Cropper",
            "David M. Cerna"
        ],
        "title": "Efficient rule induction by ignoring pointless rules",
        "abstract": "arXiv:2502.01232v1 Announce Type: new  Abstract: The goal of inductive logic programming (ILP) is to find a set of logical rules that generalises training examples and background knowledge. We introduce an ILP approach that identifies pointless rules. A rule is pointless if it contains a redundant literal or cannot discriminate against negative examples. We show that ignoring pointless rules allows an ILP system to soundly prune the hypothesis space. Our experiments on multiple domains, including visual reasoning and game playing, show that our approach can reduce learning times by 99% whilst maintaining predictive accuracies.",
        "arxiv_id": "2502.01232",
        "ARXIVID": "2502.01232",
        "COMMENT": "Does not match any specific criteria but involves visual reasoning and game playing, which might be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.00547": {
        "authors": [
            "Zaitian Wang",
            "Jian He",
            "Yu Liang",
            "Xiyuan Hu",
            "Tianhao Peng",
            "Kaixin Wang",
            "Jiakai Wang",
            "Chenlong Zhang",
            "Weili Zhang",
            "Shuang Niu",
            "Xiaoyang Xie"
        ],
        "title": "Milmer: a Framework for Multiple Instance Learning based Multimodal Emotion Recognition",
        "abstract": "arXiv:2502.00547v1 Announce Type: new  Abstract: Emotions play a crucial role in human behavior and decision-making, making emotion recognition a key area of interest in human-computer interaction (HCI). This study addresses the challenges of emotion recognition by integrating facial expression analysis with electroencephalogram (EEG) signals, introducing a novel multimodal framework-Milmer. The proposed framework employs a transformer-based fusion approach to effectively integrate visual and physiological modalities. It consists of an EEG preprocessing module, a facial feature extraction and balancing module, and a cross-modal fusion module. To enhance visual feature extraction, we fine-tune a pre-trained Swin Transformer on emotion-related datasets. Additionally, a cross-attention mechanism is introduced to balance token representation across modalities, ensuring effective feature integration. A key innovation of this work is the adoption of a multiple instance learning (MIL) approach, which extracts meaningful information from multiple facial expression images over time, capturing critical temporal dynamics often overlooked in previous studies. Extensive experiments conducted on the DEAP dataset demonstrate the superiority of the proposed framework, achieving a classification accuracy of 96.72% in the four-class emotion recognition task. Ablation studies further validate the contributions of each module, highlighting the significance of advanced feature extraction and fusion strategies in enhancing emotion recognition performance. Our code are available at https://github.com/liangyubuaa/Milmer.",
        "arxiv_id": "2502.00547",
        "ARXIVID": "2502.00547",
        "COMMENT": "Does not match any specific criteria. Focuses on multimodal emotion recognition, which is not directly related to spatial understanding, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.00156": {
        "authors": [
            "Joseph Fioresi",
            "Ishan Rajendrakumar Dave",
            "Mubarak Shah"
        ],
        "title": "ALBAR: Adversarial Learning approach to mitigate Biases in Action Recognition",
        "abstract": "arXiv:2502.00156v1 Announce Type: new  Abstract: Bias in machine learning models can lead to unfair decision making, and while it has been well-studied in the image and text domains, it remains underexplored in action recognition. Action recognition models often suffer from background bias (i.e., inferring actions based on background cues) and foreground bias (i.e., relying on subject appearance), which can be detrimental to real-life applications such as autonomous vehicles or assisted living monitoring. While prior approaches have mainly focused on mitigating background bias using specialized augmentations, we thoroughly study both biases. We propose ALBAR, a novel adversarial training method that mitigates foreground and background biases without requiring specialized knowledge of the bias attributes. Our framework applies an adversarial cross-entropy loss to the sampled static clip (where all the frames are the same) and aims to make its class probabilities uniform using a proposed entropy maximization loss. Additionally, we introduce a gradient penalty loss for regularization against the debiasing process. We evaluate our method on established background and foreground bias protocols, setting a new state-of-the-art and strongly improving combined debiasing performance by over 12% on HMDB51. Furthermore, we identify an issue of background leakage in the existing UCF101 protocol for bias evaluation which provides a shortcut to predict actions and does not provide an accurate measure of the debiasing capability of a model. We address this issue by proposing more fine-grained segmentation boundaries for the actor, where our method also outperforms existing approaches. Project Page: https://joefioresi718.github.io/ALBAR_webpage/",
        "arxiv_id": "2502.00156",
        "ARXIVID": "2502.00156",
        "COMMENT": "Does not match any specific criteria. Focuses on mitigating biases in action recognition, which is not directly related to spatial understanding, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.01403": {
        "authors": [
            "Li Zhiteng",
            "Xia Mingyuan",
            "Zhang Jingyuan",
            "Hui Zheng",
            "Kong Linghe",
            "Zhang Yulun",
            "Yang Xiaokang"
        ],
        "title": "AdaSVD: Adaptive Singular Value Decomposition for Large Language Models",
        "abstract": "arXiv:2502.01403v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved remarkable success in natural language processing (NLP) tasks, yet their substantial memory requirements present significant challenges for deployment on resource-constrained devices. Singular Value Decomposition (SVD) has emerged as a promising compression technique for LLMs, offering considerable reductions in memory overhead. However, existing SVD-based methods often struggle to effectively mitigate the errors introduced by SVD truncation, leading to a noticeable performance gap when compared to the original models. Furthermore, applying a uniform compression ratio across all transformer layers fails to account for the varying importance of different layers. To address these challenges, we propose AdaSVD, an adaptive SVD-based LLM compression approach. Specifically, AdaSVD introduces adaComp, which adaptively compensates for SVD truncation errors by alternately updating the singular matrices U and V^T. Additionally, AdaSVD introduces adaCR, which adaptively assigns layer-specific compression ratios based on the relative importance of each layer. Extensive experiments across multiple LLM families and evaluation metrics demonstrate that AdaSVD consistently outperforms state-of-the-art (SOTA) SVD-based methods, achieving superior performance with significantly reduced memory requirements. The code and models will be available at https://github.com/ZHITENGLI/AdaSVD.",
        "arxiv_id": "2502.01403",
        "ARXIVID": "2502.01403",
        "COMMENT": "Does not match any specific criteria. Focuses on compression techniques for LLMs, which is not directly related to spatial understanding, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.01051": {
        "authors": [
            "Tao Zhang",
            "Cheng Da",
            "Kun Ding",
            "Kun Jin",
            "Yan Li",
            "Tingting Gao",
            "Di Zhang",
            "Shiming Xiang",
            "Chunhong Pan"
        ],
        "title": "Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level Preference Optimization",
        "abstract": "arXiv:2502.01051v1 Announce Type: new  Abstract: Preference optimization for diffusion models aims to align them with human preferences for images. Previous methods typically leverage Vision-Language Models (VLMs) as pixel-level reward models to approximate human preferences. However, when used for step-level preference optimization, these models face challenges in handling noisy images of different timesteps and require complex transformations into pixel space. In this work, we demonstrate that diffusion models are inherently well-suited for step-level reward modeling in the latent space, as they can naturally extract features from noisy latent images. Accordingly, we propose the Latent Reward Model (LRM), which repurposes components of diffusion models to predict preferences of latent images at various timesteps. Building on LRM, we introduce Latent Preference Optimization (LPO), a method designed for step-level preference optimization directly in the latent space. Experimental results indicate that LPO not only significantly enhances performance in aligning diffusion models with general, aesthetic, and text-image alignment preferences, but also achieves 2.5-28$\\times$ training speedup compared to existing preference optimization methods. Our code will be available at https://github.com/casiatao/LPO.",
        "arxiv_id": "2502.01051",
        "ARXIVID": "2502.01051",
        "COMMENT": "Does not match any specific criteria. Focuses on preference optimization for diffusion models, which is not directly related to spatial understanding, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.01142": {
        "authors": [
            "Xinyan Guan",
            "Jiali Zeng",
            "Fandong Meng",
            "Chunlei Xin",
            "Yaojie Lu",
            "Hongyu Lin",
            "Xianpei Han",
            "Le Sun",
            "Jie Zhou"
        ],
        "title": "DeepRAG: Thinking to Retrieval Step by Step for Large Language Models",
        "abstract": "arXiv:2502.01142v1 Announce Type: new  Abstract: Large Language Models (LLMs) have shown remarkable potential in reasoning while they still suffer from severe factual hallucinations due to timeliness, accuracy, and coverage of parametric knowledge. Meanwhile, integrating reasoning with retrieval-augmented generation (RAG) remains challenging due to ineffective task decomposition and redundant retrieval, which can introduce noise and degrade response quality. In this paper, we propose DeepRAG, a framework that models retrieval-augmented reasoning as a Markov Decision Process (MDP), enabling strategic and adaptive retrieval. By iteratively decomposing queries, DeepRAG dynamically determines whether to retrieve external knowledge or rely on parametric reasoning at each step. Experiments show that DeepRAG improves retrieval efficiency while improving answer accuracy by 21.99%, demonstrating its effectiveness in optimizing retrieval-augmented reasoning.",
        "arxiv_id": "2502.01142",
        "ARXIVID": "2502.01142",
        "COMMENT": "Does not match any specific criteria. Focuses on retrieval-augmented reasoning for LLMs, which is not directly related to spatial understanding, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.00145": {
        "authors": [
            "David Speck",
            "Markus Hecher",
            "Daniel Gnad",
            "Johannes K. Fichte",
            "Augusto B. Corr\\^ea"
        ],
        "title": "Counting and Reasoning with Plans",
        "abstract": "arXiv:2502.00145v1 Announce Type: new  Abstract: Classical planning asks for a sequence of operators reaching a given goal. While the most common case is to compute a plan, many scenarios require more than that. However, quantitative reasoning on the plan space remains mostly unexplored. A fundamental problem is to count plans, which relates to the conditional probability on the plan space. Indeed, qualitative and quantitative approaches are well-established in various other areas of automated reasoning. We present the first study to quantitative and qualitative reasoning on the plan space. In particular, we focus on polynomially bounded plans. On the theoretical side, we study its complexity, which gives rise to rich reasoning modes. Since counting is hard in general, we introduce the easier notion of facets, which enables understanding the significance of operators. On the practical side, we implement quantitative reasoning for planning. Thereby, we transform a planning task into a propositional formula and use knowledge compilation to count different plans. This framework scales well to large plan spaces, while enabling rich reasoning capabilities such as learning pruning functions and explainable planning.",
        "arxiv_id": "2502.00145",
        "ARXIVID": "2502.00145",
        "COMMENT": "Does not match any specific criterion but is relevant to reasoning and planning, which may be of tangential interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.00333": {
        "authors": [
            "Kai Liu",
            "Kaicheng Yang",
            "Zheng Chen",
            "Zhiteng Li",
            "Yong Guo",
            "Wenbo Li",
            "Linghe Kong",
            "Yulun Zhang"
        ],
        "title": "BiMaCoSR: Binary One-Step Diffusion Model Leveraging Flexible Matrix Compression for Real Super-Resolution",
        "abstract": "arXiv:2502.00333v1 Announce Type: new  Abstract: While super-resolution (SR) methods based on diffusion models (DM) have demonstrated inspiring performance, their deployment is impeded due to the heavy request of memory and computation. Recent researchers apply two kinds of methods to compress or fasten the DM. One is to compress the DM into 1-bit, aka binarization, alleviating the storage and computation pressure. The other distills the multi-step DM into only one step, significantly speeding up inference process. Nonetheless, it remains impossible to deploy DM to resource-limited edge devices. To address this problem, we propose BiMaCoSR, which combines binarization and one-step distillation to obtain extreme compression and acceleration. To prevent the catastrophic collapse of the model caused by binarization, we proposed sparse matrix branch (SMB) and low rank matrixbranch (LRM). Both auxiliary branches pass the full-precision (FP) information but in different ways. SMB absorbs the extreme values and its output is high rank, carrying abundant FP information. Whereas, the design of LRMB is inspired by LoRA and is initialized with the top r SVD components, outputting low rank representation. The computation and storage overhead of our proposed branches can be safely ignored. Comprehensive comparison experiments are conducted to exhibit BiMaCoSR outperforms current state-of-the-art binarization methods and gains competitive performance compared with FP one-step model. BiMaCoSR achieves a 23.8x compression ratio and a 27.4x speedup ratio compared to FP counterpart. Our code and model are available at https://github.com/Kai-Liu001/BiMaCoSR.",
        "arxiv_id": "2502.00333",
        "ARXIVID": "2502.00333",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.01181": {
        "authors": [
            "Zhiliang Wu",
            "Kerui Chen",
            "Kun Li",
            "Hehe Fan",
            "Yi Yang"
        ],
        "title": "BVINet: Unlocking Blind Video Inpainting with Zero Annotations",
        "abstract": "arXiv:2502.01181v1 Announce Type: new  Abstract: Video inpainting aims to fill in corrupted regions of the video with plausible contents. Existing methods generally assume that the locations of corrupted regions are known, focusing primarily on the \"how to inpaint\". This reliance necessitates manual annotation of the corrupted regions using binary masks to indicate \"whereto inpaint\". However, the annotation of these masks is labor-intensive and expensive, limiting the practicality of current methods. In this paper, we expect to relax this assumption by defining a new blind video inpainting setting, enabling the networks to learn the mapping from corrupted video to inpainted result directly, eliminating the need of corrupted region annotations. Specifically, we propose an end-to-end blind video inpainting network (BVINet) to address both \"where to inpaint\" and \"how to inpaint\" simultaneously. On the one hand, BVINet can predict the masks of corrupted regions by detecting semantic-discontinuous regions of the frame and utilizing temporal consistency prior of the video. On the other hand, the predicted masks are incorporated into the BVINet, allowing it to capture valid context information from uncorrupted regions to fill in corrupted ones. Besides, we introduce a consistency loss to regularize the training parameters of BVINet. In this way, mask prediction and video completion mutually constrain each other, thereby maximizing the overall performance of the trained model. Furthermore, we customize a dataset consisting of synthetic corrupted videos, real-world corrupted videos, and their corresponding completed videos. This dataset serves as a valuable resource for advancing blind video inpainting research. Extensive experimental results demonstrate the effectiveness and superiority of our method.",
        "arxiv_id": "2502.01181",
        "ARXIVID": "2502.01181",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.01441": {
        "authors": [
            "Quan Dao",
            "Khanh Doan",
            "Di Liu",
            "Trung Le",
            "Dimitris Metaxas"
        ],
        "title": "Improved Training Technique for Latent Consistency Models",
        "abstract": "arXiv:2502.01441v1 Announce Type: new  Abstract: Consistency models are a new family of generative models capable of producing high-quality samples in either a single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the success of scaling consistency training to large-scale datasets, particularly for text-to-image and video generation tasks, is determined by performance in the latent space. In this work, we analyze the statistical differences between pixel and latent spaces, discovering that latent data often contains highly impulsive outliers, which significantly degrade the performance of iCT in the latent space. To address this, we replace Pseudo-Huber losses with Cauchy losses, effectively mitigating the impact of outliers. Additionally, we introduce a diffusion loss at early timesteps and employ optimal transport (OT) coupling to further enhance performance. Lastly, we introduce the adaptive scaling-$c$ scheduler to manage the robust training process and adopt Non-scaling LayerNorm in the architecture to better capture the statistics of the features and reduce outlier impact. With these strategies, we successfully train latent consistency models capable of high-quality sampling with one or two steps, significantly narrowing the performance gap between latent consistency and diffusion models. The implementation is released here: https://github.com/quandao10/sLCT/",
        "arxiv_id": "2502.01441",
        "ARXIVID": "2502.01441",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling in general.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.00173": {
        "authors": [
            "Rohan Chacko",
            "Nicolai Haeni",
            "Eldar Khaliullin",
            "Lin Sun",
            "Douglas Lee"
        ],
        "title": "Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation",
        "abstract": "arXiv:2502.00173v1 Announce Type: new  Abstract: We introduce Lifting By Gaussians (LBG), a novel approach for open-world instance segmentation of 3D Gaussian Splatted Radiance Fields (3DGS). Recently, 3DGS Fields have emerged as a highly efficient and explicit alternative to Neural Field-based methods for high-quality Novel View Synthesis. Our 3D instance segmentation method directly lifts 2D segmentation masks from SAM (alternately FastSAM, etc.), together with features from CLIP and DINOv2, directly fusing them onto 3DGS (or similar Gaussian radiance fields such as 2DGS). Unlike previous approaches, LBG requires no per-scene training, allowing it to operate seamlessly on any existing 3DGS reconstruction. Our approach is not only an order of magnitude faster and simpler than existing approaches; it is also highly modular, enabling 3D semantic segmentation of existing 3DGS fields without requiring a specific parametrization of the 3D Gaussians. Furthermore, our technique achieves superior semantic segmentation for 2D semantic novel view synthesis and 3D asset extraction results while maintaining flexibility and efficiency. We further introduce a novel approach to evaluate individually segmented 3D assets from 3D radiance field segmentation methods.",
        "arxiv_id": "2502.00173",
        "ARXIVID": "2502.00173",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and segmentation, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.00833": {
        "authors": [
            "Akhshan P",
            "Taneti Sanjay",
            "Chandrakala S"
        ],
        "title": "Cross multiscale vision transformer for deep fake detection",
        "abstract": "arXiv:2502.00833v1 Announce Type: new  Abstract: The proliferation of deep fake technology poses significant challenges to digital media authenticity, necessitating robust detection mechanisms. This project evaluates deep fake detection using the SP Cup's 2025 deep fake detection challenge dataset. We focused on exploring various deep learning models for detecting deep fake content, utilizing traditional deep learning techniques alongside newer architectures. Our approach involved training a series of models and rigorously assessing their performance using metrics such as accuracy.",
        "arxiv_id": "2502.00833",
        "ARXIVID": "2502.00833",
        "COMMENT": "Does not match any specific criterion but focuses on deep fake detection, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.00412": {
        "authors": [
            "Ziyu Wang",
            "Tengyu Pan",
            "Zhenyu Li",
            "Wu Ji",
            "Li Xiuxing",
            "Jianyong Wang"
        ],
        "title": "TROI: Cross-Subject Pretraining with Sparse Voxel Selection for Enhanced fMRI Visual Decoding",
        "abstract": "arXiv:2502.00412v1 Announce Type: new  Abstract: fMRI (functional Magnetic Resonance Imaging) visual decoding involves decoding the original image from brain signals elicited by visual stimuli. This often relies on manually labeled ROIs (Regions of Interest) to select brain voxels. However, these ROIs can contain redundant information and noise, reducing decoding performance. Additionally, the lack of automated ROI labeling methods hinders the practical application of fMRI visual decoding technology, especially for new subjects. This work presents TROI (Trainable Region of Interest), a novel two-stage, data-driven ROI labeling method for cross-subject fMRI decoding tasks, particularly when subject samples are limited. TROI leverages labeled ROIs in the dataset to pretrain an image decoding backbone on a cross-subject dataset, enabling efficient optimization of the input layer for new subjects without retraining the entire model from scratch. In the first stage, we introduce a voxel selection method that combines sparse mask training and low-pass filtering to quickly generate the voxel mask and determine input layer dimensions. In the second stage, we apply a learning rate rewinding strategy to fine-tune the input layer for downstream tasks. Experimental results on the same small sample dataset as the baseline method for brain visual retrieval and reconstruction tasks show that our voxel selection method surpasses the state-of-the-art method MindEye2 with an annotated ROI mask.",
        "arxiv_id": "2502.00412",
        "ARXIVID": "2502.00412",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.01002": {
        "authors": [
            "Wenfei Zhang",
            "Ruipeng Zhao",
            "Yongxiang Yao",
            "Yi Wan",
            "Peihao Wu",
            "Jiayuan Li",
            "Yansheng Li",
            "Yongjun Zhang"
        ],
        "title": "Multi-Resolution SAR and Optical Remote Sensing Image Registration Methods: A Review, Datasets, and Future Perspectives",
        "abstract": "arXiv:2502.01002v1 Announce Type: new  Abstract: Synthetic Aperture Radar (SAR) and optical image registration is essential for remote sensing data fusion, with applications in military reconnaissance, environmental monitoring, and disaster management. However, challenges arise from differences in imaging mechanisms, geometric distortions, and radiometric properties between SAR and optical images. As image resolution increases, fine SAR textures become more significant, leading to alignment issues and 3D spatial discrepancies. Two major gaps exist: the lack of a publicly available multi-resolution, multi-scene registration dataset and the absence of systematic analysis of current methods. To address this, the MultiResSAR dataset was created, containing over 10k pairs of multi-source, multi-resolution, and multi-scene SAR and optical images. Sixteen state-of-the-art algorithms were tested. Results show no algorithm achieves 100% success, and performance decreases as resolution increases, with most failing on sub-meter data. XoFTR performs best among deep learning methods (40.58%), while RIFT performs best among traditional methods (66.51%). Future research should focus on noise suppression, 3D geometric fusion, cross-view transformation modeling, and deep learning optimization for robust registration of high-resolution SAR and optical images. The dataset is available at https://github.com/betterlll/Multi-Resolution-SAR-dataset-.",
        "arxiv_id": "2502.01002",
        "ARXIVID": "2502.01002",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.00462": {
        "authors": [
            "Kihwan Ryoo",
            "Hyungtae Lim",
            "Hyun Myung"
        ],
        "title": "MambaGlue: Fast and Robust Local Feature Matching With Mamba",
        "abstract": "arXiv:2502.00462v1 Announce Type: new  Abstract: In recent years, robust matching methods using deep learning-based approaches have been actively studied and improved in computer vision tasks. However, there remains a persistent demand for both robust and fast matching techniques. To address this, we propose a novel Mamba-based local feature matching approach, called MambaGlue, where Mamba is an emerging state-of-the-art architecture rapidly gaining recognition for its superior speed in both training and inference, and promising performance compared with Transformer architectures. In particular, we propose two modules: a) MambaAttention mixer to simultaneously and selectively understand the local and global context through the Mamba-based self-attention structure and b) deep confidence score regressor, which is a multi-layer perceptron (MLP)-based architecture that evaluates a score indicating how confidently matching predictions correspond to the ground-truth correspondences. Consequently, our MambaGlue achieves a balance between robustness and efficiency in real-world applications. As verified on various public datasets, we demonstrate that our MambaGlue yields a substantial performance improvement over baseline approaches while maintaining fast inference speed. Our code will be available on https://github.com/url-kaist/MambaGlue",
        "arxiv_id": "2502.00462",
        "ARXIVID": "2502.00462",
        "COMMENT": "Does not match any specific criterion but is relevant to computer vision and machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.00019": {
        "authors": [
            "Abhishek Sharma"
        ],
        "title": "Growth Patterns of Inference",
        "abstract": "arXiv:2502.00019v1 Announce Type: new  Abstract: What properties of a first-order search space support/hinder inference? What kinds of facts would be most effective to learn? Answering these questions is essential for understanding the dynamics of deductive reasoning and creating large-scale knowledge-based learning systems that support efficient inference. We address these questions by developing a model of how the distribution of ground facts affects inference performance in search spaces. Experiments suggest that uniform search spaces are suitable for larger KBs whereas search spaces with skewed degree distribution show better performance in smaller KBs. A sharp transition in Q/A performance is seen in some cases, suggesting that analysis of the structure of search spaces with existing knowledge should be used to guide the acquisition of new ground facts in learning systems.",
        "arxiv_id": "2502.00019",
        "ARXIVID": "2502.00019",
        "COMMENT": "Does not match any specific criteria. Focuses on the properties of search spaces for inference, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.01080": {
        "authors": [
            "Dongliang Zhou",
            "Haijun Zhang",
            "Jianghong Ma",
            "Jianyang Shi"
        ],
        "title": "BC-GAN: A Generative Adversarial Network for Synthesizing a Batch of Collocated Clothing",
        "abstract": "arXiv:2502.01080v1 Announce Type: new  Abstract: Collocated clothing synthesis using generative networks has become an emerging topic in the field of fashion intelligence, as it has significant potential economic value to increase revenue in the fashion industry. In previous studies, several works have attempted to synthesize visually-collocated clothing based on a given clothing item using generative adversarial networks (GANs) with promising results. These works, however, can only accomplish the synthesis of one collocated clothing item each time. Nevertheless, users may require different clothing items to meet their multiple choices due to their personal tastes and different dressing scenarios. To address this limitation, we introduce a novel batch clothing generation framework, named BC-GAN, which is able to synthesize multiple visually-collocated clothing images simultaneously. In particular, to further improve the fashion compatibility of synthetic results, BC-GAN proposes a new fashion compatibility discriminator in a contrastive learning perspective by fully exploiting the collocation relationship among all clothing items. Our model was examined in a large-scale dataset with compatible outfits constructed by ourselves. Extensive experiment results confirmed the effectiveness of our proposed BC-GAN in comparison to state-of-the-art methods in terms of diversity, visual authenticity, and fashion compatibility.",
        "arxiv_id": "2502.01080",
        "ARXIVID": "2502.01080",
        "COMMENT": "Does not match any specific criteria. Focuses on fashion intelligence and GAN-based clothing synthesis, which is outside the specified criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.00695": {
        "authors": [
            "Linglong Wu",
            "Xuhao Shan",
            "Ruiquan Ge",
            "Ruoyu Liang",
            "Chi Zhang",
            "Yonghong Li",
            "Ahmed Elazab",
            "Huoling Luo",
            "Yunbi Liu",
            "Changmiao Wang"
        ],
        "title": "TMI-CLNet: Triple-Modal Interaction Network for Chronic Liver Disease Prognosis From Imaging, Clinical, and Radiomic Data Fusion",
        "abstract": "arXiv:2502.00695v1 Announce Type: new  Abstract: Chronic liver disease represents a significant health challenge worldwide and accurate prognostic evaluations are essential for personalized treatment plans. Recent evidence suggests that integrating multimodal data, such as computed tomography imaging, radiomic features, and clinical information, can provide more comprehensive prognostic information. However, modalities have an inherent heterogeneity, and incorporating additional modalities may exacerbate the challenges of heterogeneous data fusion. Moreover, existing multimodal fusion methods often struggle to adapt to richer medical modalities, making it difficult to capture inter-modal relationships. To overcome these limitations, We present the Triple-Modal Interaction Chronic Liver Network (TMI-CLNet). Specifically, we develop an Intra-Modality Aggregation module and a Triple-Modal Cross-Attention Fusion module, which are designed to eliminate intra-modality redundancy and extract cross-modal information, respectively. Furthermore, we design a Triple-Modal Feature Fusion loss function to align feature representations across modalities. Extensive experiments on the liver prognosis dataset demonstrate that our approach significantly outperforms existing state-of-the-art unimodal models and other multi-modal techniques. Our code is available at https://github.com/Mysterwll/liver.git.",
        "arxiv_id": "2502.00695",
        "ARXIVID": "2502.00695",
        "COMMENT": "Focuses on multi-modal data fusion for medical imaging, which is tangentially related to multi-modal learning but not directly to the criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.01584": {
        "authors": [
            "Carolyn Jane Anderson",
            "Joydeep Biswas",
            "Aleksander Boruch-Gruszecki",
            "Federico Cassano",
            "Molly Q Feldman",
            "Arjun Guha",
            "Francesca Lucchetti",
            "Zixuan Wu"
        ],
        "title": "PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models",
        "abstract": "arXiv:2502.01584v1 Announce Type: new  Abstract: Existing benchmarks for frontier models often test specialized, ``PhD-level'' knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models, however correct solutions are easy to verify, and models' mistakes are easy to spot.   Our work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models that are on par on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with ``I give up'' before providing an answer that it knows is wrong. R1 can also be remarkably ``uncertain'' in its output and in rare cases, it does not ``finish thinking,'' which suggests the need for an inference-time technique to ``wrap up'' before the context window limit is reached. We also quantify the effectiveness of reasoning longer with R1 and Gemini Thinking to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark.",
        "arxiv_id": "2502.01584",
        "ARXIVID": "2502.01584",
        "COMMENT": "Does not match any specific criteria. Focuses on reasoning challenges for LLMs, which is not directly related to spatial understanding, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.01626": {
        "authors": [
            "Le Shen",
            "Yanting Kang",
            "Rong Huang",
            "Zhijie Wang"
        ],
        "title": "MFP-VTON: Enhancing Mask-Free Person-to-Person Virtual Try-On via Diffusion Transformer",
        "abstract": "arXiv:2502.01626v1 Announce Type: new  Abstract: The garment-to-person virtual try-on (VTON) task, which aims to generate fitting images of a person wearing a reference garment, has made significant strides. However, obtaining a standard garment is often more challenging than using the garment already worn by the person. To improve ease of use, we propose MFP-VTON, a Mask-Free framework for Person-to-Person VTON. Recognizing the scarcity of person-to-person data, we adapt a garment-to-person model and dataset to construct a specialized dataset for this task. Our approach builds upon a pretrained diffusion transformer, leveraging its strong generative capabilities. During mask-free model fine-tuning, we introduce a Focus Attention loss to emphasize the garment of the reference person and the details outside the garment of the target person. Experimental results demonstrate that our model excels in both person-to-person and garment-to-person VTON tasks, generating high-fidelity fitting images.",
        "arxiv_id": "2502.01626",
        "ARXIVID": "2502.01626",
        "COMMENT": "Does not match any specific criteria. Focuses on virtual try-on tasks using diffusion transformers, which is not directly related to spatial understanding, VLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.00535": {
        "authors": [
            "David Oro",
            "Carles Fern\\'andez",
            "Xavier Martorell",
            "Javier Hernando"
        ],
        "title": "Work-Efficient Parallel Non-Maximum Suppression Kernels",
        "abstract": "arXiv:2502.00535v1 Announce Type: new  Abstract: In the context of object detection, sliding-window classifiers and single-shot Convolutional Neural Network (CNN) meta-architectures typically yield multiple overlapping candidate windows with similar high scores around the true location of a particular object. Non-Maximum Suppression (NMS) is the process of selecting a single representative candidate within this cluster of detections, so as to obtain a unique detection per object appearing on a given picture. In this paper, we present a highly scalable NMS algorithm for embedded GPU architectures that is designed from scratch to handle workloads featuring thousands of simultaneous detections on a given picture. Our kernels are directly applicable to other sequential NMS algorithms such as FeatureNMS, Soft-NMS or AdaptiveNMS that share the inner workings of the classic greedy NMS method. The obtained performance results show that our parallel NMS algorithm is capable of clustering 1024 simultaneous detected objects per frame in roughly 1 ms on both NVIDIA Tegra X1 and NVIDIA Tegra X2 on-die GPUs, while taking 2 ms on NVIDIA Tegra K1. Furthermore, our proposed parallel greedy NMS algorithm yields a 14x-40x speed up when compared to state-of-the-art NMS methods that require learning a CNN from annotated data.",
        "arxiv_id": "2502.00535",
        "ARXIVID": "2502.00535",
        "COMMENT": "Does not match any specific criterion but is relevant to efficient computation in object detection, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}