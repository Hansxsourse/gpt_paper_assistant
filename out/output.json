{
    "2503.00675": {
        "authors": [
            "Wenke E",
            "Chao Yuan",
            "Li Li",
            "Yixin Sun",
            "Yona Falinie A. Gaus",
            "Amir Atapour-Abarghouei",
            "Toby P. Breckon"
        ],
        "title": "Dur360BEV: A Real-world Single 360-degree Camera Dataset and Benchmark for Bird-Eye View Mapping in Autonomous Driving",
        "abstract": "arXiv:2503.00675v1 Announce Type: new  Abstract: We present Dur360BEV, a novel spherical camera autonomous driving dataset equipped with a high-resolution 128-channel 3D LiDAR and a RTK-refined GNSS/INS system, along with a benchmark architecture designed to generate Bird-Eye-View (BEV) maps using only a single spherical camera. This dataset and benchmark address the challenges of BEV generation in autonomous driving, particularly by reducing hardware complexity through the use of a single 360-degree camera instead of multiple perspective cameras. Within our benchmark architecture, we propose a novel spherical-image-to-BEV (SI2BEV) module that leverages spherical imagery and a refined sampling strategy to project features from 2D to 3D. Our approach also includes an innovative application of Focal Loss, specifically adapted to address the extreme class imbalance often encountered in BEV segmentation tasks. Through extensive experiments, we demonstrate that this application of Focal Loss significantly improves segmentation performance on the Dur360BEV dataset. The results show that our benchmark not only simplifies the sensor setup but also achieves competitive performance.",
        "arxiv_id": "2503.00675",
        "ARXIVID": "2503.00675",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset and method for BEV mapping in autonomous driving, focusing on novel angles like using a single 360-degree camera.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2503.01774": {
        "authors": [
            "Jay Zhangjie Wu",
            "Yuxuan Zhang",
            "Haithem Turki",
            "Xuanchi Ren",
            "Jun Gao",
            "Mike Zheng Shou",
            "Sanja Fidler",
            "Zan Gojcic",
            "Huan Ling"
        ],
        "title": "Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models",
        "abstract": "arXiv:2503.01774v1 Announce Type: new  Abstract: Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a novel pipeline designed to enhance 3D reconstruction and novel-view synthesis through single-step diffusion models. At the core of our approach is Difix, a single-step image diffusion model trained to enhance and remove artifacts in rendered novel views caused by underconstrained regions of the 3D representation. Difix serves two critical roles in our pipeline. First, it is used during the reconstruction phase to clean up pseudo-training views that are rendered from the reconstruction and then distilled back into 3D. This greatly enhances underconstrained regions and improves the overall 3D representation quality. More importantly, Difix also acts as a neural enhancer during inference, effectively removing residual artifacts arising from imperfect 3D supervision and the limited capacity of current reconstruction models. Difix3D+ is a general solution, a single model compatible with both NeRF and 3DGS representations, and it achieves an average 2$\\times$ improvement in FID score over baselines while maintaining 3D consistency.",
        "arxiv_id": "2503.01774",
        "ARXIVID": "2503.01774",
        "COMMENT": "Matches criterion 4 as it introduces a novel pipeline (Difix3D+) for improving 3D reconstructions using diffusion models, which is related to vision foundation models and their applications.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2503.01100": {
        "authors": [
            "Hanzhe Liang",
            "Jie Zhou",
            "Xuanxin Chen",
            "Jinbao Wang",
            "Can Gao"
        ],
        "title": "Fence Theorem: Preprocessing is Dual-Objective Semantic Structure Isolator in 3D Anomaly Detection",
        "abstract": "arXiv:2503.01100v1 Announce Type: new  Abstract: 3D anomaly detection (AD) is prominent but difficult due to lacking a unified theoretical foundation for preprocessing design. We establish the Fence Theorem, formalizing preprocessing as a dual-objective semantic isolator: (1) mitigating cross-semantic interference to the greatest extent feasible and (2) confining anomaly judgments to aligned semantic spaces wherever viable, thereby establishing intra-semantic comparability. Any preprocessing approach achieves this goal through a two-stage process of Emantic-Division and Spatial-Constraints stage. Through systematic deconstruction, we theoretically and experimentally subsume existing preprocessing methods under this theorem via tripartite evidence: qualitative analyses, quantitative studies, and mathematical proofs. Guided by the Fence Theorem, we implement Patch3D, consisting of Patch-Cutting and Patch-Matching modules, to segment semantic spaces and consolidate similar ones while independently modeling normal features within each space. Experiments on Anomaly-ShapeNet and Real3D-AD with different settings demonstrate that progressively finer-grained semantic alignment in preprocessing directly enhances point-level AD accuracy, providing inverse validation of the theorem's causal logic.",
        "arxiv_id": "2503.01100",
        "ARXIVID": "2503.01100",
        "COMMENT": "Matches criterion 3 as it proposes a new theoretical foundation (Fence Theorem) for preprocessing in 3D anomaly detection, which is a novel angle for embodied AI benchmarks and methods.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2503.01582": {
        "authors": [
            "Saad Ejaz",
            "Hriday Bavle",
            "Laura Ribeiro",
            "Holger Voos",
            "Jose Luis Sanchez-Lopez"
        ],
        "title": "Category-level Meta-learned NeRF Priors for Efficient Object Mapping",
        "abstract": "arXiv:2503.01582v1 Announce Type: new  Abstract: In 3D object mapping, category-level priors enable efficient object reconstruction and canonical pose estimation, requiring only a single prior per semantic category (e.g., chair, book, laptop). Recently, DeepSDF has predominantly been used as a category-level shape prior, but it struggles to reconstruct sharp geometry and is computationally expensive. In contrast, NeRFs capture fine details but have yet to be effectively integrated with category-level priors in a real-time multi-object mapping framework. To bridge this gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapper that integrates category-level priors with object-level NeRFs to enhance reconstruction efficiency while enabling canonical object pose estimation. PRENOM gets to know objects on a first-name basis by meta-learning on synthetic reconstruction tasks generated from open-source shape datasets. To account for object category variations, it employs a multi-objective genetic algorithm to optimize the NeRF architecture for each category, balancing reconstruction quality and training time. Additionally, prior-based probabilistic ray sampling directs sampling toward expected object regions, accelerating convergence and improving reconstruction quality under constrained resources. Experimental results on a low-end GPU highlight the ability of PRENOM to achieve high-quality reconstructions while maintaining computational feasibility. Specifically, comparisons with prior-free NeRF-based approaches on a synthetic dataset show a 21% lower Chamfer distance, demonstrating better reconstruction quality. Furthermore, evaluations against other approaches using shape priors on a noisy real-world dataset indicate a 13% improvement averaged across all reconstruction metrics, a boost in rotation estimation accuracy, and comparable translation and size estimation performance, while being trained for 5x less time.",
        "arxiv_id": "2503.01582",
        "ARXIVID": "2503.01582",
        "COMMENT": "Matches criterion 1 as it introduces a new methodological improvement for spatial understanding in 3D object mapping using category-level NeRF priors. Also relevant to criterion 3 as it proposes a novel approach to object mapping with meta-learned priors and optimization techniques.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.00413": {
        "authors": [
            "Tianyu Huai",
            "Jie Zhou",
            "Xingjiao Wu",
            "Qin Chen",
            "Qingchun Bai",
            "Ze Zhou",
            "Liang He"
        ],
        "title": "CL-MoE: Enhancing Multimodal Large Language Model with Dual Momentum Mixture-of-Experts for Continual Visual Question Answering",
        "abstract": "arXiv:2503.00413v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have garnered widespread attention from researchers due to their remarkable understanding and generation capabilities in visual language tasks (e.g., visual question answering). However, the rapid pace of knowledge updates in the real world makes offline training of MLLMs costly, and when faced with non-stationary data streams, MLLMs suffer from catastrophic forgetting during learning. In this paper, we propose an MLLMs-based dual momentum Mixture-of-Experts (CL-MoE) framework for continual visual question answering (VQA). We integrate MLLMs with continual learning to utilize the rich commonsense knowledge in LLMs. We introduce a Dual-Router MoE (RMoE) strategy to select the global and local experts using task-level and instance-level routers, to robustly assign weights to the experts most appropriate for the task. Then, we design a dynamic Momentum MoE (MMoE) to update the parameters of experts dynamically based on the relationships between the experts and tasks/instances, so that the model can absorb new knowledge while maintaining existing knowledge. The extensive experimental results indicate that our method achieves state-of-the-art performance on 10 VQA tasks, proving the effectiveness of our approach.",
        "arxiv_id": "2503.00413",
        "ARXIVID": "2503.00413",
        "COMMENT": "This paper proposes a continual learning framework for multimodal large language models, matching criterion 2.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.00540": {
        "authors": [
            "Shangzhe Di",
            "Zhelun Yu",
            "Guanghao Zhang",
            "Haoyuan Li",
            "Tao Zhong",
            "Hao Cheng",
            "Bolin Li",
            "Wanggui He",
            "Fangxun Shu",
            "Hao Jiang"
        ],
        "title": "Streaming Video Question-Answering with In-context Video KV-Cache Retrieval",
        "abstract": "arXiv:2503.00540v1 Announce Type: new  Abstract: We propose ReKV, a novel training-free approach that enables efficient streaming video question-answering (StreamingVQA), by seamlessly integrating with existing Video Large Language Models (Video-LLMs). Traditional VideoQA systems struggle with long videos, as they must process entire videos before responding to queries, and repeat this process for each new question. In contrast, our approach analyzes long videos in a streaming manner, allowing for prompt responses as soon as user queries are received. Building on a common Video-LLM, we first incorporate a sliding-window attention mechanism, ensuring that input frames attend to a limited number of preceding frames, thereby reducing computational overhead. To prevent information loss, we store processed video key-value caches (KV-Caches) in RAM and disk, reloading them into GPU memory as needed. Additionally, we introduce a retrieval method that leverages an external retriever or the parameters within Video-LLMs to retrieve only query-relevant KV-Caches, ensuring both efficiency and accuracy in question answering. ReKV enables the separation of video encoding and question-answering across different processes and GPUs, significantly enhancing the efficiency of StreamingVQA. Through comprehensive experimentation, we validate the efficacy and practicality of our approach, which significantly boosts efficiency and enhances applicability over existing VideoQA models.",
        "arxiv_id": "2503.00540",
        "ARXIVID": "2503.00540",
        "COMMENT": "This paper introduces a novel method for streaming video question-answering using Video-LLMs, matching criterion 2.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.01261": {
        "authors": [
            "Guotao Liang",
            "Baoquan Zhang",
            "Zhiyuan Wen",
            "Junteng Zhao",
            "Yunming Ye",
            "Kola Ye",
            "Yao He"
        ],
        "title": "Towards Improved Text-Aligned Codebook Learning: Multi-Hierarchical Codebook-Text Alignment with Long Text",
        "abstract": "arXiv:2503.01261v1 Announce Type: new  Abstract: Image quantization is a crucial technique in image generation, aimed at learning a codebook that encodes an image into a discrete token sequence. Recent advancements have seen researchers exploring learning multi-modal codebook (i.e., text-aligned codebook) by utilizing image caption semantics, aiming to enhance codebook performance in cross-modal tasks. However, existing image-text paired datasets exhibit a notable flaw in that the text descriptions tend to be overly concise, failing to adequately describe the images and provide sufficient semantic knowledge, resulting in limited alignment of text and codebook at a fine-grained level. In this paper, we propose a novel Text-Augmented Codebook Learning framework, named TA-VQ, which generates longer text for each image using the visual-language model for improved text-aligned codebook learning. However, the long text presents two key challenges: how to encode text and how to align codebook and text. To tackle two challenges, we propose to split the long text into multiple granularities for encoding, i.e., word, phrase, and sentence, so that the long text can be fully encoded without losing any key semantic knowledge. Following this, a hierarchical encoder and novel sampling-based alignment strategy are designed to achieve fine-grained codebook-text alignment. Additionally, our method can be seamlessly integrated into existing VQ models. Extensive experiments in reconstruction and various downstream tasks demonstrate its effectiveness compared to previous state-of-the-art approaches.",
        "arxiv_id": "2503.01261",
        "ARXIVID": "2503.01261",
        "COMMENT": "Matches criterion 2 as it proposes a novel framework for text-aligned codebook learning, which is relevant to multi-modal large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.01109": {
        "authors": [
            "Yansong Xu",
            "Junlin Li",
            "Wei Zhang",
            "Siyu Chen",
            "Shengyong Zhang",
            "Yuquan Leng",
            "Weijia Zhou"
        ],
        "title": "FGS-SLAM: Fourier-based Gaussian Splatting for Real-time SLAM with Sparse and Dense Map Fusion",
        "abstract": "arXiv:2503.01109v1 Announce Type: new  Abstract: 3D gaussian splatting has advanced simultaneous localization and mapping (SLAM) technology by enabling real-time positioning and the construction of high-fidelity maps. However, the uncertainty in gaussian position and initialization parameters introduces challenges, often requiring extensive iterative convergence and resulting in redundant or insufficient gaussian representations. To address this, we introduce a novel adaptive densification method based on Fourier frequency domain analysis to establish gaussian priors for rapid convergence. Additionally, we propose constructing independent and unified sparse and dense maps, where a sparse map supports efficient tracking via Generalized Iterative Closest Point (GICP) and a dense map creates high-fidelity visual representations. This is the first SLAM system leveraging frequency domain analysis to achieve high-quality gaussian mapping in real-time. Experimental results demonstrate an average frame rate of 36 FPS on Replica and TUM RGB-D datasets, achieving competitive accuracy in both localization and mapping.",
        "arxiv_id": "2503.01109",
        "ARXIVID": "2503.01109",
        "COMMENT": "Matches criterion 3 as it introduces a novel SLAM system leveraging frequency domain analysis for real-time mapping, which is a new method in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.00371": {
        "authors": [
            "Xuehao Gao",
            "Yang Yang",
            "Shaoyi Du",
            "Guo-Jun Qi",
            "Junwei Han"
        ],
        "title": "Jointly Understand Your Command and Intention:Reciprocal Co-Evolution between Scene-Aware 3D Human Motion Synthesis and Analysis",
        "abstract": "arXiv:2503.00371v1 Announce Type: new  Abstract: As two intimate reciprocal tasks, scene-aware human motion synthesis and analysis require a joint understanding between multiple modalities, including 3D body motions, 3D scenes, and textual descriptions. In this paper, we integrate these two paired processes into a Co-Evolving Synthesis-Analysis (CESA) pipeline and mutually benefit their learning. Specifically, scene-aware text-to-human synthesis generates diverse indoor motion samples from the same textual description to enrich human-scene interaction intra-class diversity, thus significantly benefiting training a robust human motion analysis system. Reciprocally, human motion analysis would enforce semantic scrutiny on each synthesized motion sample to ensure its semantic consistency with the given textual description, thus improving realistic motion synthesis. Considering that real-world indoor human motions are goal-oriented and path-guided, we propose a cascaded generation strategy that factorizes text-driven scene-specific human motion generation into three stages: goal inferring, path planning, and pose synthesizing. Coupling CESA with this powerful cascaded motion synthesis model, we jointly improve realistic human motion synthesis and robust human motion analysis in 3D scenes.",
        "arxiv_id": "2503.00371",
        "ARXIVID": "2503.00371",
        "COMMENT": "Matches criterion 1 as it introduces a novel pipeline for scene-aware 3D human motion synthesis and analysis, focusing on spatial understanding in embodied agents.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.00059": {
        "authors": [
            "Rui Hu",
            "Delai Qiu",
            "Shuyu Wei",
            "Jiaming Zhang",
            "Yining Wang",
            "Shengping Liu",
            "Jitao Sang"
        ],
        "title": "Investigating and Enhancing Vision-Audio Capability in Omnimodal Large Language Models",
        "abstract": "arXiv:2503.00059v1 Announce Type: new  Abstract: Omnimodal Large Language Models (OLLMs) have shown significant progress in integrating vision and text, but still struggle with integrating vision and audio, often exhibiting suboptimal performance when processing audio queries compared to text queries. This disparity is primarily due to insufficient alignment between vision and audio modalities during training, leading to inadequate attention to visual information when using audio queries. To mitigate this issue, we propose a Self-Knowledge Distillation (Self-KD) training method where the vision-text component of the OLLM serves as the teacher and the vision-audio component as the student. This enables the model to process audio in a manner analogous to its text processing. Our experimental results demonstrate that Self-KD is an effective method for enhancing the vision-audio capabilities of OLLMs by learning from the vision-text components, which subsequently improves the interaction between audio and images and results in improved performance on multimodal tasks.",
        "arxiv_id": "2503.00059",
        "ARXIVID": "2503.00059",
        "COMMENT": "This paper enhances vision-audio capabilities in omnimodal large language models, matching criterion 2.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2503.01794": {
        "authors": [
            "Junhyun Park",
            "Chanyu Moon",
            "Donghwan Lee",
            "Kyungsu Kim",
            "Minho Hwang"
        ],
        "title": "OFF-CLIP: Improving Normal Detection Confidence in Radiology CLIP with Simple Off-Diagonal Term Auto-Adjustment",
        "abstract": "arXiv:2503.01794v1 Announce Type: new  Abstract: Contrastive Language-Image Pre-Training (CLIP) has enabled zero-shot classification in radiology, reducing reliance on manual annotations. However, conventional contrastive learning struggles with normal case detection due to its strict intra-sample alignment, which disrupts normal sample clustering and leads to high false positives (FPs) and false negatives (FNs). To address these issues, we propose OFF-CLIP, a contrastive learning refinement that improves normal detection by introducing an off-diagonal term loss to enhance normal sample clustering and applying sentence-level text filtering to mitigate FNs by removing misaligned normal statements from abnormal reports. OFF-CLIP can be applied to radiology CLIP models without requiring any architectural modifications. Experimental results show that OFF-CLIP significantly improves normal classification, achieving a 0.61 Area under the curve (AUC) increase on VinDr-CXR over CARZero, the state-of-the-art zero-shot classification baseline, while maintaining or improving abnormal classification performance. Additionally, OFF-CLIP enhances zero-shot grounding by improving pointing game accuracy, confirming better anomaly localization. These results demonstrate OFF-CLIP's effectiveness as a robust and efficient enhancement for medical vision-language models.",
        "arxiv_id": "2503.01794",
        "ARXIVID": "2503.01794",
        "COMMENT": "Matches criterion 2 as it proposes an improvement to CLIP, a vision-language model, for radiology applications.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2503.01169": {
        "authors": [
            "Seyed Mohamad Ali Tousi",
            "Ramy Farag",
            "Jacket Demby's",
            "Gbenga Omotara",
            "John A. Lory",
            "G. N. DeSouza"
        ],
        "title": "A Zero-Shot Learning Approach for Ephemeral Gully Detection from Remote Sensing using Vision Language Models",
        "abstract": "arXiv:2503.01169v1 Announce Type: new  Abstract: Ephemeral gullies are a primary cause of soil erosion and their reliable, accurate, and early detection will facilitate significant improvements in the sustainability of global agricultural systems. In our view, prior research has not successfully addressed automated detection of ephemeral gullies from remotely sensed images, so for the first time, we present and evaluate three successful pipelines for ephemeral gully detection. Our pipelines utilize remotely sensed images, acquired from specific agricultural areas over a period of time. The pipelines were tested with various choices of Visual Language Models (VLMs), and they classified the images based on the presence of ephemeral gullies with accuracy higher than 70% and a F1-score close to 80% for positive gully detection. Additionally, we developed the first public dataset for ephemeral gully detection, labeled by a team of soil- and plant-science experts. To evaluate the proposed pipelines, we employed a variety of zero-shot classification methods based on State-of-the-Art (SOTA) open-source Vision-Language Models (VLMs). In addition to that, we compare the same pipelines with a transfer learning approach. Extensive experiments were conducted to validate the detection pipelines and to analyze the impact of hyperparameter changes in their performance. The experimental results demonstrate that the proposed zero-shot classification pipelines are highly effective in detecting ephemeral gullies in a scenario where classification datasets are scarce.",
        "arxiv_id": "2503.01169",
        "ARXIVID": "2503.01169",
        "COMMENT": "This paper applies vision-language models to detect ephemeral gullies, which aligns with criterion 4 on vision foundation models and applications.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.00743": {
        "authors": [
            "Dilxat Muhtar",
            "Enzhuo Zhang",
            "Zhenshi Li",
            "Feng Gu",
            "Yanglangxing He",
            "Pengfeng Xiao",
            "Xueliang Zhang"
        ],
        "title": "Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models",
        "abstract": "arXiv:2503.00743v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) have demonstrated great potential in interpreting remote sensing (RS) images through language-guided semantic understanding. However, the effectiveness of these VLMs critically depends on high-quality image-text training data that captures rich semantic relationships between visual content and language descriptions. Unlike natural images, RS lacks large-scale interleaved image-text pairs from web data, making data collection challenging. While current approaches rely primarily on rule-based methods or flagship VLMs for data synthesis, a systematic framework for automated quality assessment of such synthetically generated RS visionlanguage data is notably absent. To fill this gap, we propose a novel score model trained on large-scale RS visionlanguage preference data for automated quality assessment. Our empirical results demonstrate that fine-tuning CLIP or advanced VLMs (e.g., Qwen2-VL) with the top 30% of data ranked by our score model achieves superior interpretation accuracy compared to both full-data fine-tuning and CLIP-score-based ranking approaches. Furthermore, we demonstrate applications of our scoring model for reinforcement learning (RL) training and best-of-N (BoN) testtime scaling, enabling significant improvements in VLM performance for RS tasks.",
        "arxiv_id": "2503.00743",
        "ARXIVID": "2503.00743",
        "COMMENT": "Matches criterion 2 as it focuses on improving vision-language models for remote sensing applications through quality-driven data curation.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.01333": {
        "authors": [
            "Xu Liang"
        ],
        "title": "Group Relative Policy Optimization for Image Captioning",
        "abstract": "arXiv:2503.01333v1 Announce Type: new  Abstract: Image captioning tasks usually use two-stage training to complete model optimization. The first stage uses cross-entropy as the loss function for optimization, and the second stage uses self-critical sequence training (SCST) for reinforcement learning optimization. However, the SCST algorithm has certain defects. SCST relies only on a single greedy decoding result as a baseline. If the model itself is not stable enough, the greedy decoding result may be relatively worst, which will lead to a high variance of advantage estimation, further leading to unstable policy updates. In addition, SCST only compares one sampling result with the greedy decoding result, and the generation diversity is limited, which may fall into a local optimum. In this paper, we propose using the latest Group Relative Policy Optimization (GRPO) reinforcement learning algorithm as an optimization solution for the second stage. GRPO generates multiple candidate captions for the input image and then continuously optimizes the model through intragroup comparison. By constraining the amplitude of policy updates and KL divergence, the stability of the model during training is greatly guaranteed. In addition, compared to SCST, which only samples one answer, GRPO samples and generates multiple answers. Multiple candidate answers in the group cover a wider solution space. Combined with KL divergence constraints, GRPO can improve diversity while ensuring model stability. The code for this article is available at https://github.com/liangxu-one/ms-models/tree/image_caption_grpo/research/arxiv_papers/Image_Caption_GRPO.",
        "arxiv_id": "2503.01333",
        "ARXIVID": "2503.01333",
        "COMMENT": "Matches criterion 1 as it proposes a new methodological improvement (Group Relative Policy Optimization) for image captioning, which involves spatial understanding.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.00853": {
        "authors": [
            "Rui Yi Yong",
            "Samuel Picosson",
            "Arnold Wiliem"
        ],
        "title": "MTReD: 3D Reconstruction Dataset for Fly-over Videos of Maritime Domain",
        "abstract": "arXiv:2503.00853v1 Announce Type: new  Abstract: This work tackles 3D scene reconstruction for a video fly-over perspective problem in the maritime domain, with a specific emphasis on geometrically and visually sound reconstructions. This will allow for downstream tasks such as segmentation, navigation, and localization. To our knowledge, there is no dataset available in this domain. As such, we propose a novel maritime 3D scene reconstruction benchmarking dataset, named as MTReD (Maritime Three-Dimensional Reconstruction Dataset). The MTReD comprises 19 fly-over videos curated from the Internet containing ships, islands, and coastlines. As the task is aimed towards geometrical consistency and visual completeness, the dataset uses two metrics: (1) Reprojection error; and (2) Perception based metrics. We find that existing perception-based metrics, such as Learned Perceptual Image Patch Similarity (LPIPS), do not appropriately measure the completeness of a reconstructed image. Thus, we propose a novel semantic similarity metric utilizing DINOv2 features coined DiFPS (DinoV2 Features Perception Similarity). We perform initial evaluation on two baselines: (1) Structured from Motion (SfM) through Colmap; and (2) the recent state-of-the-art MASt3R model. We find that the reconstructed scenes by MASt3R have higher reprojection errors, but superior perception based metric scores. To this end, some pre-processing methods are explored, and we find a pre-processing method which improves both the reprojection error and perception-based score. We envisage our proposed MTReD to stimulate further research in these directions. The dataset and all the code will be made available in https://github.com/RuiYiYong/MTReD.",
        "arxiv_id": "2503.00853",
        "ARXIVID": "2503.00853",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset (MTReD) for 3D reconstruction in the maritime domain, which is a novel angle in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.01291": {
        "authors": [
            "Peishan Cong",
            "Ziyi Wang",
            "Yuexin Ma",
            "Xiangyu Yue"
        ],
        "title": "SemGeoMo: Dynamic Contextual Human Motion Generation with Semantic and Geometric Guidance",
        "abstract": "arXiv:2503.01291v1 Announce Type: new  Abstract: Generating reasonable and high-quality human interactive motions in a given dynamic environment is crucial for understanding, modeling, transferring, and applying human behaviors to both virtual and physical robots. In this paper, we introduce an effective method, SemGeoMo, for dynamic contextual human motion generation, which fully leverages the text-affordance-joint multi-level semantic and geometric guidance in the generation process, improving the semantic rationality and geometric correctness of generative motions. Our method achieves state-of-the-art performance on three datasets and demonstrates superior generalization capability for diverse interaction scenarios. The project page and code can be found at https://4dvlab.github.io/project_page/semgeomo/.",
        "arxiv_id": "2503.01291",
        "ARXIVID": "2503.01291",
        "COMMENT": "This paper proposes a method for human motion generation with semantic and geometric guidance, which could be relevant to spatial understanding in embodied agents (criterion 1).",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.01187": {
        "authors": [
            "Xingyuan Li",
            "Zirui Wang",
            "Yang Zou",
            "Zhixin Chen",
            "Jun Ma",
            "Zhiying Jiang",
            "Long Ma",
            "Jinyuan Liu"
        ],
        "title": "DifIISR: A Diffusion Model with Gradient Guidance for Infrared Image Super-Resolution",
        "abstract": "arXiv:2503.01187v1 Announce Type: new  Abstract: Infrared imaging is essential for autonomous driving and robotic operations as a supportive modality due to its reliable performance in challenging environments. Despite its popularity, the limitations of infrared cameras, such as low spatial resolution and complex degradations, consistently challenge imaging quality and subsequent visual tasks. Hence, infrared image super-resolution (IISR) has been developed to address this challenge. While recent developments in diffusion models have greatly advanced this field, current methods to solve it either ignore the unique modal characteristics of infrared imaging or overlook the machine perception requirements. To bridge these gaps, we propose DifIISR, an infrared image super-resolution diffusion model optimized for visual quality and perceptual performance. Our approach achieves task-based guidance for diffusion by injecting gradients derived from visual and perceptual priors into the noise during the reverse process. Specifically, we introduce an infrared thermal spectrum distribution regulation to preserve visual fidelity, ensuring that the reconstructed infrared images closely align with high-resolution images by matching their frequency components. Subsequently, we incorporate various visual foundational models as the perceptual guidance for downstream visual tasks, infusing generalizable perceptual features beneficial for detection and segmentation. As a result, our approach gains superior visual results while attaining State-Of-The-Art downstream task performance. Code is available at https://github.com/zirui0625/DifIISR",
        "arxiv_id": "2503.01187",
        "ARXIVID": "2503.01187",
        "COMMENT": "Matches criterion 4 as it applies a diffusion model to infrared image super-resolution, which is a vision foundation model application.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.00986": {
        "authors": [
            "Baoqi Pei",
            "Yifei Huang",
            "Jilan Xu",
            "Guo Chen",
            "Yuping He",
            "Lijin Yang",
            "Yali Wang",
            "Weidi Xie",
            "Yu Qiao",
            "Fei Wu",
            "Limin Wang"
        ],
        "title": "Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning",
        "abstract": "arXiv:2503.00986v1 Announce Type: new  Abstract: In egocentric video understanding, the motion of hands and objects as well as their interactions play a significant role by nature. However, existing egocentric video representation learning methods mainly focus on aligning video representation with high-level narrations, overlooking the intricate dynamics between hands and objects. In this work, we aim to integrate the modeling of fine-grained hand-object dynamics into the video representation learning process. Since no suitable data is available, we introduce HOD, a novel pipeline employing a hand-object detector and a large language model to generate high-quality narrations with detailed descriptions of hand-object dynamics. To learn these fine-grained dynamics, we propose EgoVideo, a model with a new lightweight motion adapter to capture fine-grained hand-object motion information. Through our co-training strategy, EgoVideo effectively and efficiently leverages the fine-grained hand-object dynamics in the HOD data. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple egocentric downstream tasks, including improvements of 6.3% in EK-100 multi-instance retrieval, 5.7% in EK-100 classification, and 16.3% in EGTEA classification in zero-shot settings. Furthermore, our model exhibits robust generalization capabilities in hand-object interaction and robot manipulation tasks. Code and data are available at https://github.com/OpenRobotLab/EgoHOD/.",
        "arxiv_id": "2503.00986",
        "ARXIVID": "2503.00986",
        "COMMENT": "Matches criterion 3 as it focuses on egocentric video understanding and proposes a novel pipeline for modeling hand-object dynamics, which is a novel angle in embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.01309": {
        "authors": [
            "Yijie Tang",
            "Jiazhao Zhang",
            "Yuqing Lan",
            "Yulan Guo",
            "Dezun Dong",
            "Chenyang Zhu",
            "Kai Xu"
        ],
        "title": "OnlineAnySeg: Online Zero-Shot 3D Segmentation by Visual Foundation Model Guided 2D Mask Merging",
        "abstract": "arXiv:2503.01309v1 Announce Type: new  Abstract: Online 3D open-vocabulary segmentation of a progressively reconstructed scene is both a critical and challenging task for embodied applications. With the success of visual foundation models (VFMs) in the image domain, leveraging 2D priors to address 3D online segmentation has become a prominent research focus. Since segmentation results provided by 2D priors often require spatial consistency to be lifted into final 3D segmentation, an efficient method for identifying spatial overlap among 2D masks is essential - yet existing methods rarely achieve this in real time, mainly limiting its use to offline approaches. To address this, we propose an efficient method that lifts 2D masks generated by VFMs into a unified 3D instance using a hashing technique. By employing voxel hashing for efficient 3D scene querying, our approach reduces the time complexity of costly spatial overlap queries from $O(n^2)$ to $O(n)$. Accurate spatial associations further enable 3D merging of 2D masks through simple similarity-based filtering in a zero-shot manner, making our approach more robust to incomplete and noisy data. Evaluated on the ScanNet and SceneNN benchmarks, our approach achieves state-of-the-art performance in online, open-vocabulary 3D instance segmentation with leading efficiency.",
        "arxiv_id": "2503.01309",
        "ARXIVID": "2503.01309",
        "COMMENT": "Matches criterion 4 as it leverages visual foundation models for online 3D segmentation, which is relevant to vision foundation models and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.01298": {
        "authors": [
            "Yi Wang",
            "Mushui Liu",
            "Wanggui He",
            "Longxiang Zhang",
            "Ziwei Huang",
            "Guanghao Zhang",
            "Fangxun Shu",
            "Zhong Tao",
            "Dong She",
            "Zhelun Yu",
            "Haoyuan Li",
            "Weilong Dai",
            "Mingli Song",
            "Jie Song",
            "Hao Jiang"
        ],
        "title": "MINT: Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation",
        "abstract": "arXiv:2503.01298v1 Announce Type: new  Abstract: Unified generative models have demonstrated extraordinary performance in both text and image generation. However, they tend to underperform when generating intricate images with various interwoven conditions, which is hard to solely rely on straightforward text-to-image generation. In response to this challenge, we introduce MINT, an innovative unified generative model, empowered with native multimodal chain of thought (MCoT) for enhanced image generation for the first time. Firstly, we design Mixture of Transformer Experts (MTXpert), an expert-parallel structure that effectively supports both natural language generation (NLG) and visual capabilities, while avoiding potential modality conflicts that could hinder the full potential of each modality. Building on this, we propose an innovative MCoT training paradigm, a step-by-step approach to multimodal thinking, reasoning, and reflection specifically designed to enhance image generation. This paradigm equips MINT with nuanced, element-wise decoupled alignment and a comprehensive understanding of textual and visual components. Furthermore, it fosters advanced multimodal reasoning and self-reflection, enabling the construction of images that are firmly grounded in the logical relationships between these elements. Notably, MINT has been validated to exhibit superior performance across multiple benchmarks for text-to-image (T2I) and image-to-text (I2T) tasks.",
        "arxiv_id": "2503.01298",
        "ARXIVID": "2503.01298",
        "COMMENT": "Matches criterion 2 as it introduces a unified generative model with multimodal chain of thought for enhanced image generation.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.00051": {
        "authors": [
            "Quan Quan",
            "Dun Dai"
        ],
        "title": "Correspondence-Free Pose Estimation with Patterns: A Unified Approach for Multi-Dimensional Vision",
        "abstract": "arXiv:2503.00051v1 Announce Type: new  Abstract: 6D pose estimation is a central problem in robot vision. Compared with pose estimation based on point correspondences or its robust versions, correspondence-free methods are often more flexible. However, existing correspondence-free methods often rely on feature representation alignment or end-to-end regression. For such a purpose, a new correspondence-free pose estimation method and its practical algorithms are proposed, whose key idea is the elimination of unknowns by process of addition to separate the pose estimation from correspondence. By taking the considered point sets as patterns, feature functions used to describe these patterns are introduced to establish a sufficient number of equations for optimization. The proposed method is applicable to nonlinear transformations such as perspective projection and can cover various pose estimations from 3D-to-3D points, 3D-to-2D points, and 2D-to-2D points. Experimental results on both simulation and actual data are presented to demonstrate the effectiveness of the proposed method.",
        "arxiv_id": "2503.00051",
        "ARXIVID": "2503.00051",
        "COMMENT": "Matches criterion 3 as it proposes a new correspondence-free pose estimation method, which is relevant to embodied AI and spatial understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.01785": {
        "authors": [
            "Ziyu Liu",
            "Zeyi Sun",
            "Yuhang Zang",
            "Xiaoyi Dong",
            "Yuhang Cao",
            "Haodong Duan",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "title": "Visual-RFT: Visual Reinforcement Fine-Tuning",
        "abstract": "arXiv:2503.01785v1 Announce Type: new  Abstract: Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1 learns from feedback on its answers, which is especially useful in applications when fine-tuning data is scarce. Recent open-source work like DeepSeek-R1 demonstrates that reinforcement learning with verifiable reward is one key direction in reproducing o1. While the R1-style model has demonstrated success in language models, its application in multi-modal domains remains under-explored. This work introduces Visual Reinforcement Fine-Tuning (Visual-RFT), which further extends the application areas of RFT on visual tasks. Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs) to generate multiple responses containing reasoning tokens and final answers for each input, and then uses our proposed visual perception verifiable reward functions to update the model via the policy optimization algorithm such as Group Relative Policy Optimization (GRPO). We design different verifiable reward functions for different perception tasks, such as the Intersection over Union (IoU) reward for object detection. Experimental results on fine-grained image classification, few-shot object detection, reasoning grounding, as well as open-vocabulary object detection benchmarks show the competitive performance and advanced generalization ability of Visual-RFT compared with Supervised Fine-tuning (SFT). For example, Visual-RFT improves accuracy by $24.3\\%$ over the baseline in one-shot fine-grained image classification with around 100 samples. In few-shot object detection, Visual-RFT also exceeds the baseline by $21.9$ on COCO's two-shot setting and $15.4$ on LVIS. Our Visual-RFT represents a paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven approach that enhances reasoning and adaptability for domain-specific tasks.",
        "arxiv_id": "2503.01785",
        "ARXIVID": "2503.01785",
        "COMMENT": "Matches criterion 2 as it extends reinforcement fine-tuning to visual tasks using large vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.00513": {
        "authors": [
            "Hanxun Yu",
            "Wentong Li",
            "Song Wang",
            "Junbo Chen",
            "Jianke Zhu"
        ],
        "title": "Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning",
        "abstract": "arXiv:2503.00513v1 Announce Type: new  Abstract: Despite encouraging progress in 3D scene understanding, it remains challenging to develop an effective Large Multi-modal Model (LMM) that is capable of understanding and reasoning in complex 3D environments. Most previous methods typically encode 3D point and 2D image features separately, neglecting interactions between 2D semantics and 3D object properties, as well as the spatial relationships within the 3D environment. This limitation not only hinders comprehensive representations of 3D scene, but also compromises training and inference efficiency. To address these challenges, we propose a unified Instance-aware 3D Large Multi-modal Model (Inst3D-LMM) to deal with multiple 3D scene understanding tasks simultaneously. To obtain the fine-grained instance-level visual tokens, we first introduce a novel Multi-view Cross-Modal Fusion (MCMF) module to inject the multi-view 2D semantics into their corresponding 3D geometric features. For scene-level relation-aware tokens, we further present a 3D Instance Spatial Relation (3D-ISR) module to capture the intricate pairwise spatial relationships among objects. Additionally, we perform end-to-end multi-task instruction tuning simultaneously without the subsequent task-specific fine-tuning. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods across 3D scene understanding, reasoning and grounding tasks. Source code is available at https://github.com/hanxunyu/Inst3D-LMM",
        "arxiv_id": "2503.00513",
        "ARXIVID": "2503.00513",
        "COMMENT": "Matches criterion 2 as it introduces a multi-modal large model (Inst3D-LMM) for 3D scene understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.01092": {
        "authors": [
            "Wanjun Jia",
            "Fan Yang",
            "Mengfei Duan",
            "Xianchi Chen",
            "Yinxi Wang",
            "Yiming Jiang",
            "Wenrui Chen",
            "Kailun Yang",
            "Zhiyong Li"
        ],
        "title": "One-Shot Affordance Grounding of Deformable Objects in Egocentric Organizing Scenes",
        "abstract": "arXiv:2503.01092v1 Announce Type: new  Abstract: Deformable object manipulation in robotics presents significant challenges due to uncertainties in component properties, diverse configurations, visual interference, and ambiguous prompts. These factors complicate both perception and control tasks. To address these challenges, we propose a novel method for One-Shot Affordance Grounding of Deformable Objects (OS-AGDO) in egocentric organizing scenes, enabling robots to recognize previously unseen deformable objects with varying colors and shapes using minimal samples. Specifically, we first introduce the Deformable Object Semantic Enhancement Module (DefoSEM), which enhances hierarchical understanding of the internal structure and improves the ability to accurately identify local features, even under conditions of weak component information. Next, we propose the ORB-Enhanced Keypoint Fusion Module (OEKFM), which optimizes feature extraction of key components by leveraging geometric constraints and improves adaptability to diversity and visual interference. Additionally, we propose an instance-conditional prompt based on image data and task context, effectively mitigates the issue of region ambiguity caused by prompt words. To validate these methods, we construct a diverse real-world dataset, AGDDO15, which includes 15 common types of deformable objects and their associated organizational actions. Experimental results demonstrate that our approach significantly outperforms state-of-the-art methods, achieving improvements of 6.2%, 3.2%, and 2.9% in KLD, SIM, and NSS metrics, respectively, while exhibiting high generalization performance. Source code and benchmark dataset will be publicly available at https://github.com/Dikay1/OS-AGDO.",
        "arxiv_id": "2503.01092",
        "ARXIVID": "2503.01092",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for deformable object manipulation in embodied AI with a new dataset and modules.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.01222": {
        "authors": [
            "Wenbin Wang",
            "Yongcheng Jing",
            "Liang Ding",
            "Yingjie Wang",
            "Li Shen",
            "Yong Luo",
            "Bo Du",
            "Dacheng Tao"
        ],
        "title": "Retrieval-Augmented Perception: High-Resolution Image Perception Meets Visual RAG",
        "abstract": "arXiv:2503.01222v1 Announce Type: new  Abstract: High-resolution (HR) image perception remains a key challenge in multimodal large language models (MLLMs). To overcome the limitations of existing methods, this paper shifts away from prior dedicated heuristic approaches and revisits the most fundamental idea to HR perception by enhancing the long-context capability of MLLMs, driven by recent advances in long-context techniques like retrieval-augmented generation (RAG) for general LLMs. Towards this end, this paper presents the first study exploring the use of RAG to address HR perception challenges. Specifically, we propose Retrieval-Augmented Perception (RAP), a training-free framework that retrieves and fuses relevant image crops while preserving spatial context using the proposed Spatial-Awareness Layout. To accommodate different tasks, the proposed Retrieved-Exploration Search (RE-Search) dynamically selects the optimal number of crops based on model confidence and retrieval scores. Experimental results on HR benchmarks demonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving a 43% improvement on $V^*$ Bench and 19% on HR-Bench.",
        "arxiv_id": "2503.01222",
        "ARXIVID": "2503.01222",
        "COMMENT": "Matches criterion 2 as it discusses a novel retrieval-augmented framework for high-resolution image perception in multi-modal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.01020": {
        "authors": [
            "Lie Ju",
            "Sijin Zhou",
            "Yukun Zhou",
            "Huimin Lu",
            "Zhuoting Zhu",
            "Pearse A. Keane",
            "Zongyuan Ge"
        ],
        "title": "Delving into Out-of-Distribution Detection with Medical Vision-Language Models",
        "abstract": "arXiv:2503.01020v1 Announce Type: new  Abstract: Recent advances in medical vision-language models (VLMs) demonstrate impressive performance in image classification tasks, driven by their strong zero-shot generalization capabilities. However, given the high variability and complexity inherent in medical imaging data, the ability of these models to detect out-of-distribution (OOD) data in this domain remains underexplored. In this work, we conduct the first systematic investigation into the OOD detection potential of medical VLMs. We evaluate state-of-the-art VLM-based OOD detection methods across a diverse set of medical VLMs, including both general and domain-specific purposes. To accurately reflect real-world challenges, we introduce a cross-modality evaluation pipeline for benchmarking full-spectrum OOD detection, rigorously assessing model robustness against both semantic shifts and covariate shifts. Furthermore, we propose a novel hierarchical prompt-based method that significantly enhances OOD detection performance. Extensive experiments are conducted to validate the effectiveness of our approach. The codes are available at https://github.com/PyJulie/Medical-VLMs-OOD-Detection.",
        "arxiv_id": "2503.01020",
        "ARXIVID": "2503.01020",
        "COMMENT": "Matches criterion 2 as it explores vision-language models (VLMs) in the medical domain and proposes a novel hierarchical prompt-based method.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00793": {
        "authors": [
            "Ukcheol Shin",
            "Kyunghyun Lee",
            "Jean Oh"
        ],
        "title": "Bridging Spectral-wise and Multi-spectral Depth Estimation via Geometry-guided Contrastive Learning",
        "abstract": "arXiv:2503.00793v1 Announce Type: new  Abstract: Deploying depth estimation networks in the real world requires high-level robustness against various adverse conditions to ensure safe and reliable autonomy. For this purpose, many autonomous vehicles employ multi-modal sensor systems, including an RGB camera, NIR camera, thermal camera, LiDAR, or Radar. They mainly adopt two strategies to use multiple sensors: modality-wise and multi-modal fused inference. The former method is flexible but memory-inefficient, unreliable, and vulnerable. Multi-modal fusion can provide high-level reliability, yet it needs a specialized architecture. In this paper, we propose an effective solution, named align-and-fuse strategy, for the depth estimation from multi-spectral images. In the align stage, we align embedding spaces between multiple spectrum bands to learn shareable representation across multi-spectral images by minimizing contrastive loss of global and spatially aligned local features with geometry cue. After that, in the fuse stage, we train an attachable feature fusion module that can selectively aggregate the multi-spectral features for reliable and robust prediction results. Based on the proposed method, a single-depth network can achieve both spectral-invariant and multi-spectral fused depth estimation while preserving reliability, memory efficiency, and flexibility.",
        "arxiv_id": "2503.00793",
        "ARXIVID": "2503.00793",
        "COMMENT": "Matches criterion 1 as it proposes a novel geometry-guided contrastive learning method for spatial understanding in depth estimation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00441": {
        "authors": [
            "Lixu Wang",
            "Bingqi Shang",
            "Yi Li",
            "Payal Mohapatra",
            "Wei Dong",
            "Xiao Wang",
            "Qi Zhu"
        ],
        "title": "Split Adaptation for Pre-trained Vision Transformers",
        "abstract": "arXiv:2503.00441v1 Announce Type: new  Abstract: Vision Transformers (ViTs), extensively pre-trained on large-scale datasets, have become essential to foundation models, allowing excellent performance on diverse downstream tasks with minimal adaptation. Consequently, there is growing interest in adapting pre-trained ViTs across various fields, including privacy-sensitive domains where clients are often reluctant to share their data. Existing adaptation methods typically require direct data access, rendering them infeasible under these constraints. A straightforward solution may be sending the pre-trained ViT to clients for local adaptation, which poses issues of model intellectual property protection and incurs heavy client computation overhead. To address these issues, we propose a novel split adaptation (SA) method that enables effective downstream adaptation while protecting data and models. SA, inspired by split learning (SL), segments the pre-trained ViT into a frontend and a backend, with only the frontend shared with the client for data representation extraction. But unlike regular SL, SA replaces frontend parameters with low-bit quantized values, preventing direct exposure of the model. SA allows the client to add bi-level noise to the frontend and the extracted data representations, ensuring data protection. Accordingly, SA incorporates data-level and model-level out-of-distribution enhancements to mitigate noise injection's impact on adaptation performance. Our SA focuses on the challenging few-shot adaptation and adopts patch retrieval augmentation for overfitting alleviation. Extensive experiments on multiple datasets validate SA's superiority over state-of-the-art methods and demonstrate its defense against advanced data reconstruction attacks while preventing model leakage with minimal computation cost on the client side. The source codes can be found at https://github.com/conditionWang/Split_Adaptation.",
        "arxiv_id": "2503.00441",
        "ARXIVID": "2503.00441",
        "COMMENT": "Matches criterion 4 as it discusses methodological improvements to Vision Transformers, which are foundational models in vision.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.01387": {
        "authors": [
            "Siddhant Prakash",
            "David R. Walton",
            "Rafael K. dos Anjos",
            "Anthony Steed",
            "Tobias Ritschel"
        ],
        "title": "Blind Augmentation: Calibration-free Camera Distortion Model Estimation for Real-time Mixed-reality Consistency",
        "abstract": "arXiv:2503.01387v1 Announce Type: new  Abstract: Real camera footage is subject to noise, motion blur (MB) and depth of field (DoF). In some applications these might be considered distortions to be removed, but in others it is important to model them because it would be ineffective, or interfere with an aesthetic choice, to simply remove them. In augmented reality applications where virtual content is composed into a live video feed, we can model noise, MB and DoF to make the virtual content visually consistent with the video. Existing methods for this typically suffer two main limitations. First, they require a camera calibration step to relate a known calibration target to the specific cameras response. Second, existing work require methods that can be (differentiably) tuned to the calibration, such as slow and specialized neural networks. We propose a method which estimates parameters for noise, MB and DoF instantly, which allows using off-the-shelf real-time simulation methods from e.g., a game engine in compositing augmented content. Our main idea is to unlock both features by showing how to use modern computer vision methods that can remove noise, MB and DoF from the video stream, essentially providing self-calibration. This allows to auto-tune any black-box real-time noise+MB+DoF method to deliver fast and high-fidelity augmentation consistency.",
        "arxiv_id": "2503.01387",
        "ARXIVID": "2503.01387",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for real-time camera distortion modeling, which could be relevant for embodied AI and simulators.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00901": {
        "authors": [
            "Qijie Wei",
            "Kaiheng Qian",
            "Xirong Li"
        ],
        "title": "FunBench: Benchmarking Fundus Reading Skills of MLLMs",
        "abstract": "arXiv:2503.00901v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have shown significant potential in medical image analysis. However, their capabilities in interpreting fundus images, a critical skill for ophthalmology, remain under-evaluated. Existing benchmarks lack fine-grained task divisions and fail to provide modular analysis of its two key modules, i.e., large language model (LLM) and vision encoder (VE). This paper introduces FunBench, a novel visual question answering (VQA) benchmark designed to comprehensively evaluate MLLMs' fundus reading skills. FunBench features a hierarchical task organization across four levels (modality perception, anatomy perception, lesion analysis, and disease diagnosis). It also offers three targeted evaluation modes: linear-probe based VE evaluation, knowledge-prompted LLM evaluation, and holistic evaluation. Experiments on nine open-source MLLMs plus GPT-4o reveal significant deficiencies in fundus reading skills, particularly in basic tasks such as laterality recognition. The results highlight the limitations of current MLLMs and emphasize the need for domain-specific training and improved LLMs and VEs.",
        "arxiv_id": "2503.00901",
        "ARXIVID": "2503.00901",
        "COMMENT": "Matches criterion 2 as it evaluates MLLMs in a medical domain and introduces a novel benchmark for fundus reading skills.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00068": {
        "authors": [
            "Ziyu Wu",
            "Yufan Xiong",
            "Mengting Niu",
            "Fangting Xie",
            "Quan Wan",
            "Qijun Ying",
            "Boyan Liu",
            "Xiaohui Cai"
        ],
        "title": "PI-HMR: Towards Robust In-bed Temporal Human Shape Reconstruction with Contact Pressure Sensing",
        "abstract": "arXiv:2503.00068v1 Announce Type: new  Abstract: Long-term in-bed monitoring benefits automatic and real-time health management within healthcare, and the advancement of human shape reconstruction technologies further enhances the representation and visualization of users' activity patterns. However, existing technologies are primarily based on visual cues, facing serious challenges in non-light-of-sight and privacy-sensitive in-bed scenes. Pressure-sensing bedsheets offer a promising solution for real-time motion reconstruction. Yet, limited exploration in model designs and data have hindered its further development. To tackle these issues, we propose a general framework that bridges gaps in data annotation and model design. Firstly, we introduce SMPLify-IB, an optimization method that overcomes the depth ambiguity issue in top-view scenarios through gravity constraints, enabling generating high-quality 3D human shape annotations for in-bed datasets. Then we present PI-HMR, a temporal-based human shape estimator to regress meshes from pressure sequences. By integrating multi-scale feature fusion with high-pressure distribution and spatial position priors, PI-HMR outperforms SOTA methods with 17.01mm Mean-Per-Joint-Error decrease. This work provides a whole",
        "arxiv_id": "2503.00068",
        "ARXIVID": "2503.00068",
        "COMMENT": "Matches criterion 3 as it focuses on a novel method for in-bed human shape reconstruction using pressure sensing, which is a unique angle in embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00747": {
        "authors": [
            "Fei Teng",
            "Buyin Deng",
            "Boyuan Zheng",
            "Kai Luo",
            "Kunyu Peng",
            "Jiaming Zhang",
            "Kailun Yang"
        ],
        "title": "Unifying Light Field Perception with Field of Parallax",
        "abstract": "arXiv:2503.00747v1 Announce Type: new  Abstract: Field of Parallax (FoP)}, a spatial field that distills the common features from different LF representations to provide flexible and consistent support for multi-task learning. FoP is built upon three core features--projection difference, adjacency divergence, and contextual consistency--which are essential for cross-task adaptability. To implement FoP, we design a two-step angular adapter: the first step captures angular-specific differences, while the second step consolidates contextual consistency to ensure robust representation. Leveraging the FoP-based representation, we introduce the LFX framework, the first to handle arbitrary LF representations seamlessly, unifying LF multi-task vision. We evaluated LFX across three different tasks, achieving new state-of-the-art results, compared with previous task-specific architectures: 84.74% in mIoU for semantic segmentation on UrbanLF, 0.84% in AP for object detection on PKU, and 0.030 in MAE and 0.026 in MAE for salient object detection on Duftv2 and PKU, respectively. The source code will be made publicly available at https://github.com/warriordby/LFX.",
        "arxiv_id": "2503.00747",
        "ARXIVID": "2503.00747",
        "COMMENT": "Matches criterion 4 as it introduces a novel framework for light field perception, which is related to vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00132": {
        "authors": [
            "Anzhe Chen",
            "Hongxiang Yu",
            "Shuxin Li",
            "Yuxi Chen",
            "Zhongxiang Zhou",
            "Wentao Sun",
            "Rong Xiong",
            "Yue Wang"
        ],
        "title": "CNSv2: Probabilistic Correspondence Encoded Neural Image Servo",
        "abstract": "arXiv:2503.00132v1 Announce Type: new  Abstract: Visual servo based on traditional image matching methods often requires accurate keypoint correspondence for high precision control. However, keypoint detection or matching tends to fail in challenging scenarios with inconsistent illuminations or textureless objects, resulting significant performance degradation. Previous approaches, including our proposed Correspondence encoded Neural image Servo policy (CNS), attempted to alleviate these issues by integrating neural control strategies. While CNS shows certain improvement against error correspondence over conventional image-based controllers, it could not fully resolve the limitations arising from poor keypoint detection and matching. In this paper, we continue to address this problem and propose a new solution: Probabilistic Correspondence Encoded Neural Image Servo (CNSv2). CNSv2 leverages probabilistic feature matching to improve robustness in challenging scenarios. By redesigning the architecture to condition on multimodal feature matching, CNSv2 achieves high precision, improved robustness across diverse scenes and runs in real-time. We validate CNSv2 with simulations and real-world experiments, demonstrating its effectiveness in overcoming the limitations of detector-based methods in visual servo tasks.",
        "arxiv_id": "2503.00132",
        "ARXIVID": "2503.00132",
        "COMMENT": "Matches criterion 1 as it proposes a new method for spatial understanding in visual servo tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.01628": {
        "authors": [
            "William Michael Laprade",
            "Jesper Cairo Westergaard",
            "Svend Christensen",
            "Mads Nielsen",
            "Anders Bjorholm Dahl"
        ],
        "title": "A General Purpose Spectral Foundational Model for Both Proximal and Remote Sensing Spectral Imaging",
        "abstract": "arXiv:2503.01628v1 Announce Type: new  Abstract: Spectral imaging data acquired via multispectral and hyperspectral cameras can have hundreds of channels, where each channel records the reflectance at a specific wavelength and bandwidth. Time and resource constraints limit our ability to collect large spectral datasets, making it difficult to build and train predictive models from scratch. In the RGB domain, we can often alleviate some of the limitations of smaller datasets by using pretrained foundational models as a starting point. However, most existing foundation models are pretrained on large datasets of 3-channel RGB images, severely limiting their effectiveness when used with spectral imaging data. The few spectral foundation models that do exist usually have one of two limitations: (1) they are built and trained only on remote sensing data limiting their application in proximal spectral imaging, (2) they utilize the more widely available multispectral imaging datasets with less than 15 channels restricting their use with hundred-channel hyperspectral images. To alleviate these issues, we propose a large-scale foundational model and dataset built upon the masked autoencoder architecture that takes advantage of spectral channel encoding, spatial-spectral masking and ImageNet pretraining for an adaptable and robust model for downstream spectral imaging tasks.",
        "arxiv_id": "2503.01628",
        "ARXIVID": "2503.01628",
        "COMMENT": "Matches criterion 4 as it proposes a spectral foundational model for spectral imaging tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.00936": {
        "authors": [
            "Yuji Wang",
            "Jingchen Ni",
            "Yong Liu",
            "Chun Yuan",
            "Yansong Tang"
        ],
        "title": "IteRPrimE: Zero-shot Referring Image Segmentation with Iterative Grad-CAM Refinement and Primary Word Emphasis",
        "abstract": "arXiv:2503.00936v1 Announce Type: new  Abstract: Zero-shot Referring Image Segmentation (RIS) identifies the instance mask that best aligns with a specified referring expression without training and fine-tuning, significantly reducing the labor-intensive annotation process. Despite achieving commendable results, previous CLIP-based models have a critical drawback: the models exhibit a notable reduction in their capacity to discern relative spatial relationships of objects. This is because they generate all possible masks on an image and evaluate each masked region for similarity to the given expression, often resulting in decreased sensitivity to direct positional clues in text inputs. Moreover, most methods have weak abilities to manage relationships between primary words and their contexts, causing confusion and reduced accuracy in identifying the correct target region. To address these challenges, we propose IteRPrimE (Iterative Grad-CAM Refinement and Primary word Emphasis), which leverages a saliency heatmap through Grad-CAM from a Vision-Language Pre-trained (VLP) model for image-text matching. An iterative Grad-CAM refinement strategy is introduced to progressively enhance the model's focus on the target region and overcome positional insensitivity, creating a self-correcting effect. Additionally, we design the Primary Word Emphasis module to help the model handle complex semantic relations, enhancing its ability to attend to the intended object. Extensive experiments conducted on the RefCOCO/+/g, and PhraseCut benchmarks demonstrate that IteRPrimE outperforms previous state-of-the-art zero-shot methods, particularly excelling in out-of-domain scenarios.",
        "arxiv_id": "2503.00936",
        "ARXIVID": "2503.00936",
        "COMMENT": "Matches criterion 2 as it focuses on zero-shot referring image segmentation using vision-language pre-trained models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.01210": {
        "authors": [
            "Guanyao Wu",
            "Haoyu Liu",
            "Hongming Fu",
            "Yichuan Peng",
            "Jinyuan Liu",
            "Xin Fan",
            "Risheng Liu"
        ],
        "title": "Every SAM Drop Counts: Embracing Semantic Priors for Multi-Modality Image Fusion and Beyond",
        "abstract": "arXiv:2503.01210v1 Announce Type: new  Abstract: Multi-modality image fusion, particularly infrared and visible image fusion, plays a crucial role in integrating diverse modalities to enhance scene understanding. Early research primarily focused on visual quality, yet challenges remain in preserving fine details, making it difficult to adapt to subsequent tasks. Recent approaches have shifted towards task-specific design, but struggle to achieve the ``The Best of Both Worlds'' due to inconsistent optimization goals. To address these issues, we propose a novel method that leverages the semantic knowledge from the Segment Anything Model (SAM) to Grow the quality of fusion results and Establish downstream task adaptability, namely SAGE. Specifically, we design a Semantic Persistent Attention (SPA) Module that efficiently maintains source information via the persistent repository while extracting high-level semantic priors from SAM. More importantly, to eliminate the impractical dependence on SAM during inference, we introduce a bi-level optimization-driven distillation mechanism with triplet losses, which allow the student network to effectively extract knowledge at the feature, pixel, and contrastive semantic levels, thereby removing reliance on the cumbersome SAM model. Extensive experiments show that our method achieves a balance between high-quality visual results and downstream task adaptability while maintaining practical deployment efficiency.",
        "arxiv_id": "2503.01210",
        "ARXIVID": "2503.01210",
        "COMMENT": "Matches criterion 4 as it leverages vision foundation models (SAM) for multi-modality image fusion.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.01645": {
        "authors": [
            "Zhendong Wang",
            "Jianmin Bao",
            "Shuyang Gu",
            "Dong Chen",
            "Wengang Zhou",
            "Houqiang Li"
        ],
        "title": "DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models",
        "abstract": "arXiv:2503.01645v1 Announce Type: new  Abstract: In this paper, we present DesignDiffusion, a simple yet effective framework for the novel task of synthesizing design images from textual descriptions. A primary challenge lies in generating accurate and style-consistent textual and visual content. Existing works in a related task of visual text generation often focus on generating text within given specific regions, which limits the creativity of generation models, resulting in style or color inconsistencies between textual and visual elements if applied to design image generation. To address this issue, we propose an end-to-end, one-stage diffusion-based framework that avoids intricate components like position and layout modeling. Specifically, the proposed framework directly synthesizes textual and visual design elements from user prompts. It utilizes a distinctive character embedding derived from the visual text to enhance the input prompt, along with a character localization loss for enhanced supervision during text generation. Furthermore, we employ a self-play Direct Preference Optimization fine-tuning strategy to improve the quality and accuracy of the synthesized visual text. Extensive experiments demonstrate that DesignDiffusion achieves state-of-the-art performance in design image generation.",
        "arxiv_id": "2503.01645",
        "ARXIVID": "2503.01645",
        "COMMENT": "Matches criterion 4 as it focuses on a diffusion-based framework for text-to-design image generation, which is an application of vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.01416": {
        "authors": [
            "Ramanathan Rajendiran",
            "Debaditya Roy",
            "Basura Fernando"
        ],
        "title": "Learning to Generate Long-term Future Narrations Describing Activities of Daily Living",
        "abstract": "arXiv:2503.01416v1 Announce Type: new  Abstract: Anticipating future events is crucial for various application domains such as healthcare, smart home technology, and surveillance. Narrative event descriptions provide context-rich information, enhancing a system's future planning and decision-making capabilities. We propose a novel task: $\\textit{long-term future narration generation}$, which extends beyond traditional action anticipation by generating detailed narrations of future daily activities. We introduce a visual-language model, ViNa, specifically designed to address this challenging task. ViNa integrates long-term videos and corresponding narrations to generate a sequence of future narrations that predict subsequent events and actions over extended time horizons. ViNa extends existing multimodal models that perform only short-term predictions or describe observed videos by generating long-term future narrations for a broader range of daily activities. We also present a novel downstream application that leverages the generated narrations called future video retrieval to help users improve planning for a task by visualizing the future. We evaluate future narration generation on the largest egocentric dataset Ego4D.",
        "arxiv_id": "2503.01416",
        "ARXIVID": "2503.01416",
        "COMMENT": "This paper proposes a visual-language model for long-term future narration generation, which could be tangentially related to embodied AI but does not directly match any specific criteria.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2503.00276": {
        "authors": [
            "Haoxin Li",
            "Yingchen Yu",
            "Qilong Wu",
            "Hanwang Zhang",
            "Boyang Li",
            "Song Bai"
        ],
        "title": "Learning to Animate Images from A Few Videos to Portray Delicate Human Actions",
        "abstract": "arXiv:2503.00276v1 Announce Type: new  Abstract: Despite recent progress, video generative models still struggle to animate human actions from static images, particularly when handling uncommon actions whose training data are limited. In this paper, we investigate the task of learning to animate human actions from a small number of videos -- 16 or fewer -- which is highly valuable in real-world applications like video and movie production. Few-shot learning of generalizable motion patterns while ensuring smooth transitions from the initial reference image is exceedingly challenging. We propose FLASH (Few-shot Learning to Animate and Steer Humans), which improves motion generalization by aligning motion features and inter-frame correspondence relations between videos that share the same motion but have different appearances. This approach minimizes overfitting to visual appearances in the limited training data and enhances the generalization of learned motion patterns. Additionally, FLASH extends the decoder with additional layers to compensate lost details in the latent space, fostering smooth transitions from the initial reference image. Experiments demonstrate that FLASH effectively animates images with unseen human or scene appearances into specified actions while maintaining smooth transitions from the reference image.",
        "arxiv_id": "2503.00276",
        "ARXIVID": "2503.00276",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling in multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.01220": {
        "authors": [
            "Jiqing Wu",
            "Ingrid Berg",
            "Yawei Li",
            "Ender Konukoglu",
            "Viktor H. Koelzer"
        ],
        "title": "Tera-MIND: Tera-scale mouse brain simulation via spatial mRNA-guided diffusion",
        "abstract": "arXiv:2503.01220v1 Announce Type: new  Abstract: Holistic 3D modeling of molecularly defined brain structures is crucial for understanding complex brain functions. Emerging tissue profiling technologies enable the construction of a comprehensive atlas of the mammalian brain with sub-cellular resolution and spatially resolved gene expression data. However, such tera-scale volumetric datasets present significant computational challenges in understanding complex brain functions within their native 3D spatial context. Here, we propose the novel generative approach $\\textbf{Tera-MIND}$, which can simulate $\\textbf{Tera}$-scale $\\textbf{M}$ouse bra$\\textbf{IN}s$ in 3D using a patch-based and boundary-aware $\\textbf{D}$iffusion model. Taking spatial transcriptomic data as the conditional input, we generate virtual mouse brains with comprehensive cellular morphological detail at teravoxel scale. Through the lens of 3D $gene$-$gene$ self-attention, we identify spatial molecular interactions for key transcriptomic pathways in the murine brain, exemplified by glutamatergic and dopaminergic neuronal systems. Importantly, these $in$-$silico$ biological findings are consistent and reproducible across three tera-scale virtual mouse brains. Therefore, Tera-MIND showcases a promising path toward efficient and generative simulations of whole organ systems for biomedical research. Project website: $\\href{http://musikisomorphie.github.io/Tera-MIND.html}{https}$",
        "arxiv_id": "2503.01220",
        "ARXIVID": "2503.01220",
        "COMMENT": "Does not match any specific criteria but is related to generative modeling for brain simulation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.01458": {
        "authors": [
            "Xu Wan",
            "Chao Yang",
            "Cheng Yang",
            "Jie Song",
            "Mingyang Sun"
        ],
        "title": "SrSv: Integrating Sequential Rollouts with Sequential Value Estimation for Multi-agent Reinforcement Learning",
        "abstract": "arXiv:2503.01458v1 Announce Type: new  Abstract: Although multi-agent reinforcement learning (MARL) has shown its success across diverse domains, extending its application to large-scale real-world systems still faces significant challenges. Primarily, the high complexity of real-world environments exacerbates the credit assignment problem, substantially reducing training efficiency. Moreover, the variability of agent populations in large-scale scenarios necessitates scalable decision-making mechanisms. To address these challenges, we propose a novel framework: Sequential rollout with Sequential value estimation (SrSv). This framework aims to capture agent interdependence and provide a scalable solution for cooperative MARL. Specifically, SrSv leverages the autoregressive property of the Transformer model to handle varying populations through sequential action rollout. Furthermore, to capture the interdependence of policy distributions and value functions among multiple agents, we introduce an innovative sequential value estimation methodology and integrates the value approximation into an attention-based sequential model. We evaluate SrSv on three benchmarks: Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, and DubinsCars. Experimental results demonstrate that SrSv significantly outperforms baseline methods in terms of training efficiency without compromising convergence performance. Moreover, when implemented in a large-scale DubinsCar system with 1,024 agents, our framework surpasses existing benchmarks, highlighting the excellent scalability of SrSv.",
        "arxiv_id": "2503.01458",
        "ARXIVID": "2503.01458",
        "COMMENT": "Does not match any specific criteria. Focuses on multi-agent reinforcement learning, which is not directly related to spatial understanding, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.00643": {
        "authors": [
            "Yante Li",
            "Hanwen Qi",
            "Haoyu Chen",
            "Xinlian Liang",
            "Guoying Zhao"
        ],
        "title": "Deep Change Monitoring: A Hyperbolic Representative Learning Framework and a Dataset for Long-term Fine-grained Tree Change Detection",
        "abstract": "arXiv:2503.00643v1 Announce Type: new  Abstract: In environmental protection, tree monitoring plays an essential role in maintaining and improving ecosystem health. However, precise monitoring is challenging because existing datasets fail to capture continuous fine-grained changes in trees due to low-resolution images and high acquisition costs. In this paper, we introduce UAVTC, a large-scale, long-term, high-resolution dataset collected using UAVs equipped with cameras, specifically designed to detect individual Tree Changes (TCs). UAVTC includes rich annotations and statistics based on biological knowledge, offering a fine-grained view for tree monitoring. To address environmental influences and effectively model the hierarchical diversity of physiological TCs, we propose a novel Hyperbolic Siamese Network (HSN) for TC detection, enabling compact and hierarchical representations of dynamic tree changes.   Extensive experiments show that HSN can effectively capture complex hierarchical changes and provide a robust solution for fine-grained TC detection. In addition, HSN generalizes well to cross-domain face anti-spoofing task, highlighting its broader significance in AI. We believe our work, combining ecological insights and interdisciplinary expertise, will benefit the community by offering a new benchmark and innovative AI technologies.",
        "arxiv_id": "2503.00643",
        "ARXIVID": "2503.00643",
        "COMMENT": "Does not match any specific criteria. Focuses on tree monitoring and hyperbolic learning, which is not directly related to spatial understanding, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.00389": {
        "authors": [
            "Yuto Shibata",
            "Yusuke Oumi",
            "Go Irie",
            "Akisato Kimura",
            "Yoshimitsu Aoki",
            "Mariko Isogawa"
        ],
        "title": "BGM2Pose: Active 3D Human Pose Estimation with Non-Stationary Sounds",
        "abstract": "arXiv:2503.00389v1 Announce Type: new  Abstract: We propose BGM2Pose, a non-invasive 3D human pose estimation method using arbitrary music (e.g., background music) as active sensing signals. Unlike existing approaches that significantly limit practicality by employing intrusive chirp signals within the audible range, our method utilizes natural music that causes minimal discomfort to humans. Estimating human poses from standard music presents significant challenges. In contrast to sound sources specifically designed for measurement, regular music varies in both volume and pitch. These dynamic changes in signals caused by music are inevitably mixed with alterations in the sound field resulting from human motion, making it hard to extract reliable cues for pose estimation. To address these challenges, BGM2Pose introduces a Contrastive Pose Extraction Module that employs contrastive learning and hard negative sampling to eliminate musical components from the recorded data, isolating the pose information. Additionally, we propose a Frequency-wise Attention Module that enables the model to focus on subtle acoustic variations attributable to human movement by dynamically computing attention across frequency bands. Experiments suggest that our method outperforms the existing methods, demonstrating substantial potential for real-world applications. Our datasets and code will be made publicly available.",
        "arxiv_id": "2503.00389",
        "ARXIVID": "2503.00389",
        "COMMENT": "Does not match any specific criteria. Focuses on human pose estimation using music signals, which is not directly related to spatial understanding, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.00226": {
        "authors": [
            "Yufei Guo",
            "Xiaode Liu",
            "Yuanpei Chen",
            "Weihang Peng",
            "Yuhan Zhang",
            "Zhe Ma"
        ],
        "title": "Spiking Transformer:Introducing Accurate Addition-Only Spiking Self-Attention for Transformer",
        "abstract": "arXiv:2503.00226v1 Announce Type: new  Abstract: Transformers have demonstrated outstanding performance across a wide range of tasks, owing to their self-attention mechanism, but they are highly energy-consuming. Spiking Neural Networks have emerged as a promising energy-efficient alternative to traditional Artificial Neural Networks, leveraging event-driven computation and binary spikes for information transfer. The combination of Transformers' capabilities with the energy efficiency of SNNs offers a compelling opportunity. This paper addresses the challenge of adapting the self-attention mechanism of Transformers to the spiking paradigm by introducing a novel approach: Accurate Addition-Only Spiking Self-Attention (A$^2$OS$^2$A). Unlike existing methods that rely solely on binary spiking neurons for all components of the self-attention mechanism, our approach integrates binary, ReLU, and ternary spiking neurons. This hybrid strategy significantly improves accuracy while preserving non-multiplicative computations. Moreover, our method eliminates the need for softmax and scaling operations. Extensive experiments show that the A$^2$OS$^2$A-based Spiking Transformer outperforms existing SNN-based Transformers on several datasets, even achieving an accuracy of 78.66\\% on ImageNet-1K. Our work represents a significant advancement in SNN-based Transformer models, offering a more accurate and efficient solution for real-world applications.",
        "arxiv_id": "2503.00226",
        "ARXIVID": "2503.00226",
        "COMMENT": "Does not match any specific criteria. Focuses on spiking transformers, which is not directly related to spatial understanding, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.00516": {
        "authors": [
            "Jiawen Zhu",
            "Huayi Tang",
            "Xin Chen",
            "Xinying Wang",
            "Dong Wang",
            "Huchuan Lu"
        ],
        "title": "Two-stream Beats One-stream: Asymmetric Siamese Network for Efficient Visual Tracking",
        "abstract": "arXiv:2503.00516v1 Announce Type: new  Abstract: Efficient tracking has garnered attention for its ability to operate on resource-constrained platforms for real-world deployment beyond desktop GPUs. Current efficient trackers mainly follow precision-oriented trackers, adopting a one-stream framework with lightweight modules. However, blindly adhering to the one-stream paradigm may not be optimal, as incorporating template computation in every frame leads to redundancy, and pervasive semantic interaction between template and search region places stress on edge devices. In this work, we propose a novel asymmetric Siamese tracker named \\textbf{AsymTrack} for efficient tracking. AsymTrack disentangles template and search streams into separate branches, with template computing only once during initialization to generate modulation signals. Building on this architecture, we devise an efficient template modulation mechanism to unidirectional inject crucial cues into the search features, and design an object perception enhancement module that integrates abstract semantics and local details to overcome the limited representation in lightweight tracker. Extensive experiments demonstrate that AsymTrack offers superior speed-precision trade-offs across different platforms compared to the current state-of-the-arts. For instance, AsymTrack-T achieves 60.8\\% AUC on LaSOT and 224/81/84 FPS on GPU/CPU/AGX, surpassing HiT-Tiny by 6.0\\% AUC with higher speeds. The code is available at https://github.com/jiawen-zhu/AsymTrack.",
        "arxiv_id": "2503.00516",
        "ARXIVID": "2503.00516",
        "COMMENT": "This paper introduces an efficient visual tracking method, which is not directly related to any specific criteria but may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01257": {
        "authors": [
            "Xuan Zhu",
            "Jijun Xiang",
            "Xianqi Wang",
            "Longliang Liu",
            "Yu Wang",
            "Hong Zhang",
            "Fei Guo",
            "Xin Yang"
        ],
        "title": "SVDC: Consistent Direct Time-of-Flight Video Depth Completion with Frequency Selective Fusion",
        "abstract": "arXiv:2503.01257v1 Announce Type: new  Abstract: Lightweight direct Time-of-Flight (dToF) sensors are ideal for 3D sensing on mobile devices. However, due to the manufacturing constraints of compact devices and the inherent physical principles of imaging, dToF depth maps are sparse and noisy. In this paper, we propose a novel video depth completion method, called SVDC, by fusing the sparse dToF data with the corresponding RGB guidance. Our method employs a multi-frame fusion scheme to mitigate the spatial ambiguity resulting from the sparse dToF imaging. Misalignment between consecutive frames during multi-frame fusion could cause blending between object edges and the background, which results in a loss of detail. To address this, we introduce an adaptive frequency selective fusion (AFSF) module, which automatically selects convolution kernel sizes to fuse multi-frame features. Our AFSF utilizes a channel-spatial enhancement attention (CSEA) module to enhance features and generates an attention map as fusion weights. The AFSF ensures edge detail recovery while suppressing high-frequency noise in smooth regions. To further enhance temporal consistency, We propose a cross-window consistency loss to ensure consistent predictions across different windows, effectively reducing flickering. Our proposed SVDC achieves optimal accuracy and consistency on the TartanAir and Dynamic Replica datasets. Code is available at https://github.com/Lan1eve/SVDC.",
        "arxiv_id": "2503.01257",
        "ARXIVID": "2503.01257",
        "COMMENT": "This paper focuses on video depth completion using RGB guidance, which is not directly related to any specific criteria but may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00237": {
        "authors": [
            "Erik Miehling",
            "Karthikeyan Natesan Ramamurthy",
            "Kush R. Varshney",
            "Matthew Riemer",
            "Djallel Bouneffouf",
            "John T. Richards",
            "Amit Dhurandhar",
            "Elizabeth M. Daly",
            "Michael Hind",
            "Prasanna Sattigeri",
            "Dennis Wei",
            "Ambrish Rawat",
            "Jasmina Gajcin",
            "Werner Geyer"
        ],
        "title": "Agentic AI Needs a Systems Theory",
        "abstract": "arXiv:2503.00237v1 Announce Type: new  Abstract: The endowment of AI with reasoning capabilities and some degree of agency is widely viewed as a path toward more capable and generalizable systems. Our position is that the current development of agentic AI requires a more holistic, systems-theoretic perspective in order to fully understand their capabilities and mitigate any emergent risks. The primary motivation for our position is that AI development is currently overly focused on individual model capabilities, often ignoring broader emergent behavior, leading to a significant underestimation in the true capabilities and associated risks of agentic AI. We describe some fundamental mechanisms by which advanced capabilities can emerge from (comparably simpler) agents simply due to their interaction with the environment and other agents. Informed by an extensive amount of existing literature from various fields, we outline mechanisms for enhanced agent cognition, emergent causal reasoning ability, and metacognitive awareness. We conclude by presenting some key open challenges and guidance for the development of agentic AI. We emphasize that a systems-level perspective is essential for better understanding, and purposefully shaping, agentic AI systems.",
        "arxiv_id": "2503.00237",
        "ARXIVID": "2503.00237",
        "COMMENT": "This paper discusses a systems-theoretic perspective for agentic AI, which could be tangentially related to embodied AI but does not directly match any specific criteria.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01087": {
        "authors": [
            "Jon Donnelly",
            "Zhicheng Guo",
            "Alina Jade Barnett",
            "Hayden McTavish",
            "Chaofan Chen",
            "Cynthia Rudin"
        ],
        "title": "Rashomon Sets for Prototypical-Part Networks: Editing Interpretable Models in Real-Time",
        "abstract": "arXiv:2503.01087v1 Announce Type: new  Abstract: Interpretability is critical for machine learning models in high-stakes settings because it allows users to verify the model's reasoning. In computer vision, prototypical part models (ProtoPNets) have become the dominant model type to meet this need. Users can easily identify flaws in ProtoPNets, but fixing problems in a ProtoPNet requires slow, difficult retraining that is not guaranteed to resolve the issue. This problem is called the \"interaction bottleneck.\" We solve the interaction bottleneck for ProtoPNets by simultaneously finding many equally good ProtoPNets (i.e., a draw from a \"Rashomon set\"). We show that our framework - called Proto-RSet - quickly produces many accurate, diverse ProtoPNets, allowing users to correct problems in real time while maintaining performance guarantees with respect to the training set. We demonstrate the utility of this method in two settings: 1) removing synthetic bias introduced to a bird identification model and 2) debugging a skin cancer identification model. This tool empowers non-machine-learning experts, such as clinicians or domain experts, to quickly refine and correct machine learning models without repeated retraining by machine learning experts.",
        "arxiv_id": "2503.01087",
        "ARXIVID": "2503.01087",
        "COMMENT": "Does not match any specific criteria but is related to interpretability in computer vision models, which aligns with your friend's general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00811": {
        "authors": [
            "Lu Ma",
            "Kaibo Cao",
            "Hao Liang",
            "Jiaxin Lin",
            "Zhuang Li",
            "Yuhong Liu",
            "Jihong Zhang",
            "Wentao Zhang",
            "Bin Cui"
        ],
        "title": "Evaluating and Predicting Distorted Human Body Parts for Generated Images",
        "abstract": "arXiv:2503.00811v1 Announce Type: new  Abstract: Recent advancements in text-to-image (T2I) models enable high-quality image synthesis, yet generating anatomically accurate human figures remains challenging. AI-generated images frequently exhibit distortions such as proliferated limbs, missing fingers, deformed extremities, or fused body parts. Existing evaluation metrics like Inception Score (IS) and Fr\\'echet Inception Distance (FID) lack the granularity to detect these distortions, while human preference-based metrics focus on abstract quality assessments rather than anatomical fidelity. To address this gap, we establish the first standards for identifying human body distortions in AI-generated images and introduce Distortion-5K, a comprehensive dataset comprising 4,700 annotated images of normal and malformed human figures across diverse styles and distortion types. Based on this dataset, we propose ViT-HD, a Vision Transformer-based model tailored for detecting human body distortions in AI-generated images, which outperforms state-of-the-art segmentation models and visual language models, achieving an F1 score of 0.899 and IoU of 0.831 on distortion localization. Additionally, we construct the Human Distortion Benchmark with 500 human-centric prompts to evaluate four popular T2I models using trained ViT-HD, revealing that nearly 50\\% of generated images contain distortions. This work pioneers a systematic approach to evaluating anatomical accuracy in AI-generated humans, offering tools to advance the fidelity of T2I models and their real-world applicability. The Distortion-5K dataset, trained ViT-HD will soon be released in our GitHub repository: \\href{https://github.com/TheRoadQaQ/Predicting-Distortion}{https://github.com/TheRoadQaQ/Predicting-Distortion}.",
        "arxiv_id": "2503.00811",
        "ARXIVID": "2503.00811",
        "COMMENT": "Does not match any specific criteria but is related to evaluating and improving text-to-image models, which aligns with your friend's general interest in generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00410": {
        "authors": [
            "Zhaoyi Tian",
            "Feifeng Wang",
            "Shiwei Wang",
            "Zihao Zhou",
            "Yao Zhu",
            "Liquan Shen"
        ],
        "title": "High Dynamic Range Video Compression: A Large-Scale Benchmark Dataset and A Learned Bit-depth Scalable Compression Algorithm",
        "abstract": "arXiv:2503.00410v1 Announce Type: new  Abstract: Recently, learned video compression (LVC) is undergoing a period of rapid development. However, due to absence of large and high-quality high dynamic range (HDR) video training data, LVC on HDR video is still unexplored. In this paper, we are the first to collect a large-scale HDR video benchmark dataset, named HDRVD2K, featuring huge quantity, diverse scenes and multiple motion types. HDRVD2K fills gaps of video training data and facilitate the development of LVC on HDR videos. Based on HDRVD2K, we further propose the first learned bit-depth scalable video compression (LBSVC) network for HDR videos by effectively exploiting bit-depth redundancy between videos of multiple dynamic ranges. To achieve this, we first propose a compression-friendly bit-depth enhancement module (BEM) to effectively predict original HDR videos based on compressed tone-mapped low dynamic range (LDR) videos and dynamic range prior, instead of reducing redundancy only through spatio-temporal predictions. Our method greatly improves the reconstruction quality and compression performance on HDR videos. Extensive experiments demonstrate the effectiveness of HDRVD2K on learned HDR video compression and great compression performance of our proposed LBSVC network. Code and dataset will be released in https://github.com/sdkinda/HDR-Learned-Video-Coding.",
        "arxiv_id": "2503.00410",
        "ARXIVID": "2503.00410",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and machine learning in the context of HDR video compression.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00881": {
        "authors": [
            "You Shen",
            "Zhipeng Zhang",
            "Xinyang Li",
            "Yansong Qu",
            "Yu Lin",
            "Shengchuan Zhang",
            "Liujuan Cao"
        ],
        "title": "Evolving High-Quality Rendering and Reconstruction in a Unified Framework with Contribution-Adaptive Regularization",
        "abstract": "arXiv:2503.00881v1 Announce Type: new  Abstract: Representing 3D scenes from multiview images is a core challenge in computer vision and graphics, which requires both precise rendering and accurate reconstruction. Recently, 3D Gaussian Splatting (3DGS) has garnered significant attention for its high-quality rendering and fast inference speed. Yet, due to the unstructured and irregular nature of Gaussian point clouds, ensuring accurate geometry reconstruction remains difficult. Existing methods primarily focus on geometry regularization, with common approaches including primitive-based and dual-model frameworks. However, the former suffers from inherent conflicts between rendering and reconstruction, while the latter is computationally and storage-intensive. To address these challenges, we propose CarGS, a unified model leveraging Contribution-adaptive regularization to achieve simultaneous, high-quality rendering and surface reconstruction. The essence of our framework is learning adaptive contribution for Gaussian primitives by squeezing the knowledge from geometry regularization into a compact MLP. Additionally, we introduce a geometry-guided densification strategy with clues from both normals and Signed Distance Fields (SDF) to improve the capability of capturing high-frequency details. Our design improves the mutual learning of the two tasks, meanwhile its unified structure does not require separate models as in dual-model based approaches, guaranteeing efficiency. Extensive experiments demonstrate the ability to achieve state-of-the-art (SOTA) results in both rendering fidelity and reconstruction accuracy while maintaining real-time speed and minimal storage size.",
        "arxiv_id": "2503.00881",
        "ARXIVID": "2503.00881",
        "COMMENT": "Does not match any specific criterion but is relevant to 3D scene representation and rendering, which aligns with your friend's general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00495": {
        "authors": [
            "Xuanchen Li",
            "Jianyu Wang",
            "Yuhao Cheng",
            "Yikun Zeng",
            "Xingyu Ren",
            "Wenhan Zhu",
            "Weiming Zhao",
            "Yichao Yan"
        ],
        "title": "Towards High-fidelity 3D Talking Avatar with Personalized Dynamic Texture",
        "abstract": "arXiv:2503.00495v1 Announce Type: new  Abstract: Significant progress has been made for speech-driven 3D face animation, but most works focus on learning the motion of mesh/geometry, ignoring the impact of dynamic texture. In this work, we reveal that dynamic texture plays a key role in rendering high-fidelity talking avatars, and introduce a high-resolution 4D dataset \\textbf{TexTalk4D}, consisting of 100 minutes of audio-synced scan-level meshes with detailed 8K dynamic textures from 100 subjects. Based on the dataset, we explore the inherent correlation between motion and texture, and propose a diffusion-based framework \\textbf{TexTalker} to simultaneously generate facial motions and dynamic textures from speech. Furthermore, we propose a novel pivot-based style injection strategy to capture the complicity of different texture and motion styles, which allows disentangled control. TexTalker, as the first method to generate audio-synced facial motion with dynamic texture, not only outperforms the prior arts in synthesising facial motions, but also produces realistic textures that are consistent with the underlying facial movements. Project page: https://xuanchenli.github.io/TexTalk/.",
        "arxiv_id": "2503.00495",
        "ARXIVID": "2503.00495",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and multi-modal learning, which aligns with your friend's general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01009": {
        "authors": [
            "Jinzhao Li",
            "Nan Jiang",
            "Yexiang Xue"
        ],
        "title": "An Exact Solver for Satisfiability Modulo Counting with Probabilistic Circuits",
        "abstract": "arXiv:2503.01009v1 Announce Type: new  Abstract: Satisfiability Modulo Counting (SMC) is a recently proposed general language to reason about problems integrating statistical and symbolic artificial intelligence. An SMC formula is an extended SAT formula in which the truth values of a few Boolean variables are determined by probabilistic inference. Existing approximate solvers optimize surrogate objectives, which lack formal guarantees. Current exact solvers directly integrate SAT solvers and probabilistic inference solvers resulting in slow performance because of many back-and-forth invocations of both solvers. We propose KOCO-SMC, an integrated exact SMC solver that efficiently tracks lower and upper bounds in the probabilistic inference process. It enhances computational efficiency by enabling early estimation of probabilistic inference using only partial variable assignments, whereas existing methods require full variable assignments. In the experiment, we compare KOCO-SMC with currently available approximate and exact SMC solvers on large-scale datasets and real-world applications. Our approach delivers high-quality solutions with high efficiency.",
        "arxiv_id": "2503.01009",
        "ARXIVID": "2503.01009",
        "COMMENT": "Does not match any specific criteria but is relevant to probabilistic reasoning and symbolic AI, which are tangentially related to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01633": {
        "authors": [
            "Luyi Qiu",
            "Tristan Till",
            "Xiaobao Guo",
            "Adams Wai-Kin Kong"
        ],
        "title": "SparseMamba-PCL: Scribble-Supervised Medical Image Segmentation via SAM-Guided Progressive Collaborative Learning",
        "abstract": "arXiv:2503.01633v1 Announce Type: new  Abstract: Scribble annotations significantly reduce the cost and labor required for dense labeling in large medical datasets with complex anatomical structures. However, current scribble-supervised learning methods are limited in their ability to effectively propagate sparse annotation labels to dense segmentation masks and accurately segment object boundaries. To address these issues, we propose a Progressive Collaborative Learning framework that leverages novel algorithms and the Med-SAM foundation model to enhance information quality during training. (1) We enrich ground truth scribble segmentation labels through a new algorithm, propagating scribbles to estimate object boundaries. (2) We enhance feature representation by optimizing Med-SAM-guided training through the fusion of feature embeddings from Med-SAM and our proposed Sparse Mamba network. This enriched representation also facilitates the fine-tuning of the Med-SAM decoder with enriched scribbles. (3) For inference, we introduce a Sparse Mamba network, which is highly capable of capturing local and global dependencies by replacing the traditional sequential patch processing method with a skip-sampling procedure. Experiments on the ACDC, CHAOS, and MSCMRSeg datasets validate the effectiveness of our framework, outperforming nine state-of-the-art methods. Our code is available at \\href{https://github.com/QLYCode/SparseMamba-PCL}{SparseMamba-PCL.git}.",
        "arxiv_id": "2503.01633",
        "ARXIVID": "2503.01633",
        "COMMENT": "Does not match any specific criteria but is relevant to medical image segmentation using foundation models, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00359": {
        "authors": [
            "Qianqian Shen",
            "Yunhan Zhao",
            "Nahyun Kwon",
            "Jeeeun Kim",
            "Yanan Li",
            "Shu Kong"
        ],
        "title": "Solving Instance Detection from an Open-World Perspective",
        "abstract": "arXiv:2503.00359v1 Announce Type: new  Abstract: Instance detection (InsDet) aims to localize specific object instances within a novel scene imagery based on given visual references. Technically, it requires proposal detection to identify all possible object instances, followed by instance-level matching to pinpoint the ones of interest. Its open-world nature supports its wide-ranging applications from robotics to AR/VR, but also presents significant challenges: methods must generalize to unknown testing data distributions because (1) the testing scene imagery is unseen during training, and (2) there are domain gaps between visual references and detected proposals. Existing methods attempt to tackle these challenges by synthesizing diverse training examples or utilizing off-the-shelf foundation models (FMs). However, they only partially capitalize the available open-world information. In this paper, we approach InsDet from an Open-World perspective, introducing our method IDOW. We find that, while pretrained FMs yield high recall in instance detection, they are not specifically optimized for instance-level feature matching. To address this, we adapt pretrained FMs for improved instance-level matching using open-world data. Our approach incorporates metric learning along with novel data augmentations, which sample distractors as negative examples and synthesize novel-view instances to enrich the visual references. Extensive experiments demonstrate that our method significantly outperforms prior works, achieving >10 AP over previous results on two recently released challenging benchmark datasets in both conventional and novel instance detection settings.",
        "arxiv_id": "2503.00359",
        "ARXIVID": "2503.00359",
        "COMMENT": "Does not match any specific criteria but is relevant to open-world instance detection, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01792": {
        "authors": [
            "Andrei Buliga",
            "Chiara Di Francescomarino",
            "Chiara Ghidini",
            "Marco Montali",
            "Massimiliano Ronzani"
        ],
        "title": "Generating Counterfactual Explanations Under Temporal Constraints",
        "abstract": "arXiv:2503.01792v1 Announce Type: new  Abstract: Counterfactual explanations are one of the prominent eXplainable Artificial Intelligence (XAI) techniques, and suggest changes to input data that could alter predictions, leading to more favourable outcomes. Existing counterfactual methods do not readily apply to temporal domains, such as that of process mining, where data take the form of traces of activities that must obey to temporal background knowledge expressing which dynamics are possible and which not. Specifically, counterfactuals generated off-the-shelf may violate the background knowledge, leading to inconsistent explanations. This work tackles this challenge by introducing a novel approach for generating temporally constrained counterfactuals, guaranteed to comply by design with background knowledge expressed in Linear Temporal Logic on process traces (LTLp). We do so by infusing automata-theoretic techniques for LTLp inside a genetic algorithm for counterfactual generation. The empirical evaluation shows that the generated counterfactuals are temporally meaningful and more interpretable for applications involving temporal dependencies.",
        "arxiv_id": "2503.01792",
        "ARXIVID": "2503.01792",
        "COMMENT": "Does not match any specific criteria but is relevant to explainable AI and temporal reasoning, which are tangentially related to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01122": {
        "authors": [
            "Shizhan Liu",
            "Hao Zheng",
            "Hang Yu",
            "Jianguo Li"
        ],
        "title": "ACCORD: Alleviating Concept Coupling through Dependence Regularization for Text-to-Image Diffusion Personalization",
        "abstract": "arXiv:2503.01122v1 Announce Type: new  Abstract: Image personalization has garnered attention for its ability to customize Text-to-Image generation using only a few reference images. However, a key challenge in image personalization is the issue of conceptual coupling, where the limited number of reference images leads the model to form unwanted associations between the personalization target and other concepts. Current methods attempt to tackle this issue indirectly, leading to a suboptimal balance between text control and personalization fidelity. In this paper, we take a direct approach to the concept coupling problem through statistical analysis, revealing that it stems from two distinct sources of dependence discrepancies. We therefore propose two complementary plug-and-play loss functions: Denoising Decouple Loss and Prior Decouple loss, each designed to minimize one type of dependence discrepancy. Extensive experiments demonstrate that our approach achieves a superior trade-off between text control and personalization fidelity.",
        "arxiv_id": "2503.01122",
        "ARXIVID": "2503.01122",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling and statistical tricks in text-to-image diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01175": {
        "authors": [
            "Hongye Cheng",
            "Tianyu Wang",
            "Guangsi Shi",
            "Zexing Zhao",
            "Yanwei Fu"
        ],
        "title": "HOP: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech Gesture Generation",
        "abstract": "arXiv:2503.01175v1 Announce Type: new  Abstract: Co-speech gestures are crucial non-verbal cues that enhance speech clarity and expressiveness in human communication, which have attracted increasing attention in multimodal research. While the existing methods have made strides in gesture accuracy, challenges remain in generating diverse and coherent gestures, as most approaches assume independence among multimodal inputs and lack explicit modeling of their interactions. In this work, we propose a novel multimodal learning method named HOP for co-speech gesture generation that captures the heterogeneous entanglement between gesture motion, audio rhythm, and text semantics, enabling the generation of coordinated gestures. By leveraging spatiotemporal graph modeling, we achieve the alignment of audio and action. Moreover, to enhance modality coherence, we build the audio-text semantic representation based on a reprogramming module, which is beneficial for cross-modality adaptation. Our approach enables the trimodal system to learn each other's features and represent them in the form of topological entanglement. Extensive experiments demonstrate that HOP achieves state-of-the-art performance, offering more natural and expressive co-speech gesture generation. More information, codes, and demos are available here: https://star-uu-wang.github.io/HOP/",
        "arxiv_id": "2503.01175",
        "ARXIVID": "2503.01175",
        "COMMENT": "Does not match any specific criteria. Focuses on co-speech gesture generation, which is not directly related to spatial understanding, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01642": {
        "authors": [
            "Wenjie Wu",
            "Yongcheng Jing",
            "Yingjie Wang",
            "Wenbin Hu",
            "Dacheng Tao"
        ],
        "title": "Graph-Augmented Reasoning: Evolving Step-by-Step Knowledge Graph Retrieval for LLM Reasoning",
        "abstract": "arXiv:2503.01642v1 Announce Type: new  Abstract: Recent large language model (LLM) reasoning, despite its success, suffers from limited domain knowledge, susceptibility to hallucinations, and constrained reasoning depth, particularly in small-scale models deployed in resource-constrained environments. This paper presents the first investigation into integrating step-wise knowledge graph retrieval with step-wise reasoning to address these challenges, introducing a novel paradigm termed as graph-augmented reasoning. Our goal is to enable frozen, small-scale LLMs to retrieve and process relevant mathematical knowledge in a step-wise manner, enhancing their problem-solving abilities without additional training. To this end, we propose KG-RAR, a framework centered on process-oriented knowledge graph construction, a hierarchical retrieval strategy, and a universal post-retrieval processing and reward model (PRP-RM) that refines retrieved information and evaluates each reasoning step. Experiments on the Math500 and GSM8K benchmarks across six models demonstrate that KG-RAR yields encouraging results, achieving a 20.73\\% relative improvement with Llama-3B on Math500.",
        "arxiv_id": "2503.01642",
        "ARXIVID": "2503.01642",
        "COMMENT": "Does not match any specific criteria. Focuses on integrating knowledge graph retrieval with LLM reasoning, which is not directly related to spatial understanding, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01288": {
        "authors": [
            "Chong Wang",
            "Lanqing Guo",
            "Zixuan Fu",
            "Siyuan Yang",
            "Hao Cheng",
            "Alex C. Kot",
            "Bihan Wen"
        ],
        "title": "Reconciling Stochastic and Deterministic Strategies for Zero-shot Image Restoration using Diffusion Model in Dual",
        "abstract": "arXiv:2503.01288v1 Announce Type: new  Abstract: Plug-and-play (PnP) methods offer an iterative strategy for solving image restoration (IR) problems in a zero-shot manner, using a learned \\textit{discriminative denoiser} as the implicit prior. More recently, a sampling-based variant of this approach, which utilizes a pre-trained \\textit{generative diffusion model}, has gained great popularity for solving IR problems through stochastic sampling. The IR results using PnP with a pre-trained diffusion model demonstrate distinct advantages compared to those using discriminative denoisers, \\ie improved perceptual quality while sacrificing the data fidelity. The unsatisfactory results are due to the lack of integration of these strategies in the IR tasks. In this work, we propose a novel zero-shot IR scheme, dubbed Reconciling Diffusion Model in Dual (RDMD), which leverages only a \\textbf{single} pre-trained diffusion model to construct \\textbf{two} complementary regularizers. Specifically, the diffusion model in RDMD will iteratively perform deterministic denoising and stochastic sampling, aiming to achieve high-fidelity image restoration with appealing perceptual quality. RDMD also allows users to customize the distortion-perception tradeoff with a single hyperparameter, enhancing the adaptability of the restoration process in different practical scenarios. Extensive experiments on several IR tasks demonstrate that our proposed method could achieve superior results compared to existing approaches on both the FFHQ and ImageNet datasets.",
        "arxiv_id": "2503.01288",
        "ARXIVID": "2503.01288",
        "COMMENT": "Does not match any specific criteria. Focuses on image restoration using diffusion models, which is not directly related to spatial understanding, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01751": {
        "authors": [
            "Marco Scialanga",
            "Thibault Laugel",
            "Vincent Grari",
            "Marcin Detyniecki"
        ],
        "title": "SAKE: Steering Activations for Knowledge Editing",
        "abstract": "arXiv:2503.01751v1 Announce Type: new  Abstract: As Large Langue Models have been shown to memorize real-world facts, the need to update this knowledge in a controlled and efficient manner arises. Designed with these constraints in mind, Knowledge Editing (KE) approaches propose to alter specific facts in pretrained models. However, they have been shown to suffer from several limitations, including their lack of contextual robustness and their failure to generalize to logical implications related to the fact. To overcome these issues, we propose SAKE, a steering activation method that models a fact to be edited as a distribution rather than a single prompt. Leveraging Optimal Transport, SAKE alters the LLM behavior over a whole fact-related distribution, defined as paraphrases and logical implications. Several numerical experiments demonstrate the effectiveness of this method: SAKE is thus able to perform more robust edits than its existing counterparts.",
        "arxiv_id": "2503.01751",
        "ARXIVID": "2503.01751",
        "COMMENT": "Does not match any specific criteria. Focuses on knowledge editing in LLMs, which is not directly related to spatial understanding, VLLMs/MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00823": {
        "authors": [
            "Bowen Zheng",
            "Da-Wei Zhou",
            "Han-Jia Ye",
            "De-Chuan Zhan"
        ],
        "title": "Task-Agnostic Guided Feature Expansion for Class-Incremental Learning",
        "abstract": "arXiv:2503.00823v1 Announce Type: new  Abstract: The ability to learn new concepts while preserve the learned knowledge is desirable for learning systems in Class-Incremental Learning (CIL). Recently, feature expansion of the model become a prevalent solution for CIL, where the old features are fixed during the training of the new task while new features are expanded for the new tasks. However, such task-specific features learned from the new task may collide with the old features, leading to misclassification between tasks. Therefore, the expanded model is often encouraged to capture diverse features from the new task, aiming to avoid such collision. However, the existing solution is largely restricted to the samples from the current task, because of the poor accessibility to previous samples. To promote the learning and transferring of diverse features across tasks, we propose a framework called Task-Agnostic Guided Feature Expansion (TagFex). Firstly, it captures task-agnostic features continually with a separate model, providing extra task-agnostic features for subsequent tasks. Secondly, to obtain useful features from the task-agnostic model for the current task, it aggregates the task-agnostic features with the task-specific feature using a merge attention. Then the aggregated feature is transferred back into the task-specific feature for inference, helping the task-specific model capture diverse features. Extensive experiments show the effectiveness and superiority of TagFex on various CIL settings. Code is available at https://github.com/bwnzheng/TagFex_CVPR2025.",
        "arxiv_id": "2503.00823",
        "ARXIVID": "2503.00823",
        "COMMENT": "Does not match any specific criteria but is related to class-incremental learning, which is tangentially relevant to machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00905": {
        "authors": [
            "Zhu Liu",
            "Zijun Wang",
            "Jinyuan Liu",
            "Fanqi Meng",
            "Long Ma",
            "Risheng Liu"
        ],
        "title": "DEAL: Data-Efficient Adversarial Learning for High-Quality Infrared Imaging",
        "abstract": "arXiv:2503.00905v1 Announce Type: new  Abstract: Thermal imaging is often compromised by dynamic, complex degradations caused by hardware limitations and unpredictable environmental factors. The scarcity of high-quality infrared data, coupled with the challenges of dynamic, intricate degradations, makes it difficult to recover details using existing methods. In this paper, we introduce thermal degradation simulation integrated into the training process via a mini-max optimization, by modeling these degraded factors as adversarial attacks on thermal images. The simulation is dynamic to maximize objective functions, thus capturing a broad spectrum of degraded data distributions. This approach enables training with limited data, thereby improving model performance.Additionally, we introduce a dual-interaction network that combines the benefits of spiking neural networks with scale transformation to capture degraded features with sharp spike signal intensities. This architecture ensures compact model parameters while preserving efficient feature representation. Extensive experiments demonstrate that our method not only achieves superior visual quality under diverse single and composited degradation, but also delivers a significant reduction in processing when trained on only fifty clear images, outperforming existing techniques in efficiency and accuracy. The source code will be available at https://github.com/LiuZhu-CV/DEAL.",
        "arxiv_id": "2503.00905",
        "ARXIVID": "2503.00905",
        "COMMENT": "Does not match any specific criteria but is related to adversarial learning for infrared imaging, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00848": {
        "authors": [
            "BoCheng Li",
            "WenJuan Zhang",
            "Bing Zhang",
            "YiLing Yao",
            "YaNing Wang"
        ],
        "title": "PSRGS:Progressive Spectral Residual of 3D Gaussian for High-Frequency Recovery",
        "abstract": "arXiv:2503.00848v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3D GS) achieves impressive results in novel view synthesis for small, single-object scenes through Gaussian ellipsoid initialization and adaptive density control. However, when applied to large-scale remote sensing scenes, 3D GS faces challenges: the point clouds generated by Structure-from-Motion (SfM) are often sparse, and the inherent smoothing behavior of 3D GS leads to over-reconstruction in high-frequency regions, where have detailed textures and color variations. This results in the generation of large, opaque Gaussian ellipsoids that cause gradient artifacts. Moreover, the simultaneous optimization of both geometry and texture may lead to densification of Gaussian ellipsoids at incorrect geometric locations, resulting in artifacts in other views. To address these issues, we propose PSRGS, a progressive optimization scheme based on spectral residual maps. Specifically, we create a spectral residual significance map to separate low-frequency and high-frequency regions. In the low-frequency region, we apply depth-aware and depth-smooth losses to initialize the scene geometry with low threshold. For the high-frequency region, we use gradient features with higher threshold to split and clone ellipsoids, refining the scene. The sampling rate is determined by feature responses and gradient loss. Finally, we introduce a pre-trained network that jointly computes perceptual loss from multiple views, ensuring accurate restoration of high-frequency details in both Gaussian ellipsoids geometry and color. We conduct experiments on multiple datasets to assess the effectiveness of our method, which demonstrates competitive rendering quality, especially in recovering texture details in high-frequency regions.",
        "arxiv_id": "2503.00848",
        "ARXIVID": "2503.00848",
        "COMMENT": "Does not match any specific criteria but is related to high-frequency recovery in 3D Gaussian splatting, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00677": {
        "authors": [
            "Zhiqi Kang",
            "Liyuan Wang",
            "Xingxing Zhang",
            "Karteek Alahari"
        ],
        "title": "Advancing Prompt-Based Methods for Replay-Independent General Continual Learning",
        "abstract": "arXiv:2503.00677v1 Announce Type: new  Abstract: General continual learning (GCL) is a broad concept to describe real-world continual learning (CL) problems, which are often characterized by online data streams without distinct transitions between tasks, i.e., blurry task boundaries. Such requirements result in poor initial performance, limited generalizability, and severe catastrophic forgetting, heavily impacting the effectiveness of mainstream GCL models trained from scratch. While the use of a frozen pretrained backbone with appropriate prompt tuning can partially address these challenges, such prompt-based methods remain suboptimal for CL of remaining tunable parameters on the fly. In this regard, we propose an innovative approach named MISA (Mask and Initial Session Adaption) to advance prompt-based methods in GCL. It includes a forgetting-aware initial session adaption that employs pretraining data to initialize prompt parameters and improve generalizability, as well as a non-parametric logit mask of the output layers to mitigate catastrophic forgetting. Empirical results demonstrate substantial performance gains of our approach compared to recent competitors, especially without a replay buffer (e.g., up to 18.39%, 22.06%, and 11.96% performance lead on CIFAR-100, Tiny-ImageNet, and ImageNet-R, respectively). Moreover, our approach features the plug-in nature for prompt-based methods, independence of replay, ease of implementation, and avoidance of CL-relevant hyperparameters, serving as a strong baseline for GCL research. Our source code is publicly available at https://github.com/kangzhiq/MISA",
        "arxiv_id": "2503.00677",
        "ARXIVID": "2503.00677",
        "COMMENT": "Does not match any specific criteria but is related to continual learning, which is tangentially relevant to your friend's interest in machine learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01139": {
        "authors": [
            "Junyi Li",
            "Yongqiang Chen",
            "Chenxi Liu",
            "Qianyi Cai",
            "Tongliang Liu",
            "Bo Han",
            "Kun Zhang",
            "Hui Xiong"
        ],
        "title": "Can Large Language Models Help Experimental Design for Causal Discovery?",
        "abstract": "arXiv:2503.01139v1 Announce Type: new  Abstract: Designing proper experiments and selecting optimal intervention targets is a longstanding problem in scientific or causal discovery. Identifying the underlying causal structure from observational data alone is inherently difficult.Obtaining interventional data, on the other hand, is crucial to causal discovery, yet it is usually expensive and time-consuming to gather sufficient interventional data to facilitate causal discovery.Previous approaches commonly utilize uncertainty or gradient signals to determine the intervention targets. However, numerical-based approaches may yield suboptimal results due to the inaccurate estimation of the guiding signals at the beginning when with limited interventional data. In this work, we investigate a different approach, whether we can leverage Large Language Models (LLMs) to assist with the intervention targeting in causal discovery by making use of the rich world knowledge about the experimental design in LLMs.Specifically, we present \\oursfull (\\ours) -- a robust framework that effectively incorporates LLMs to augment existing numerical approaches for the intervention targeting in causal discovery. Across $4$ realistic benchmark scales, \\ours demonstrates significant improvements and robustness over existing methods and even surpasses humans, which demonstrates the usefulness of LLMs in assisting with experimental design for scientific discovery.",
        "arxiv_id": "2503.01139",
        "ARXIVID": "2503.01139",
        "COMMENT": "Does not match any specific criteria but explores the use of LLMs in experimental design, which is tangentially related to your friend's interest in clever statistical tricks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01715": {
        "authors": [
            "Antoni Bigata",
            "Micha{\\l} Stypu{\\l}kowski",
            "Rodrigo Mira",
            "Stella Bounareli",
            "Konstantinos Vougioukas",
            "Zoe Landgraf",
            "Nikita Drobyshev",
            "Maciej Zieba",
            "Stavros Petridis",
            "Maja Pantic"
        ],
        "title": "KeyFace: Expressive Audio-Driven Facial Animation for Long Sequences via KeyFrame Interpolation",
        "abstract": "arXiv:2503.01715v1 Announce Type: new  Abstract: Current audio-driven facial animation methods achieve impressive results for short videos but suffer from error accumulation and identity drift when extended to longer durations. Existing methods attempt to mitigate this through external spatial control, increasing long-term consistency but compromising the naturalness of motion. We propose KeyFace, a novel two-stage diffusion-based framework, to address these issues. In the first stage, keyframes are generated at a low frame rate, conditioned on audio input and an identity frame, to capture essential facial expressions and movements over extended periods of time. In the second stage, an interpolation model fills in the gaps between keyframes, ensuring smooth transitions and temporal coherence. To further enhance realism, we incorporate continuous emotion representations and handle a wide range of non-speech vocalizations (NSVs), such as laughter and sighs. We also introduce two new evaluation metrics for assessing lip synchronization and NSV generation. Experimental results show that KeyFace outperforms state-of-the-art methods in generating natural, coherent facial animations over extended durations, successfully encompassing NSVs and continuous emotions.",
        "arxiv_id": "2503.01715",
        "ARXIVID": "2503.01715",
        "COMMENT": "Does not match any specific criteria but is related to audio-driven facial animation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00250": {
        "authors": [
            "Yanan Niu",
            "Roy Sarkis",
            "Demetri Psaltis",
            "Mario Paolone",
            "Christophe Moser",
            "Luisa Lambertini"
        ],
        "title": "Solar Multimodal Transformer: Intraday Solar Irradiance Predictor using Public Cameras and Time Series",
        "abstract": "arXiv:2503.00250v1 Announce Type: new  Abstract: Accurate intraday solar irradiance forecasting is crucial for optimizing dispatch planning and electricity trading. For this purpose, we introduce a novel and effective approach that includes three distinguishing components from the literature: 1) the uncommon use of single-frame public camera imagery; 2) solar irradiance time series scaled with a proposed normalization step, which boosts performance; and 3) a lightweight multimodal model, called Solar Multimodal Transformer (SMT), that delivers accurate short-term solar irradiance forecasting by combining images and scaled time series. Benchmarking against Solcast, a leading solar forecasting service provider, our model improved prediction accuracy by 25.95%. Our approach allows for easy adaptation to various camera specifications, offering broad applicability for real-world solar forecasting challenges.",
        "arxiv_id": "2503.00250",
        "ARXIVID": "2503.00250",
        "COMMENT": "Does not match any specific criteria but is related to multi-modal solar irradiance prediction.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00429": {
        "authors": [
            "Jingyi Yang",
            "Xun Lin",
            "Zitong Yu",
            "Liepiao Zhang",
            "Xin Liu",
            "Hui Li",
            "Xiaochen Yuan",
            "Xiaochun Cao"
        ],
        "title": "DADM: Dual Alignment of Domain and Modality for Face Anti-spoofing",
        "abstract": "arXiv:2503.00429v1 Announce Type: new  Abstract: With the availability of diverse sensor modalities (i.e., RGB, Depth, Infrared) and the success of multi-modal learning, multi-modal face anti-spoofing (FAS) has emerged as a prominent research focus. The intuition behind it is that leveraging multiple modalities can uncover more intrinsic spoofing traces. However, this approach presents more risk of misalignment. We identify two main types of misalignment: (1) \\textbf{Intra-domain modality misalignment}, where the importance of each modality varies across different attacks. For instance, certain modalities (e.g., Depth) may be non-defensive against specific attacks (e.g., 3D mask), indicating that each modality has unique strengths and weaknesses in countering particular attacks. Consequently, simple fusion strategies may fall short. (2) \\textbf{Inter-domain modality misalignment}, where the introduction of additional modalities exacerbates domain shifts, potentially overshadowing the benefits of complementary fusion. To tackle (1), we propose a alignment module between modalities based on mutual information, which adaptively enhances favorable modalities while suppressing unfavorable ones. To address (2), we employ a dual alignment optimization method that aligns both sub-domain hyperplanes and modality angle margins, thereby mitigating domain gaps. Our method, dubbed \\textbf{D}ual \\textbf{A}lignment of \\textbf{D}omain and \\textbf{M}odality (DADM), achieves state-of-the-art performance in extensive experiments across four challenging protocols demonstrating its robustness in multi-modal domain generalization scenarios. The codes will be released soon.",
        "arxiv_id": "2503.00429",
        "ARXIVID": "2503.00429",
        "COMMENT": "Does not match any specific criteria but is related to multi-modal face anti-spoofing.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00364": {
        "authors": [
            "Yaowei Guo",
            "Jiazheng Xing",
            "Xiaojun Hou",
            "Shuo Xin",
            "Juntao Jiang",
            "Demetri Terzopoulos",
            "Chenfanfu Jiang",
            "Yong Liu"
        ],
        "title": "CFSum: A Transformer-Based Multi-Modal Video Summarization Framework With Coarse-Fine Fusion",
        "abstract": "arXiv:2503.00364v1 Announce Type: new  Abstract: Video summarization, by selecting the most informative and/or user-relevant parts of original videos to create concise summary videos, has high research value and consumer demand in today's video proliferation era. Multi-modal video summarization that accomodates user input has become a research hotspot. However, current multi-modal video summarization methods suffer from two limitations. First, existing methods inadequately fuse information from different modalities and cannot effectively utilize modality-unique features. Second, most multi-modal methods focus on video and text modalities, neglecting the audio modality, despite the fact that audio information can be very useful in certain types of videos. In this paper we propose CFSum, a transformer-based multi-modal video summarization framework with coarse-fine fusion. CFSum exploits video, text, and audio modal features as input, and incorporates a two-stage transformer-based feature fusion framework to fully utilize modality-unique information. In the first stage, multi-modal features are fused simultaneously to perform initial coarse-grained feature fusion, then, in the second stage, video and audio features are explicitly attended with the text representation yielding more fine-grained information interaction. The CFSum architecture gives equal importance to each modality, ensuring that each modal feature interacts deeply with the other modalities. Our extensive comparative experiments against prior methods and ablation studies on various datasets confirm the effectiveness and superiority of CFSum.",
        "arxiv_id": "2503.00364",
        "ARXIVID": "2503.00364",
        "COMMENT": "Does not match any specific criteria but is related to multi-modal video summarization.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00828": {
        "authors": [
            "Yalun Dai",
            "Lingao Xiao",
            "Ivor W. Tsang",
            "Yang He"
        ],
        "title": "Training-Free Dataset Pruning for Instance Segmentation",
        "abstract": "arXiv:2503.00828v1 Announce Type: new  Abstract: Existing dataset pruning techniques primarily focus on classification tasks, limiting their applicability to more complex and practical tasks like instance segmentation. Instance segmentation presents three key challenges: pixel-level annotations, instance area variations, and class imbalances, which significantly complicate dataset pruning efforts. Directly adapting existing classification-based pruning methods proves ineffective due to their reliance on time-consuming model training process. To address this, we propose a novel Training-Free Dataset Pruning (TFDP) method for instance segmentation. Specifically, we leverage shape and class information from image annotations to design a Shape Complexity Score (SCS), refining it into a Scale-Invariant (SI-SCS) and Class-Balanced (CB-SCS) versions to address instance area variations and class imbalances, all without requiring model training. We achieve state-of-the-art results on VOC 2012, Cityscapes, and COCO datasets, generalizing well across CNN and Transformer architectures. Remarkably, our approach accelerates the pruning process by an average of 1349$\\times$ on COCO compared to the adapted baselines. Source code is available at: https://github.com/he-y/dataset-pruning-for-instance-segmentation",
        "arxiv_id": "2503.00828",
        "ARXIVID": "2503.00828",
        "COMMENT": "Does not match any specific criteria but is related to dataset pruning for instance segmentation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.00952": {
        "authors": [
            "Jinhong Wang",
            "Jintai Chen",
            "Jian Liu",
            "Dongqi Tang",
            "Danny Z. Chen",
            "Jian Wu"
        ],
        "title": "A Survey on Ordinal Regression: Applications, Advances and Prospects",
        "abstract": "arXiv:2503.00952v1 Announce Type: new  Abstract: Ordinal regression refers to classifying object instances into ordinal categories. Ordinal regression is crucial for applications in various areas like facial age estimation, image aesthetics assessment, and even cancer staging, due to its capability to utilize ordered information effectively. More importantly, it also enhances model interpretation by considering category order, aiding the understanding of data trends and causal relationships. Despite significant recent progress, challenges remain, and further investigation of ordinal regression techniques and applications is essential to guide future research. In this survey, we present a comprehensive examination of advances and applications of ordinal regression. By introducing a systematic taxonomy, we meticulously classify the pertinent techniques and applications into three well-defined categories based on different strategies and objectives: Continuous Space Discretization, Distribution Ordering Learning, and Ambiguous Instance Delving. This categorization enables a structured exploration of diverse insights in ordinal regression problems, providing a framework for a more comprehensive understanding and evaluation of this field and its related applications. To our best knowledge, this is the first systematic survey of ordinal regression, which lays a foundation for future research in this fundamental and generic domain.",
        "arxiv_id": "2503.00952",
        "ARXIVID": "2503.00952",
        "COMMENT": "Does not match any specific criteria. Focuses on ordinal regression, which is not directly related to spatial understanding, VLLMs, MLLMs, embodied AI, or vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.01136": {
        "authors": [
            "Xiongfei Su",
            "Siyuan Li",
            "Yuning Cui",
            "Miao Cao",
            "Yulun Zhang",
            "Zheng Chen",
            "Zongliang Wu",
            "Zedong Wang",
            "Yuanlong Zhang",
            "Xin Yuan"
        ],
        "title": "Prior-guided Hierarchical Harmonization Network for Efficient Image Dehazing",
        "abstract": "arXiv:2503.01136v1 Announce Type: new  Abstract: Image dehazing is a crucial task that involves the enhancement of degraded images to recover their sharpness and textures. While vision Transformers have exhibited impressive results in diverse dehazing tasks, their quadratic complexity and lack of dehazing priors pose significant drawbacks for real-world applications.   In this paper, guided by triple priors, Bright Channel Prior (BCP), Dark Channel Prior (DCP), and Histogram Equalization (HE), we propose a \\textit{P}rior-\\textit{g}uided Hierarchical \\textit{H}armonization Network (PGH$^2$Net) for image dehazing. PGH$^2$Net is built upon the UNet-like architecture with an efficient encoder and decoder, consisting of two module types: (1) Prior aggregation module that injects B/DCP and selects diverse contexts with gating attention. (2) Feature harmonization modules that subtract low-frequency components from spatial and channel aspects and learn more informative feature distributions to equalize the feature maps.",
        "arxiv_id": "2503.01136",
        "ARXIVID": "2503.01136",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and image enhancement.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.00052": {
        "authors": [
            "Yan Su",
            "Qiulin Wu",
            "Weizhen Li",
            "Chengchang Pan",
            "Honggang Qi"
        ],
        "title": "RURA-Net: A general disease diagnosis method based on Zero-Shot Learning",
        "abstract": "arXiv:2503.00052v1 Announce Type: new  Abstract: The training of deep learning models relies on a large amount of labeled data. However, the high cost of medical labeling seriously hinders the development of deep learning in the medical field. Our study proposes a general disease diagnosis approach based on Zero-Shot Learning. The Siamese neural network is used to find similar diseases for the target diseases, and the U-Net segmentation model is used to accurately segment the key lesions of the disease. Finally, based on the ResNet-Agglomerative clustering algorithm, a clustering model is trained on a large number of sample data of similar diseases to obtain a approximate diagnosis of the target disease. Zero-Shot Learning of the target disease is then successfully achieved. To evaluate the validity of the model, we validated our method on a dataset of ophthalmic diseases in CFP modality. The external dataset was used to test its performance, and the accuracy=0.8395, precision=0.8094, recall=0.8463, F1 Score=0.8274, AUC=0.9226, which exceeded the indexes of most Few-Shot Learning and One-Shot Learning models. It proves that our method has great potential and reference value in the medical field, where annotation data is usually scarce and expensive to obtain.",
        "arxiv_id": "2503.00052",
        "ARXIVID": "2503.00052",
        "COMMENT": "Does not match any specific criterion but is relevant to zero-shot learning and medical applications, which aligns with your friend's general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.01292": {
        "authors": [
            "Yurui Pan",
            "Lidong Wang",
            "Yuchao Chen",
            "Wenbing Zhu",
            "Bo Peng",
            "Mingmin Chi"
        ],
        "title": "PA-CLIP: Enhancing Zero-Shot Anomaly Detection through Pseudo-Anomaly Awareness",
        "abstract": "arXiv:2503.01292v1 Announce Type: new  Abstract: In industrial anomaly detection (IAD), accurately identifying defects amidst diverse anomalies and under varying imaging conditions remains a significant challenge. Traditional approaches often struggle with high false-positive rates, frequently misclassifying normal shadows and surface deformations as defects, an issue that becomes particularly pronounced in products with complex and intricate surface features. To address these challenges, we introduce PA-CLIP, a zero-shot anomaly detection method that reduces background noise and enhances defect detection through a pseudo-anomaly-based framework. The proposed method integrates a multiscale feature aggregation strategy for capturing detailed global and local information, two memory banks for distinguishing background information, including normal patterns and pseudo-anomalies, from true anomaly features, and a decision-making module designed to minimize false positives caused by environmental variations while maintaining high defect sensitivity. Demonstrated on the MVTec AD and VisA datasets, PA-CLIP outperforms existing zero-shot methods, providing a robust solution for industrial defect detection.",
        "arxiv_id": "2503.01292",
        "ARXIVID": "2503.01292",
        "COMMENT": "Does not match any specific criterion but is relevant to zero-shot learning and anomaly detection, which aligns with your friend's general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.00515": {
        "authors": [
            "Songlin Dong",
            "Yuhang He",
            "Zhengdong Zhou",
            "Haoyu Luo",
            "Xing Wei",
            "Alex C. Kot",
            "Yihong Gong"
        ],
        "title": "Class-Independent Increment: An Efficient Approach for Multi-label Class-Incremental Learning",
        "abstract": "arXiv:2503.00515v1 Announce Type: new  Abstract: Current research on class-incremental learning primarily focuses on single-label classification tasks. However, real-world applications often involve multi-label scenarios, such as image retrieval and medical imaging. Therefore, this paper focuses on the challenging yet practical multi-label class-incremental learning (MLCIL) problem. In addition to the challenge of catastrophic forgetting, MLCIL encounters issues related to feature confusion, encompassing inter-session and intra-feature confusion. To address these problems, we propose a novel MLCIL approach called class-independent increment (CLIN). Specifically, in contrast to existing methods that extract image-level features, we propose a class-independent incremental network (CINet) to extract multiple class-level embeddings for multi-label samples. It learns and preserves the knowledge of different classes by constructing class-specific tokens. On this basis, we develop two novel loss functions, optimizing the learning of class-specific tokens and class-level embeddings, respectively. These losses aim to distinguish between new and old classes, further alleviating the problem of feature confusion. Extensive experiments on MS-COCO and PASCAL VOC datasets demonstrate the effectiveness of our method for improving recognition performance and mitigating forgetting on various MLCIL tasks.",
        "arxiv_id": "2503.00515",
        "ARXIVID": "2503.00515",
        "COMMENT": "Does not match any specific criterion but is relevant to machine learning and incremental learning, which aligns with your friend's general interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.00697": {
        "authors": [
            "Yiyang Lin",
            "Danling Jiang",
            "Xinyu Liu",
            "Yun Miao",
            "Yixuan Yuan"
        ],
        "title": "CREATE-FFPE: Cross-Resolution Compensated and Multi-Frequency Enhanced FS-to-FFPE Stain Transfer for Intraoperative IHC Images",
        "abstract": "arXiv:2503.00697v1 Announce Type: new  Abstract: In the immunohistochemical (IHC) analysis during surgery, frozen-section (FS) images are used to determine the benignity or malignancy of the tumor. However, FS image faces problems such as image contamination and poor nuclear detail, which may disturb the pathologist's diagnosis. In contrast, formalin-fixed and paraffin-embedded (FFPE) image has a higher staining quality, but it requires quite a long time to prepare and thus is not feasible during surgery. To help pathologists observe IHC images with high quality in surgery, this paper proposes a Cross-REsolution compensATed and multi-frequency Enhanced FS-to-FFPE (CREATE-FFPE) stain transfer framework, which is the first FS-to-FFPE method for the intraoperative IHC images. To solve the slide contamination and poor nuclear detail mentioned above, we propose the cross-resolution compensation module (CRCM) and the wavelet detail guidance module (WDGM). Specifically, CRCM compensates for information loss due to contamination by providing more tissue information across multiple resolutions, while WDGM produces the desirable details in a wavelet way, and the details can be used to guide the stain transfer to be more precise. Experiments show our method can beat all the competing methods on our dataset. In addition, the FID has decreased by 44.4%, and KID*100 has decreased by 71.2% by adding the proposed CRCM and WDGM in ablation studies, and the performance of a downstream microsatellite instability prediction task with public dataset can be greatly improved by performing our FS-to-FFPE stain transfer.",
        "arxiv_id": "2503.00697",
        "ARXIVID": "2503.00697",
        "COMMENT": "Does not match any specific criteria but is relevant to generative modeling in medical imaging, which is tangentially related to your friend's interests.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}