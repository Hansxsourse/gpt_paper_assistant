{
    "2508.18265": {
        "authors": [
            "Weiyun Wang",
            "Zhangwei Gao",
            "Lixin Gu",
            "Hengjun Pu",
            "Long Cui",
            "Xingguang Wei",
            "Zhaoyang Liu",
            "Linglin Jing",
            "Shenglong Ye",
            "Jie Shao",
            "Zhaokai Wang",
            "Zhe Chen",
            "Hongjie Zhang",
            "Ganlin Yang",
            "Haomin Wang",
            "Qi Wei",
            "Jinhui Yin",
            "Wenhao Li",
            "Erfei Cui",
            "Guanzhou Chen",
            "Zichen Ding",
            "Changyao Tian",
            "Zhenyu Wu",
            "Jingjing Xie",
            "Zehao Li",
            "Bowen Yang",
            "Yuchen Duan",
            "Xuehui Wang",
            "Songze Li",
            "Xiangyu Zhao",
            "Haodong Duan",
            "Nianchen Deng",
            "Bin Fu",
            "Yinan He",
            "Yi Wang",
            "Conghui He",
            "Botian Shi",
            "Junjun He",
            "Yingtong Xiong",
            "Han Lv",
            "Lijun Wu",
            "Wenqi Shao",
            "Kaipeng Zhang",
            "Huipeng Deng",
            "Biqing Qi",
            "Jiaye Ge",
            "Qipeng Guo",
            "Wenwei Zhang",
            "Wanli Ouyang",
            "Limin Wang",
            "Min Dou",
            "Xizhou Zhu",
            "Tong Lu",
            "Dahua Lin",
            "Jifeng Dai",
            "Bowen Zhou",
            "Weijie Su",
            "Kai Chen",
            "Yu Qiao",
            "Wenhai Wang",
            "Gen Luo"
        ],
        "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency",
        "abstract": "arXiv:2508.18265v1 Announce Type: new  Abstract: We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning performance and a 4.05$\\times$ inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.",
        "arxiv_id": "2508.18265",
        "ARXIVID": "2508.18265",
        "COMMENT": "No criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17436": {
        "authors": [
            "Qitong Zhang",
            "Jieqing Feng"
        ],
        "title": "Disentangled Geometry and Appearance for Efficient Multi-View Surface Reconstruction and Rendering",
        "abstract": "arXiv:2508.17436v1 Announce Type: new  Abstract: This paper addresses the limitations of neural rendering-based multi-view surface reconstruction methods, which require an additional mesh extraction step that is inconvenient and would produce poor-quality surfaces with mesh aliasing, restricting downstream applications. Building on the explicit mesh representation and differentiable rasterization framework, this work proposes an efficient solution that preserves the high efficiency of this framework while significantly improving reconstruction quality and versatility. Specifically, we introduce a disentangled geometry and appearance model that does not rely on deep networks, enhancing learning and broadening applicability. A neural deformation field is constructed to incorporate global geometric context, enhancing geometry learning, while a novel regularization constrains geometric features passed to a neural shader to ensure its accuracy and boost shading. For appearance, a view-invariant diffuse term is separated and baked into mesh vertices, further improving rendering efficiency. Experimental results demonstrate that the proposed method achieves state-of-the-art training (4.84 minutes) and rendering (0.023 seconds) speeds, with reconstruction quality that is competitive with top-performing methods. Moreover, the method enables practical applications such as mesh and texture editing, showcasing its versatility and application potential. This combination of efficiency, competitive quality, and broad applicability makes our approach a valuable contribution to multi-view surface reconstruction and rendering.",
        "arxiv_id": "2508.17436",
        "ARXIVID": "2508.17436",
        "COMMENT": "No criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17885": {
        "authors": [
            "Raul Balmez",
            "Alexandru Brateanu",
            "Ciprian Orhei",
            "Codruta Ancuti",
            "Cosmin Ancuti"
        ],
        "title": "ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement",
        "abstract": "arXiv:2508.17885v1 Announce Type: new  Abstract: We introduce ISALux, a novel transformer-based approach for Low-Light Image Enhancement (LLIE) that seamlessly integrates illumination and semantic priors. Our architecture includes an original self-attention block, Hybrid Illumination and Semantics-Aware Multi-Headed Self- Attention (HISA-MSA), which integrates illumination and semantic segmentation maps for en- hanced feature extraction. ISALux employs two self-attention modules to independently process illumination and semantic features, selectively enriching each other to regulate luminance and high- light structural variations in real-world scenarios. A Mixture of Experts (MoE)-based Feed-Forward Network (FFN) enhances contextual learning, with a gating mechanism conditionally activating the top K experts for specialized processing. To address overfitting in LLIE methods caused by distinct light patterns in benchmarking datasets, we enhance the HISA-MSA module with low-rank matrix adaptations (LoRA). Extensive qualitative and quantitative evaluations across multiple specialized datasets demonstrate that ISALux is competitive with state-of-the-art (SOTA) methods. Addition- ally, an ablation study highlights the contribution of each component in the proposed model. Code will be released upon publication.",
        "arxiv_id": "2508.17885",
        "ARXIVID": "2508.17885",
        "COMMENT": "No criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17364": {
        "authors": [
            "Guoqing Zhang",
            "Xingtong Ge",
            "Lu Shi",
            "Xin Zhang",
            "Muqing Xue",
            "Wanru Xu",
            "Yigang Cen"
        ],
        "title": "Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation",
        "abstract": "arXiv:2508.17364v1 Announce Type: new  Abstract: The image-to-image generation task aims to produce controllable images by leveraging conditional inputs and prompt instructions. However, existing methods often train separate control branches for each type of condition, leading to redundant model structures and inefficient use of computational resources. To address this, we propose a Unified image-to-image Generation (UniGen) framework that supports diverse conditional inputs while enhancing generation efficiency and expressiveness. Specifically, to tackle the widely existing parameter redundancy and computational inefficiency in controllable conditional generation architectures, we propose the Condition Modulated Expert (CoMoE) module. This module aggregates semantically similar patch features and assigns them to dedicated expert modules for visual representation and conditional modeling. By enabling independent modeling of foreground features under different conditions, CoMoE effectively mitigates feature entanglement and redundant computation in multi-condition scenarios. Furthermore, to bridge the information gap between the backbone and control branches, we propose WeaveNet, a dynamic, snake-like connection mechanism that enables effective interaction between global text-level control from the backbone and fine-grained control from conditional branches. Extensive experiments on the Subjects-200K and MultiGen-20M datasets across various conditional image generation tasks demonstrate that our method consistently achieves state-of-the-art performance, validating its advantages in both versatility and effectiveness. The code has been uploaded to https://github.com/gavin-gqzhang/UniGen.",
        "arxiv_id": "2508.17364",
        "ARXIVID": "2508.17364",
        "COMMENT": "No criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17283": {
        "authors": [
            "Breenda Das",
            "Lennart Purucker",
            "Timur Carstensen",
            "Frank Hutter"
        ],
        "title": "Quickly Tuning Foundation Models for Image Segmentation",
        "abstract": "arXiv:2508.17283v1 Announce Type: new  Abstract: Foundation models like SAM (Segment Anything Model) exhibit strong zero-shot image segmentation performance, but often fall short on domain-specific tasks. Fine-tuning these models typically requires significant manual effort and domain expertise. In this work, we introduce QTT-SEG, a meta-learning-driven approach for automating and accelerating the fine-tuning of SAM for image segmentation. Built on the Quick-Tune hyperparameter optimization framework, QTT-SEG predicts high-performing configurations using meta-learned cost and performance models, efficiently navigating a search space of over 200 million possibilities. We evaluate QTT-SEG on eight binary and five multiclass segmentation datasets under tight time constraints. Our results show that QTT-SEG consistently improves upon SAM's zero-shot performance and surpasses AutoGluon Multimodal, a strong AutoML baseline, on most binary tasks within three minutes. On multiclass datasets, QTT-SEG delivers consistent gains as well. These findings highlight the promise of meta-learning in automating model adaptation for specialized segmentation tasks. Code available at: https://github.com/ds-brx/QTT-SEG/",
        "arxiv_id": "2508.17283",
        "ARXIVID": "2508.17283",
        "COMMENT": "No criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17299": {
        "authors": [
            "Zhihao Chen",
            "Qi Gao",
            "Zilong Li",
            "Junping Zhang",
            "Yi Zhang",
            "Jun Zhao",
            "Hongming Shan"
        ],
        "title": "FoundDiff: Foundational Diffusion Model for Generalizable Low-Dose CT Denoising",
        "abstract": "arXiv:2508.17299v1 Announce Type: new  Abstract: Low-dose computed tomography (CT) denoising is crucial for reduced radiation exposure while ensuring diagnostically acceptable image quality. Despite significant advancements driven by deep learning (DL) in recent years, existing DL-based methods, typically trained on a specific dose level and anatomical region, struggle to handle diverse noise characteristics and anatomical heterogeneity during varied scanning conditions, limiting their generalizability and robustness in clinical scenarios. In this paper, we propose FoundDiff, a foundational diffusion model for unified and generalizable LDCT denoising across various dose levels and anatomical regions. FoundDiff employs a two-stage strategy: (i) dose-anatomy perception and (ii) adaptive denoising. First, we develop a dose- and anatomy-aware contrastive language image pre-training model (DA-CLIP) to achieve robust dose and anatomy perception by leveraging specialized contrastive learning strategies to learn continuous representations that quantify ordinal dose variations and identify salient anatomical regions. Second, we design a dose- and anatomy-aware diffusion model (DA-Diff) to perform adaptive and generalizable denoising by synergistically integrating the learned dose and anatomy embeddings from DACLIP into diffusion process via a novel dose and anatomy conditional block (DACB) based on Mamba. Extensive experiments on two public LDCT datasets encompassing eight dose levels and three anatomical regions demonstrate superior denoising performance of FoundDiff over existing state-of-the-art methods and the remarkable generalization to unseen dose levels. The codes and models are available at https://github.com/hao1635/FoundDiff.",
        "arxiv_id": "2508.17299",
        "ARXIVID": "2508.17299",
        "COMMENT": "No criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17614": {
        "authors": [
            "Aowen Wang",
            "Wei Li",
            "Hao Luo",
            "Mengxing Ao",
            "Chenyu Zhu",
            "Xinyang Li",
            "Fan Wang"
        ],
        "title": "JCo-MVTON: Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-on",
        "abstract": "arXiv:2508.17614v1 Announce Type: new  Abstract: Virtual try-on systems have long been hindered by heavy reliance on human body masks, limited fine-grained control over garment attributes, and poor generalization to real-world, in-the-wild scenarios. In this paper, we propose JCo-MVTON (Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-On), a novel framework that overcomes these limitations by integrating diffusion-based image generation with multi-modal conditional fusion. Built upon a Multi-Modal Diffusion Transformer (MM-DiT) backbone, our approach directly incorporates diverse control signals -- such as the reference person image and the target garment image -- into the denoising process through dedicated conditional pathways that fuse features within the self-attention layers. This fusion is further enhanced with refined positional encodings and attention masks, enabling precise spatial alignment and improved garment-person integration. To address data scarcity and quality, we introduce a bidirectional generation strategy for dataset construction: one pipeline uses a mask-based model to generate realistic reference images, while a symmetric ``Try-Off'' model, trained in a self-supervised manner, recovers the corresponding garment images. The synthesized dataset undergoes rigorous manual curation, allowing iterative improvement in visual fidelity and diversity. Experiments demonstrate that JCo-MVTON achieves state-of-the-art performance on public benchmarks including DressCode, significantly outperforming existing methods in both quantitative metrics and human evaluations. Moreover, it shows strong generalization in real-world applications, surpassing commercial systems.",
        "arxiv_id": "2508.17614",
        "ARXIVID": "2508.17614",
        "COMMENT": "No criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.18271": {
        "authors": [
            "Haitang Feng",
            "Jie Liu",
            "Jie Tang",
            "Gangshan Wu",
            "Beiqi Chen",
            "Jianhuang Lai",
            "Guangcong Wang"
        ],
        "title": "ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models",
        "abstract": "arXiv:2508.18271v1 Announce Type: new  Abstract: 3D inpainting often relies on multi-view 2D image inpainting, where the inherent inconsistencies across different inpainted views can result in blurred textures, spatial discontinuities, and distracting visual artifacts. These inconsistencies pose significant challenges when striving for accurate and realistic 3D object completion, particularly in applications that demand high fidelity and structural coherence. To overcome these limitations, we propose ObjFiller-3D, a novel method designed for the completion and editing of high-quality and consistent 3D objects. Instead of employing a conventional 2D image inpainting model, our approach leverages a curated selection of state-of-the-art video editing model to fill in the masked regions of 3D objects. We analyze the representation gap between 3D and videos, and propose an adaptation of a video inpainting model for 3D scene inpainting. In addition, we introduce a reference-based 3D inpainting method to further enhance the quality of reconstruction. Experiments across diverse datasets show that compared to previous methods, ObjFiller-3D produces more faithful and fine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of 0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for practical deployment in real-world 3D editing applications. Project page: https://objfiller3d.github.io/ Code: https://github.com/objfiller3d/ObjFiller-3D .",
        "arxiv_id": "2508.18271",
        "ARXIVID": "2508.18271",
        "COMMENT": "No criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2508.17062": {
        "authors": [
            "Peng Hu",
            "Yu Gu",
            "Liang Luo",
            "Fuji Ren"
        ],
        "title": "SSG-Dit: A Spatial Signal Guided Framework for Controllable Video Generation",
        "abstract": "arXiv:2508.17062v1 Announce Type: new  Abstract: Controllable video generation aims to synthesize video content that aligns precisely with user-provided conditions, such as text descriptions and initial images. However, a significant challenge persists in this domain: existing models often struggle to maintain strong semantic consistency, frequently generating videos that deviate from the nuanced details specified in the prompts. To address this issue, we propose SSG-DiT (Spatial Signal Guided Diffusion Transformer), a novel and efficient framework for high-fidelity controllable video generation. Our approach introduces a decoupled two-stage process. The first stage, Spatial Signal Prompting, generates a spatially aware visual prompt by leveraging the rich internal representations of a pre-trained multi-modal model. This prompt, combined with the original text, forms a joint condition that is then injected into a frozen video DiT backbone via our lightweight and parameter-efficient SSG-Adapter. This unique design, featuring a dual-branch attention mechanism, allows the model to simultaneously harness its powerful generative priors while being precisely steered by external spatial signals. Extensive experiments demonstrate that SSG-DiT achieves state-of-the-art performance, outperforming existing models on multiple key metrics in the VBench benchmark, particularly in spatial relationship control and overall consistency.",
        "arxiv_id": "2508.17062",
        "ARXIVID": "2508.17062",
        "COMMENT": "No criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}