{
    "2508.08488": {
        "authors": [
            "Ankan Deria",
            "Dwarikanath Mahapatra",
            "Behzad Bozorgtabar",
            "Mohna Chakraborty",
            "Snehashis Chakraborty",
            "Sudipta Roy"
        ],
        "title": "MuGa-VTON: Multi-Garment Virtual Try-On via Diffusion Transformers with Prompt Customization",
        "abstract": "arXiv:2508.08488v1 Announce Type: new  Abstract: Virtual try-on seeks to generate photorealistic images of individuals in desired garments, a task that must simultaneously preserve personal identity and garment fidelity for practical use in fashion retail and personalization. However, existing methods typically handle upper and lower garments separately, rely on heavy preprocessing, and often fail to preserve person-specific cues such as tattoos, accessories, and body shape-resulting in limited realism and flexibility. To this end, we introduce MuGa-VTON, a unified multi-garment diffusion framework that jointly models upper and lower garments together with person identity in a shared latent space. Specifically, we proposed three key modules: the Garment Representation Module (GRM) for capturing both garment semantics, the Person Representation Module (PRM) for encoding identity and pose cues, and the A-DiT fusion module, which integrates garment, person, and text-prompt features through a diffusion transformer. This architecture supports prompt-based customization, allowing fine-grained garment modifications with minimal user input. Extensive experiments on the VITON-HD and DressCode benchmarks demonstrate that MuGa-VTON outperforms existing methods in both qualitative and quantitative evaluations, producing high-fidelity, identity-preserving results suitable for real-world virtual try-on applications.",
        "arxiv_id": "2508.08488",
        "ARXIVID": "2508.08488",
        "COMMENT": "No criteria match closely.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    }
}