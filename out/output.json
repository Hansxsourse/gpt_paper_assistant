{
    "2502.14865": {
        "authors": [
            "Sara Ghaboura",
            "Ketan More",
            "Ritesh Thawkar",
            "Wafa Alghallabi",
            "Omkar Thawakar",
            "Fahad Shahbaz Khan",
            "Hisham Cholakkal",
            "Salman Khan",
            "Rao Muhammad Anwer"
        ],
        "title": "Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical and Cultural Artifacts",
        "abstract": "arXiv:2502.14865v1 Announce Type: new  Abstract: Understanding historical and cultural artifacts demands human expertise and advanced computational techniques, yet the process remains complex and time-intensive. While large multimodal models offer promising support, their evaluation and improvement require a standardized benchmark. To address this, we introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning 266 distinct cultures across 10 major historical regions. Designed for AI-driven analysis of manuscripts, artworks, inscriptions, and archaeological discoveries, TimeTravel provides a structured dataset and robust evaluation framework to assess AI models' capabilities in classification, interpretation, and historical comprehension. By integrating AI with historical research, TimeTravel fosters AI-powered tools for historians, archaeologists, researchers, and cultural tourists to extract valuable insights while ensuring technology contributes meaningfully to historical discovery and cultural heritage preservation. We evaluate contemporary AI models on TimeTravel, highlighting their strengths and identifying areas for improvement. Our goal is to establish AI as a reliable partner in preserving cultural heritage, ensuring that technological advancements contribute meaningfully to historical discovery. Our code is available at: \\url{https://github.com/mbzuai-oryx/TimeTravel}.",
        "arxiv_id": "2502.14865",
        "ARXIVID": "2502.14865",
        "COMMENT": "This paper introduces TimeTravel, a benchmark for evaluating large multimodal models on historical and cultural artifacts. It matches criterion 3 as it focuses on a new benchmark for multimodal models, which could be relevant for embodied AI applications. The focus on historical artifacts is a novel angle.",
        "RELEVANCE": 6,
        "NOVELTY": 8
    },
    "2502.14149": {
        "authors": [
            "Runlong He",
            "Danyal Z. Khan",
            "Evangelos B. Mazomenos",
            "Hani J. Marcus",
            "Danail Stoyanov",
            "Matthew J. Clarkson",
            "Mobarakol Islam"
        ],
        "title": "PitVQA++: Vector Matrix-Low-Rank Adaptation for Open-Ended Visual Question Answering in Pituitary Surgery",
        "abstract": "arXiv:2502.14149v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) in visual question answering (VQA) offer a unique opportunity to enhance intra-operative decision-making, promote intuitive interactions, and significantly advancing surgical education. However, the development of VLMs for surgical VQA is challenging due to limited datasets and the risk of overfitting and catastrophic forgetting during full fine-tuning of pretrained weights. While parameter-efficient techniques like Low-Rank Adaptation (LoRA) and Matrix of Rank Adaptation (MoRA) address adaptation challenges, their uniform parameter distribution overlooks the feature hierarchy in deep networks, where earlier layers, that learn general features, require more parameters than later ones. This work introduces PitVQA++ with an open-ended PitVQA dataset and vector matrix-low-rank adaptation (Vector-MoLoRA), an innovative VLM fine-tuning approach for adapting GPT-2 to pituitary surgery. Open-Ended PitVQA comprises around 101,803 frames from 25 procedural videos with 745,972 question-answer sentence pairs, covering key surgical elements such as phase and step recognition, context understanding, tool detection, localization, and interactions recognition. Vector-MoLoRA incorporates the principles of LoRA and MoRA to develop a matrix-low-rank adaptation strategy that employs vector ranking to allocate more parameters to earlier layers, gradually reducing them in the later layers. Our approach, validated on the Open-Ended PitVQA and EndoVis18-VQA datasets, effectively mitigates catastrophic forgetting while significantly enhancing performance over recent baselines. Furthermore, our risk-coverage analysis highlights its enhanced reliability and trustworthiness in handling uncertain predictions. Our source code and dataset is available at~\\url{https://github.com/HRL-Mike/PitVQA-Plus}.",
        "arxiv_id": "2502.14149",
        "ARXIVID": "2502.14149",
        "COMMENT": "This paper introduces PitVQA++, a new dataset and fine-tuning method for visual question answering in surgical contexts. It matches criterion 2 as it involves a novel adaptation of vision-language models (VLMs) for a specific domain. The dataset and fine-tuning method are interesting but domain-specific.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2502.14846": {
        "authors": [
            "Yue Yang",
            "Ajay Patel",
            "Matt Deitke",
            "Tanmay Gupta",
            "Luca Weihs",
            "Andrew Head",
            "Mark Yatskar",
            "Chris Callison-Burch",
            "Ranjay Krishna",
            "Aniruddha Kembhavi",
            "Christopher Clark"
        ],
        "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation",
        "abstract": "arXiv:2502.14846v1 Announce Type: new  Abstract: Reasoning about images with rich text, such as charts and documents, is a critical application of vision-language models (VLMs). However, VLMs often struggle in these domains due to the scarcity of diverse text-rich vision-language data. To address this challenge, we present CoSyn, a framework that leverages the coding capabilities of text-only large language models (LLMs) to automatically create synthetic text-rich multimodal data. Given input text describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts an LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic images. With the underlying code as textual representations of the synthetic images, CoSyn can generate high-quality instruction-tuning data, again relying on a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K images and 2.7M rows of vision-language instruction-tuning data. Comprehensive experiments on seven benchmarks demonstrate that models trained on our synthetic data achieve state-of-the-art performance among competitive open-source models, including Llama 3.2, and surpass proprietary models such as GPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing data, enabling VLMs to ground information within input images, showcasing its potential for developing multimodal agents capable of acting in real-world environments.",
        "arxiv_id": "2502.14846",
        "ARXIVID": "2502.14846",
        "COMMENT": "Matches criterion 2 as it presents CoSyn, a framework for generating synthetic multimodal data for vision-language models, and demonstrates its effectiveness on benchmarks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2502.14156": {
        "authors": [
            "Katie Z Luo",
            "Minh-Quan Dao",
            "Zhenzhen Liu",
            "Mark Campbell",
            "Wei-Lun Chao",
            "Kilian Q. Weinberger",
            "Ezio Malis",
            "Vincent Fremont",
            "Bharath Hariharan",
            "Mao Shan",
            "Stewart Worrall",
            "Julie Stephany Berrio Perez"
        ],
        "title": "Mixed Signals: A Diverse Point Cloud Dataset for Heterogeneous LiDAR V2X Collaboration",
        "abstract": "arXiv:2502.14156v1 Announce Type: new  Abstract: Vehicle-to-everything (V2X) collaborative perception has emerged as a promising solution to address the limitations of single-vehicle perception systems. However, existing V2X datasets are limited in scope, diversity, and quality. To address these gaps, we present Mixed Signals, a comprehensive V2X dataset featuring 45.1k point clouds and 240.6k bounding boxes collected from three connected autonomous vehicles (CAVs) equipped with two different types of LiDAR sensors, plus a roadside unit with dual LiDARs. Our dataset provides precisely aligned point clouds and bounding box annotations across 10 classes, ensuring reliable data for perception training. We provide detailed statistical analysis on the quality of our dataset and extensively benchmark existing V2X methods on it. Mixed Signals V2X Dataset is one of the highest quality, large-scale datasets publicly available for V2X perception research. Details on the website https://mixedsignalsdataset.cs.cornell.edu/.",
        "arxiv_id": "2502.14156",
        "ARXIVID": "2502.14156",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for V2X collaborative perception, addressing gaps in diversity and quality.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.14064": {
        "authors": [
            "Shansong Wang",
            "Mojtaba Safari",
            "Qiang Li",
            "Chih-Wei Chang",
            "Richard LJ Qiu",
            "Justin Roper",
            "David S. Yu",
            "Xiaofeng Yang"
        ],
        "title": "Triad: Vision Foundation Model for 3D Magnetic Resonance Imaging",
        "abstract": "arXiv:2502.14064v1 Announce Type: new  Abstract: Vision foundation models (VFMs) are pre-trained on extensive image datasets to learn general representations for diverse types of data. These models can subsequently be fine-tuned for specific downstream tasks, significantly boosting performance across a broad range of applications. However, existing vision foundation models that claim to be applicable to various radiology tasks are mostly pre-trained on 3D computed tomography (CT), which benefits from the availability of extensive 3D CT databases. Significant differences between CT and magnetic resonance imaging (MRI) in imaging principles, signal characteristics, and data distribution may hinder their practical performance and versatility in MRI-specific applications. Here, we propose Triad, a vision foundation model for 3D MRI. Triad adopts a widely used autoencoder architecture to learn robust representations from 131,170 3D MRI volumes and uses organ-independent imaging descriptions to constrain the semantic distribution of the visual modality. The above pre-training dataset is called Triad-131K, which is currently the largest 3D MRI pre-training dataset. We evaluate Triad across three tasks, namely, organ/tumor segmentation, organ/cancer classification, and medical image registration, in two data modalities (within-domain and out-of-domain) settings using 25 downstream datasets. By initializing models with Triad's pre-trained weights, nnUNet-Triad improves segmentation performance by 6.88% compared to nnUNet-Scratch across 17 datasets. Swin-B-Triad achieves a 3.97% improvement over Swin-B-Scratch in classification tasks across five datasets. SwinUNETR-Triad improves by 4.00% compared to SwinUNETR-Scratch in registration tasks across two datasets. Our study demonstrates that pre-training can maximize performance when the data modalities and organs of upstream and downstream tasks are consistent.",
        "arxiv_id": "2502.14064",
        "ARXIVID": "2502.14064",
        "COMMENT": "Matches criterion 4 as it introduces Triad, a vision foundation model for 3D MRI, and demonstrates its applications across multiple tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.14520": {
        "authors": [
            "Meng Wang",
            "Fan Wu",
            "Ruihui Li",
            "Yunchuan Qin",
            "Zhuo Tang",
            "Kenli Li"
        ],
        "title": "Learning Temporal 3D Semantic Scene Completion via Optical Flow Guidance",
        "abstract": "arXiv:2502.14520v1 Announce Type: new  Abstract: 3D Semantic Scene Completion (SSC) provides comprehensive scene geometry and semantics for autonomous driving perception, which is crucial for enabling accurate and reliable decision-making. However, existing SSC methods are limited to capturing sparse information from the current frame or naively stacking multi-frame temporal features, thereby failing to acquire effective scene context. These approaches ignore critical motion dynamics and struggle to achieve temporal consistency. To address the above challenges, we propose a novel temporal SSC method FlowScene: Learning Temporal 3D Semantic Scene Completion via Optical Flow Guidance. By leveraging optical flow, FlowScene can integrate motion, different viewpoints, occlusions, and other contextual cues, thereby significantly improving the accuracy of 3D scene completion. Specifically, our framework introduces two key components: (1) a Flow-Guided Temporal Aggregation module that aligns and aggregates temporal features using optical flow, capturing motion-aware context and deformable structures; and (2) an Occlusion-Guided Voxel Refinement module that injects occlusion masks and temporally aggregated features into 3D voxel space, adaptively refining voxel representations for explicit geometric modeling. Experimental results demonstrate that FlowScene achieves state-of-the-art performance on the SemanticKITTI and SSCBench-KITTI-360 benchmarks.",
        "arxiv_id": "2502.14520",
        "ARXIVID": "2502.14520",
        "COMMENT": "Matches criterion 1 as it proposes a novel method for temporal 3D semantic scene completion using optical flow guidance, which relates to spatial understanding in embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.14786": {
        "authors": [
            "Michael Tschannen",
            "Alexey Gritsenko",
            "Xiao Wang",
            "Muhammad Ferjad Naeem",
            "Ibrahim Alabdulmohsin",
            "Nikhil Parthasarathy",
            "Talfan Evans",
            "Lucas Beyer",
            "Ye Xia",
            "Basil Mustafa",
            "Olivier H\\'enaff",
            "Jeremiah Harmsen",
            "Andreas Steiner",
            "Xiaohua Zhai"
        ],
        "title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features",
        "abstract": "arXiv:2502.14786v1 Announce Type: new  Abstract: We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success of the original SigLIP. In this second iteration, we extend the original image-text training objective with several prior, independently developed techniques into a unified recipe -- this includes captioning-based pretraining, self-supervised losses (self-distillation, masked prediction) and online data curation. With these changes, SigLIP 2 models outperform their SigLIP counterparts at all model scales in core capabilities, including zero-shot classification, image-text retrieval, and transfer performance when extracting visual representations for Vision-Language Models (VLMs). Furthermore, the new training recipe leads to significant improvements on localization and dense prediction tasks. We also train variants which support multiple resolutions and preserve the input's native aspect ratio. Finally, we train on a more diverse data-mixture that includes de-biasing techniques, leading to much better multilingual understanding and improved fairness. To allow users to trade off inference cost with performance, we release model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M), and g (1B).",
        "arxiv_id": "2502.14786",
        "ARXIVID": "2502.14786",
        "COMMENT": "Matches criterion 2 as it introduces SigLIP 2, a new multilingual vision-language encoder with improvements in semantic understanding and dense features.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.14282": {
        "authors": [
            "Haowei Liu",
            "Xi Zhang",
            "Haiyang Xu",
            "Yuyang Wanyan",
            "Junyang Wang",
            "Ming Yan",
            "Ji Zhang",
            "Chunfeng Yuan",
            "Changsheng Xu",
            "Weiming Hu",
            "Fei Huang"
        ],
        "title": "PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC",
        "abstract": "arXiv:2502.14282v1 Announce Type: new  Abstract: In the field of MLLM-based GUI agents, compared to smartphones, the PC scenario not only features a more complex interactive environment, but also involves more intricate intra- and inter-app workflows. To address these issues, we propose a hierarchical agent framework named PC-Agent. Specifically, from the perception perspective, we devise an Active Perception Module (APM) to overcome the inadequate abilities of current MLLMs in perceiving screenshot content. From the decision-making perspective, to handle complex user instructions and interdependent subtasks more effectively, we propose a hierarchical multi-agent collaboration architecture that decomposes decision-making processes into Instruction-Subtask-Action levels. Within this architecture, three agents (i.e., Manager, Progress and Decision) are set up for instruction decomposition, progress tracking and step-by-step decision-making respectively. Additionally, a Reflection agent is adopted to enable timely bottom-up error feedback and adjustment. We also introduce a new benchmark PC-Eval with 25 real-world complex instructions. Empirical results on PC-Eval show that our PC-Agent achieves a 32% absolute improvement of task success rate over previous state-of-the-art methods. The code will be publicly available.",
        "arxiv_id": "2502.14282",
        "ARXIVID": "2502.14282",
        "COMMENT": "Matches criterion 3 as it proposes a hierarchical multi-agent framework for complex task automation, focusing on novel embodied AI methods.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.14834": {
        "authors": [
            "Shangqing Tu",
            "Yucheng Wang",
            "Daniel Zhang-Li",
            "Yushi Bai",
            "Jifan Yu",
            "Yuhao Wu",
            "Lei Hou",
            "Huiqin Liu",
            "Zhiyuan Liu",
            "Bin Xu",
            "Juanzi Li"
        ],
        "title": "LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models",
        "abstract": "arXiv:2502.14834v1 Announce Type: new  Abstract: Existing Large Vision-Language Models (LVLMs) can process inputs with context lengths up to 128k visual and text tokens, yet they struggle to generate coherent outputs beyond 1,000 words. We find that the primary limitation is the absence of long output examples during supervised fine-tuning (SFT). To tackle this issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158 examples, each with multiple input images, an instruction, and corresponding outputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that maintain high-fidelity to the input images, we employ Direct Preference Optimization (DPO) to the SFT model. Given the high cost of collecting human feedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which breaks long outputs into segments and uses iterative corrections to form preference pairs with the original outputs. Additionally, we develop MMLongBench-Write, a benchmark featuring six tasks to evaluate the long-generation capabilities of VLMs. Our 7B parameter model, trained with LongWriter-V-22k and IterDPO, achieves impressive performance on this benchmark, outperforming larger proprietary models like GPT-4o. Code and data: https://github.com/THU-KEG/LongWriter-V",
        "arxiv_id": "2502.14834",
        "ARXIVID": "2502.14834",
        "COMMENT": "Matches criterion 2 as it focuses on improving vision-language models for ultra-long and high-fidelity generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.14140": {
        "authors": [
            "Yiming Huang",
            "Zhiyang Dou",
            "Lingjie Liu"
        ],
        "title": "ModSkill: Physical Character Skill Modularization",
        "abstract": "arXiv:2502.14140v1 Announce Type: new  Abstract: Human motion is highly diverse and dynamic, posing challenges for imitation learning algorithms that aim to generalize motor skills for controlling simulated characters. Previous methods typically rely on a universal full-body controller for tracking reference motion (tracking-based model) or a unified full-body skill embedding space (skill embedding). However, these approaches often struggle to generalize and scale to larger motion datasets. In this work, we introduce a novel skill learning framework, ModSkill, that decouples complex full-body skills into compositional, modular skills for independent body parts. Our framework features a skill modularization attention layer that processes policy observations into modular skill embeddings that guide low-level controllers for each body part. We also propose an Active Skill Learning approach with Generative Adaptive Sampling, using large motion generation models to adaptively enhance policy learning in challenging tracking scenarios. Our results show that this modularized skill learning framework, enhanced by generative sampling, outperforms existing methods in precise full-body motion tracking and enables reusable skill embeddings for diverse goal-driven tasks.",
        "arxiv_id": "2502.14140",
        "ARXIVID": "2502.14140",
        "COMMENT": "Matches criterion 3 as it introduces a novel modular skill learning framework for embodied AI with generative sampling.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2502.14125": {
        "authors": [
            "Zhenhan Huang",
            "Tejaswini Pedapati",
            "Pin-Yu Chen",
            "Jianxi Gao"
        ],
        "title": "Modular Prompt Learning Improves Vision-Language Models",
        "abstract": "arXiv:2502.14125v1 Announce Type: new  Abstract: Pre-trained vision-language models are able to interpret visual concepts and language semantics. Prompt learning, a method of constructing prompts for text encoders or image encoders, elicits the potentials of pre-trained models and readily adapts them to new scenarios. Compared to fine-tuning, prompt learning enables the model to achieve comparable or better performance using fewer trainable parameters. Besides, prompt learning freezes the pre-trained model and avoids the catastrophic forgetting issue in the fine-tuning. Continuous prompts inserted into the input of every transformer layer (i.e. deep prompts) can improve the performances of pre-trained models on downstream tasks. For i-th transformer layer, the inserted prompts replace previously inserted prompts in the $(i-1)$-th layer. Although the self-attention mechanism contextualizes newly inserted prompts for the current layer and embeddings from the previous layer's output, removing all inserted prompts from the previous layer inevitably loses information contained in the continuous prompts. In this work, we propose Modular Prompt Learning (MPL) that is designed to promote the preservation of information contained in the inserted prompts. We evaluate the proposed method on base-to-new generalization and cross-dataset tasks. On average of 11 datasets, our method achieves 0.7% performance gain on the base-to-new generalization task compared to the state-of-the-art method. The largest improvement on the individual dataset is 10.7% (EuroSAT dataset).",
        "arxiv_id": "2502.14125",
        "ARXIVID": "2502.14125",
        "COMMENT": "Matches criterion 4 as it proposes modular prompt learning to improve vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 5
    },
    "2502.14779": {
        "authors": [
            "Hongji Yang",
            "Wencheng Han",
            "Yucheng Zhou",
            "Jianbing Shen"
        ],
        "title": "DC-ControlNet: Decoupling Inter- and Intra-Element Conditions in Image Generation with Diffusion Models",
        "abstract": "arXiv:2502.14779v1 Announce Type: new  Abstract: In this paper, we introduce DC (Decouple)-ControlNet, a highly flexible and precisely controllable framework for multi-condition image generation. The core idea behind DC-ControlNet is to decouple control conditions, transforming global control into a hierarchical system that integrates distinct elements, contents, and layouts. This enables users to mix these individual conditions with greater flexibility, leading to more efficient and accurate image generation control. Previous ControlNet-based models rely solely on global conditions, which affect the entire image and lack the ability of element- or region-specific control. This limitation reduces flexibility and can cause condition misunderstandings in multi-conditional image generation. To address these challenges, we propose both intra-element and Inter-element Controllers in DC-ControlNet. The Intra-Element Controller handles different types of control signals within individual elements, accurately describing the content and layout characteristics of the object. For interactions between elements, we introduce the Inter-Element Controller, which accurately handles multi-element interactions and occlusion based on user-defined relationships. Extensive evaluations show that DC-ControlNet significantly outperforms existing ControlNet models and Layout-to-Image generative models in terms of control flexibility and precision in multi-condition control.",
        "arxiv_id": "2502.14779",
        "ARXIVID": "2502.14779",
        "COMMENT": "This paper introduces DC-ControlNet, a novel framework for multi-condition image generation with diffusion models. While it focuses on image generation, it does not directly address spatial understanding or embodied agents, nor does it involve vision-language models or benchmarks for embodied AI. It is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2502.14815": {
        "authors": [
            "Lingjiao Chen",
            "Jared Quincy Davis",
            "Boris Hanin",
            "Peter Bailis",
            "Matei Zaharia",
            "James Zou",
            "Ion Stoica"
        ],
        "title": "Optimizing Model Selection for Compound AI Systems",
        "abstract": "arXiv:2502.14815v1 Announce Type: new  Abstract: Compound AI systems that combine multiple LLM calls, such as self-refine and multi-agent-debate, achieve strong performance on many AI tasks. We address a core question in optimizing compound systems: for each LLM call or module in the system, how should one decide which LLM to use? We show that these LLM choices have a large effect on quality, but the search space is exponential. We propose LLMSelector, an efficient framework for model selection in compound systems, which leverages two key empirical insights: (i) end-to-end performance is often monotonic in how well each module performs, with all other modules held fixed, and (ii) per-module performance can be estimated accurately by an LLM. Building upon these insights, LLMSelector iteratively selects one module and allocates to it the model with the highest module-wise performance, as estimated by an LLM, until no further gain is possible. LLMSelector is applicable to any compound system with a bounded number of modules, and its number of API calls scales linearly with the number of modules, achieving high-quality model allocation both empirically and theoretically. Experiments with popular compound systems such as multi-agent debate and self-refine using LLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector confers 5%-70% accuracy gains compared to using the same LLM for all modules.",
        "arxiv_id": "2502.14815",
        "ARXIVID": "2502.14815",
        "COMMENT": "Does not match any specific criteria but involves optimization in compound AI systems, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.14155": {
        "authors": [
            "Animesh Nighojkar",
            "Bekhzodbek Moydinboyev",
            "My Duong",
            "John Licato"
        ],
        "title": "Giving AI Personalities Leads to More Human-Like Reasoning",
        "abstract": "arXiv:2502.14155v1 Announce Type: new  Abstract: In computational cognitive modeling, capturing the full spectrum of human judgment and decision-making processes, beyond just optimal behaviors, is a significant challenge. This study explores whether Large Language Models (LLMs) can emulate the breadth of human reasoning by predicting both intuitive, fast System 1 and deliberate, slow System 2 processes. We investigate the potential of AI to mimic diverse reasoning behaviors across a human population, addressing what we call the {\\em full reasoning spectrum problem}. We designed reasoning tasks using a novel generalization of the Natural Language Inference (NLI) format to evaluate LLMs' ability to replicate human reasoning. The questions were crafted to elicit both System 1 and System 2 responses. Human responses were collected through crowd-sourcing and the entire distribution was modeled, rather than just the majority of the answers. We used personality-based prompting inspired by the Big Five personality model to elicit AI responses reflecting specific personality traits, capturing the diversity of human reasoning, and exploring how personality traits influence LLM outputs. Combined with genetic algorithms to optimize the weighting of these prompts, this method was tested alongside traditional machine learning models. The results show that LLMs can mimic human response distributions, with open-source models like Llama and Mistral outperforming proprietary GPT models. Personality-based prompting, especially when optimized with genetic algorithms, significantly enhanced LLMs' ability to predict human response distributions, suggesting that capturing suboptimal, naturalistic reasoning may require modeling techniques incorporating diverse reasoning styles and psychological profiles. The study concludes that personality-based prompting combined with genetic algorithms is promising for enhancing AI's \\textit{human-ness} in reasoning.",
        "arxiv_id": "2502.14155",
        "ARXIVID": "2502.14155",
        "COMMENT": "Does not match any specific criteria but explores reasoning diversity in LLMs, which may be tangentially interesting.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.14801": {
        "authors": [
            "Cheng Li",
            "Keyuan Zhou",
            "Tong Liu",
            "Yu Wang",
            "Mingqiao Zhuang",
            "Huan-ang Gao",
            "Bu Jin",
            "Hao Zhao"
        ],
        "title": "AVD2: Accident Video Diffusion for Accident Video Description",
        "abstract": "arXiv:2502.14801v1 Announce Type: new  Abstract: Traffic accidents present complex challenges for autonomous driving, often featuring unpredictable scenarios that hinder accurate system interpretation and responses.Nonetheless, prevailing methodologies fall short in elucidating the causes of accidents and proposing preventive measures due to the paucity of training data specific to accident scenarios.In this work, we introduce AVD2 (Accident Video Diffusion for Accident Video Description), a novel framework that enhances accident scene understanding by generating accident videos that aligned with detailed natural language descriptions and reasoning, resulting in the contributed EMM-AU (Enhanced Multi-Modal Accident Video Understanding) dataset. Empirical results reveal that the integration of the EMM-AU dataset establishes state-of-the-art performance across both automated metrics and human evaluations, markedly advancing the domains of accident analysis and prevention. Project resources are available at https://an-answer-tree.github.io",
        "arxiv_id": "2502.14801",
        "ARXIVID": "2502.14801",
        "COMMENT": "Does not match any specific criteria but is related to multi-modal learning and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2502.14063": {
        "authors": [
            "Rui Zhao",
            "Zeyu Zhang",
            "Yi Xu",
            "Yi Yao",
            "Yan Huang",
            "Wenxin Zhang",
            "Zirui Song",
            "Xiuying Chen",
            "Yang Zhao"
        ],
        "title": "PedDet: Adaptive Spectral Optimization for Multimodal Pedestrian Detection",
        "abstract": "arXiv:2502.14063v1 Announce Type: new  Abstract: Pedestrian detection in intelligent transportation systems has made significant progress but faces two critical challenges: (1) insufficient fusion of complementary information between visible and infrared spectra, particularly in complex scenarios, and (2) sensitivity to illumination changes, such as low-light or overexposed conditions, leading to degraded performance. To address these issues, we propose PedDet, an adaptive spectral optimization complementarity framework specifically enhanced and optimized for multispectral pedestrian detection. PedDet introduces the Multi-scale Spectral Feature Perception Module (MSFPM) to adaptively fuse visible and infrared features, enhancing robustness and flexibility in feature extraction. Additionally, the Illumination Robustness Feature Decoupling Module (IRFDM) improves detection stability under varying lighting by decoupling pedestrian and background features. We further design a contrastive alignment to enhance intermodal feature discrimination. Experiments on LLVIP and MSDS datasets demonstrate that PedDet achieves state-of-the-art performance, improving the mAP by 6.6% with superior detection accuracy even in low-light conditions, marking a significant step forward for road safety. Code will be available at https://github.com/AIGeeksGroup/PedDet.",
        "arxiv_id": "2502.14063",
        "ARXIVID": "2502.14063",
        "COMMENT": "Does not match any specific criteria but involves multi-modal learning for pedestrian detection.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.14168": {
        "authors": [
            "Zhengeng Yang",
            "Hongshan Yu",
            "Jianjun Zhang",
            "Qiang Tang",
            "Ajmal Mian"
        ],
        "title": "Deep learning based infrared small object segmentation: Challenges and future directions",
        "abstract": "arXiv:2502.14168v1 Announce Type: new  Abstract: Infrared sensing is a core method for supporting unmanned systems, such as autonomous vehicles and drones. Recently, infrared sensors have been widely deployed on mobile and stationary platforms for detection and classification of objects from long distances and in wide field of views. Given its success in the vision image analysis domain, deep learning has also been applied for object recognition in infrared images. However, techniques that have proven successful in visible light perception face new challenges in the infrared domain. These challenges include extremely low signal-to-noise ratios in infrared images, very small and blurred objects of interest, and limited availability of labeled/unlabeled training data due to the specialized nature of infrared sensors. Numerous methods have been proposed in the literature for the detection and classification of small objects in infrared images achieving varied levels of success. There is a need for a survey paper that critically analyzes existing techniques in this domain, identifies unsolved challenges and provides future research directions. This paper fills the gap and offers a concise and insightful review of deep learning-based methods. It also identifies the challenges faced by existing infrared object segmentation methods and provides a structured review of existing infrared perception methods from the perspective of these challenges and highlights the motivations behind the various approaches. Finally, this review suggests promising future directions based on recent advancements within this domain.",
        "arxiv_id": "2502.14168",
        "ARXIVID": "2502.14168",
        "COMMENT": "Does not match any specific criteria but is related to computer vision challenges in infrared imaging.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2502.14456": {
        "authors": [
            "Ran Ding",
            "Ziyu Zhang",
            "Ying Zhu",
            "Ziqian Kong",
            "Peilan Xu"
        ],
        "title": "Narrative-Driven Travel Planning: Geoculturally-Grounded Script Generation with Evolutionary Itinerary Optimization",
        "abstract": "arXiv:2502.14456v1 Announce Type: new  Abstract: To enhance tourists' experiences and immersion, this paper proposes a narrative-driven travel planning framework called NarrativeGuide, which generates a geoculturally-grounded narrative script for travelers, offering a novel, role-playing experience for their journey. In the initial stage, NarrativeGuide constructs a knowledge graph for attractions within a city, then configures the worldview, character setting, and exposition based on the knowledge graph. Using this foundation, the knowledge graph is combined to generate an independent scene unit for each attraction. During the itinerary planning stage, NarrativeGuide models narrative-driven travel planning as an optimization problem, utilizing a genetic algorithm (GA) to refine the itinerary. Before evaluating the candidate itinerary, transition scripts are generated for each pair of adjacent attractions, which, along with the scene units, form a complete script. The weighted sum of script coherence, travel time, and attraction scores is then used as the fitness value to update the candidate solution set. Experimental results across four cities, i.e., Nanjing and Yangzhou in China, Paris in France, and Berlin in Germany, demonstrate significant improvements in narrative coherence and cultural fit, alongside a notable reduction in travel time and an increase in the quality of visited attractions. Our study highlights that incorporating external evolutionary optimization effectively addresses the limitations of large language models in travel planning.Our codes are available at https://github.com/Evan01225/Narrative-Driven-Travel-Planning.",
        "arxiv_id": "2502.14456",
        "ARXIVID": "2502.14456",
        "COMMENT": "Does not match any specific criteria but involves optimization and generative modeling in a novel application area.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}