{
    "2503.15138": {
        "authors": [
            "Mingzhe Zheng",
            "Yongqi Xu",
            "Haojian Huang",
            "Xuran Ma",
            "Yexin Liu",
            "Wenjie Shu",
            "Yatian Pang",
            "Feilong Tang",
            "Qifeng Chen",
            "Harry Yang",
            "Ser-Nam Lim"
        ],
        "title": "VideoGen-of-Thought: Step-by-step generating multi-shot video with minimal manual intervention",
        "abstract": "arXiv:2503.15138v1 Announce Type: new  Abstract: Current video generation models excel at short clips but fail to produce cohesive multi-shot narratives due to disjointed visual dynamics and fractured storylines. Existing solutions either rely on extensive manual scripting/editing or prioritize single-shot fidelity over cross-scene continuity, limiting their practicality for movie-like content. We introduce VideoGen-of-Thought (VGoT), a step-by-step framework that automates multi-shot video synthesis from a single sentence by systematically addressing three core challenges: (1) Narrative Fragmentation: Existing methods lack structured storytelling. We propose dynamic storyline modeling, which first converts the user prompt into concise shot descriptions, then elaborates them into detailed, cinematic specifications across five domains (character dynamics, background continuity, relationship evolution, camera movements, HDR lighting), ensuring logical narrative progression with self-validation. (2) Visual Inconsistency: Existing approaches struggle with maintaining visual consistency across shots. Our identity-aware cross-shot propagation generates identity-preserving portrait (IPP) tokens that maintain character fidelity while allowing trait variations (expressions, aging) dictated by the storyline. (3) Transition Artifacts: Abrupt shot changes disrupt immersion. Our adjacent latent transition mechanisms implement boundary-aware reset strategies that process adjacent shots' features at transition points, enabling seamless visual flow while preserving narrative continuity. VGoT generates multi-shot videos that outperform state-of-the-art baselines by 20.4% in within-shot face consistency and 17.4% in style consistency, while achieving over 100% better cross-shot consistency and 10x fewer manual adjustments than alternatives.",
        "arxiv_id": "2503.15138",
        "ARXIVID": "2503.15138",
        "COMMENT": "Matches criterion 2 as it introduces a novel framework for multi-shot video generation with minimal manual intervention, leveraging multi-modal techniques.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.15283": {
        "authors": [
            "Teng-Fang Hsiao",
            "Bo-Kai Ruan",
            "Yi-Lun Wu",
            "Tzu-Ling Lin",
            "Hong-Han Shuai"
        ],
        "title": "TF-TI2I: Training-Free Text-and-Image-to-Image Generation via Multi-Modal Implicit-Context Learning in Text-to-Image Models",
        "abstract": "arXiv:2503.15283v1 Announce Type: new  Abstract: Text-and-Image-To-Image (TI2I), an extension of Text-To-Image (T2I), integrates image inputs with textual instructions to enhance image generation. Existing methods often partially utilize image inputs, focusing on specific elements like objects or styles, or they experience a decline in generation quality with complex, multi-image instructions. To overcome these challenges, we introduce Training-Free Text-and-Image-to-Image (TF-TI2I), which adapts cutting-edge T2I models such as SD3 without the need for additional training. Our method capitalizes on the MM-DiT architecture, in which we point out that textual tokens can implicitly learn visual information from vision tokens. We enhance this interaction by extracting a condensed visual representation from reference images, facilitating selective information sharing through Reference Contextual Masking -- this technique confines the usage of contextual tokens to instruction-relevant visual information. Additionally, our Winner-Takes-All module mitigates distribution shifts by prioritizing the most pertinent references for each vision token. Addressing the gap in TI2I evaluation, we also introduce the FG-TI2I Bench, a comprehensive benchmark tailored for TI2I and compatible with existing T2I methods. Our approach shows robust performance across various benchmarks, confirming its effectiveness in handling complex image-generation tasks.",
        "arxiv_id": "2503.15283",
        "ARXIVID": "2503.15283",
        "COMMENT": "Matches criterion 2 as it introduces a novel method for multi-modal text-and-image-to-image generation using advanced T2I models.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2503.15475": {
        "authors": [
            "Foundation AI Team",
            "Kiran Bhat",
            "Nishchaie Khanna",
            "Karun Channa",
            "Tinghui Zhou",
            "Yiheng Zhu",
            "Xiaoxia Sun",
            "Charles Shang",
            "Anirudh Sudarshan",
            "Maurice Chu",
            "Daiqing Li",
            "Kangle Deng",
            "Jean-Philippe Fauconnier",
            "Tijmen Verhulsdonck",
            "Maneesh Agrawala",
            "Kayvon Fatahalian",
            "Alexander Weiss",
            "Christian Reiser",
            "Ravi Kiran Chirravuri",
            "Ravali Kandur",
            "Alejandro Pelaez",
            "Akash Garg",
            "Michael Palleschi",
            "Jessica Wang",
            "Skylar Litz",
            "Leon Liu",
            "Anying Li",
            "David Harmon",
            "Derek Liu",
            "Liangjun Feng",
            "Denis Goupil",
            "Lukas Kuczynski",
            "Jihyun Yoon",
            "Naveen Marri",
            "Peiye Zhuang",
            "Yinan Zhang",
            "Brian Yin",
            "Haomiao Jiang",
            "Marcel van Workum",
            "Thomas Lane",
            "Bryce Erickson",
            "Salil Pathare",
            "Kyle Price",
            "Anupam Singh",
            "David Baszucki"
        ],
        "title": "Cube: A Roblox View of 3D Intelligence",
        "abstract": "arXiv:2503.15475v1 Announce Type: new  Abstract: Foundation models trained on vast amounts of data have demonstrated remarkable reasoning and generation capabilities in the domains of text, images, audio and video. Our goal at Roblox is to build such a foundation model for 3D intelligence, a model that can support developers in producing all aspects of a Roblox experience, from generating 3D objects and scenes to rigging characters for animation to producing programmatic scripts describing object behaviors. We discuss three key design requirements for such a 3D foundation model and then present our first step towards building such a model. We expect that 3D geometric shapes will be a core data type and describe our solution for 3D shape tokenizer. We show how our tokenization scheme can be used in applications for text-to-shape generation, shape-to-text generation and text-to-scene generation. We demonstrate how these applications can collaborate with existing large language models (LLMs) to perform scene analysis and reasoning. We conclude with a discussion outlining our path to building a fully unified foundation model for 3D intelligence.",
        "arxiv_id": "2503.15475",
        "ARXIVID": "2503.15475",
        "COMMENT": "Matches criterion 4 as it discusses 3D foundation models and their applications.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2503.14830": {
        "authors": [
            "Junfeng Ni",
            "Yu Liu",
            "Ruijie Lu",
            "Zirui Zhou",
            "Song-Chun Zhu",
            "Yixin Chen",
            "Siyuan Huang"
        ],
        "title": "Decompositional Neural Scene Reconstruction with Generative Diffusion Prior",
        "abstract": "arXiv:2503.14830v1 Announce Type: new  Abstract: Decompositional reconstruction of 3D scenes, with complete shapes and detailed texture of all objects within, is intriguing for downstream applications but remains challenging, particularly with sparse views as input. Recent approaches incorporate semantic or geometric regularization to address this issue, but they suffer significant degradation in underconstrained areas and fail to recover occluded regions. We argue that the key to solving this problem lies in supplementing missing information for these areas. To this end, we propose DP-Recon, which employs diffusion priors in the form of Score Distillation Sampling (SDS) to optimize the neural representation of each individual object under novel views. This provides additional information for the underconstrained areas, but directly incorporating diffusion prior raises potential conflicts between the reconstruction and generative guidance. Therefore, we further introduce a visibility-guided approach to dynamically adjust the per-pixel SDS loss weights. Together these components enhance both geometry and appearance recovery while remaining faithful to input images. Extensive experiments across Replica and ScanNet++ demonstrate that our method significantly outperforms SOTA methods. Notably, it achieves better object reconstruction under 10 views than the baselines under 100 views. Our method enables seamless text-based editing for geometry and appearance through SDS optimization and produces decomposed object meshes with detailed UV maps that support photorealistic Visual effects (VFX) editing. The project page is available at https://dp-recon.github.io/.",
        "arxiv_id": "2503.14830",
        "ARXIVID": "2503.14830",
        "COMMENT": "Matches criterion 4 as it focuses on vision foundation models and their application in 3D scene reconstruction with generative diffusion priors.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2503.15470": {
        "authors": [
            "Boshen Xu",
            "Yuting Mei",
            "Xinbi Liu",
            "Sipeng Zheng",
            "Qin Jin"
        ],
        "title": "EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining",
        "abstract": "arXiv:2503.15470v1 Announce Type: new  Abstract: Egocentric video-language pretraining has significantly advanced video representation learning. Humans perceive and interact with a fully 3D world, developing spatial awareness that extends beyond text-based understanding. However, most previous works learn from 1D text or 2D visual cues, such as bounding boxes, which inherently lack 3D understanding. To bridge this gap, we introduce EgoDTM, an Egocentric Depth- and Text-aware Model, jointly trained through large-scale 3D-aware video pretraining and video-text contrastive learning. EgoDTM incorporates a lightweight 3D-aware decoder to efficiently learn 3D-awareness from pseudo depth maps generated by depth estimation models. To further facilitate 3D-aware video pretraining, we enrich the original brief captions with hand-object visual cues by organically combining several foundation models. Extensive experiments demonstrate EgoDTM's superior performance across diverse downstream tasks, highlighting its superior 3D-aware visual understanding. Our code will be released at https://github.com/xuboshen/EgoDTM.",
        "arxiv_id": "2503.15470",
        "ARXIVID": "2503.15470",
        "COMMENT": "This paper introduces a novel 3D-aware egocentric video-language pretraining model, which aligns with criterion 1 and 3.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.15451": {
        "authors": [
            "Lixing Xiao",
            "Shunlin Lu",
            "Huaijin Pi",
            "Ke Fan",
            "Liang Pan",
            "Yueer Zhou",
            "Ziyong Feng",
            "Xiaowei Zhou",
            "Sida Peng",
            "Jingbo Wang"
        ],
        "title": "MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space",
        "abstract": "arXiv:2503.15451v1 Announce Type: new  Abstract: This paper addresses the challenge of text-conditioned streaming motion generation, which requires us to predict the next-step human pose based on variable-length historical motions and incoming texts. Existing methods struggle to achieve streaming motion generation, e.g., diffusion models are constrained by pre-defined motion lengths, while GPT-based methods suffer from delayed response and error accumulation problem due to discretized non-causal tokenization. To solve these problems, we propose MotionStreamer, a novel framework that incorporates a continuous causal latent space into a probabilistic autoregressive model. The continuous latents mitigate information loss caused by discretization and effectively reduce error accumulation during long-term autoregressive generation. In addition, by establishing temporal causal dependencies between current and historical motion latents, our model fully utilizes the available information to achieve accurate online motion decoding. Experiments show that our method outperforms existing approaches while offering more applications, including multi-round generation, long-term generation, and dynamic motion composition. Project Page: https://zju3dv.github.io/MotionStreamer/",
        "arxiv_id": "2503.15451",
        "ARXIVID": "2503.15451",
        "COMMENT": "This paper introduces MotionStreamer, a framework for text-conditioned streaming motion generation, which aligns with criterion 3 on novel methods in embodied AI.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.14526": {
        "authors": [
            "Yu Fang",
            "Yue Yang",
            "Xinghao Zhu",
            "Kaiyuan Zheng",
            "Gedas Bertasius",
            "Daniel Szafir",
            "Mingyu Ding"
        ],
        "title": "ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis",
        "abstract": "arXiv:2503.14526v1 Announce Type: new  Abstract: Vision-language-action (VLA) models present a promising paradigm by training policies directly on real robot datasets like Open X-Embodiment. However, the high cost of real-world data collection hinders further data scaling, thereby restricting the generalizability of VLAs. In this paper, we introduce ReBot, a novel real-to-sim-to-real approach for scaling real robot datasets and adapting VLA models to target domains, which is the last-mile deployment challenge in robot manipulation. Specifically, ReBot replays real-world robot trajectories in simulation to diversify manipulated objects (real-to-sim), and integrates the simulated movements with inpainted real-world background to synthesize physically realistic and temporally consistent robot videos (sim-to-real). Our approach has several advantages: 1) it enjoys the benefit of real data to minimize the sim-to-real gap; 2) it leverages the scalability of simulation; and 3) it can generalize a pretrained VLA to a target domain with fully automated data pipelines. Extensive experiments in both simulation and real-world environments show that ReBot significantly enhances the performance and robustness of VLAs. For example, in SimplerEnv with the WidowX robot, ReBot improved the in-domain performance of Octo by 7.2% and OpenVLA by 21.8%, and out-of-domain generalization by 19.9% and 9.4%, respectively. For real-world evaluation with a Franka robot, ReBot increased the success rates of Octo by 17% and OpenVLA by 20%. More information can be found at: https://yuffish.github.io/rebot/",
        "arxiv_id": "2503.14526",
        "ARXIVID": "2503.14526",
        "COMMENT": "Matches criterion 3 as it focuses on a novel real-to-sim-to-real approach for scaling robot learning datasets, which is relevant to embodied AI benchmarks and methods.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2503.14905": {
        "authors": [
            "Siwei Wen",
            "Junyan Ye",
            "Peilin Feng",
            "Hengrui Kang",
            "Zichen Wen",
            "Yize Chen",
            "Jiang Wu",
            "Wenjun Wu",
            "Conghui He",
            "Weijia Li"
        ],
        "title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation",
        "abstract": "arXiv:2503.14905v1 Announce Type: new  Abstract: With the rapid advancement of Artificial Intelligence Generated Content (AIGC) technologies, synthetic images have become increasingly prevalent in everyday life, posing new challenges for authenticity assessment and detection. Despite the effectiveness of existing methods in evaluating image authenticity and locating forgeries, these approaches often lack human interpretability and do not fully address the growing complexity of synthetic data. To tackle these challenges, we introduce FakeVLM, a specialized large multimodal model designed for both general synthetic image and DeepFake detection tasks. FakeVLM not only excels in distinguishing real from fake images but also provides clear, natural language explanations for image artifacts, enhancing interpretability. Additionally, we present FakeClue, a comprehensive dataset containing over 100,000 images across seven categories, annotated with fine-grained artifact clues in natural language. FakeVLM demonstrates performance comparable to expert models while eliminating the need for additional classifiers, making it a robust solution for synthetic data detection. Extensive evaluations across multiple datasets confirm the superiority of FakeVLM in both authenticity classification and artifact explanation tasks, setting a new benchmark for synthetic image detection. The dataset and code will be released in: https://github.com/opendatalab/FakeVLM.",
        "arxiv_id": "2503.14905",
        "ARXIVID": "2503.14905",
        "COMMENT": "Matches criterion 2 as it introduces a large multimodal model (FakeVLM) for synthetic image detection with artifact explanation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.14974": {
        "authors": [
            "Yifan Li",
            "Shuai Yang",
            "Jiaying Liu"
        ],
        "title": "Language-based Image Colorization: A Benchmark and Beyond",
        "abstract": "arXiv:2503.14974v1 Announce Type: new  Abstract: Image colorization aims to bring colors back to grayscale images. Automatic image colorization methods, which requires no additional guidance, struggle to generate high-quality images due to color ambiguity, and provides limited user controllability. Thanks to the emergency of cross-modality datasets and models, language-based colorization methods are proposed to fully utilize the efficiency and flexibly of text descriptions to guide colorization. In view of the lack of a comprehensive review of language-based colorization literature, we conduct a thorough analysis and benchmarking. We first briefly summarize existing automatic colorization methods. Then, we focus on language-based methods and point out their core challenge on cross-modal alignment. We further divide these methods into two categories: one attempts to train a cross-modality network from scratch, while the other utilizes the pre-trained cross-modality model to establish the textual-visual correspondence. Based on the analyzed limitations of existing language-based methods, we propose a simple yet effective method based on distilled diffusion model. Extensive experiments demonstrate that our simple baseline can produces better results than previous complex methods with 14 times speed up. To the best of our knowledge, this is the first comprehensive review and benchmark on language-based image colorization field, providing meaningful insights for the community. The code is available at https://github.com/lyf1212/Color-Turbo.",
        "arxiv_id": "2503.14974",
        "ARXIVID": "2503.14974",
        "COMMENT": "Matches criterion 2 as it discusses language-based image colorization using cross-modal alignment and diffusion models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.15185": {
        "authors": [
            "Gyeongrok Oh",
            "Sungjune Kim",
            "Heeju Ko",
            "Hyung-gun Chi",
            "Jinkyu Kim",
            "Dongwook Lee",
            "Daehyun Ji",
            "Sungjoon Choi",
            "Sujin Jang",
            "Sangpil Kim"
        ],
        "title": "3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware View Transformation",
        "abstract": "arXiv:2503.15185v1 Announce Type: new  Abstract: The resolution of voxel queries significantly influences the quality of view transformation in camera-based 3D occupancy prediction. However, computational constraints and the practical necessity for real-time deployment require smaller query resolutions, which inevitably leads to an information loss. Therefore, it is essential to encode and preserve rich visual details within limited query sizes while ensuring a comprehensive representation of 3D occupancy. To this end, we introduce ProtoOcc, a novel occupancy network that leverages prototypes of clustered image segments in view transformation to enhance low-resolution context. In particular, the mapping of 2D prototypes onto 3D voxel queries encodes high-level visual geometries and complements the loss of spatial information from reduced query resolutions. Additionally, we design a multi-perspective decoding strategy to efficiently disentangle the densely compressed visual cues into a high-dimensional 3D occupancy scene. Experimental results on both Occ3D and SemanticKITTI benchmarks demonstrate the effectiveness of the proposed method, showing clear improvements over the baselines. More importantly, ProtoOcc achieves competitive performance against the baselines even with 75\\% reduced voxel resolution.",
        "arxiv_id": "2503.15185",
        "ARXIVID": "2503.15185",
        "COMMENT": "Matches criterion 1 as it introduces a novel method for spatial understanding in 3D occupancy prediction using prototype-aware view transformation.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.15406": {
        "authors": [
            "Jisu Nam",
            "Soowon Son",
            "Zhan Xu",
            "Jing Shi",
            "Difan Liu",
            "Feng Liu",
            "Aashish Misraa",
            "Seungryong Kim",
            "Yang Zhou"
        ],
        "title": "Visual Persona: Foundation Model for Full-Body Human Customization",
        "abstract": "arXiv:2503.15406v1 Announce Type: new  Abstract: We introduce Visual Persona, a foundation model for text-to-image full-body human customization that, given a single in-the-wild human image, generates diverse images of the individual guided by text descriptions. Unlike prior methods that focus solely on preserving facial identity, our approach captures detailed full-body appearance, aligning with text descriptions for body structure and scene variations. Training this model requires large-scale paired human data, consisting of multiple images per individual with consistent full-body identities, which is notoriously difficult to obtain. To address this, we propose a data curation pipeline leveraging vision-language models to evaluate full-body appearance consistency, resulting in Visual Persona-500K, a dataset of 580k paired human images across 100k unique identities. For precise appearance transfer, we introduce a transformer encoder-decoder architecture adapted to a pre-trained text-to-image diffusion model, which augments the input image into distinct body regions, encodes these regions as local appearance features, and projects them into dense identity embeddings independently to condition the diffusion model for synthesizing customized images. Visual Persona consistently surpasses existing approaches, generating high-quality, customized images from in-the-wild inputs. Extensive ablation studies validate design choices, and we demonstrate the versatility of Visual Persona across various downstream tasks.",
        "arxiv_id": "2503.15406",
        "ARXIVID": "2503.15406",
        "COMMENT": "Matches criterion 4 as it discusses a vision foundation model (Visual Persona) and its application in full-body human customization.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.14939": {
        "authors": [
            "Tengjin Weng",
            "Jingyi Wang",
            "Wenhao Jiang",
            "Zhong Ming"
        ],
        "title": "VisNumBench: Evaluating Number Sense of Multimodal Large Language Models",
        "abstract": "arXiv:2503.14939v1 Announce Type: new  Abstract: Can Multimodal Large Language Models (MLLMs) develop an intuitive number sense similar to humans? Targeting this problem, we introduce Visual Number Benchmark (VisNumBench) to evaluate the number sense abilities of MLLMs across a wide range of visual numerical tasks. VisNumBench consists of about 1,900 multiple-choice question-answer pairs derived from both synthetic and real-world visual data, covering seven visual numerical attributes and four types of visual numerical estimation tasks. Our experiments on VisNumBench led to the following key findings: (i) The 17 MLLMs we tested, including open-source models such as Qwen2.5-VL and InternVL2.5, as well as proprietary models like GPT-4o and Gemini 2.0 Flash, perform significantly below human levels in number sense-related tasks. (ii) Multimodal mathematical models and multimodal chain-of-thought (CoT) models did not exhibit significant improvements in number sense abilities. (iii) Stronger MLLMs with larger parameter sizes and broader general abilities demonstrate modest gains in number sense abilities. We believe VisNumBench will serve as a valuable resource for the research community, encouraging further advancements in enhancing MLLMs' number sense abilities. All benchmark resources, including code and datasets, will be publicly available at https://wwwtttjjj.github.io/VisNumBench/.",
        "arxiv_id": "2503.14939",
        "ARXIVID": "2503.14939",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (VisNumBench) for evaluating number sense in MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2503.14607": {
        "authors": [
            "Shuo Xing",
            "Zezhou Sun",
            "Shuangyu Xie",
            "Kaiyuan Chen",
            "Yanjia Huang",
            "Yuping Wang",
            "Jiachen Li",
            "Dezhen Song",
            "Zhengzhong Tu"
        ],
        "title": "Can Large Vision Language Models Read Maps Like a Human?",
        "abstract": "arXiv:2503.14607v1 Announce Type: new  Abstract: In this paper, we introduce MapBench-the first dataset specifically designed for human-readable, pixel-based map-based outdoor navigation, curated from complex path finding scenarios. MapBench comprises over 1600 pixel space map path finding problems from 100 diverse maps. In MapBench, LVLMs generate language-based navigation instructions given a map image and a query with beginning and end landmarks. For each map, MapBench provides Map Space Scene Graph (MSSG) as an indexing data structure to convert between natural language and evaluate LVLM-generated results. We demonstrate that MapBench significantly challenges state-of-the-art LVLMs both zero-shot prompting and a Chain-of-Thought (CoT) augmented reasoning framework that decomposes map navigation into sequential cognitive processes. Our evaluation of both open-source and closed-source LVLMs underscores the substantial difficulty posed by MapBench, revealing critical limitations in their spatial reasoning and structured decision-making capabilities. We release all the code and dataset in https://github.com/taco-group/MapBench.",
        "arxiv_id": "2503.14607",
        "ARXIVID": "2503.14607",
        "COMMENT": "This paper introduces MapBench, a dataset for map-based outdoor navigation, which aligns with criterion 3 on novel benchmarks in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.14919": {
        "authors": [
            "Junyu Shi",
            "Lijiang Liu",
            "Yong Sun",
            "Zhiyuan Zhang",
            "Jinni Zhou",
            "Qiang Nie"
        ],
        "title": "GenM$^3$: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation",
        "abstract": "arXiv:2503.14919v1 Announce Type: new  Abstract: Scaling up motion datasets is crucial to enhance motion generation capabilities. However, training on large-scale multi-source datasets introduces data heterogeneity challenges due to variations in motion content. To address this, we propose Generative Pretrained Multi-path Motion Model (GenM$^3$), a comprehensive framework designed to learn unified motion representations. GenM$^3$ comprises two components: 1) a Multi-Expert VQ-VAE (MEVQ-VAE) that adapts to different dataset distributions to learn a unified discrete motion representation, and 2) a Multi-path Motion Transformer (MMT) that improves intra-modal representations by using separate modality-specific pathways, each with densely activated experts to accommodate variations within that modality, and improves inter-modal alignment by the text-motion shared pathway. To enable large-scale training, we integrate and unify 11 high-quality motion datasets (approximately 220 hours of motion data) and augment it with textual annotations (nearly 10,000 motion sequences labeled by a large language model and 300+ by human experts). After training on our integrated dataset, GenM$^3$ achieves a state-of-the-art FID of 0.035 on the HumanML3D benchmark, surpassing state-of-the-art methods by a large margin. It also demonstrates strong zero-shot generalization on IDEA400 dataset, highlighting its effectiveness and adaptability across diverse motion scenarios.",
        "arxiv_id": "2503.14919",
        "ARXIVID": "2503.14919",
        "COMMENT": "This paper introduces a generative pretrained multi-path motion model for text-conditional human motion generation, which aligns with criterion 3 on novel methods in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.15285": {
        "authors": [
            "Yuanchao Yue",
            "Zhengxin Li",
            "Wei Zhang",
            "Hui Yuan"
        ],
        "title": "PAPI-Reg: Patch-to-Pixel Solution for Efficient Cross-Modal Registration between LiDAR Point Cloud and Camera Image",
        "abstract": "arXiv:2503.15285v1 Announce Type: new  Abstract: The primary requirement for cross-modal data fusion is the precise alignment of data from different sensors. However, the calibration between LiDAR point clouds and camera images is typically time-consuming and needs external calibration board or specific environmental features. Cross-modal registration effectively solves this problem by aligning the data directly without requiring external calibration. However, due to the domain gap between the point cloud and the image, existing methods rarely achieve satisfactory registration accuracy while maintaining real-time performance. To address this issue, we propose a framework that projects point clouds into several 2D representations for matching with camera images, which not only leverages the geometric characteristic of LiDAR point clouds more effectively but also bridge the domain gap between the point cloud and image. Moreover, to tackle the challenges of cross modal differences and the limited overlap between LiDAR point clouds and images in the image matching task, we introduce a multi-scale feature extraction network to effectively extract features from both camera images and the projection maps of LiDAR point cloud. Additionally, we propose a patch-to-pixel matching network to provide more effective supervision and achieve higher accuracy. We validate the performance of our model through experiments on the KITTI and nuScenes datasets. Our network achieves real-time performance and extremely high registration accuracy. On the KITTI dataset, our model achieves a registration accuracy rate of over 99\\%.",
        "arxiv_id": "2503.15285",
        "ARXIVID": "2503.15285",
        "COMMENT": "This paper proposes a framework for cross-modal registration between LiDAR point clouds and camera images, which aligns with criterion 3.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.15022": {
        "authors": [
            "Saad Lahlali",
            "Sandra Kara",
            "Hejer Ammar",
            "Florian Chabot",
            "Nicolas Granger",
            "Herv\\'e Le Borgne",
            "Quoc-Cuong Pham"
        ],
        "title": "xMOD: Cross-Modal Distillation for 2D/3D Multi-Object Discovery from 2D motion",
        "abstract": "arXiv:2503.15022v1 Announce Type: new  Abstract: Object discovery, which refers to the task of localizing objects without human annotations, has gained significant attention in 2D image analysis. However, despite this growing interest, it remains under-explored in 3D data, where approaches rely exclusively on 3D motion, despite its several challenges. In this paper, we present a novel framework that leverages advances in 2D object discovery which are based on 2D motion to exploit the advantages of such motion cues being more flexible and generalizable and to bridge the gap between 2D and 3D modalities. Our primary contributions are twofold: (i) we introduce DIOD-3D, the first baseline for multi-object discovery in 3D data using 2D motion, incorporating scene completion as an auxiliary task to enable dense object localization from sparse input data; (ii) we develop xMOD, a cross-modal training framework that integrates 2D and 3D data while always using 2D motion cues. xMOD employs a teacher-student training paradigm across the two modalities to mitigate confirmation bias by leveraging the domain gap. During inference, the model supports both RGB-only and point cloud-only inputs. Additionally, we propose a late-fusion technique tailored to our pipeline that further enhances performance when both modalities are available at inference. We evaluate our approach extensively on synthetic (TRIP-PD) and challenging real-world datasets (KITTI and Waymo). Notably, our approach yields a substantial performance improvement compared with the 2D object discovery state-of-the-art on all datasets with gains ranging from +8.7 to +15.1 in F1@50 score. The code is available at https://github.com/CEA-LIST/xMOD",
        "arxiv_id": "2503.15022",
        "ARXIVID": "2503.15022",
        "COMMENT": "This paper presents a cross-modal framework for 2D/3D multi-object discovery, which aligns with criterion 3.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.14945": {
        "authors": [
            "Yanhao Wu",
            "Haoyang Zhang",
            "Tianwei Lin",
            "Lichao Huang",
            "Shujie Luo",
            "Rui Wu",
            "Congpei Qiu",
            "Wei Ke",
            "Tong Zhang"
        ],
        "title": "Generating Multimodal Driving Scenes via Next-Scene Prediction",
        "abstract": "arXiv:2503.14945v1 Announce Type: new  Abstract: Generative models in Autonomous Driving (AD) enable diverse scene creation, yet existing methods fall short by only capturing a limited range of modalities, restricting the capability of generating controllable scenes for comprehensive evaluation of AD systems. In this paper, we introduce a multimodal generation framework that incorporates four major data modalities, including a novel addition of map modality. With tokenized modalities, our scene sequence generation framework autoregressively predicts each scene while managing computational demands through a two-stage approach. The Temporal AutoRegressive (TAR) component captures inter-frame dynamics for each modality while the Ordered AutoRegressive (OAR) component aligns modalities within each scene by sequentially predicting tokens in a fixed order. To maintain coherence between map and ego-action modalities, we introduce the Action-aware Map Alignment (AMA) module, which applies a transformation based on the ego-action to maintain coherence between these modalities. Our framework effectively generates complex, realistic driving scenes over extended sequences, ensuring multimodal consistency and offering fine-grained control over scene elements.",
        "arxiv_id": "2503.14945",
        "ARXIVID": "2503.14945",
        "COMMENT": "Matches criterion 4 as it discusses multimodal generation and applications in autonomous driving.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.14911": {
        "authors": [
            "Siyuan Yan",
            "Ming Hu",
            "Yiwen Jiang",
            "Xieji Li",
            "Hao Fei",
            "Philipp Tschandl",
            "Harald Kittler",
            "Zongyuan Ge"
        ],
        "title": "Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology",
        "abstract": "arXiv:2503.14911v1 Announce Type: new  Abstract: The emergence of vision-language models has transformed medical AI, enabling unprecedented advances in diagnostic capability and clinical applications. However, progress in dermatology has lagged behind other medical domains due to the lack of standard image-text pairs. Existing dermatological datasets are limited in both scale and depth, offering only single-label annotations across a narrow range of diseases instead of rich textual descriptions, and lacking the crucial clinical context needed for real-world applications. To address these limitations, we present Derm1M, the first large-scale vision-language dataset for dermatology, comprising 1,029,761 image-text pairs. Built from diverse educational resources and structured around a standard ontology collaboratively developed by experts, Derm1M provides comprehensive coverage for over 390 skin conditions across four hierarchical levels and 130 clinical concepts with rich contextual information such as medical history, symptoms, and skin tone. To demonstrate Derm1M potential in advancing both AI research and clinical application, we pretrained a series of CLIP-like models, collectively called DermLIP, on this dataset. The DermLIP family significantly outperforms state-of-the-art foundation models on eight diverse datasets across multiple tasks, including zero-shot skin disease classification, clinical and artifacts concept identification, few-shot/full-shot learning, and cross-modal retrieval. Our dataset and code will be public.",
        "arxiv_id": "2503.14911",
        "ARXIVID": "2503.14911",
        "COMMENT": "Matches criterion 4 as it introduces a large-scale vision-language dataset for dermatology, which is related to vision foundation models and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2503.15035": {
        "authors": [
            "Sungjae Lee",
            "Yeonjoo Hong",
            "Kwang In Kim"
        ],
        "title": "GraspCorrect: Robotic Grasp Correction via Vision-Language Model-Guided Feedback",
        "abstract": "arXiv:2503.15035v1 Announce Type: new  Abstract: Despite significant advancements in robotic manipulation, achieving consistent and stable grasping remains a fundamental challenge, often limiting the successful execution of complex tasks. Our analysis reveals that even state-of-the-art policy models frequently exhibit unstable grasping behaviors, leading to failure cases that create bottlenecks in real-world robotic applications. To address these challenges, we introduce GraspCorrect, a plug-and-play module designed to enhance grasp performance through vision-language model-guided feedback. GraspCorrect employs an iterative visual question-answering framework with two key components: grasp-guided prompting, which incorporates task-specific constraints, and object-aware sampling, which ensures the selection of physically feasible grasp candidates. By iteratively generating intermediate visual goals and translating them into joint-level actions, GraspCorrect significantly improves grasp stability and consistently enhances task success rates across existing policy models in the RLBench and CALVIN datasets.",
        "arxiv_id": "2503.15035",
        "ARXIVID": "2503.15035",
        "COMMENT": "Matches criterion 3 as it introduces a vision-language model-guided feedback mechanism for robotic grasp correction, which is relevant to embodied AI methods.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2503.15208": {
        "authors": [
            "Jiazhe Guo",
            "Yikang Ding",
            "Xiwu Chen",
            "Shuo Chen",
            "Bohan Li",
            "Yingshuang Zou",
            "Xiaoyang Lyu",
            "Feiyang Tan",
            "Xiaojuan Qi",
            "Zhiheng Li",
            "Hao Zhao"
        ],
        "title": "DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation",
        "abstract": "arXiv:2503.15208v1 Announce Type: new  Abstract: Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. A key challenge lies in finding an efficient and generalizable geometric representation that seamlessly connects temporal and spatial synthesis. To address this, we propose DiST-4D, the first disentangled spatiotemporal diffusion framework for 4D driving scene generation, which leverages metric depth as the core geometric representation. DiST-4D decomposes the problem into two diffusion processes: DiST-T, which predicts future metric depth and multi-view RGB sequences directly from past observations, and DiST-S, which enables spatial NVS by training only on existing viewpoints while enforcing cycle consistency. This cycle consistency mechanism introduces a forward-backward rendering constraint, reducing the generalization gap between observed and unseen viewpoints. Metric depth is essential for both accurate reliable forecasting and accurate spatial NVS, as it provides a view-consistent geometric representation that generalizes well to unseen perspectives. Experiments demonstrate that DiST-4D achieves state-of-the-art performance in both temporal prediction and NVS tasks, while also delivering competitive performance in planning-related evaluations.",
        "arxiv_id": "2503.15208",
        "ARXIVID": "2503.15208",
        "COMMENT": "Matches criterion 4 as it introduces a novel framework for 4D driving scene generation, which is related to vision foundation models and their applications.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2503.15284": {
        "authors": [
            "Yuanchao Yue",
            "Hui Yuan",
            "Qinglong Miao",
            "Xiaolong Mao",
            "Raouf Hamzaoui",
            "Peter Eisert"
        ],
        "title": "EdgeRegNet: Edge Feature-based Multimodal Registration Network between Images and LiDAR Point Clouds",
        "abstract": "arXiv:2503.15284v1 Announce Type: new  Abstract: Cross-modal data registration has long been a critical task in computer vision, with extensive applications in autonomous driving and robotics. Accurate and robust registration methods are essential for aligning data from different modalities, forming the foundation for multimodal sensor data fusion and enhancing perception systems' accuracy and reliability. The registration task between 2D images captured by cameras and 3D point clouds captured by Light Detection and Ranging (LiDAR) sensors is usually treated as a visual pose estimation problem. High-dimensional feature similarities from different modalities are leveraged to identify pixel-point correspondences, followed by pose estimation techniques using least squares methods. However, existing approaches often resort to downsampling the original point cloud and image data due to computational constraints, inevitably leading to a loss in precision. Additionally, high-dimensional features extracted using different feature extractors from various modalities require specific techniques to mitigate cross-modal differences for effective matching. To address these challenges, we propose a method that uses edge information from the original point clouds and images for cross-modal registration. We retain crucial information from the original data by extracting edge points and pixels, enhancing registration accuracy while maintaining computational efficiency. The use of edge points and edge pixels allows us to introduce an attention-based feature exchange block to eliminate cross-modal disparities. Furthermore, we incorporate an optimal matching layer to improve correspondence identification. We validate the accuracy of our method on the KITTI and nuScenes datasets, demonstrating its state-of-the-art performance.",
        "arxiv_id": "2503.15284",
        "ARXIVID": "2503.15284",
        "COMMENT": "Matches criterion 3 as it proposes a novel method for cross-modal registration between images and LiDAR point clouds, which is relevant to embodied AI and spatial understanding.",
        "RELEVANCE": 6,
        "NOVELTY": 7
    },
    "2503.14944": {
        "authors": [
            "Zihan Cao",
            "Yu Zhong",
            "Ziqi Wang",
            "Liang-Jian Deng"
        ],
        "title": "MMAIF: Multi-task and Multi-degradation All-in-One for Image Fusion with Language Guidance",
        "abstract": "arXiv:2503.14944v1 Announce Type: new  Abstract: Image fusion, a fundamental low-level vision task, aims to integrate multiple image sequences into a single output while preserving as much information as possible from the input. However, existing methods face several significant limitations: 1) requiring task- or dataset-specific models; 2) neglecting real-world image degradations (\\textit{e.g.}, noise), which causes failure when processing degraded inputs; 3) operating in pixel space, where attention mechanisms are computationally expensive; and 4) lacking user interaction capabilities. To address these challenges, we propose a unified framework for multi-task, multi-degradation, and language-guided image fusion. Our framework includes two key components: 1) a practical degradation pipeline that simulates real-world image degradations and generates interactive prompts to guide the model; 2) an all-in-one Diffusion Transformer (DiT) operating in latent space, which fuses a clean image conditioned on both the degraded inputs and the generated prompts. Furthermore, we introduce principled modifications to the original DiT architecture to better suit the fusion task. Based on this framework, we develop two versions of the model: Regression-based and Flow Matching-based variants. Extensive qualitative and quantitative experiments demonstrate that our approach effectively addresses the aforementioned limitations and outperforms previous restoration+fusion and all-in-one pipelines. Codes are available at https://github.com/294coder/MMAIF.",
        "arxiv_id": "2503.14944",
        "ARXIVID": "2503.14944",
        "COMMENT": "This paper matches criterion 2 as it introduces a multi-modal framework for image fusion with language guidance, which aligns with VLLMs/MLLMs.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.15019": {
        "authors": [
            "Shengqiong Wu",
            "Hao Fei",
            "Jingkang Yang",
            "Xiangtai Li",
            "Juncheng Li",
            "Hanwang Zhang",
            "Tat-seng Chua"
        ],
        "title": "Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene",
        "abstract": "arXiv:2503.15019v1 Announce Type: new  Abstract: The latest emerged 4D Panoptic Scene Graph (4D-PSG) provides an advanced-ever representation for comprehensively modeling the dynamic 4D visual real world. Unfortunately, current pioneering 4D-PSG research can primarily suffer from data scarcity issues severely, as well as the resulting out-of-vocabulary problems; also, the pipeline nature of the benchmark generation method can lead to suboptimal performance. To address these challenges, this paper investigates a novel framework for 4D-PSG generation that leverages rich 2D visual scene annotations to enhance 4D scene learning. First, we introduce a 4D Large Language Model (4D-LLM) integrated with a 3D mask decoder for end-to-end generation of 4D-PSG. A chained SG inference mechanism is further designed to exploit LLMs' open-vocabulary capabilities to infer accurate and comprehensive object and relation labels iteratively. Most importantly, we propose a 2D-to-4D visual scene transfer learning framework, where a spatial-temporal scene transcending strategy effectively transfers dimension-invariant features from abundant 2D SG annotations to 4D scenes, effectively compensating for data scarcity in 4D-PSG. Extensive experiments on the benchmark data demonstrate that we strikingly outperform baseline models by a large margin, highlighting the effectiveness of our method.",
        "arxiv_id": "2503.15019",
        "ARXIVID": "2503.15019",
        "COMMENT": "Matches criterion 3 as it proposes a novel framework for 4D panoptic scene graph generation, addressing data scarcity and leveraging 2D annotations.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.15485": {
        "authors": [
            "Zineng Tang",
            "Long Lian",
            "Seun Eisape",
            "XuDong Wang",
            "Roei Herzig",
            "Adam Yala",
            "Alane Suhr",
            "Trevor Darrell",
            "David M. Chan"
        ],
        "title": "TULIP: Towards Unified Language-Image Pretraining",
        "abstract": "arXiv:2503.15485v1 Announce Type: new  Abstract: Despite the recent success of image-text contrastive models like CLIP and SigLIP, these models often struggle with vision-centric tasks that demand high-fidelity image understanding, such as counting, depth estimation, and fine-grained object recognition. These models, by performing language alignment, tend to prioritize high-level semantics over visual understanding, weakening their image understanding. On the other hand, vision-focused models are great at processing visual information but struggle to understand language, limiting their flexibility for language-driven tasks. In this work, we introduce TULIP, an open-source, drop-in replacement for existing CLIP-like models. Our method leverages generative data augmentation, enhanced image-image and text-text contrastive learning, and image/text reconstruction regularization to learn fine-grained visual features while preserving global semantic alignment. Our approach, scaling to over 1B parameters, outperforms existing state-of-the-art (SOTA) models across multiple benchmarks, establishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to a $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot classification, and improving vision-language models, achieving over $3\\times$ higher scores than SigLIP on MMVP. Our code/checkpoints are available at https://tulip-berkeley.github.io",
        "arxiv_id": "2503.15485",
        "ARXIVID": "2503.15485",
        "COMMENT": "Matches criterion 4 as it introduces a vision foundation model (TULIP) with applications in fine-grained visual tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.14698": {
        "authors": [
            "Yiming Wang",
            "Lucy Chai",
            "Xuan Luo",
            "Michael Niemeyer",
            "Manuel Lagunas",
            "Stephen Lombardi",
            "Siyu Tang",
            "Tiancheng Sun"
        ],
        "title": "SplatVoxel: History-Aware Novel View Streaming without Temporal Training",
        "abstract": "arXiv:2503.14698v1 Announce Type: new  Abstract: We study the problem of novel view streaming from sparse-view videos, which aims to generate a continuous sequence of high-quality, temporally consistent novel views as new input frames arrive. However, existing novel view synthesis methods struggle with temporal coherence and visual fidelity, leading to flickering and inconsistency. To address these challenges, we introduce history-awareness, leveraging previous frames to reconstruct the scene and improve quality and stability. We propose a hybrid splat-voxel feed-forward scene reconstruction approach that combines Gaussian Splatting to propagate information over time, with a hierarchical voxel grid for temporal fusion. Gaussian primitives are efficiently warped over time using a motion graph that extends 2D tracking models to 3D motion, while a sparse voxel transformer integrates new temporal observations in an error-aware manner. Crucially, our method does not require training on multi-view video datasets, which are currently limited in size and diversity, and can be directly applied to sparse-view video streams in a history-aware manner at inference time. Our approach achieves state-of-the-art performance in both static and streaming scene reconstruction, effectively reducing temporal artifacts and visual artifacts while running at interactive rates (15 fps with 350ms delay) on a single H100 GPU. Project Page: https://19reborn.github.io/SplatVoxel/",
        "arxiv_id": "2503.14698",
        "ARXIVID": "2503.14698",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for scene reconstruction in embodied AI, focusing on temporal coherence and streaming applications.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.15369": {
        "authors": [
            "Yinan Liang",
            "Ziwei Wang",
            "Xiuwei Xu",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "title": "EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language Models",
        "abstract": "arXiv:2503.15369v1 Announce Type: new  Abstract: While multimodal large language models demonstrate strong performance in complex reasoning tasks, they pose significant challenges related to model complexity during deployment, especially for resource-limited devices. In this paper, we propose an automatic pruning method for large vision-language models to enhance the efficiency of multimodal reasoning. Conventional methods rely on the training data of the original model to select the proper pruning ratio for different network components. However, these methods are impractical for large vision-language models due to the unaffordable search costs caused by web-scale training corpus. In contrast, our approach only leverages a small number of samples to search for the desired pruning policy by maximizing its generalization ability on unknown training data while maintaining the model accuracy, which enables the achievement of an optimal trade-off between accuracy and efficiency for large visual language models. Specifically, we formulate the generalization gap of the pruning strategy using the structural risk minimization principle. Based on both task performance and generalization capability, we iteratively search for the optimal pruning policy within a given search space and optimize the vision projector to evolve the search space with higher upper bound of performance. We conduct extensive experiments on the ScienceQA, Vizwiz, MM-vet, and LLaVA-Bench datasets for the task of visual question answering. Using only 64 samples for pruning policy search, EfficientLLaVA achieves an accuracy of 83.05% on ScienceQA, along with a $\\times$ 1.8 speedup compared to the dense LLaVA-v1.5-7B model.",
        "arxiv_id": "2503.15369",
        "ARXIVID": "2503.15369",
        "COMMENT": "This paper proposes an auto-pruning method for large vision-language models, which aligns with criterion 2.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.15412": {
        "authors": [
            "Fereshteh Forghani",
            "Jason J. Yu",
            "Tristan Aumentado-Armstrong",
            "Konstantinos G. Derpanis",
            "Marcus A. Brubaker"
        ],
        "title": "Learn Your Scales: Towards Scale-Consistent Generative Novel View Synthesis",
        "abstract": "arXiv:2503.15412v1 Announce Type: new  Abstract: Conventional depth-free multi-view datasets are captured using a moving monocular camera without metric calibration. The scales of camera positions in this monocular setting are ambiguous. Previous methods have acknowledged scale ambiguity in multi-view data via various ad-hoc normalization pre-processing steps, but have not directly analyzed the effect of incorrect scene scales on their application. In this paper, we seek to understand and address the effect of scale ambiguity when used to train generative novel view synthesis methods (GNVS). In GNVS, new views of a scene or object can be minimally synthesized given a single image and are, thus, unconstrained, necessitating the use of generative methods. The generative nature of these models captures all aspects of uncertainty, including any uncertainty of scene scales, which act as nuisance variables for the task. We study the effect of scene scale ambiguity in GNVS when sampled from a single image by isolating its effect on the resulting models and, based on these intuitions, define new metrics that measure the scale inconsistency of generated views. We then propose a framework to estimate scene scales jointly with the GNVS model in an end-to-end fashion. Empirically, we show that our method reduces the scale inconsistency of generated views without the complexity or downsides of previous scale normalization methods. Further, we show that removing this ambiguity improves generated image quality of the resulting GNVS model.",
        "arxiv_id": "2503.15412",
        "ARXIVID": "2503.15412",
        "COMMENT": "This paper addresses scale ambiguity in generative novel view synthesis, which is relevant to vision foundation models and their applications (criterion 4).",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.15106": {
        "authors": [
            "Amir Hamza",
            "Andrea Caraffa",
            "Davide Boscaini",
            "Fabio Poiesi"
        ],
        "title": "Distilling 3D distinctive local descriptors for 6D pose estimation",
        "abstract": "arXiv:2503.15106v1 Announce Type: new  Abstract: Three-dimensional local descriptors are crucial for encoding geometric surface properties, making them essential for various point cloud understanding tasks. Among these descriptors, GeDi has demonstrated strong zero-shot 6D pose estimation capabilities but remains computationally impractical for real-world applications due to its expensive inference process. \\textit{Can we retain GeDi's effectiveness while significantly improving its efficiency?} In this paper, we explore this question by introducing a knowledge distillation framework that trains an efficient student model to regress local descriptors from a GeDi teacher. Our key contributions include: an efficient large-scale training procedure that ensures robustness to occlusions and partial observations while operating under compute and storage constraints, and a novel loss formulation that handles weak supervision from non-distinctive teacher descriptors. We validate our approach on five BOP Benchmark datasets and demonstrate a significant reduction in inference time while maintaining competitive performance with existing methods, bringing zero-shot 6D pose estimation closer to real-time feasibility. Project Website: https://tev-fbk.github.io/dGeDi/",
        "arxiv_id": "2503.15106",
        "ARXIVID": "2503.15106",
        "COMMENT": "This paper introduces a knowledge distillation framework for efficient 3D local descriptors in 6D pose estimation, which is relevant to spatial understanding (criterion 1).",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.14973": {
        "authors": [
            "Rishav Rishav",
            "Somjit Nath",
            "Vincent Michalski",
            "Samira Ebrahimi Kahou"
        ],
        "title": "Behaviour Discovery and Attribution for Explainable Reinforcement Learning",
        "abstract": "arXiv:2503.14973v1 Announce Type: new  Abstract: Explaining the decisions made by reinforcement learning (RL) agents is critical for building trust and ensuring reliability in real-world applications. Traditional approaches to explainability often rely on saliency analysis, which can be limited in providing actionable insights. Recently, there has been growing interest in attributing RL decisions to specific trajectories within a dataset. However, these methods often generalize explanations to long trajectories, potentially involving multiple distinct behaviors. Often, providing multiple more fine grained explanations would improve clarity. In this work, we propose a framework for behavior discovery and action attribution to behaviors in offline RL trajectories. Our method identifies meaningful behavioral segments, enabling more precise and granular explanations associated with high level agent behaviors. This approach is adaptable across diverse environments with minimal modifications, offering a scalable and versatile solution for behavior discovery and attribution for explainable RL.",
        "arxiv_id": "2503.14973",
        "ARXIVID": "2503.14973",
        "COMMENT": "This paper focuses on explainable reinforcement learning by proposing a framework for behavior discovery and action attribution, which aligns with criterion 3 on novel methods in embodied AI.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.15337": {
        "authors": [
            "Hao Tan",
            "Zichang Tan",
            "Jun Li",
            "Ajian Liu",
            "Jun Wan",
            "Zhen Lei"
        ],
        "title": "Recover and Match: Open-Vocabulary Multi-Label Recognition through Knowledge-Constrained Optimal Transport",
        "abstract": "arXiv:2503.15337v1 Announce Type: new  Abstract: Identifying multiple novel classes in an image, known as open-vocabulary multi-label recognition, is a challenging task in computer vision. Recent studies explore the transfer of powerful vision-language models such as CLIP. However, these approaches face two critical challenges: (1) The local semantics of CLIP are disrupted due to its global pre-training objectives, resulting in unreliable regional predictions. (2) The matching property between image regions and candidate labels has been neglected, relying instead on naive feature aggregation such as average pooling, which leads to spurious predictions from irrelevant regions. In this paper, we present RAM (Recover And Match), a novel framework that effectively addresses the above issues. To tackle the first problem, we propose Ladder Local Adapter (LLA) to enforce refocusing on local regions, recovering local semantics in a memory-friendly way. For the second issue, we propose Knowledge-Constrained Optimal Transport (KCOT) to suppress meaningless matching to non-GT labels by formulating the task as an optimal transport problem. As a result, RAM achieves state-of-the-art performance on various datasets from three distinct domains, and shows great potential to boost the existing methods. Code: https://github.com/EricTan7/RAM.",
        "arxiv_id": "2503.15337",
        "ARXIVID": "2503.15337",
        "COMMENT": "This paper matches criterion 4 as it leverages vision-language models (CLIP) for open-vocabulary multi-label recognition, which is related to vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.14935": {
        "authors": [
            "Chongjun Tu",
            "Lin Zhang",
            "Pengtao Chen",
            "Peng Ye",
            "Xianfang Zeng",
            "Wei Cheng",
            "Gang Yu",
            "Tao Chen"
        ],
        "title": "FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion Understanding",
        "abstract": "arXiv:2503.14935v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable capabilities in video content understanding but still struggle with fine-grained motion comprehension. To comprehensively assess the motion understanding ability of existing MLLMs, we introduce FAVOR-Bench, comprising 1,776 videos with structured manual annotations of various motions. Our benchmark includes both close-ended and open-ended tasks. For close-ended evaluation, we carefully design 8,184 multiple-choice question-answer pairs spanning six distinct sub-tasks. For open-ended evaluation, we develop both a novel cost-efficient LLM-free and a GPT-assisted caption assessment method, where the former can enhance benchmarking interpretability and reproducibility. Comprehensive experiments with 21 state-of-the-art MLLMs reveal significant limitations in their ability to comprehend and describe detailed temporal dynamics in video motions. To alleviate this limitation, we further build FAVOR-Train, a dataset consisting of 17,152 videos with fine-grained motion annotations. The results of finetuning Qwen2.5-VL on FAVOR-Train yield consistent improvements on motion-related tasks of TVBench, MotionBench and our FAVOR-Bench. Comprehensive assessment results demonstrate that the proposed FAVOR-Bench and FAVOR-Train provide valuable tools to the community for developing more powerful video understanding models. Project page: \\href{https://favor-bench.github.io/}{https://favor-bench.github.io/}.",
        "arxiv_id": "2503.14935",
        "ARXIVID": "2503.14935",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (FAVOR-Bench) for fine-grained video motion understanding, focusing on multimodal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.14665": {
        "authors": [
            "Parker Ewen",
            "Hao Chen",
            "Seth Isaacson",
            "Joey Wilson",
            "Katherine A. Skinner",
            "Ram Vasudevan"
        ],
        "title": "These Magic Moments: Differentiable Uncertainty Quantification of Radiance Field Models",
        "abstract": "arXiv:2503.14665v1 Announce Type: new  Abstract: This paper introduces a novel approach to uncertainty quantification for radiance fields by leveraging higher-order moments of the rendering equation. Uncertainty quantification is crucial for downstream tasks including view planning and scene understanding, where safety and robustness are paramount. However, the high dimensionality and complexity of radiance fields pose significant challenges for uncertainty quantification, limiting the use of these uncertainty quantification methods in high-speed decision-making. We demonstrate that the probabilistic nature of the rendering process enables efficient and differentiable computation of higher-order moments for radiance field outputs, including color, depth, and semantic predictions. Our method outperforms existing radiance field uncertainty estimation techniques while offering a more direct, computationally efficient, and differentiable formulation without the need for post-processing.Beyond uncertainty quantification, we also illustrate the utility of our approach in downstream applications such as next-best-view (NBV) selection and active ray sampling for neural radiance field training. Extensive experiments on synthetic and real-world scenes confirm the efficacy of our approach, which achieves state-of-the-art performance while maintaining simplicity.",
        "arxiv_id": "2503.14665",
        "ARXIVID": "2503.14665",
        "COMMENT": "Matches criterion 3 as it introduces a novel uncertainty quantification method for radiance fields, relevant to embodied AI and scene understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.15166": {
        "authors": [
            "\\`Alex Pujol Vidal",
            "Sergio Escalera",
            "Kamal Nasrollahi",
            "Thomas B. Moeslund"
        ],
        "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU",
        "abstract": "arXiv:2503.15166v1 Announce Type: new  Abstract: Machine unlearning methods have become increasingly important for selective concept removal in large pre-trained models. While recent work has explored unlearning in Euclidean contrastive vision-language models, the effectiveness of concept removal in hyperbolic spaces remains unexplored. This paper investigates machine unlearning in hyperbolic contrastive learning by adapting Alignment Calibration to MERU, a model that embeds images and text in hyperbolic space to better capture semantic hierarchies. Through systematic experiments and ablation studies, we demonstrate that hyperbolic geometry offers distinct advantages for concept removal, achieving near perfect forgetting with reasonable performance on retained concepts, particularly when scaling to multiple concept removal. Our approach introduces hyperbolic-specific components including entailment calibration and norm regularization that leverage the unique properties of hyperbolic space. Comparative analysis with Euclidean models reveals fundamental differences in unlearning dynamics, with hyperbolic unlearning reorganizing the semantic hierarchy while Euclidean approaches merely disconnect cross-modal associations. These findings not only advance machine unlearning techniques but also provide insights into the geometric properties that influence concept representation and removal in multimodal models. Source code available at https://github.com/alex-pv01/HAC",
        "arxiv_id": "2503.15166",
        "ARXIVID": "2503.15166",
        "COMMENT": "Matches criterion 2 as it explores multimodal contrastive learning in hyperbolic spaces, which is relevant to vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.14895": {
        "authors": [
            "Shuo Li",
            "Jiajun Sun",
            "Guodong Zheng",
            "Xiaoran Fan",
            "Yujiong Shen",
            "Yi Lu",
            "Zhiheng Xi",
            "Yuming Yang",
            "Wenming Tan",
            "Tao Ji",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations",
        "abstract": "arXiv:2503.14895v1 Announce Type: new  Abstract: Recently, multimodal large language models (MLLMs) have demonstrated remarkable performance in visual-language tasks. However, the authenticity of the responses generated by MLLMs is often compromised by object hallucinations. We identify that a key cause of these hallucinations is the model's over-susceptibility to specific image frequency features in detecting objects. In this paper, we introduce Multi-Frequency Perturbations (MFP), a simple, cost-effective, and pluggable method that leverages both low-frequency and high-frequency features of images to perturb visual feature representations and explicitly suppress redundant frequency-domain features during inference, thereby mitigating hallucinations. Experimental results demonstrate that our method significantly mitigates object hallucinations across various model architectures. Furthermore, as a training-time method, MFP can be combined with inference-time methods to achieve state-of-the-art performance on the CHAIR benchmark.",
        "arxiv_id": "2503.14895",
        "ARXIVID": "2503.14895",
        "COMMENT": "Matches criterion 2 as it addresses object hallucinations in multimodal large language models (MLLMs) with a novel perturbation method.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.14953": {
        "authors": [
            "Yang Liu",
            "Wentao Feng",
            "Zhuoyao Liu",
            "Shudong Huang",
            "Jiancheng Lv"
        ],
        "title": "Aligning Information Capacity Between Vision and Language via Dense-to-Sparse Feature Distillation for Image-Text Matching",
        "abstract": "arXiv:2503.14953v1 Announce Type: new  Abstract: Enabling Visual Semantic Models to effectively handle multi-view description matching has been a longstanding challenge. Existing methods typically learn a set of embeddings to find the optimal match for each view's text and compute similarity. However, the visual and text embeddings learned through these approaches have limited information capacity and are prone to interference from locally similar negative samples. To address this issue, we argue that the information capacity of embeddings is crucial and propose Dense-to-Sparse Feature Distilled Visual Semantic Embedding (D2S-VSE), which enhances the information capacity of sparse text by leveraging dense text distillation. Specifically, D2S-VSE is a two-stage framework. In the pre-training stage, we align images with dense text to enhance the information capacity of visual semantic embeddings. In the fine-tuning stage, we optimize two tasks simultaneously, distilling dense text embeddings to sparse text embeddings while aligning images and sparse texts, enhancing the information capacity of sparse text embeddings. Our proposed D2S-VSE model is extensively evaluated on the large-scale MS-COCO and Flickr30K datasets, demonstrating its superiority over recent state-of-the-art methods.",
        "arxiv_id": "2503.14953",
        "ARXIVID": "2503.14953",
        "COMMENT": "Matches criterion 2 as it discusses improvements in visual-language models (VLLMs) for image-text matching.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.15264": {
        "authors": [
            "Hengrui Kang",
            "Siwei Wen",
            "Zichen Wen",
            "Junyan Ye",
            "Weijia Li",
            "Peilin Feng",
            "Baichuan Zhou",
            "Bin Wang",
            "Dahua Lin",
            "Linfeng Zhang",
            "Conghui He"
        ],
        "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection",
        "abstract": "arXiv:2503.15264v1 Announce Type: new  Abstract: The rapid advancements in generative technology have emerged as a double-edged sword. While offering powerful tools that enhance convenience, they also pose significant social concerns. As defenders, current synthetic image detection methods often lack artifact-level textual interpretability and are overly focused on image manipulation detection, and current datasets usually suffer from outdated generators and a lack of fine-grained annotations. In this paper, we introduce SynthScars, a high-quality and diverse dataset consisting of 12,236 fully synthetic images with human-expert annotations. It features 4 distinct image content types, 3 categories of artifacts, and fine-grained annotations covering pixel-level segmentation, detailed textual explanations, and artifact category labels. Furthermore, we propose LEGION (LEarning to Ground and explain for Synthetic Image detectiON), a multimodal large language model (MLLM)-based image forgery analysis framework that integrates artifact detection, segmentation, and explanation. Building upon this capability, we further explore LEGION as a controller, integrating it into image refinement pipelines to guide the generation of higher-quality and more realistic images. Extensive experiments show that LEGION outperforms existing methods across multiple benchmarks, particularly surpassing the second-best traditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score. Moreover, the refined images generated under its guidance exhibit stronger alignment with human preferences. The code, model, and dataset will be released.",
        "arxiv_id": "2503.15264",
        "ARXIVID": "2503.15264",
        "COMMENT": "This paper proposes a multimodal framework for synthetic image detection, which aligns with criterion 2.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.14547": {
        "authors": [
            "Shuheng Li",
            "Jiayun Zhang",
            "Xiaohan Fu",
            "Xiyuan Zhang",
            "Jingbo Shang",
            "Rajesh K. Gupta"
        ],
        "title": "Matching Skeleton-based Activity Representations with Heterogeneous Signals for HAR",
        "abstract": "arXiv:2503.14547v1 Announce Type: new  Abstract: In human activity recognition (HAR), activity labels have typically been encoded in one-hot format, which has a recent shift towards using textual representations to provide contextual knowledge. Here, we argue that HAR should be anchored to physical motion data, as motion forms the basis of activity and applies effectively across sensing systems, whereas text is inherently limited. We propose SKELAR, a novel HAR framework that pretrains activity representations from skeleton data and matches them with heterogeneous HAR signals. Our method addresses two major challenges: (1) capturing core motion knowledge without context-specific details. We achieve this through a self-supervised coarse angle reconstruction task that recovers joint rotation angles, invariant to both users and deployments; (2) adapting the representations to downstream tasks with varying modalities and focuses. To address this, we introduce a self-attention matching module that dynamically prioritizes relevant body parts in a data-driven manner. Given the lack of corresponding labels in existing skeleton data, we establish MASD, a new HAR dataset with IMU, WiFi, and skeleton, collected from 20 subjects performing 27 activities. This is the first broadly applicable HAR dataset with time-synchronized data across three modalities. Experiments show that SKELAR achieves the state-of-the-art performance in both full-shot and few-shot settings. We also demonstrate that SKELAR can effectively leverage synthetic skeleton data to extend its use in scenarios without skeleton collections.",
        "arxiv_id": "2503.14547",
        "ARXIVID": "2503.14547",
        "COMMENT": "This paper proposes SKELAR, a framework for human activity recognition using skeleton data, which is tangentially related to criterion 1 on spatial understanding.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.15435": {
        "authors": [
            "Baolu Li",
            "Zongzhe Xu",
            "Jinlong Li",
            "Xinyu Liu",
            "Jianwu Fang",
            "Xiaopeng Li",
            "Hongkai Yu"
        ],
        "title": "V2X-DG: Domain Generalization for Vehicle-to-Everything Cooperative Perception",
        "abstract": "arXiv:2503.15435v1 Announce Type: new  Abstract: LiDAR-based Vehicle-to-Everything (V2X) cooperative perception has demonstrated its impact on the safety and effectiveness of autonomous driving. Since current cooperative perception algorithms are trained and tested on the same dataset, the generalization ability of cooperative perception systems remains underexplored. This paper is the first work to study the Domain Generalization problem of LiDAR-based V2X cooperative perception (V2X-DG) for 3D detection based on four widely-used open source datasets: OPV2V, V2XSet, V2V4Real and DAIR-V2X. Our research seeks to sustain high performance not only within the source domain but also across other unseen domains, achieved solely through training on source domain. To this end, we propose Cooperative Mixup Augmentation based Generalization (CMAG) to improve the model generalization capability by simulating the unseen cooperation, which is designed compactly for the domain gaps in cooperative perception. Furthermore, we propose a constraint for the regularization of the robust generalized feature representation learning: Cooperation Feature Consistency (CFC), which aligns the intermediately fused features of the generalized cooperation by CMAG and the early fused features of the original cooperation in source domain. Extensive experiments demonstrate that our approach achieves significant performance gains when generalizing to other unseen datasets while it also maintains strong performance on the source dataset.",
        "arxiv_id": "2503.15435",
        "ARXIVID": "2503.15435",
        "COMMENT": "This paper proposes a domain generalization method for LiDAR-based V2X cooperative perception, which is tangentially related to criterion 3 on novel benchmarks or methods in embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.15060": {
        "authors": [
            "Imanol G. Estepa",
            "Jes\\'us M. Rodr\\'iguez-de-Vera",
            "Ignacio Saras\\'ua",
            "Bhalaji Nagarajan",
            "Petia Radeva"
        ],
        "title": "Conjuring Positive Pairs for Efficient Unification of Representation Learning and Image Synthesis",
        "abstract": "arXiv:2503.15060v1 Announce Type: new  Abstract: While representation learning and generative modeling seek to understand visual data, unifying both domains remains unexplored. Recent Unified Self-Supervised Learning (SSL) methods have started to bridge the gap between both paradigms. However, they rely solely on semantic token reconstruction, which requires an external tokenizer during training -- introducing a significant overhead. In this work, we introduce Sorcen, a novel unified SSL framework, incorporating a synergic Contrastive-Reconstruction objective. Our Contrastive objective, \"Echo Contrast\", leverages the generative capabilities of Sorcen, eliminating the need for additional image crops or augmentations during training. Sorcen \"generates\" an echo sample in the semantic token space, forming the contrastive positive pair. Sorcen operates exclusively on precomputed tokens, eliminating the need for an online token transformation during training, thereby significantly reducing computational overhead. Extensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the previous Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear probing, unconditional image generation, few-shot learning, and transfer learning, respectively, while being 60.8% more efficient. Additionally, Sorcen surpasses previous single-crop MIM SoTA in linear probing and achieves SoTA performance in unconditional image generation, highlighting significant improvements and breakthroughs in Unified SSL models.",
        "arxiv_id": "2503.15060",
        "ARXIVID": "2503.15060",
        "COMMENT": "Matches criterion 1 as it introduces a novel unified SSL framework for representation learning and image synthesis, which could be relevant to spatial intelligence in embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.15417": {
        "authors": [
            "Harold Haodong Chen",
            "Haojian Huang",
            "Xianfeng Wu",
            "Yexin Liu",
            "Yajing Bai",
            "Wen-Jie Shu",
            "Harry Yang",
            "Ser-Nam Lim"
        ],
        "title": "Temporal Regularization Makes Your Video Generator Stronger",
        "abstract": "arXiv:2503.15417v1 Announce Type: new  Abstract: Temporal quality is a critical aspect of video generation, as it ensures consistent motion and realistic dynamics across frames. However, achieving high temporal coherence and diversity remains challenging. In this work, we explore temporal augmentation in video generation for the first time, and introduce FluxFlow for initial investigation, a strategy designed to enhance temporal quality. Operating at the data level, FluxFlow applies controlled temporal perturbations without requiring architectural modifications. Extensive experiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow significantly improves temporal coherence and diversity across various video generation models, including U-Net, DiT, and AR-based architectures, while preserving spatial fidelity. These findings highlight the potential of temporal augmentation as a simple yet effective approach to advancing video generation quality.",
        "arxiv_id": "2503.15417",
        "ARXIVID": "2503.15417",
        "COMMENT": "This paper does not match any specific criteria but explores temporal augmentation in video generation, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.14928": {
        "authors": [
            "Jiaxin Ye",
            "Hongming Shan"
        ],
        "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video",
        "abstract": "arXiv:2503.14928v1 Announce Type: new  Abstract: Vision-guided speech generation aims to produce authentic speech from facial appearance or lip motions without relying on auditory signals, offering significant potential for applications such as dubbing in filmmaking and assisting individuals with aphonia. Despite recent progress, existing methods struggle to achieve unified cross-modal alignment across semantics, timbre, and emotional prosody from visual cues, prompting us to propose Consistent Video-to-Speech (CV2S) as an extended task to enhance cross-modal consistency. To tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal diffusion framework that generates faithful speech using only visual input, operating within a discrete space. Specifically, we propose a discrete lip aligner that predicts discrete speech tokens from lip videos to capture semantic information, while an error detector identifies misaligned tokens, which are subsequently refined through masked language modeling with BERT. To further enhance the expressiveness of the generated speech, we develop a style diffusion transformer equipped with a face-style adapter that adaptively customizes identity and prosody dynamics across both the channel and temporal dimensions while ensuring synchronization with lip-aware semantic features. Extensive experiments demonstrate that ImaginTalk can generate high-fidelity speech with more accurate semantic details and greater expressiveness in timbre and emotion compared to state-of-the-art baselines. Demos are shown at our project page: https://imagintalk.github.io.",
        "arxiv_id": "2503.14928",
        "ARXIVID": "2503.14928",
        "COMMENT": "This paper introduces a cross-modal diffusion framework for vision-guided speech generation, which does not directly match any specific criterion but is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.14604": {
        "authors": [
            "Sara Sarto",
            "Marcella Cornia",
            "Rita Cucchiara"
        ],
        "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives",
        "abstract": "arXiv:2503.14604v1 Announce Type: new  Abstract: The evaluation of machine-generated image captions is a complex and evolving challenge. With the advent of Multimodal Large Language Models (MLLMs), image captioning has become a core task, increasing the need for robust and reliable evaluation metrics. This survey provides a comprehensive overview of advancements in image captioning evaluation, analyzing the evolution, strengths, and limitations of existing metrics. We assess these metrics across multiple dimensions, including correlation with human judgment, ranking accuracy, and sensitivity to hallucinations. Additionally, we explore the challenges posed by the longer and more detailed captions generated by MLLMs and examine the adaptability of current metrics to these stylistic variations. Our analysis highlights some limitations of standard evaluation approaches and suggests promising directions for future research in image captioning assessment.",
        "arxiv_id": "2503.14604",
        "ARXIVID": "2503.14604",
        "COMMENT": "This paper discusses evaluation metrics for image captioning in the context of MLLMs, which aligns with criterion 2.",
        "RELEVANCE": 5,
        "NOVELTY": 4
    },
    "2503.14979": {
        "authors": [
            "Yaxiong Chen",
            "Junjian Hu",
            "Chunlei Li",
            "Zixuan Zheng",
            "Jingliang Hu",
            "Yilei Shi",
            "Shengwu Xiong",
            "Xiao Xiang Zhu",
            "Lichao Mou"
        ],
        "title": "One-Shot Medical Video Object Segmentation via Temporal Contrastive Memory Networks",
        "abstract": "arXiv:2503.14979v1 Announce Type: new  Abstract: Video object segmentation is crucial for the efficient analysis of complex medical video data, yet it faces significant challenges in data availability and annotation. We introduce the task of one-shot medical video object segmentation, which requires separating foreground and background pixels throughout a video given only the mask annotation of the first frame. To address this problem, we propose a temporal contrastive memory network comprising image and mask encoders to learn feature representations, a temporal contrastive memory bank that aligns embeddings from adjacent frames while pushing apart distant ones to explicitly model inter-frame relationships and stores these features, and a decoder that fuses encoded image features and memory readouts for segmentation. We also collect a diverse, multi-source medical video dataset spanning various modalities and anatomies to benchmark this task. Extensive experiments demonstrate state-of-the-art performance in segmenting both seen and unseen structures from a single exemplar, showing ability to generalize from scarce labels. This highlights the potential to alleviate annotation burdens for medical video analysis. Code is available at https://github.com/MedAITech/TCMN.",
        "arxiv_id": "2503.14979",
        "ARXIVID": "2503.14979",
        "COMMENT": "This paper introduces a temporal contrastive memory network for one-shot medical video object segmentation, which is tangentially related to criterion 4 on vision foundation models and applications.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2503.14958": {
        "authors": [
            "Zixuan Zheng",
            "Yilei Shi",
            "Chunlei Li",
            "Jingliang Hu",
            "Xiao Xiang Zhu",
            "Lichao Mou"
        ],
        "title": "Reducing Annotation Burden: Exploiting Image Knowledge for Few-Shot Medical Video Object Segmentation via Spatiotemporal Consistency Relearning",
        "abstract": "arXiv:2503.14958v1 Announce Type: new  Abstract: Few-shot video object segmentation aims to reduce annotation costs; however, existing methods still require abundant dense frame annotations for training, which are scarce in the medical domain. We investigate an extremely low-data regime that utilizes annotations from only a few video frames and leverages existing labeled images to minimize costly video annotations. Specifically, we propose a two-phase framework. First, we learn a few-shot segmentation model using labeled images. Subsequently, to improve performance without full supervision, we introduce a spatiotemporal consistency relearning approach on medical videos that enforces consistency between consecutive frames. Constraints are also enforced between the image model and relearning model at both feature and prediction levels. Experiments demonstrate the superiority of our approach over state-of-the-art few-shot segmentation methods. Our model bridges the gap between abundant annotated medical images and scarce, sparsely labeled medical videos to achieve strong video segmentation performance in this low data regime. Code is available at https://github.com/MedAITech/RAB.",
        "arxiv_id": "2503.14958",
        "ARXIVID": "2503.14958",
        "COMMENT": "This paper proposes a framework for few-shot medical video object segmentation, which is tangentially related to criterion 4 on vision foundation models and applications.",
        "RELEVANCE": 4,
        "NOVELTY": 5
    },
    "2503.14910": {
        "authors": [
            "Jingyi Liao",
            "Xun Xu",
            "Yongyi Su",
            "Rong-Cheng Tu",
            "Yifan Liu",
            "Dacheng Tao",
            "Xulei Yang"
        ],
        "title": "Robust Distribution Alignment for Industrial Anomaly Detection under Distribution Shift",
        "abstract": "arXiv:2503.14910v1 Announce Type: new  Abstract: Anomaly detection plays a crucial role in quality control for industrial applications. However, ensuring robustness under unseen domain shifts such as lighting variations or sensor drift remains a significant challenge. Existing methods attempt to address domain shifts by training generalizable models but often rely on prior knowledge of target distributions and can hardly generalise to backbones designed for other data modalities. To overcome these limitations, we build upon memory-bank-based anomaly detection methods, optimizing a robust Sinkhorn distance on limited target training data to enhance generalization to unseen target domains. We evaluate the effectiveness on both 2D and 3D anomaly detection benchmarks with simulated distribution shifts. Our proposed method demonstrates superior results compared with state-of-the-art anomaly detection and domain adaptation methods.",
        "arxiv_id": "2503.14910",
        "ARXIVID": "2503.14910",
        "COMMENT": "Does not closely match any specific criterion but is related to anomaly detection, which is tangentially relevant to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.14957": {
        "authors": [
            "Thanh-Son Nguyen",
            "Hong Yang",
            "Tzeh Yuan Neoh",
            "Hao Zhang",
            "Ee Yeo Keat",
            "Basura Fernando"
        ],
        "title": "Neuro Symbolic Knowledge Reasoning for Procedural Video Question Answering",
        "abstract": "arXiv:2503.14957v1 Announce Type: new  Abstract: This paper introduces a new video question-answering (VQA) dataset that challenges models to leverage procedural knowledge for complex reasoning. It requires recognizing visual entities, generating hypotheses, and performing contextual, causal, and counterfactual reasoning. To address this, we propose neuro symbolic reasoning module that integrates neural networks and LLM-driven constrained reasoning over variables for interpretable answer generation. Results show that combining LLMs with structured knowledge reasoning with logic enhances procedural reasoning on the STAR benchmark and our dataset. Code and dataset at https://github.com/LUNAProject22/KML soon.",
        "arxiv_id": "2503.14957",
        "ARXIVID": "2503.14957",
        "COMMENT": "Does not closely match any specific criterion but is generally related to multi-modal learning and reasoning.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.14736": {
        "authors": [
            "Yilan Dong",
            "Haohe Liu",
            "Qing Wang",
            "Jiahao Yang",
            "Wenqing Wang",
            "Gregory Slabaugh",
            "Shanxin Yuan"
        ],
        "title": "HandSplat: Embedding-Driven Gaussian Splatting for High-Fidelity Hand Rendering",
        "abstract": "arXiv:2503.14736v1 Announce Type: new  Abstract: Existing 3D Gaussian Splatting (3DGS) methods for hand rendering rely on rigid skeletal motion with an oversimplified non-rigid motion model, which fails to capture fine geometric and appearance details. Additionally, they perform densification based solely on per-point gradients and process poses independently, ignoring spatial and temporal correlations. These limitations lead to geometric detail loss, temporal instability, and inefficient point distribution. To address these issues, we propose HandSplat, a novel Gaussian Splatting-based framework that enhances both fidelity and stability for hand rendering. To improve fidelity, we extend standard 3DGS attributes with implicit geometry and appearance embeddings for finer non-rigid motion modeling while preserving the static hand characteristic modeled by original 3DGS attributes. Additionally, we introduce a local gradient-aware densification strategy that dynamically refines Gaussian density in high-variation regions. To improve stability, we incorporate pose-conditioned attribute regularization to encourage attribute consistency across similar poses, mitigating temporal artifacts. Extensive experiments on InterHand2.6M demonstrate that HandSplat surpasses existing methods in fidelity and stability while achieving real-time performance. We will release the code and pre-trained models upon acceptance.",
        "arxiv_id": "2503.14736",
        "ARXIVID": "2503.14736",
        "COMMENT": "This paper does not match any specific criteria but focuses on improving fidelity and stability in 3D hand rendering, which may be of tangential interest.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15144": {
        "authors": [
            "Xing He",
            "Zhe Zhu",
            "Liangliang Nan",
            "Honghua Chen",
            "Jing Qin",
            "Mingqiang Wei"
        ],
        "title": "PointSFDA: Source-free Domain Adaptation for Point Cloud Completion",
        "abstract": "arXiv:2503.15144v1 Announce Type: new  Abstract: Conventional methods for point cloud completion, typically trained on synthetic datasets, face significant challenges when applied to out-of-distribution real-world scans. In this paper, we propose an effective yet simple source-free domain adaptation framework for point cloud completion, termed \\textbf{PointSFDA}. Unlike unsupervised domain adaptation that reduces the domain gap by directly leveraging labeled source data, PointSFDA uses only a pretrained source model and unlabeled target data for adaptation, avoiding the need for inaccessible source data in practical scenarios. Being the first source-free domain adaptation architecture for point cloud completion, our method offers two core contributions. First, we introduce a coarse-to-fine distillation solution to explicitly transfer the global geometry knowledge learned from the source dataset. Second, as noise may be introduced due to domain gaps, we propose a self-supervised partial-mask consistency training strategy to learn local geometry information in the target domain. Extensive experiments have validated that our method significantly improves the performance of state-of-the-art networks in cross-domain shape completion. Our code is available at \\emph{\\textcolor{magenta}{https://github.com/Starak-x/PointSFDA}}.",
        "arxiv_id": "2503.15144",
        "ARXIVID": "2503.15144",
        "COMMENT": "This paper does not match any of the specific criteria but is tangentially related to spatial understanding in the context of point cloud completion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.14517": {
        "authors": [
            "Hejia Chen",
            "Haoxian Zhang",
            "Shoulong Zhang",
            "Xiaoqiang Liu",
            "Sisi Zhuang",
            "Yuan Zhang",
            "Pengfei Wan",
            "Di Zhang",
            "Shuai Li"
        ],
        "title": "Cafe-Talk: Generating 3D Talking Face Animation with Multimodal Coarse- and Fine-grained Control",
        "abstract": "arXiv:2503.14517v1 Announce Type: new  Abstract: Speech-driven 3D talking face method should offer both accurate lip synchronization and controllable expressions. Previous methods solely adopt discrete emotion labels to globally control expressions throughout sequences while limiting flexible fine-grained facial control within the spatiotemporal domain. We propose a diffusion-transformer-based 3D talking face generation model, Cafe-Talk, which simultaneously incorporates coarse- and fine-grained multimodal control conditions. Nevertheless, the entanglement of multiple conditions challenges achieving satisfying performance. To disentangle speech audio and fine-grained conditions, we employ a two-stage training pipeline. Specifically, Cafe-Talk is initially trained using only speech audio and coarse-grained conditions. Then, a proposed fine-grained control adapter gradually adds fine-grained instructions represented by action units (AUs), preventing unfavorable speech-lip synchronization. To disentangle coarse- and fine-grained conditions, we design a swap-label training mechanism, which enables the dominance of the fine-grained conditions. We also devise a mask-based CFG technique to regulate the occurrence and intensity of fine-grained control. In addition, a text-based detector is introduced with text-AU alignment to enable natural language user input and further support multimodal control. Extensive experimental results prove that Cafe-Talk achieves state-of-the-art lip synchronization and expressiveness performance and receives wide acceptance in fine-grained control in user studies. Project page: https://harryxd2018.github.io/cafe-talk/",
        "arxiv_id": "2503.14517",
        "ARXIVID": "2503.14517",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to generative modeling and multimodal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.14938": {
        "authors": [
            "Zhong Ji",
            "Ci Liu",
            "Jingren Liu",
            "Chen Tang",
            "Yanwei Pang",
            "Xuelong Li"
        ],
        "title": "Optimal Transport Adapter Tuning for Bridging Modality Gaps in Few-Shot Remote Sensing Scene Classification",
        "abstract": "arXiv:2503.14938v1 Announce Type: new  Abstract: Few-Shot Remote Sensing Scene Classification (FS-RSSC) presents the challenge of classifying remote sensing images with limited labeled samples. Existing methods typically emphasize single-modal feature learning, neglecting the potential benefits of optimizing multi-modal representations. To address this limitation, we propose a novel Optimal Transport Adapter Tuning (OTAT) framework aimed at constructing an ideal Platonic representational space through optimal transport (OT) theory. This framework seeks to harmonize rich visual information with less dense textual cues, enabling effective cross-modal information transfer and complementarity. Central to this approach is the Optimal Transport Adapter (OTA), which employs a cross-modal attention mechanism to enrich textual representations and facilitate subsequent better information interaction. By transforming the network optimization into an OT optimization problem, OTA establishes efficient pathways for balanced information exchange between modalities. Moreover, we introduce a sample-level Entropy-Aware Weighted (EAW) loss, which combines difficulty-weighted similarity scores with entropy-based regularization. This loss function provides finer control over the OT optimization process, enhancing its solvability and stability. Our framework offers a scalable and efficient solution for advancing multimodal learning in remote sensing applications. Extensive experiments on benchmark datasets demonstrate that OTAT achieves state-of-the-art performance in FS-RSSC, significantly improving the model performance and generalization.",
        "arxiv_id": "2503.14938",
        "ARXIVID": "2503.14938",
        "COMMENT": "This paper focuses on few-shot remote sensing scene classification using optimal transport, which does not directly match any specific criterion but is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15465": {
        "authors": [
            "Ruichen Chen",
            "Keith G. Mills",
            "Di Niu"
        ],
        "title": "FP4DiT: Towards Effective Floating Point Quantization for Diffusion Transformers",
        "abstract": "arXiv:2503.15465v1 Announce Type: new  Abstract: Diffusion Models (DM) have revolutionized the text-to-image visual generation process. However, the large computational cost and model footprint of DMs hinders practical deployment, especially on edge devices. Post-training quantization (PTQ) is a lightweight method to alleviate these burdens without the need for training or fine-tuning. While recent DM PTQ methods achieve W4A8 on integer-based PTQ, two key limitations remain: First, while most existing DM PTQ methods evaluate on classical DMs like Stable Diffusion XL, 1.5 or earlier, which use convolutional U-Nets, newer Diffusion Transformer (DiT) models like the PixArt series, Hunyuan and others adopt fundamentally different transformer backbones to achieve superior image synthesis. Second, integer (INT) quantization is prevailing in DM PTQ but doesn't align well with the network weight and activation distribution, while Floating-Point Quantization (FPQ) is still under-investigated, yet it holds the potential to better align the weight and activation distributions in low-bit settings for DiT. In response, we introduce FP4DiT, a PTQ method that leverages FPQ to achieve W4A6 quantization. Specifically, we extend and generalize the Adaptive Rounding PTQ technique to adequately calibrate weight quantization for FPQ and demonstrate that DiT activations depend on input patch data, necessitating robust online activation quantization techniques. Experimental results demonstrate that FP4DiT outperforms integer-based PTQ at W4A6 and W4A8 precision and generates convincing visual content on PixArt-$\\alpha$, PixArt-$\\Sigma$ and Hunyuan in terms of several T2I metrics such as HPSv2 and CLIP.",
        "arxiv_id": "2503.15465",
        "ARXIVID": "2503.15465",
        "COMMENT": "This paper introduces a quantization method for diffusion transformers, which is tangentially related to vision foundation models but does not directly match any specific criterion.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.14863": {
        "authors": [
            "Hengkang Wang",
            "Yang Liu",
            "Huidong Liu",
            "Chien-Chih Wang",
            "Yanhui Guo",
            "Hongdong Li",
            "Bryan Wang",
            "Ju Sun"
        ],
        "title": "Temporal-Consistent Video Restoration with Pre-trained Diffusion Models",
        "abstract": "arXiv:2503.14863v1 Announce Type: new  Abstract: Video restoration (VR) aims to recover high-quality videos from degraded ones. Although recent zero-shot VR methods using pre-trained diffusion models (DMs) show good promise, they suffer from approximation errors during reverse diffusion and insufficient temporal consistency. Moreover, dealing with 3D video data, VR is inherently computationally intensive. In this paper, we advocate viewing the reverse process in DMs as a function and present a novel Maximum a Posterior (MAP) framework that directly parameterizes video frames in the seed space of DMs, eliminating approximation errors. We also introduce strategies to promote bilevel temporal consistency: semantic consistency by leveraging clustering structures in the seed space, and pixel-level consistency by progressive warping with optical flow refinements. Extensive experiments on multiple virtual reality tasks demonstrate superior visual quality and temporal consistency achieved by our method compared to the state-of-the-art.",
        "arxiv_id": "2503.14863",
        "ARXIVID": "2503.14863",
        "COMMENT": "This paper focuses on video restoration using pre-trained diffusion models, which does not directly match any specific criterion but is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15056": {
        "authors": [
            "Suhyeon Lee",
            "Kwanyoung Kim",
            "Jong Chul Ye"
        ],
        "title": "Single-Step Bidirectional Unpaired Image Translation Using Implicit Bridge Consistency Distillation",
        "abstract": "arXiv:2503.15056v1 Announce Type: new  Abstract: Unpaired image-to-image translation has seen significant progress since the introduction of CycleGAN. However, methods based on diffusion models or Schr\\\"odinger bridges have yet to be widely adopted in real-world applications due to their iterative sampling nature. To address this challenge, we propose a novel framework, Implicit Bridge Consistency Distillation (IBCD), which enables single-step bidirectional unpaired translation without using adversarial loss. IBCD extends consistency distillation by using a diffusion implicit bridge model that connects PF-ODE trajectories between distributions. Additionally, we introduce two key improvements: 1) distribution matching for consistency distillation and 2) adaptive weighting method based on distillation difficulty. Experimental results demonstrate that IBCD achieves state-of-the-art performance on benchmark datasets in a single generation step. Project page available at https://hyn2028.github.io/project_page/IBCD/index.html",
        "arxiv_id": "2503.15056",
        "ARXIVID": "2503.15056",
        "COMMENT": "Does not match any specific criterion but is related to unpaired image translation and generative modeling.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.14654": {
        "authors": [
            "Jonas Dornbusch",
            "Emanuel Pfarr",
            "Florin-Alexandru Vasluianu",
            "Frank Werner",
            "Radu Timofte"
        ],
        "title": "A Simple Combination of Diffusion Models for Better Quality Trade-Offs in Image Denoising",
        "abstract": "arXiv:2503.14654v1 Announce Type: new  Abstract: Diffusion models have garnered considerable interest in computer vision, owing both to their capacity to synthesize photorealistic images and to their proven effectiveness in image reconstruction tasks. However, existing approaches fail to efficiently balance the high visual quality of diffusion models with the low distortion achieved by previous image reconstruction methods. Specifically, for the fundamental task of additive Gaussian noise removal, we first illustrate an intuitive method for leveraging pretrained diffusion models. Further, we introduce our proposed Linear Combination Diffusion Denoiser (LCDD), which unifies two complementary inference procedures - one that leverages the model's generative potential and another that ensures faithful signal recovery. By exploiting the inherent structure of the denoising samples, LCDD achieves state-of-the-art performance and offers controlled, well-behaved trade-offs through a simple scalar hyperparameter adjustment.",
        "arxiv_id": "2503.14654",
        "ARXIVID": "2503.14654",
        "COMMENT": "Does not match any specific criterion but is related to diffusion models and image denoising.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15404": {
        "authors": [
            "Yuchen Ren",
            "Zhengyu Zhao",
            "Chenhao Lin",
            "Bo Yang",
            "Lu Zhou",
            "Zhe Liu",
            "Chao Shen"
        ],
        "title": "Improving Adversarial Transferability on Vision Transformers via Forward Propagation Refinement",
        "abstract": "arXiv:2503.15404v1 Announce Type: new  Abstract: Vision Transformers (ViTs) have been widely applied in various computer vision and vision-language tasks. To gain insights into their robustness in practical scenarios, transferable adversarial examples on ViTs have been extensively studied. A typical approach to improving adversarial transferability is by refining the surrogate model. However, existing work on ViTs has restricted their surrogate refinement to backward propagation. In this work, we instead focus on Forward Propagation Refinement (FPR) and specifically refine two key modules of ViTs: attention maps and token embeddings. For attention maps, we propose Attention Map Diversification (AMD), which diversifies certain attention maps and also implicitly imposes beneficial gradient vanishing during backward propagation. For token embeddings, we propose Momentum Token Embedding (MTE), which accumulates historical token embeddings to stabilize the forward updates in both the Attention and MLP blocks. We conduct extensive experiments with adversarial examples transferred from ViTs to various CNNs and ViTs, demonstrating that our FPR outperforms the current best (backward) surrogate refinement by up to 7.0\\% on average. We also validate its superiority against popular defenses and its compatibility with other transfer methods. Codes and appendix are available at https://github.com/RYC-98/FPR.",
        "arxiv_id": "2503.15404",
        "ARXIVID": "2503.15404",
        "COMMENT": "Does not match any specific criterion but is related to adversarial transferability in vision transformers.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15126": {
        "authors": [
            "Haoyu Ji",
            "Bowen Chen",
            "Weihong Ren",
            "Wenze Huang",
            "Zhihao Yang",
            "Zhiyong Wang",
            "Honghai Liu"
        ],
        "title": "Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action Segmentation",
        "abstract": "arXiv:2503.15126v1 Announce Type: new  Abstract: Skeleton-based Temporal Action Segmentation (STAS) aims to segment and recognize various actions from long, untrimmed sequences of human skeletal movements. Current STAS methods typically employ spatio-temporal modeling to establish dependencies among joints as well as frames, and utilize one-hot encoding with cross-entropy loss for frame-wise classification supervision. However, these methods overlook the intrinsic correlations among joints and actions within skeletal features, leading to a limited understanding of human movements. To address this, we propose a Text-Derived Relational Graph-Enhanced Network (TRG-Net) that leverages prior graphs generated by Large Language Models (LLM) to enhance both modeling and supervision. For modeling, the Dynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived Joint Graphs (TJG) with channel- and frame-level dynamic adaptation to effectively model spatial relations, while integrating spatio-temporal core features during temporal modeling. For supervision, the Absolute-Relative Inter-Class Supervision (ARIS) method employs contrastive learning between action features and text embeddings to regularize the absolute class distributions, and utilizes Text-Derived Action Graphs (TAG) to capture the relative inter-class relationships among action features. Additionally, we propose a Spatial-Aware Enhancement Processing (SAEP) method, which incorporates random joint occlusion and axial rotation to enhance spatial generalization. Performance evaluations on four public datasets demonstrate that TRG-Net achieves state-of-the-art results.",
        "arxiv_id": "2503.15126",
        "ARXIVID": "2503.15126",
        "COMMENT": "Does not match any specific criterion but is related to skeleton-based action segmentation and LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15017": {
        "authors": [
            "Yunwei Lan",
            "Zhigao Cui",
            "Chang Liu",
            "Jialun Peng",
            "Nian Wang",
            "Xin Luo",
            "Dong Liu"
        ],
        "title": "Exploiting Diffusion Prior for Real-World Image Dehazing with Unpaired Training",
        "abstract": "arXiv:2503.15017v1 Announce Type: new  Abstract: Unpaired training has been verified as one of the most effective paradigms for real scene dehazing by learning from unpaired real-world hazy and clear images. Although numerous studies have been proposed, current methods demonstrate limited generalization for various real scenes due to limited feature representation and insufficient use of real-world prior. Inspired by the strong generative capabilities of diffusion models in producing both hazy and clear images, we exploit diffusion prior for real-world image dehazing, and propose an unpaired framework named Diff-Dehazer. Specifically, we leverage diffusion prior as bijective mapping learners within the CycleGAN, a classic unpaired learning framework. Considering that physical priors contain pivotal statistics information of real-world data, we further excavate real-world knowledge by integrating physical priors into our framework. Furthermore, we introduce a new perspective for adequately leveraging the representation ability of diffusion models by removing degradation in image and text modalities, so as to improve the dehazing effect. Extensive experiments on multiple real-world datasets demonstrate the superior performance of our method. Our code https://github.com/ywxjm/Diff-Dehazer.",
        "arxiv_id": "2503.15017",
        "ARXIVID": "2503.15017",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling and image dehazing.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15293": {
        "authors": [
            "Hangtao Zhang",
            "Yichen Wang",
            "Shihui Yan",
            "Chenyu Zhu",
            "Ziqi Zhou",
            "Linshan Hou",
            "Shengshan Hu",
            "Minghui Li",
            "Yanjun Zhang",
            "Leo Yu Zhang"
        ],
        "title": "Test-Time Backdoor Detection for Object Detection Models",
        "abstract": "arXiv:2503.15293v1 Announce Type: new  Abstract: Object detection models are vulnerable to backdoor attacks, where attackers poison a small subset of training samples by embedding a predefined trigger to manipulate prediction. Detecting poisoned samples (i.e., those containing triggers) at test time can prevent backdoor activation. However, unlike image classification tasks, the unique characteristics of object detection -- particularly its output of numerous objects -- pose fresh challenges for backdoor detection. The complex attack effects (e.g., \"ghost\" object emergence or \"vanishing\" object) further render current defenses fundamentally inadequate. To this end, we design TRAnsformation Consistency Evaluation (TRACE), a brand-new method for detecting poisoned samples at test time in object detection. Our journey begins with two intriguing observations: (1) poisoned samples exhibit significantly more consistent detection results than clean ones across varied backgrounds. (2) clean samples show higher detection consistency when introduced to different focal information. Based on these phenomena, TRACE applies foreground and background transformations to each test sample, then assesses transformation consistency by calculating the variance in objects confidences. TRACE achieves black-box, universal backdoor detection, with extensive experiments showing a 30% improvement in AUROC over state-of-the-art defenses and resistance to adaptive attacks.",
        "arxiv_id": "2503.15293",
        "ARXIVID": "2503.15293",
        "COMMENT": "Does not match any specific criterion but is related to object detection and adversarial robustness.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.14757": {
        "authors": [
            "Marcelo Sanchez",
            "Gil Triginer",
            "Ignacio Sarasua",
            "Lara Raad",
            "Coloma Ballester"
        ],
        "title": "RETHINED: A New Benchmark and Baseline for Real-Time High-Resolution Image Inpainting On Edge Devices",
        "abstract": "arXiv:2503.14757v1 Announce Type: new  Abstract: Existing image inpainting methods have shown impressive completion results for low-resolution images. However, most of these algorithms fail at high resolutions and require powerful hardware, limiting their deployment on edge devices. Motivated by this, we propose the first baseline for REal-Time High-resolution image INpainting on Edge Devices (RETHINED) that is able to inpaint at ultra-high-resolution and can run in real-time ($\\leq$ 30ms) in a wide variety of mobile devices. A simple, yet effective novel method formed by a lightweight Convolutional Neural Network (CNN) to recover structure, followed by a resolution-agnostic patch replacement mechanism to provide detailed texture. Specially our pipeline leverages the structural capacity of CNN and the high-level detail of patch-based methods, which is a key component for high-resolution image inpainting. To demonstrate the real application of our method, we conduct an extensive analysis on various mobile-friendly devices and demonstrate similar inpainting performance while being $\\mathrm{100 \\times faster}$ than existing state-of-the-art methods. Furthemore, we realease DF8K-Inpainting, the first free-form mask UHD inpainting dataset.",
        "arxiv_id": "2503.14757",
        "ARXIVID": "2503.14757",
        "COMMENT": "Does not match any specific criterion but is related to computer vision and edge device applications.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.14880": {
        "authors": [
            "Henrique Morimitsu",
            "Xiaobin Zhu",
            "Roberto M. Cesar Jr.",
            "Xiangyang Ji",
            "Xu-Cheng Yin"
        ],
        "title": "DPFlow: Adaptive Optical Flow Estimation with a Dual-Pyramid Framework",
        "abstract": "arXiv:2503.14880v1 Announce Type: new  Abstract: Optical flow estimation is essential for video processing tasks, such as restoration and action recognition. The quality of videos is constantly increasing, with current standards reaching 8K resolution. However, optical flow methods are usually designed for low resolution and do not generalize to large inputs due to their rigid architectures. They adopt downscaling or input tiling to reduce the input size, causing a loss of details and global information. There is also a lack of optical flow benchmarks to judge the actual performance of existing methods on high-resolution samples. Previous works only conducted qualitative high-resolution evaluations on hand-picked samples. This paper fills this gap in optical flow estimation in two ways. We propose DPFlow, an adaptive optical flow architecture capable of generalizing up to 8K resolution inputs while trained with only low-resolution samples. We also introduce Kubric-NK, a new benchmark for evaluating optical flow methods with input resolutions ranging from 1K to 8K. Our high-resolution evaluation pushes the boundaries of existing methods and reveals new insights about their generalization capabilities. Extensive experimental results show that DPFlow achieves state-of-the-art results on the MPI-Sintel, KITTI 2015, Spring, and other high-resolution benchmarks.",
        "arxiv_id": "2503.14880",
        "ARXIVID": "2503.14880",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and machine learning due to its focus on optical flow estimation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15161": {
        "authors": [
            "Yang Li",
            "Soumya Snigdha Kundu",
            "Maxence Boels",
            "Toktam Mahmoodi",
            "Sebastien Ourselin",
            "Tom Vercauteren",
            "Prokar Dasgupta",
            "Jonathan Shapey",
            "Alejandro Granados"
        ],
        "title": "UltraFlwr -- An Efficient Federated Medical and Surgical Object Detection Framework",
        "abstract": "arXiv:2503.15161v1 Announce Type: new  Abstract: Object detection shows promise for medical and surgical applications such as cell counting and tool tracking. However, its faces multiple real-world edge deployment challenges including limited high-quality annotated data, data sharing restrictions, and computational constraints. In this work, we introduce UltraFlwr, a framework for federated medical and surgical object detection. By leveraging Federated Learning (FL), UltraFlwr enables decentralized model training across multiple sites without sharing raw data. To further enhance UltraFlwr's efficiency, we propose YOLO-PA, a set of novel Partial Aggregation (PA) strategies specifically designed for YOLO models in FL. YOLO-PA significantly reduces communication overhead by up to 83% per round while maintaining performance comparable to Full Aggregation (FA) strategies. Our extensive experiments on BCCD and m2cai16-tool-locations datasets demonstrate that YOLO-PA not only provides better client models compared to client-wise centralized training and FA strategies, but also facilitates efficient training and deployment across resource-constrained edge devices. Further, we also establish one of the first benchmarks in federated medical and surgical object detection. This paper advances the feasibility of training and deploying detection models on the edge, making federated object detection more practical for time-critical and resource-constrained medical and surgical applications. UltraFlwr is publicly available at https://github.com/KCL-BMEIS/UltraFlwr.",
        "arxiv_id": "2503.15161",
        "ARXIVID": "2503.15161",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and machine learning due to its focus on federated object detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.14824": {
        "authors": [
            "Zikun Zhou",
            "Yushuai Sun",
            "Wenjie Pei",
            "Xin Li",
            "Yaowei Wang"
        ],
        "title": "Prototype Perturbation for Relaxing Alignment Constraints in Backward-Compatible Learning",
        "abstract": "arXiv:2503.14824v1 Announce Type: new  Abstract: The traditional paradigm to update retrieval models requires re-computing the embeddings of the gallery data, a time-consuming and computationally intensive process known as backfilling. To circumvent backfilling, Backward-Compatible Learning (BCL) has been widely explored, which aims to train a new model compatible with the old one. Many previous works focus on effectively aligning the embeddings of the new model with those of the old one to enhance the backward-compatibility. Nevertheless, such strong alignment constraints would compromise the discriminative ability of the new model, particularly when different classes are closely clustered and hard to distinguish in the old feature space. To address this issue, we propose to relax the constraints by introducing perturbations to the old feature prototypes. This allows us to align the new feature space with a pseudo-old feature space defined by these perturbed prototypes, thereby preserving the discriminative ability of the new model in backward-compatible learning. We have developed two approaches for calculating the perturbations: Neighbor-Driven Prototype Perturbation (NDPP) and Optimization-Driven Prototype Perturbation (ODPP). Particularly, they take into account the feature distributions of not only the old but also the new models to obtain proper perturbations along with new model updating. Extensive experiments on the landmark and commodity datasets demonstrate that our approaches perform favorably against state-of-the-art BCL algorithms.",
        "arxiv_id": "2503.14824",
        "ARXIVID": "2503.14824",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and machine learning due to its focus on backward-compatible learning.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15295": {
        "authors": [
            "Aoting Zhang",
            "Dongbao Yang",
            "Chang Liu",
            "Xiaopeng Hong",
            "Miao Shang",
            "Yu Zhou"
        ],
        "title": "DCA: Dividing and Conquering Amnesia in Incremental Object Detection",
        "abstract": "arXiv:2503.15295v1 Announce Type: new  Abstract: Incremental object detection (IOD) aims to cultivate an object detector that can continuously localize and recognize novel classes while preserving its performance on previous classes. Existing methods achieve certain success by improving knowledge distillation and exemplar replay for transformer-based detection frameworks, but the intrinsic forgetting mechanisms remain underexplored. In this paper, we dive into the cause of forgetting and discover forgetting imbalance between localization and recognition in transformer-based IOD, which means that localization is less-forgetting and can generalize to future classes, whereas catastrophic forgetting occurs primarily on recognition. Based on these insights, we propose a Divide-and-Conquer Amnesia (DCA) strategy, which redesigns the transformer-based IOD into a localization-then-recognition process. DCA can well maintain and transfer the localization ability, leaving decoupled fragile recognition to be specially conquered. To reduce feature drift in recognition, we leverage semantic knowledge encoded in pre-trained language models to anchor class representations within a unified feature space across incremental tasks. This involves designing a duplex classifier fusion and embedding class semantic features into the recognition decoding process in the form of queries. Extensive experiments validate that our approach achieves state-of-the-art performance, especially for long-term incremental scenarios. For example, under the four-step setting on MS-COCO, our DCA strategy significantly improves the final AP by 6.9%.",
        "arxiv_id": "2503.15295",
        "ARXIVID": "2503.15295",
        "COMMENT": "Does not match any specific criterion but is generally relevant to computer vision and machine learning due to its focus on incremental object detection.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.14760": {
        "authors": [
            "Kai Armstrong",
            "Alexander Rodrigues",
            "Alexander P. Willmott",
            "Lei Zhang",
            "Xujiong Ye"
        ],
        "title": "Validation of Human Pose Estimation and Human Mesh Recovery for Extracting Clinically Relevant Motion Data from Videos",
        "abstract": "arXiv:2503.14760v1 Announce Type: new  Abstract: This work aims to discuss the current landscape of kinematic analysis tools, ranging from the state-of-the-art in sports biomechanics such as inertial measurement units (IMUs) and retroreflective marker-based optical motion capture (MoCap) to more novel approaches from the field of computing such as human pose estimation and human mesh recovery. Primarily, this comparative analysis aims to validate the use of marker-less MoCap techniques in a clinical setting by showing that these marker-less techniques are within a reasonable range for kinematics analysis compared to the more cumbersome and less portable state-of-the-art tools. Not only does marker-less motion capture using human pose estimation produce results in-line with the results of both the IMU and MoCap kinematics but also benefits from a reduced set-up time and reduced practical knowledge and expertise to set up. Overall, while there is still room for improvement when it comes to the quality of the data produced, we believe that this compromise is within the room of error that these low-speed actions that are used in small clinical tests.",
        "arxiv_id": "2503.14760",
        "ARXIVID": "2503.14760",
        "COMMENT": "Does not closely match any specific criterion but is related to computer vision applications in human pose estimation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15004": {
        "authors": [
            "Annalena Bl\\\"ansdorf",
            "Tristan Wirth",
            "Arne Rak",
            "Thomas P\\\"ollabauer",
            "Volker Knauthe",
            "Arjan Kuijper"
        ],
        "title": "Semantic Segmentation of Transparent and Opaque Drinking Glasses with the Help of Zero-shot Learning",
        "abstract": "arXiv:2503.15004v1 Announce Type: new  Abstract: Segmenting transparent structures in images is challenging since they are difficult to distinguish from the background. Common examples are drinking glasses, which are a ubiquitous part of our lives and appear in many different shapes and sizes. In this work we propose TransCaGNet, a modified version of the zero-shot model CaGNet. We exchange the segmentation backbone with the architecture of Trans4Trans to be capable of segmenting transparent objects. Since some glasses are rarely captured, we use zeroshot learning to be able to create semantic segmentations of glass categories not given during training. We propose a novel synthetic dataset covering a diverse set of different environmental conditions. Additionally we capture a real-world evaluation dataset since most applications take place in the real world. Comparing our model with Zeg-Clip we are able to show that TransCaGNet produces better mean IoU and accuracy values while ZegClip outperforms it mostly for unseen classes. To improve the segmentation results, we combine the semantic segmentation of the models with the segmentation results of SAM 2. Our evaluation emphasizes that distinguishing between different classes is challenging for the models due to similarity, points of view, or coverings. Taking this behavior into account, we assign glasses multiple possible categories. The modification leads to an improvement up to 13.68% for the mean IoU and up to 17.88% for the mean accuracy values on the synthetic dataset. Using our difficult synthetic dataset for training, the models produce even better results on the real-world dataset. The mean IoU is improved up to 5.55% and the mean accuracy up to 5.72% on the real-world dataset.",
        "arxiv_id": "2503.15004",
        "ARXIVID": "2503.15004",
        "COMMENT": "Does not closely match any specific criterion but is generally relevant to computer vision and segmentation tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    }
}