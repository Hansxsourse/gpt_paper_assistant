{
    "2503.15558": {
        "authors": [
            "NVIDIA",
            ":",
            "Alisson Azzolini",
            "Hannah Brandon",
            "Prithvijit Chattopadhyay",
            "Huayu Chen",
            "Jinju Chu",
            "Yin Cui",
            "Jenna Diamond",
            "Yifan Ding",
            "Francesco Ferroni",
            "Rama Govindaraju",
            "Jinwei Gu",
            "Siddharth Gururani",
            "Imad El Hanafi",
            "Zekun Hao",
            "Jacob Huffman",
            "Jingyi Jin",
            "Brendan Johnson",
            "Rizwan Khan",
            "George Kurian",
            "Elena Lantz",
            "Nayeon Lee",
            "Zhaoshuo Li",
            "Xuan Li",
            "Tsung-Yi Lin",
            "Yen-Chen Lin",
            "Ming-Yu Liu",
            "Andrew Mathau",
            "Yun Ni",
            "Lindsey Pavao",
            "Wei Ping",
            "David W. Romero",
            "Misha Smelyanskiy",
            "Shuran Song",
            "Lyne Tchapmi",
            "Andrew Z. Wang",
            "Boxin Wang",
            "Haoxiang Wang",
            "Fangyin Wei",
            "Jiashu Xu",
            "Yao Xu",
            "Xiaodong Yang",
            "Zhuolin Yang",
            "Xiaohui Zeng",
            "Zhe Zhang"
        ],
        "title": "Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning",
        "abstract": "arXiv:2503.15558v1 Announce Type: new  Abstract: Physical AI systems need to perceive, understand, and perform complex actions in the physical world. In this paper, we present the Cosmos-Reason1 models that can understand the physical world and generate appropriate embodied decisions (e.g., next step action) in natural language through long chain-of-thought reasoning processes. We begin by defining key capabilities for Physical AI reasoning, with a focus on physical common sense and embodied reasoning. To represent physical common sense, we use a hierarchical ontology that captures fundamental knowledge about space, time, and physics. For embodied reasoning, we rely on a two-dimensional ontology that generalizes across different physical embodiments. Building on these capabilities, we develop two multimodal large language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data and train our models in four stages: vision pre-training, general supervised fine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL) as the post-training. To evaluate our models, we build comprehensive benchmarks for physical common sense and embodied reasoning according to our ontologies. Evaluation results show that Physical AI SFT and reinforcement learning bring significant improvements. To facilitate the development of Physical AI, we will make our code and pre-trained models available under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-reason1.",
        "arxiv_id": "2503.15558",
        "ARXIVID": "2503.15558",
        "COMMENT": "Matches criteria 1 and 3 as it introduces new multimodal large language models (Cosmos-Reason1) for embodied reasoning and builds benchmarks for physical common sense and embodied reasoning.",
        "RELEVANCE": 10,
        "NOVELTY": 8
    },
    "2503.15887": {
        "authors": [
            "Haochen Wang",
            "Kai Hu",
            "Liangcai Gao"
        ],
        "title": "DocVideoQA: Towards Comprehensive Understanding of Document-Centric Videos through Question Answering",
        "abstract": "arXiv:2503.15887v1 Announce Type: new  Abstract: Remote work and online courses have become important methods of knowledge dissemination, leading to a large number of document-based instructional videos. Unlike traditional video datasets, these videos mainly feature rich-text images and audio that are densely packed with information closely tied to the visual content, requiring advanced multimodal understanding capabilities. However, this domain remains underexplored due to dataset availability and its inherent complexity. In this paper, we introduce the DocVideoQA task and dataset for the first time, comprising 1454 videos across 23 categories with a total duration of about 828 hours. The dataset is annotated with 154k question-answer pairs generated manually and via GPT, assessing models' comprehension, temporal awareness, and modality integration capabilities. Initially, we establish a baseline using open-source MLLMs. Recognizing the challenges in modality comprehension for document-centric videos, we present DV-LLaMA, a robust video MLLM baseline. Our method enhances unimodal feature extraction with diverse instruction-tuning data and employs contrastive learning to strengthen modality integration. Through fine-tuning, the LLM is equipped with audio-visual capabilities, leading to significant improvements in document-centric video understanding. Extensive testing on the DocVideoQA dataset shows that DV-LLaMA significantly outperforms existing models. We'll release the code and dataset to facilitate future research.",
        "arxiv_id": "2503.15887",
        "ARXIVID": "2503.15887",
        "COMMENT": "This paper matches criterion 2 as it introduces a new dataset and method for multi-modal large language models in document-centric video understanding.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.16413": {
        "authors": [
            "Xueyan Zou",
            "Yuchen Song",
            "Ri-Zhao Qiu",
            "Xuanbin Peng",
            "Jianglong Ye",
            "Sifei Liu",
            "Xiaolong Wang"
        ],
        "title": "M3: 3D-Spatial MultiModal Memory",
        "abstract": "arXiv:2503.16413v1 Announce Type: new  Abstract: We present 3D Spatial MultiModal Memory (M3), a multimodal memory system designed to retain information about medium-sized static scenes through video sources for visual perception. By integrating 3D Gaussian Splatting techniques with foundation models, M3 builds a multimodal memory capable of rendering feature representations across granularities, encompassing a wide range of knowledge. In our exploration, we identify two key challenges in previous works on feature splatting: (1) computational constraints in storing high-dimensional features for each Gaussian primitive, and (2) misalignment or information loss between distilled features and foundation model features. To address these challenges, we propose M3 with key components of principal scene components and Gaussian memory attention, enabling efficient training and inference. To validate M3, we conduct comprehensive quantitative evaluations of feature similarity and downstream tasks, as well as qualitative visualizations to highlight the pixel trace of Gaussian memory attention. Our approach encompasses a diverse range of foundation models, including vision-language models (VLMs), perception models, and large multimodal and language models (LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy M3's feature field in indoor scenes on a quadruped robot. Notably, we claim that M3 is the first work to address the core compression challenges in 3D feature distillation.",
        "arxiv_id": "2503.16413",
        "ARXIVID": "2503.16413",
        "COMMENT": "This paper matches criterion 1 as it focuses on spatial intelligence and memory for embodied agents using 3D Gaussian Splatting and foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.16289": {
        "authors": [
            "Inwoo Hwang",
            "Bing Zhou",
            "Young Min Kim",
            "Jian Wang",
            "Chuan Guo"
        ],
        "title": "SceneMI: Motion In-betweening for Modeling Human-Scene Interactions",
        "abstract": "arXiv:2503.16289v1 Announce Type: new  Abstract: Modeling human-scene interactions (HSI) is essential for understanding and simulating everyday human behaviors. Recent approaches utilizing generative modeling have made progress in this domain; however, they are limited in controllability and flexibility for real-world applications. To address these challenges, we propose reformulating the HSI modeling problem as Scene-aware Motion In-betweening -- a more tractable and practical task. We introduce SceneMI, a framework that supports several practical applications, including keyframe-guided character animation in 3D scenes and enhancing the motion quality of imperfect HSI data. SceneMI employs dual scene descriptors to comprehensively encode global and local scene context. Furthermore, our framework leverages the inherent denoising nature of diffusion models to generalize on noisy keyframes. Experimental results demonstrate SceneMI's effectiveness in scene-aware keyframe in-betweening and generalization to the real-world GIMO dataset, where motions and scenes are acquired by noisy IMU sensors and smartphones. We further showcase SceneMI's applicability in HSI reconstruction from monocular videos.",
        "arxiv_id": "2503.16289",
        "ARXIVID": "2503.16289",
        "COMMENT": "Matches criteria 3 as it introduces a new framework (SceneMI) for modeling human-scene interactions with practical applications and novel methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2503.16399": {
        "authors": [
            "Chen Chen",
            "Zhirui Wang",
            "Taowei Sheng",
            "Yi Jiang",
            "Yundu Li",
            "Peirui Cheng",
            "Luning Zhang",
            "Kaiqiang Chen",
            "Yanfeng Hu",
            "Xue Yang",
            "Xian Sun"
        ],
        "title": "SA-Occ: Satellite-Assisted 3D Occupancy Prediction in Real World",
        "abstract": "arXiv:2503.16399v1 Announce Type: new  Abstract: Existing vision-based 3D occupancy prediction methods are inherently limited in accuracy due to their exclusive reliance on street-view imagery, neglecting the potential benefits of incorporating satellite views. We propose SA-Occ, the first Satellite-Assisted 3D occupancy prediction model, which leverages GPS & IMU to integrate historical yet readily available satellite imagery into real-time applications, effectively mitigating limitations of ego-vehicle perceptions, involving occlusions and degraded performance in distant regions. To address the core challenges of cross-view perception, we propose: 1) Dynamic-Decoupling Fusion, which resolves inconsistencies in dynamic regions caused by the temporal asynchrony between satellite and street views; 2) 3D-Proj Guidance, a module that enhances 3D feature extraction from inherently 2D satellite imagery; and 3) Uniform Sampling Alignment, which aligns the sampling density between street and satellite views. Evaluated on Occ3D-nuScenes, SA-Occ achieves state-of-the-art performance, especially among single-frame methods, with a 39.05% mIoU (a 6.97% improvement), while incurring only 6.93 ms of additional latency per frame. Our code and newly curated dataset are available at https://github.com/chenchen235/SA-Occ.",
        "arxiv_id": "2503.16399",
        "ARXIVID": "2503.16399",
        "COMMENT": "This paper matches criterion 3 as it introduces a novel benchmark and method for 3D occupancy prediction using satellite and street-view imagery.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2503.15778": {
        "authors": [
            "Boshra Khalili",
            "Andrew W. Smyth"
        ],
        "title": "AutoDrive-QA- Automated Generation of Multiple-Choice Questions for Autonomous Driving Datasets Using Large Vision-Language Models",
        "abstract": "arXiv:2503.15778v1 Announce Type: new  Abstract: In autonomous driving, open-ended question answering often suffers from unreliable evaluations because freeform responses require either complex metrics or subjective human judgment. To address this challenge, we introduce AutoDrive-QA, an automatic pipeline that converts existing driving QA datasets (including DriveLM, NuScenes-QA, and LingoQA) into a structured multiple-choice question (MCQ) format. This benchmark systematically assesses perception, prediction, and planning tasks, providing a standardized and objective evaluation framework. AutoDrive-QA employs an automated pipeline that leverages large language models (LLMs) to generate high-quality, contextually relevant distractors based on domain-specific error patterns commonly found in autonomous driving scenarios. To evaluate both general capabilities and generalization performance, we test the benchmark on three public datasets and conduct zero-shot experiments on an unseen dataset. The zero-shot evaluations reveal that GPT-4V leads with 69.57% accuracy -- achieving 74.94% in Perception, 65.33% in Prediction, and 68.45% in Planning -- demonstrating that while all models excel in Perception, they struggle in Prediction. Consequently, AutoDrive-QA establishes a rigorous, unbiased standard for integrating and evaluating different vision-language models across various autonomous driving datasets, thereby improving generalization in this field. We release all the codes in the AutoDrive-QA GitHub Repository.",
        "arxiv_id": "2503.15778",
        "ARXIVID": "2503.15778",
        "COMMENT": "Matches criteria 3 as it introduces a new benchmark (AutoDrive-QA) for evaluating vision-language models in autonomous driving datasets.",
        "RELEVANCE": 8,
        "NOVELTY": 6
    },
    "2503.15835": {
        "authors": [
            "Yiren Lu",
            "Yunlai Zhou",
            "Disheng Liu",
            "Tuo Liang",
            "Yu Yin"
        ],
        "title": "BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting",
        "abstract": "arXiv:2503.15835v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has shown remarkable potential for static scene reconstruction, and recent advancements have extended its application to dynamic scenes. However, the quality of reconstructions depends heavily on high-quality input images and precise camera poses, which are not that trivial to fulfill in real-world scenarios. Capturing dynamic scenes with handheld monocular cameras, for instance, typically involves simultaneous movement of both the camera and objects within a single exposure. This combined motion frequently results in image blur that existing methods cannot adequately handle. To address these challenges, we introduce BARD-GS, a novel approach for robust dynamic scene reconstruction that effectively handles blurry inputs and imprecise camera poses. Our method comprises two main components: 1) camera motion deblurring and 2) object motion deblurring. By explicitly decomposing motion blur into camera motion blur and object motion blur and modeling them separately, we achieve significantly improved rendering results in dynamic regions. In addition, we collect a real-world motion blur dataset of dynamic scenes to evaluate our approach. Extensive experiments demonstrate that BARD-GS effectively reconstructs high-quality dynamic scenes under realistic conditions, significantly outperforming existing methods.",
        "arxiv_id": "2503.15835",
        "ARXIVID": "2503.15835",
        "COMMENT": "Matches criterion 3 as it introduces a novel method for dynamic scene reconstruction with a focus on handling blurry inputs and imprecise camera poses, which is a novel angle in embodied AI.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.16394": {
        "authors": [
            "Akhil Perincherry",
            "Jacob Krantz",
            "Stefan Lee"
        ],
        "title": "Do Visual Imaginations Improve Vision-and-Language Navigation Agents?",
        "abstract": "arXiv:2503.16394v1 Announce Type: new  Abstract: Vision-and-Language Navigation (VLN) agents are tasked with navigating an unseen environment using natural language instructions. In this work, we study if visual representations of sub-goals implied by the instructions can serve as navigational cues and lead to increased navigation performance. To synthesize these visual representations or imaginations, we leverage a text-to-image diffusion model on landmark references contained in segmented instructions. These imaginations are provided to VLN agents as an added modality to act as landmark cues and an auxiliary loss is added to explicitly encourage relating these with their corresponding referring expressions. Our findings reveal an increase in success rate (SR) of around 1 point and up to 0.5 points in success scaled by inverse path length (SPL) across agents. These results suggest that the proposed approach reinforces visual understanding compared to relying on language instructions alone. Code and data for our work can be found at https://www.akhilperincherry.com/VLN-Imagine-website/.",
        "arxiv_id": "2503.16394",
        "ARXIVID": "2503.16394",
        "COMMENT": "This paper matches criterion 3 as it explores a novel method for improving vision-and-language navigation agents using visual imaginations.",
        "RELEVANCE": 7,
        "NOVELTY": 6
    },
    "2503.16282": {
        "authors": [
            "Zhaochong An",
            "Guolei Sun",
            "Yun Liu",
            "Runjia Li",
            "Junlin Han",
            "Ender Konukoglu",
            "Serge Belongie"
        ],
        "title": "Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model",
        "abstract": "arXiv:2503.16282v1 Announce Type: new  Abstract: Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to new classes with few support samples while retaining base class segmentation. Existing GFS-PCS methods enhance prototypes via interacting with support or query features but remain limited by sparse knowledge from few-shot samples. Meanwhile, 3D vision-language models (3D VLMs), generalizing across open-world novel classes, contain rich but noisy novel class knowledge. In this work, we introduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels from 3D VLMs with precise yet sparse few-shot samples to maximize the strengths of both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label selection to filter low-quality regions, followed by an adaptive infilling strategy that combines knowledge from pseudo-label contexts and few-shot samples to adaptively label the filtered, unlabeled areas. Additionally, we design a novel-base mix strategy to embed few-shot samples into training scenes, preserving essential context for improved novel class learning. Moreover, recognizing the limited diversity in current GFS-PCS benchmarks, we introduce two challenging benchmarks with diverse novel classes for comprehensive generalization evaluation. Experiments validate the effectiveness of our framework across models and datasets. Our approach and benchmarks provide a solid foundation for advancing GFS-PCS in the real world. The code is at https://github.com/ZhaochongAn/GFS-VL",
        "arxiv_id": "2503.16282",
        "ARXIVID": "2503.16282",
        "COMMENT": "Matches criterion 4 as it applies vision-language models to 3D point cloud segmentation, which is a novel application of vision foundation models.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.15948": {
        "authors": [
            "Elisei Rykov",
            "Kseniia Petrushina",
            "Kseniia Titova",
            "Alexander Panchenko",
            "Vasily Konovalov"
        ],
        "title": "Don't Fight Hallucinations, Use Them: Estimating Image Realism using NLI over Atomic Facts",
        "abstract": "arXiv:2503.15948v1 Announce Type: new  Abstract: Quantifying the realism of images remains a challenging problem in the field of artificial intelligence. For example, an image of Albert Einstein holding a smartphone violates common-sense because modern smartphone were invented after Einstein's death. We introduce a novel method for assessing image realism using Large Vision-Language Models (LVLMs) and Natural Language Inference (NLI). Our approach is based on the premise that LVLMs may generate hallucinations when confronted with images that defy common sense. Using LVLM to extract atomic facts from these images, we obtain a mix of accurate facts and erroneous hallucinations. We proceed by calculating pairwise entailment scores among these facts, subsequently aggregating these values to yield a singular reality score. This process serves to identify contradictions between genuine facts and hallucinatory elements, signaling the presence of images that violate common sense. Our approach has achieved a new state-of-the-art performance in zero-shot mode on the WHOOPS! dataset.",
        "arxiv_id": "2503.15948",
        "ARXIVID": "2503.15948",
        "COMMENT": "Matches criterion 2 as it uses large vision-language models (LVLMs) for assessing image realism with a novel statistical approach involving hallucinations and entailment scores.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.15949": {
        "authors": [
            "Yaxiong Chen",
            "Minghong Wei",
            "Zixuan Zheng",
            "Jingliang Hu",
            "Yilei Shi",
            "Shengwu Xiong",
            "Xiao Xiang Zhu",
            "Lichao Mou"
        ],
        "title": "CausalCLIPSeg: Unlocking CLIP's Potential in Referring Medical Image Segmentation with Causal Intervention",
        "abstract": "arXiv:2503.15949v1 Announce Type: new  Abstract: Referring medical image segmentation targets delineating lesions indicated by textual descriptions. Aligning visual and textual cues is challenging due to their distinct data properties. Inspired by large-scale pre-trained vision-language models, we propose CausalCLIPSeg, an end-to-end framework for referring medical image segmentation that leverages CLIP. Despite not being trained on medical data, we enforce CLIP's rich semantic space onto the medical domain by a tailored cross-modal decoding method to achieve text-to-pixel alignment. Furthermore, to mitigate confounding bias that may cause the model to learn spurious correlations instead of meaningful causal relationships, CausalCLIPSeg introduces a causal intervention module which self-annotates confounders and excavates causal features from inputs for segmentation judgments. We also devise an adversarial min-max game to optimize causal features while penalizing confounding ones. Extensive experiments demonstrate the state-of-the-art performance of our proposed method. Code is available at https://github.com/WUTCM-Lab/CausalCLIPSeg.",
        "arxiv_id": "2503.15949",
        "ARXIVID": "2503.15949",
        "COMMENT": "Matches criterion 2 as it leverages CLIP, a vision-language model, for medical image segmentation with causal intervention, which is a novel application.",
        "RELEVANCE": 6,
        "NOVELTY": 6
    },
    "2503.15851": {
        "authors": [
            "Zhou Zhenglin",
            "Ma Fan",
            "Fan Hehe",
            "Chua Tat-Seng"
        ],
        "title": "Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video Diffusion",
        "abstract": "arXiv:2503.15851v1 Announce Type: new  Abstract: Animatable head avatar generation typically requires extensive data for training. To reduce the data requirements, a natural solution is to leverage existing data-free static avatar generation methods, such as pre-trained diffusion models with score distillation sampling (SDS), which align avatars with pseudo ground-truth outputs from the diffusion model. However, directly distilling 4D avatars from video diffusion often leads to over-smooth results due to spatial and temporal inconsistencies in the generated video. To address this issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial and temporal consistency dataset for 4D avatar reconstruction using the video diffusion model. Specifically, Zero-1-to-A iteratively constructs video datasets and optimizes animatable avatars in a progressive manner, ensuring that avatar quality increases smoothly and consistently throughout the learning process. This progressive learning involves two stages: (1) Spatial Consistency Learning fixes expressions and learns from front-to-side views, and (2) Temporal Consistency Learning fixes views and learns from relaxed to exaggerated expressions, generating 4D avatars in a simple-to-complex manner. Extensive experiments demonstrate that Zero-1-to-A improves fidelity, animation quality, and rendering speed compared to existing diffusion-based methods, providing a solution for lifelike avatar creation. Code is publicly available at: https://github.com/ZhenglinZhou/Zero-1-to-A.",
        "arxiv_id": "2503.15851",
        "ARXIVID": "2503.15851",
        "COMMENT": "This paper introduces Zero-1-to-A, a method for animatable head avatar generation using video diffusion, which aligns with criterion 4 as it applies diffusion models in a novel way.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.16318": {
        "authors": [
            "Edgar Sucar",
            "Zihang Lai",
            "Eldar Insafutdinov",
            "Andrea Vedaldi"
        ],
        "title": "Dynamic Point Maps: A Versatile Representation for Dynamic 3D Reconstruction",
        "abstract": "arXiv:2503.16318v1 Announce Type: new  Abstract: DUSt3R has recently shown that one can reduce many tasks in multi-view geometry, including estimating camera intrinsics and extrinsics, reconstructing the scene in 3D, and establishing image correspondences, to the prediction of a pair of viewpoint-invariant point maps, i.e., pixel-aligned point clouds defined in a common reference frame. This formulation is elegant and powerful, but unable to tackle dynamic scenes. To address this challenge, we introduce the concept of Dynamic Point Maps (DPM), extending standard point maps to support 4D tasks such as motion segmentation, scene flow estimation, 3D object tracking, and 2D correspondence. Our key intuition is that, when time is introduced, there are several possible spatial and time references that can be used to define the point maps. We identify a minimal subset of such combinations that can be regressed by a network to solve the sub tasks mentioned above. We train a DPM predictor on a mixture of synthetic and real data and evaluate it across diverse benchmarks for video depth prediction, dynamic point cloud reconstruction, 3D scene flow and object pose tracking, achieving state-of-the-art performance. Code, models and additional results are available at https://www.robots.ox.ac.uk/~vgg/research/dynamic-point-maps/.",
        "arxiv_id": "2503.16318",
        "ARXIVID": "2503.16318",
        "COMMENT": "This paper introduces Dynamic Point Maps for dynamic 3D reconstruction, which aligns with criterion 3 as it proposes a novel representation for dynamic scenes.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.16125": {
        "authors": [
            "Jiangyi Wang",
            "Na Zhao"
        ],
        "title": "Uncertainty Meets Diversity: A Comprehensive Active Learning Framework for Indoor 3D Object Detection",
        "abstract": "arXiv:2503.16125v1 Announce Type: new  Abstract: Active learning has emerged as a promising approach to reduce the substantial annotation burden in 3D object detection tasks, spurring several initiatives in outdoor environments. However, its application in indoor environments remains unexplored. Compared to outdoor 3D datasets, indoor datasets face significant challenges, including fewer training samples per class, a greater number of classes, more severe class imbalance, and more diverse scene types and intra-class variances. This paper presents the first study on active learning for indoor 3D object detection, where we propose a novel framework tailored for this task. Our method incorporates two key criteria - uncertainty and diversity - to actively select the most ambiguous and informative unlabeled samples for annotation. The uncertainty criterion accounts for both inaccurate detections and undetected objects, ensuring that the most ambiguous samples are prioritized. Meanwhile, the diversity criterion is formulated as a joint optimization problem that maximizes the diversity of both object class distributions and scene types, using a new Class-aware Adaptive Prototype (CAP) bank. The CAP bank dynamically allocates representative prototypes to each class, helping to capture varying intra-class diversity across different categories. We evaluate our method on SUN RGB-D and ScanNetV2, where it outperforms baselines by a significant margin, achieving over 85% of fully-supervised performance with just 10% of the annotation budget.",
        "arxiv_id": "2503.16125",
        "ARXIVID": "2503.16125",
        "COMMENT": "Matches criterion 3 as it introduces a novel active learning framework for indoor 3D object detection, addressing previously unexplored challenges.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.16429": {
        "authors": [
            "Xiaoyang Wu",
            "Daniel DeTone",
            "Duncan Frost",
            "Tianwei Shen",
            "Chris Xie",
            "Nan Yang",
            "Jakob Engel",
            "Richard Newcombe",
            "Hengshuang Zhao",
            "Julian Straub"
        ],
        "title": "Sonata: Self-Supervised Learning of Reliable Point Representations",
        "abstract": "arXiv:2503.16429v1 Announce Type: new  Abstract: In this paper, we question whether we have a reliable self-supervised point cloud model that can be used for diverse 3D tasks via simple linear probing, even with limited data and minimal computation. We find that existing 3D self-supervised learning approaches fall short when evaluated on representation quality through linear probing. We hypothesize that this is due to what we term the \"geometric shortcut\", which causes representations to collapse to low-level spatial features. This challenge is unique to 3D and arises from the sparse nature of point cloud data. We address it through two key strategies: obscuring spatial information and enhancing the reliance on input features, ultimately composing a Sonata of 140k point clouds through self-distillation. Sonata is simple and intuitive, yet its learned representations are strong and reliable: zero-shot visualizations demonstrate semantic grouping, alongside strong spatial reasoning through nearest-neighbor relationships. Sonata demonstrates exceptional parameter and data efficiency, tripling linear probing accuracy (from 21.8% to 72.5%) on ScanNet and nearly doubling performance with only 1% of the data compared to previous approaches. Full fine-tuning further advances SOTA across both 3D indoor and outdoor perception tasks.",
        "arxiv_id": "2503.16429",
        "ARXIVID": "2503.16429",
        "COMMENT": "Matches criterion 3 as it proposes a novel self-supervised learning method (Sonata) for 3D tasks, addressing unique challenges in point cloud data.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.16185": {
        "authors": [
            "Peihao Wu",
            "Yongxiang Yao",
            "Wenfei Zhang",
            "Dong Wei",
            "Yi Wan",
            "Yansheng Li",
            "Yongjun Zhang"
        ],
        "title": "MapGlue: Multimodal Remote Sensing Image Matching",
        "abstract": "arXiv:2503.16185v1 Announce Type: new  Abstract: Multimodal remote sensing image (MRSI) matching is pivotal for cross-modal fusion, localization, and object detection, but it faces severe challenges due to geometric, radiometric, and viewpoint discrepancies across imaging modalities. Existing unimodal datasets lack scale and diversity, limiting deep learning solutions. This paper proposes MapGlue, a universal MRSI matching framework, and MapData, a large-scale multimodal dataset addressing these gaps. Our contributions are twofold. MapData, a globally diverse dataset spanning 233 sampling points, offers original images (7,000x5,000 to 20,000x15,000 pixels). After rigorous cleaning, it provides 121,781 aligned electronic map-visible image pairs (512x512 pixels) with hybrid manual-automated ground truth, addressing the scarcity of scalable multimodal benchmarks. MapGlue integrates semantic context with a dual graph-guided mechanism to extract cross-modal invariant features. This structure enables global-to-local interaction, enhancing descriptor robustness against modality-specific distortions. Extensive evaluations on MapData and five public datasets demonstrate MapGlue's superiority in matching accuracy under complex conditions, outperforming state-of-the-art methods. Notably, MapGlue generalizes effectively to unseen modalities without retraining, highlighting its adaptability. This work addresses longstanding challenges in MRSI matching by combining scalable dataset construction with a robust, semantics-driven framework. Furthermore, MapGlue shows strong generalization capabilities on other modality matching tasks for which it was not specifically trained. The dataset and code are available at https://github.com/PeihaoWu/MapGlue.",
        "arxiv_id": "2503.16185",
        "ARXIVID": "2503.16185",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark (MapData) and a novel method (MapGlue) for multimodal remote sensing image matching.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.15917": {
        "authors": [
            "Beilei Cui",
            "Long Bai",
            "Mobarakol Islam",
            "An Wang",
            "Zhiqi Ma",
            "Yiming Huang",
            "Feng Li",
            "Zhen Chen",
            "Zhongliang Jiang",
            "Nassir Navab",
            "Hongliang Ren"
        ],
        "title": "Learning to Efficiently Adapt Foundation Models for Self-Supervised Endoscopic 3D Scene Reconstruction from Any Cameras",
        "abstract": "arXiv:2503.15917v1 Announce Type: new  Abstract: Accurate 3D scene reconstruction is essential for numerous medical tasks. Given the challenges in obtaining ground truth data, there has been an increasing focus on self-supervised learning (SSL) for endoscopic depth estimation as a basis for scene reconstruction. While foundation models have shown remarkable progress in visual tasks, their direct application to the medical domain often leads to suboptimal results. However, the visual features from these models can still enhance endoscopic tasks, emphasizing the need for efficient adaptation strategies, which still lack exploration currently. In this paper, we introduce Endo3DAC, a unified framework for endoscopic scene reconstruction that efficiently adapts foundation models. We design an integrated network capable of simultaneously estimating depth maps, relative poses, and camera intrinsic parameters. By freezing the backbone foundation model and training only the specially designed Gated Dynamic Vector-Based Low-Rank Adaptation (GDV-LoRA) with separate decoder heads, Endo3DAC achieves superior depth and pose estimation while maintaining training efficiency. Additionally, we propose a 3D scene reconstruction pipeline that optimizes depth maps' scales, shifts, and a few parameters based on our integrated network. Extensive experiments across four endoscopic datasets demonstrate that Endo3DAC significantly outperforms other state-of-the-art methods while requiring fewer trainable parameters. To our knowledge, we are the first to utilize a single network that only requires surgical videos to perform both SSL depth estimation and scene reconstruction tasks. The code will be released upon acceptance.",
        "arxiv_id": "2503.15917",
        "ARXIVID": "2503.15917",
        "COMMENT": "Endo3DAC introduces a framework for self-supervised 3D scene reconstruction in endoscopy, aligning with criterion 1 as it focuses on spatial understanding and intelligence in embodied agents.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.16420": {
        "authors": [
            "Paul Engstler",
            "Aleksandar Shtedritski",
            "Iro Laina",
            "Christian Rupprecht",
            "Andrea Vedaldi"
        ],
        "title": "SynCity: Training-Free Generation of 3D Worlds",
        "abstract": "arXiv:2503.16420v1 Announce Type: new  Abstract: We address the challenge of generating 3D worlds from textual descriptions. We propose SynCity, a training- and optimization-free approach, which leverages the geometric precision of pre-trained 3D generative models and the artistic versatility of 2D image generators to create large, high-quality 3D spaces. While most 3D generative models are object-centric and cannot generate large-scale worlds, we show how 3D and 2D generators can be combined to generate ever-expanding scenes. Through a tile-based approach, we allow fine-grained control over the layout and the appearance of scenes. The world is generated tile-by-tile, and each new tile is generated within its world-context and then fused with the scene. SynCity generates compelling and immersive scenes that are rich in detail and diversity.",
        "arxiv_id": "2503.16420",
        "ARXIVID": "2503.16420",
        "COMMENT": "This paper proposes SynCity, a method for generating 3D worlds from textual descriptions, which aligns with criterion 3 as it introduces a novel approach to generating 3D environments without training.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.15877": {
        "authors": [
            "Tiange Xiang",
            "Kai Li",
            "Chengjiang Long",
            "Christian H\\\"ane",
            "Peihong Guo",
            "Scott Delp",
            "Ehsan Adeli",
            "Li Fei-Fei"
        ],
        "title": "Repurposing 2D Diffusion Models with Gaussian Atlas for 3D Generation",
        "abstract": "arXiv:2503.15877v1 Announce Type: new  Abstract: Recent advances in text-to-image diffusion models have been driven by the increasing availability of paired 2D data. However, the development of 3D diffusion models has been hindered by the scarcity of high-quality 3D data, resulting in less competitive performance compared to their 2D counterparts. To address this challenge, we propose repurposing pre-trained 2D diffusion models for 3D object generation. We introduce Gaussian Atlas, a novel representation that utilizes dense 2D grids, enabling the fine-tuning of 2D diffusion models to generate 3D Gaussians. Our approach demonstrates successful transfer learning from a pre-trained 2D diffusion model to a 2D manifold flattened from 3D structures. To support model training, we compile GaussianVerse, a large-scale dataset comprising 205K high-quality 3D Gaussian fittings of various 3D objects. Our experimental results show that text-to-image diffusion models can be effectively adapted for 3D content generation, bridging the gap between 2D and 3D modeling.",
        "arxiv_id": "2503.15877",
        "ARXIVID": "2503.15877",
        "COMMENT": "Matches criteria 4 as it repurposes 2D diffusion models for 3D generation, bridging 2D and 3D modeling in vision foundation models.",
        "RELEVANCE": 5,
        "NOVELTY": 7
    },
    "2503.15867": {
        "authors": [
            "Rohit Kundu",
            "Athula Balachandran",
            "Amit K. Roy-Chowdhury"
        ],
        "title": "TruthLens: Explainable DeepFake Detection for Face Manipulated and Fully Synthetic Data",
        "abstract": "arXiv:2503.15867v1 Announce Type: new  Abstract: Detecting DeepFakes has become a crucial research area as the widespread use of AI image generators enables the effortless creation of face-manipulated and fully synthetic content, yet existing methods are often limited to binary classification (real vs. fake) and lack interpretability. To address these challenges, we propose TruthLens, a novel and highly generalizable framework for DeepFake detection that not only determines whether an image is real or fake but also provides detailed textual reasoning for its predictions. Unlike traditional methods, TruthLens effectively handles both face-manipulated DeepFakes and fully AI-generated content while addressing fine-grained queries such as \"Does the eyes/nose/mouth look real or fake?\"   The architecture of TruthLens combines the global contextual understanding of multimodal large language models like PaliGemma2 with the localized feature extraction capabilities of vision-only models like DINOv2. This hybrid design leverages the complementary strengths of both models, enabling robust detection of subtle manipulations while maintaining interpretability. Extensive experiments on diverse datasets demonstrate that TruthLens outperforms state-of-the-art methods in detection accuracy (by 2-14%) and explainability, in both in-domain and cross-data settings, generalizing effectively across traditional and emerging manipulation techniques.",
        "arxiv_id": "2503.15867",
        "ARXIVID": "2503.15867",
        "COMMENT": "Matches criterion 2 as it combines multimodal large language models (VLLMs) with vision-only models for explainable DeepFake detection.",
        "RELEVANCE": 6,
        "NOVELTY": 5
    },
    "2503.15661": {
        "authors": [
            "Shravan Nayak",
            "Xiangru Jian",
            "Kevin Qinghong Lin",
            "Juan A. Rodriguez",
            "Montek Kalsi",
            "Rabiul Awal",
            "Nicolas Chapados",
            "M. Tamer \\\"Ozsu",
            "Aishwarya Agrawal",
            "David Vazquez",
            "Christopher Pal",
            "Perouz Taslakian",
            "Spandana Gella",
            "Sai Rajeswar"
        ],
        "title": "UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction",
        "abstract": "arXiv:2503.15661v1 Announce Type: new  Abstract: Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate tasks like document editing and file management can greatly enhance computer workflows. While existing research focuses on online settings, desktop environments, critical for many professional and everyday tasks, remain underexplored due to data collection challenges and licensing issues. We introduce UI-Vision, the first comprehensive, license-permissive benchmark for offline, fine-grained evaluation of computer use agents in real-world desktop environments. Unlike online benchmarks, UI-Vision provides: (i) dense, high-quality annotations of human demonstrations, including bounding boxes, UI labels, and action trajectories (clicks, drags, and keyboard inputs) across 83 software applications, and (ii) three fine-to-coarse grained tasks-Element Grounding, Layout Grounding, and Action Prediction-with well-defined metrics to rigorously evaluate agents' performance in desktop environments. Our evaluation reveals critical limitations in state-of-the-art models like UI-TARS-72B, including issues with understanding professional software, spatial reasoning, and complex actions like drag-and-drop. These findings highlight the challenges in developing fully autonomous computer use agents. By releasing UI-Vision as open-source, we aim to advance the development of more capable agents for real-world desktop tasks.",
        "arxiv_id": "2503.15661",
        "ARXIVID": "2503.15661",
        "COMMENT": "This paper introduces UI-Vision, a benchmark for GUI-based visual perception and interaction, which aligns with criterion 3 as it proposes a new benchmark for embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.16397": {
        "authors": [
            "Nikita Starodubcev",
            "Denis Kuznedelev",
            "Artem Babenko",
            "Dmitry Baranchuk"
        ],
        "title": "Scale-wise Distillation of Diffusion Models",
        "abstract": "arXiv:2503.16397v1 Announce Type: new  Abstract: We present SwD, a scale-wise distillation framework for diffusion models (DMs), which effectively employs next-scale prediction ideas for diffusion-based few-step generators. In more detail, SwD is inspired by the recent insights relating diffusion processes to the implicit spectral autoregression. We suppose that DMs can initiate generation at lower data resolutions and gradually upscale the samples at each denoising step without loss in performance while significantly reducing computational costs. SwD naturally integrates this idea into existing diffusion distillation methods based on distribution matching. Also, we enrich the family of distribution matching approaches by introducing a novel patch loss enforcing finer-grained similarity to the target distribution. When applied to state-of-the-art text-to-image diffusion models, SwD approaches the inference times of two full resolution steps and significantly outperforms the counterparts under the same computation budget, as evidenced by automated metrics and human preference studies.",
        "arxiv_id": "2503.16397",
        "ARXIVID": "2503.16397",
        "COMMENT": "This paper presents SwD, a scale-wise distillation framework for diffusion models, which aligns with criterion 4 as it focuses on improving diffusion models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.15621": {
        "authors": [
            "Federico Cocchi",
            "Nicholas Moratelli",
            "Davide Caffagni",
            "Sara Sarto",
            "Lorenzo Baraldi",
            "Marcella Cornia",
            "Rita Cucchiara"
        ],
        "title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning",
        "abstract": "arXiv:2503.15621v1 Announce Type: new  Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has highlighted the critical roles of both the visual backbone and the underlying language model. While prior work has primarily focused on scaling these components to billions of parameters, the trade-offs between model size, architecture, and performance remain underexplored. Additionally, inconsistencies in training data and evaluation protocols have hindered direct comparisons, making it difficult to derive optimal design choices. In this paper, we introduce LLaVA-MORE, a new family of MLLMs that integrates recent language models with diverse visual backbones. To ensure fair comparisons, we employ a unified training protocol applied consistently across all architectures. Our analysis systematically explores both small- and medium-scale LLMs -- including Phi-4, LLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and instruction following, while examining the relationship between model size and performance. Beyond evaluating the LLM impact on final results, we conduct a comprehensive study of various visual encoders, ranging from CLIP-based architectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional experiments investigate the effects of increased image resolution and variations in pre-training datasets. Overall, our results provide insights into the design of more effective MLLMs, offering a reproducible evaluation framework that facilitates direct comparisons and can guide future model development. Our source code and trained models are publicly available at: https://github.com/aimagelab/LLaVA-MORE.",
        "arxiv_id": "2503.15621",
        "ARXIVID": "2503.15621",
        "COMMENT": "This paper introduces LLaVA-MORE, a study on MLLMs with a focus on visual backbones and language models, which aligns with criterion 2.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.15927": {
        "authors": [
            "Hui Zhang",
            "Tingwei Gao",
            "Jie Shao",
            "Zuxuan Wu"
        ],
        "title": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to Accelerate Diffusion Transformers",
        "abstract": "arXiv:2503.15927v1 Announce Type: new  Abstract: Diffusion models have demonstrated impressive generation capabilities, particularly with recent advancements leveraging transformer architectures to improve both visual and artistic quality. However, Diffusion Transformers (DiTs) continue to encounter challenges related to low inference speed, primarily due to the iterative denoising process. To address this issue, we propose BlockDance, a training-free approach that explores feature similarities at adjacent time steps to accelerate DiTs. Unlike previous feature-reuse methods that lack tailored reuse strategies for features at different scales, BlockDance prioritizes the identification of the most structurally similar features, referred to as Structurally Similar Spatio-Temporal (STSS) features. These features are primarily located within the structure-focused blocks of the transformer during the later stages of denoising. BlockDance caches and reuses these highly similar features to mitigate redundant computation, thereby accelerating DiTs while maximizing consistency with the generated results of the original model. Furthermore, considering the diversity of generated content and the varying distributions of redundant features, we introduce BlockDance-Ada, a lightweight decision-making network tailored for instance-specific acceleration. BlockDance-Ada dynamically allocates resources and provides superior content quality. Both BlockDance and BlockDance-Ada have proven effective across various generation tasks and models, achieving accelerations between 25% and 50% while maintaining generation quality.",
        "arxiv_id": "2503.15927",
        "ARXIVID": "2503.15927",
        "COMMENT": "This paper proposes a novel method (BlockDance) to accelerate Diffusion Transformers by reusing structurally similar spatio-temporal features, which aligns with criterion 1.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.16036": {
        "authors": [
            "Zhihang Liu",
            "Chen-Wei Xie",
            "Pandeng Li",
            "Liming Zhao",
            "Longxiang Tang",
            "Yun Zheng",
            "Chuanbin Liu",
            "Hongtao Xie"
        ],
        "title": "Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models",
        "abstract": "arXiv:2503.16036v1 Announce Type: new  Abstract: Recent Multi-modal Large Language Models (MLLMs) have been challenged by the computational overhead resulting from massive video frames, often alleviated through compression strategies. However, the visual content is not equally contributed to user instructions, existing strategies (\\eg, average pool) inevitably lead to the loss of potentially useful information. To tackle this, we propose the Hybrid-level Instruction Injection Strategy for Conditional Token Compression in MLLMs (HICom), utilizing the instruction as a condition to guide the compression from both local and global levels. This encourages the compression to retain the maximum amount of user-focused information while reducing visual tokens to minimize computational burden. Specifically, the instruction condition is injected into the grouped visual tokens at the local level and the learnable tokens at the global level, and we conduct the attention mechanism to complete the conditional compression. From the hybrid-level compression, the instruction-relevant visual parts are highlighted while the temporal-spatial structure is also preserved for easier understanding of LLMs. To further unleash the potential of HICom, we introduce a new conditional pre-training stage with our proposed dataset HICom-248K. Experiments show that our HICom can obtain distinguished video understanding ability with fewer tokens, increasing the performance by 2.43\\% average on three multiple-choice QA benchmarks and saving 78.8\\% tokens compared with the SOTA method. The code is available at https://github.com/lntzm/HICom.",
        "arxiv_id": "2503.16036",
        "ARXIVID": "2503.16036",
        "COMMENT": "Matches criterion 2 as it proposes a novel token compression strategy for MLLMs, improving video understanding in multi-modal tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.15816": {
        "authors": [
            "Abduljaleel Adejumo",
            "Faegheh Yeganli",
            "Clifford Broni-bediako",
            "Aoran Xiao",
            "Naoto Yokoya",
            "Mennatullah Siam"
        ],
        "title": "A Vision Centric Remote Sensing Benchmark",
        "abstract": "arXiv:2503.15816v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in vision-language tasks but their remote sensing (RS) counterpart are relatively under explored. Unlike natural images, RS imagery presents unique challenges that current MLLMs struggle to handle, particularly in visual grounding and spatial reasoning. This study investigates the limitations of CLIP-based MLLMs in RS, highlighting their failure to differentiate visually distinct yet semantically similar RS images. To address this, we introduce a remote sensing multimodal visual patterns (RSMMVP) benchmark. It is designed to evaluate MLLMs in RS tasks by identifying the CLIP-blind pairs, where CLIP-based models incorrectly assign high similarity scores to visually distinct RS images. Through a visual question answering (VQA) evaluation, we analyze the performance of state-of-the-art MLLMs, revealing significant limitations in RS specific representation learning. The results provide valuable insights into the weaknesses of CLIP-based visual encoding and offer a foundation for future research to develop more effective MLLMs tailored for remote sensing applications.",
        "arxiv_id": "2503.15816",
        "ARXIVID": "2503.15816",
        "COMMENT": "Matches criterion 4 as it evaluates MLLMs in remote sensing and introduces a benchmark for vision-language tasks in this domain.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.16422": {
        "authors": [
            "Yuheng Yuan",
            "Qiuhong Shen",
            "Xingyi Yang",
            "Xinchao Wang"
        ],
        "title": "1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering",
        "abstract": "arXiv:2503.16422v1 Announce Type: new  Abstract: 4D Gaussian Splatting (4DGS) has recently gained considerable attention as a method for reconstructing dynamic scenes. Despite achieving superior quality, 4DGS typically requires substantial storage and suffers from slow rendering speed. In this work, we delve into these issues and identify two key sources of temporal redundancy. (Q1) \\textbf{Short-Lifespan Gaussians}: 4DGS uses a large portion of Gaussians with short temporal span to represent scene dynamics, leading to an excessive number of Gaussians. (Q2) \\textbf{Inactive Gaussians}: When rendering, only a small subset of Gaussians contributes to each frame. Despite this, all Gaussians are processed during rasterization, resulting in redundant computation overhead. To address these redundancies, we present \\textbf{4DGS-1K}, which runs at over 1000 FPS on modern GPUs. For Q1, we introduce the Spatial-Temporal Variation Score, a new pruning criterion that effectively removes short-lifespan Gaussians while encouraging 4DGS to capture scene dynamics using Gaussians with longer temporal spans. For Q2, we store a mask for active Gaussians across consecutive frames, significantly reducing redundant computations in rendering. Compared to vanilla 4DGS, our method achieves a $41\\times$ reduction in storage and $9\\times$ faster rasterization speed on complex dynamic scenes, while maintaining comparable visual quality. Please see our project page at https://4DGS-1K.github.io.",
        "arxiv_id": "2503.16422",
        "ARXIVID": "2503.16422",
        "COMMENT": "This paper proposes 4DGS-1K, a method for efficient dynamic scene rendering, aligning with criterion 4 as it focuses on vision foundation models and their applications.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.16068": {
        "authors": [
            "Longbin Ji",
            "Lei Zhong",
            "Pengfei Wei",
            "Changjian Li"
        ],
        "title": "PoseTraj: Pose-Aware Trajectory Control in Video Diffusion",
        "abstract": "arXiv:2503.16068v1 Announce Type: new  Abstract: Recent advancements in trajectory-guided video generation have achieved notable progress. However, existing models still face challenges in generating object motions with potentially changing 6D poses under wide-range rotations, due to limited 3D understanding. To address this problem, we introduce PoseTraj, a pose-aware video dragging model for generating 3D-aligned motion from 2D trajectories. Our method adopts a novel two-stage pose-aware pretraining framework, improving 3D understanding across diverse trajectories. Specifically, we propose a large-scale synthetic dataset PoseTraj-10K, containing 10k videos of objects following rotational trajectories, and enhance the model perception of object pose changes by incorporating 3D bounding boxes as intermediate supervision signals. Following this, we fine-tune the trajectory-controlling module on real-world videos, applying an additional camera-disentanglement module to further refine motion accuracy. Experiments on various benchmark datasets demonstrate that our method not only excels in 3D pose-aligned dragging for rotational trajectories but also outperforms existing baselines in trajectory accuracy and video quality.",
        "arxiv_id": "2503.16068",
        "ARXIVID": "2503.16068",
        "COMMENT": "PoseTraj introduces a pose-aware video generation model with a focus on 3D understanding, aligning with criterion 3 as it proposes a novel method for trajectory-guided video generation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.15940": {
        "authors": [
            "Yaxiong Chen",
            "Chuang Du",
            "Chunlei Li",
            "Jingliang Hu",
            "Yilei Shi",
            "Shengwu Xiong",
            "Xiao Xiang Zhu",
            "Lichao Mou"
        ],
        "title": "UniCrossAdapter: Multimodal Adaptation of CLIP for Radiology Report Generation",
        "abstract": "arXiv:2503.15940v1 Announce Type: new  Abstract: Automated radiology report generation aims to expedite the tedious and error-prone reporting process for radiologists. While recent works have made progress, learning to align medical images and textual findings remains challenging due to the relative scarcity of labeled medical data. For example, datasets for this task are much smaller than those used for image captioning in computer vision. In this work, we propose to transfer representations from CLIP, a large-scale pre-trained vision-language model, to better capture cross-modal semantics between images and texts. However, directly applying CLIP is suboptimal due to the domain gap between natural images and radiology. To enable efficient adaptation, we introduce UniCrossAdapter, lightweight adapter modules that are incorporated into CLIP and fine-tuned on the target task while keeping base parameters fixed. The adapters are distributed across modalities and their interaction to enhance vision-language alignment. Experiments on two public datasets demonstrate the effectiveness of our approach, advancing state-of-the-art in radiology report generation. The proposed transfer learning framework provides a means of harnessing semantic knowledge from large-scale pre-trained models to tackle data-scarce medical vision-language tasks. Code is available at https://github.com/chauncey-tow/MRG-CLIP.",
        "arxiv_id": "2503.15940",
        "ARXIVID": "2503.15940",
        "COMMENT": "This paper proposes UniCrossAdapter for adapting CLIP to radiology report generation. It aligns with criterion 2 as it involves adapting a vision-language model (CLIP) for a specific application.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.15875": {
        "authors": [
            "Haiguang Wang",
            "Daqi Liu",
            "Hongwei Xie",
            "Haisong Liu",
            "Enhui Ma",
            "Kaicheng Yu",
            "Limin Wang",
            "Bing Wang"
        ],
        "title": "MiLA: Multi-view Intensive-fidelity Long-term Video Generation World Model for Autonomous Driving",
        "abstract": "arXiv:2503.15875v1 Announce Type: new  Abstract: In recent years, data-driven techniques have greatly advanced autonomous driving systems, but the need for rare and diverse training data remains a challenge, requiring significant investment in equipment and labor. World models, which predict and generate future environmental states, offer a promising solution by synthesizing annotated video data for training. However, existing methods struggle to generate long, consistent videos without accumulating errors, especially in dynamic scenes. To address this, we propose MiLA, a novel framework for generating high-fidelity, long-duration videos up to one minute. MiLA utilizes a Coarse-to-Re(fine) approach to both stabilize video generation and correct distortion of dynamic objects. Additionally, we introduce a Temporal Progressive Denoising Scheduler and Joint Denoising and Correcting Flow modules to improve the quality of generated videos. Extensive experiments on the nuScenes dataset show that MiLA achieves state-of-the-art performance in video generation quality. For more information, visit the project website: https://github.com/xiaomi-mlab/mila.github.io.",
        "arxiv_id": "2503.15875",
        "ARXIVID": "2503.15875",
        "COMMENT": "MiLA introduces a novel framework for long-term video generation for autonomous driving, which aligns with criterion 3 as it focuses on a new method for generating training data for embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.16421": {
        "authors": [
            "Quanhao Li",
            "Zhen Xing",
            "Rui Wang",
            "Hui Zhang",
            "Qi Dai",
            "Zuxuan Wu"
        ],
        "title": "MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance",
        "abstract": "arXiv:2503.16421v1 Announce Type: new  Abstract: Recent advances in video generation have led to remarkable improvements in visual quality and temporal coherence. Upon this, trajectory-controllable video generation has emerged to enable precise object motion control through explicitly defined spatial paths. However, existing methods struggle with complex object movements and multi-object motion control, resulting in imprecise trajectory adherence, poor object consistency, and compromised visual quality. Furthermore, these methods only support trajectory control in a single format, limiting their applicability in diverse scenarios. Additionally, there is no publicly available dataset or benchmark specifically tailored for trajectory-controllable video generation, hindering robust training and systematic evaluation. To address these challenges, we introduce MagicMotion, a novel image-to-video generation framework that enables trajectory control through three levels of conditions from dense to sparse: masks, bounding boxes, and sparse boxes. Given an input image and trajectories, MagicMotion seamlessly animates objects along defined trajectories while maintaining object consistency and visual quality. Furthermore, we present MagicData, a large-scale trajectory-controlled video dataset, along with an automated pipeline for annotation and filtering. We also introduce MagicBench, a comprehensive benchmark that assesses both video quality and trajectory control accuracy across different numbers of objects. Extensive experiments demonstrate that MagicMotion outperforms previous methods across various metrics. Our project page are publicly available at https://quanhaol.github.io/magicmotion-site.",
        "arxiv_id": "2503.16421",
        "ARXIVID": "2503.16421",
        "COMMENT": "MagicMotion introduces a new framework for trajectory-controllable video generation and a benchmark (MagicBench), aligning with criterion 3 as it focuses on a novel benchmark and method for embodied AI.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.16423": {
        "authors": [
            "Ron Campos",
            "Ashmal Vayani",
            "Parth Parag Kulkarni",
            "Rohit Gupta",
            "Aritra Dutta",
            "Mubarak Shah"
        ],
        "title": "GAEA: A Geolocation Aware Conversational Model",
        "abstract": "arXiv:2503.16423v1 Announce Type: new  Abstract: Image geolocalization, in which, traditionally, an AI model predicts the precise GPS coordinates of an image is a challenging task with many downstream applications. However, the user cannot utilize the model to further their knowledge other than the GPS coordinate; the model lacks an understanding of the location and the conversational ability to communicate with the user. In recent days, with tremendous progress of large multimodal models (LMMs) proprietary and open-source researchers have attempted to geolocalize images via LMMs. However, the issues remain unaddressed; beyond general tasks, for more specialized downstream tasks, one of which is geolocalization, LMMs struggle. In this work, we propose to solve this problem by introducing a conversational model GAEA that can provide information regarding the location of an image, as required by a user. No large-scale dataset enabling the training of such a model exists. Thus we propose a comprehensive dataset GAEA with 800K images and around 1.6M question answer pairs constructed by leveraging OpenStreetMap (OSM) attributes and geographical context clues. For quantitative evaluation, we propose a diverse benchmark comprising 4K image-text pairs to evaluate conversational capabilities equipped with diverse question types. We consider 11 state-of-the-art open-source and proprietary LMMs and demonstrate that GAEA significantly outperforms the best open-source model, LLaVA-OneVision by 25.69% and the best proprietary model, GPT-4o by 8.28%. Our dataset, model and codes are available",
        "arxiv_id": "2503.16423",
        "ARXIVID": "2503.16423",
        "COMMENT": "Matches criterion 2 as it introduces a geolocation-aware conversational model leveraging multimodal capabilities.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.15947": {
        "authors": [
            "Tianyi Hu",
            "Qingxu Fu",
            "Zhiqiang Pu",
            "Yuan Wang",
            "Tenghai Qiu"
        ],
        "title": "Unreal-MAP: Unreal-Engine-Based General Platform for Multi-Agent Reinforcement Learning",
        "abstract": "arXiv:2503.15947v1 Announce Type: new  Abstract: In this paper, we propose Unreal Multi-Agent Playground (Unreal-MAP), an MARL general platform based on the Unreal-Engine (UE). Unreal-MAP allows users to freely create multi-agent tasks using the vast visual and physical resources available in the UE community, and deploy state-of-the-art (SOTA) MARL algorithms within them. Unreal-MAP is user-friendly in terms of deployment, modification, and visualization, and all its components are open-source. We also develop an experimental framework compatible with algorithms ranging from rule-based to learning-based provided by third-party frameworks. Lastly, we deploy several SOTA algorithms in example tasks developed via Unreal-MAP, and conduct corresponding experimental analyses. We believe Unreal-MAP can play an important role in the MARL field by closely integrating existing algorithms with user-customized tasks, thus advancing the field of MARL.",
        "arxiv_id": "2503.15947",
        "ARXIVID": "2503.15947",
        "COMMENT": "Matches criterion 3 as it introduces a new simulator platform for multi-agent reinforcement learning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.16263": {
        "authors": [
            "Ayberk Acar",
            "Mariana Smith",
            "Lidia Al-Zogbi",
            "Tanner Watts",
            "Fangjie Li",
            "Hao Li",
            "Nural Yilmaz",
            "Paul Maria Scheikl",
            "Jesse F. d'Almeida",
            "Susheela Sharma",
            "Lauren Branscombe",
            "Tayfun Efe Ertop",
            "Robert J. Webster III",
            "Ipek Oguz",
            "Alan Kuntz",
            "Axel Krieger",
            "Jie Ying Wu"
        ],
        "title": "From Monocular Vision to Autonomous Action: Guiding Tumor Resection via 3D Reconstruction",
        "abstract": "arXiv:2503.16263v1 Announce Type: new  Abstract: Surgical automation requires precise guidance and understanding of the scene. Current methods in the literature rely on bulky depth cameras to create maps of the anatomy, however this does not translate well to space-limited clinical applications. Monocular cameras are small and allow minimally invasive surgeries in tight spaces but additional processing is required to generate 3D scene understanding. We propose a 3D mapping pipeline that uses only RGB images to create segmented point clouds of the target anatomy. To ensure the most precise reconstruction, we compare different structure from motion algorithms' performance on mapping the central airway obstructions, and test the pipeline on a downstream task of tumor resection. In several metrics, including post-procedure tissue model evaluation, our pipeline performs comparably to RGB-D cameras and, in some cases, even surpasses their performance. These promising results demonstrate that automation guidance can be achieved in minimally invasive procedures with monocular cameras. This study is a step toward the complete autonomy of surgical robots.",
        "arxiv_id": "2503.16263",
        "ARXIVID": "2503.16263",
        "COMMENT": "Matches criterion 1 as it focuses on spatial understanding using monocular vision for surgical automation.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.16378": {
        "authors": [
            "Tzu-Yun Tseng",
            "Alexey Nekrasov",
            "Malcolm Burdorf",
            "Bastian Leibe",
            "Julie Stephany Berrio",
            "Mao Shan",
            "Stewart Worrall"
        ],
        "title": "Panoptic-CUDAL Technical Report: Rural Australia Point Cloud Dataset in Rainy Conditions",
        "abstract": "arXiv:2503.16378v1 Announce Type: new  Abstract: Existing autonomous driving datasets are predominantly oriented towards well-structured urban settings and favorable weather conditions, leaving the complexities of rural environments and adverse weather conditions largely unaddressed. Although some datasets encompass variations in weather and lighting, bad weather scenarios do not appear often. Rainfall can significantly impair sensor functionality, introducing noise and reflections in LiDAR and camera data and reducing the system's capabilities for reliable environmental perception and safe navigation. We introduce the Panoptic-CUDAL dataset, a novel dataset purpose-built for panoptic segmentation in rural areas subject to rain. By recording high-resolution LiDAR, camera, and pose data, Panoptic-CUDAL offers a diverse, information-rich dataset in a challenging scenario. We present analysis of the recorded data and provide baseline results for panoptic and semantic segmentation methods on LiDAR point clouds. The dataset can be found here: https://robotics.sydney.edu.au/our-research/intelligent-transportation-systems/",
        "arxiv_id": "2503.16378",
        "ARXIVID": "2503.16378",
        "COMMENT": "Matches criterion 3 as it introduces a new benchmark dataset for autonomous driving in rural and rainy conditions.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.15672": {
        "authors": [
            "William Ljungbergh",
            "Adam Lilja",
            "Adam Tonderski. Arvid Laveno Ling",
            "Carl Lindstr\\\"om",
            "Willem Verbeke",
            "Junsheng Fu",
            "Christoffer Petersson",
            "Lars Hammarstrand",
            "Michael Felsberg"
        ],
        "title": "GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for Autonomous Driving",
        "abstract": "arXiv:2503.15672v1 Announce Type: new  Abstract: Self-supervised pre-training based on next-token prediction has enabled large language models to capture the underlying structure of text, and has led to unprecedented performance on a large array of tasks when applied at scale. Similarly, autonomous driving generates vast amounts of spatiotemporal data, alluding to the possibility of harnessing scale to learn the underlying geometric and semantic structure of the environment and its evolution over time. In this direction, we propose a geometric and semantic self-supervised pre-training method, GASP, that learns a unified representation by predicting, at any queried future point in spacetime, (1) general occupancy, capturing the evolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle path through the environment; and (3) distilled high-level features from a vision foundation model. By modeling geometric and semantic 4D occupancy fields instead of raw sensor measurements, the model learns a structured, generalizable representation of the environment and its evolution through time. We validate GASP on multiple autonomous driving benchmarks, demonstrating significant improvements in semantic occupancy forecasting, online mapping, and ego trajectory prediction. Our results demonstrate that continuous 4D geometric and semantic occupancy prediction provides a scalable and effective pre-training paradigm for autonomous driving. For code and additional visualizations, see \\href{https://research.zenseact.com/publications/gasp/.",
        "arxiv_id": "2503.15672",
        "ARXIVID": "2503.15672",
        "COMMENT": "Matches criterion 4 as it incorporates vision foundation models for autonomous driving tasks.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.16260": {
        "authors": [
            "Zijian Li",
            "Jingjing Fu",
            "Lei Song",
            "Jiang Bian",
            "Jun Zhang",
            "Rui Wang"
        ],
        "title": "Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart Reasoning Data",
        "abstract": "arXiv:2503.16260v1 Announce Type: new  Abstract: Visual reasoning is crucial for multimodal large language models (MLLMs) to address complex chart queries, yet high-quality rationale data remains scarce. Existing methods leveraged (M)LLMs for data generation, but direct prompting often yields limited precision and diversity. In this paper, we propose \\textit{Chain of Functions (CoF)}, a novel programmatic reasoning data generation pipeline that utilizes freely-explored reasoning paths as supervision to ensure data precision and diversity. Specifically, it starts with human-free exploration among the atomic functions (e.g., maximum data and arithmetic operations) to generate diverse function chains, which are then translated into linguistic rationales and questions with only a moderate open-sourced LLM. \\textit{CoF} provides multiple benefits: 1) Precision: function-governed generation reduces hallucinations compared to freeform generation; 2) Diversity: enumerating function chains enables varied question taxonomies; 3) Explainability: function chains serve as built-in rationales, allowing fine-grained evaluation beyond overall accuracy; 4) Practicality: eliminating reliance on extremely large models. Employing \\textit{CoF}, we construct the \\textit{ChartCoF} dataset, with 1.4k complex reasoning Q\\&A for fine-grained analysis and 50k Q\\&A for reasoning enhancement. The fine-grained evaluation on \\textit{ChartCoF} reveals varying performance across question taxonomies for each MLLM, and the experiments also show that finetuning with \\textit{ChartCoF} achieves state-of-the-art performance among same-scale MLLMs on widely used benchmarks. Furthermore, the novel paradigm of function-governed rationale generation in \\textit{CoF} could inspire broader applications beyond charts.",
        "arxiv_id": "2503.16260",
        "ARXIVID": "2503.16260",
        "COMMENT": "Matches criteria 2 as it focuses on generating fine-grained chart reasoning data for multimodal large language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.15886": {
        "authors": [
            "Hui Liu",
            "Wenya Wang",
            "Kecheng Chen",
            "Jie Liu",
            "Yibing Liu",
            "Tiexin Qin",
            "Peisong He",
            "Xinghao Jiang",
            "Haoliang Li"
        ],
        "title": "Enhancing Zero-Shot Image Recognition in Vision-Language Models through Human-like Concept Guidance",
        "abstract": "arXiv:2503.15886v1 Announce Type: new  Abstract: In zero-shot image recognition tasks, humans demonstrate remarkable flexibility in classifying unseen categories by composing known simpler concepts. However, existing vision-language models (VLMs), despite achieving significant progress through large-scale natural language supervision, often underperform in real-world applications because of sub-optimal prompt engineering and the inability to adapt effectively to target classes. To address these issues, we propose a Concept-guided Human-like Bayesian Reasoning (CHBR) framework. Grounded in Bayes' theorem, CHBR models the concept used in human image recognition as latent variables and formulates this task by summing across potential concepts, weighted by a prior distribution and a likelihood function. To tackle the intractable computation over an infinite concept space, we introduce an importance sampling algorithm that iteratively prompts large language models (LLMs) to generate discriminative concepts, emphasizing inter-class differences. We further propose three heuristic approaches involving Average Likelihood, Confidence Likelihood, and Test Time Augmentation (TTA) Likelihood, which dynamically refine the combination of concepts based on the test image. Extensive evaluations across fifteen datasets demonstrate that CHBR consistently outperforms existing state-of-the-art zero-shot generalization methods.",
        "arxiv_id": "2503.15886",
        "ARXIVID": "2503.15886",
        "COMMENT": "Matches criteria 2 as it proposes a novel framework (CHBR) for improving zero-shot image recognition in vision-language models.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.16106": {
        "authors": [
            "Mohamad Hassan N C",
            "Divyam Gupta",
            "Mainak Singha",
            "Sai Bhargav Rongali",
            "Ankit Jha",
            "Muhammad Haris Khan",
            "Biplab Banerjee"
        ],
        "title": "OSLoPrompt: Bridging Low-Supervision Challenges and Open-Set Domain Generalization in CLIP",
        "abstract": "arXiv:2503.16106v1 Announce Type: new  Abstract: We introduce Low-Shot Open-Set Domain Generalization (LSOSDG), a novel paradigm unifying low-shot learning with open-set domain generalization (ODG). While prompt-based methods using models like CLIP have advanced DG, they falter in low-data regimes (e.g., 1-shot) and lack precision in detecting open-set samples with fine-grained semantics related to training classes. To address these challenges, we propose OSLOPROMPT, an advanced prompt-learning framework for CLIP with two core innovations. First, to manage limited supervision across source domains and improve DG, we introduce a domain-agnostic prompt-learning mechanism that integrates adaptable domain-specific cues and visually guided semantic attributes through a novel cross-attention module, besides being supported by learnable domain- and class-generic visual prompts to enhance cross-modal adaptability. Second, to improve outlier rejection during inference, we classify unfamiliar samples as \"unknown\" and train specialized prompts with systematically synthesized pseudo-open samples that maintain fine-grained relationships to known classes, generated through a targeted query strategy with off-the-shelf foundation models. This strategy enhances feature learning, enabling our model to detect open samples with varied granularity more effectively. Extensive evaluations across five benchmarks demonstrate that OSLOPROMPT establishes a new state-of-the-art in LSOSDG, significantly outperforming existing methods.",
        "arxiv_id": "2503.16106",
        "ARXIVID": "2503.16106",
        "COMMENT": "Matches criteria 4 as it focuses on improving CLIP-based vision-language models for domain generalization and low-shot learning.",
        "RELEVANCE": 5,
        "NOVELTY": 6
    },
    "2503.15647": {
        "authors": [
            "Jumanh Atoum",
            "Garrison L. H. Johnston",
            "Nabil Simaan",
            "Jie Ying Wu"
        ],
        "title": "Multi-Modal Gesture Recognition from Video and Surgical Tool Pose Information via Motion Invariants",
        "abstract": "arXiv:2503.15647v1 Announce Type: new  Abstract: Recognizing surgical gestures in real-time is a stepping stone towards automated activity recognition, skill assessment, intra-operative assistance, and eventually surgical automation. The current robotic surgical systems provide us with rich multi-modal data such as video and kinematics. While some recent works in multi-modal neural networks learn the relationships between vision and kinematics data, current approaches treat kinematics information as independent signals, with no underlying relation between tool-tip poses. However, instrument poses are geometrically related, and the underlying geometry can aid neural networks in learning gesture representation. Therefore, we propose combining motion invariant measures (curvature and torsion) with vision and kinematics data using a relational graph network to capture the underlying relations between different data streams. We show that gesture recognition improves when combining invariant signals with tool position, achieving 90.3\\% frame-wise accuracy on the JIGSAWS suturing dataset. Our results show that motion invariant signals coupled with position are better representations of gesture motion compared to traditional position and quaternion representations. Our results highlight the need for geometric-aware modeling of kinematics for gesture recognition.",
        "arxiv_id": "2503.15647",
        "ARXIVID": "2503.15647",
        "COMMENT": "This paper introduces a novel method for gesture recognition using multi-modal data, which is tangentially related to embodied AI but does not directly match any criterion.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2503.15973": {
        "authors": [
            "Zichen Liu",
            "Kunlun Xu",
            "Bing Su",
            "Xu Zou",
            "Yuxin Peng",
            "Jiahuan Zhou"
        ],
        "title": "STOP: Integrated Spatial-Temporal Dynamic Prompting for Video Understanding",
        "abstract": "arXiv:2503.15973v1 Announce Type: new  Abstract: Pre-trained on tremendous image-text pairs, vision-language models like CLIP have demonstrated promising zero-shot generalization across numerous image-based tasks. However, extending these capabilities to video tasks remains challenging due to limited labeled video data and high training costs. Recent video prompting methods attempt to adapt CLIP for video tasks by introducing learnable prompts, but they typically rely on a single static prompt for all video sequences, overlooking the diverse temporal dynamics and spatial variations that exist across frames. This limitation significantly hinders the model's ability to capture essential temporal information for effective video understanding. To address this, we propose an integrated Spatial-TempOral dynamic Prompting (STOP) model which consists of two complementary modules, the intra-frame spatial prompting and inter-frame temporal prompting. Our intra-frame spatial prompts are designed to adaptively highlight discriminative regions within each frame by leveraging intra-frame attention and temporal variation, allowing the model to focus on areas with substantial temporal dynamics and capture fine-grained spatial details. Additionally, to highlight the varying importance of frames for video understanding, we further introduce inter-frame temporal prompts, dynamically inserting prompts between frames with high temporal variance as measured by frame similarity. This enables the model to prioritize key frames and enhances its capacity to understand temporal dependencies across sequences. Extensive experiments on various video benchmarks demonstrate that STOP consistently achieves superior performance against state-of-the-art methods. The code is available at https://github.com/zhoujiahuan1991/CVPR2025-STOP.",
        "arxiv_id": "2503.15973",
        "ARXIVID": "2503.15973",
        "COMMENT": "This paper introduces a novel prompting method for video understanding, which is tangentially related to vision foundation models (criterion 4).",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2503.16412": {
        "authors": [
            "Ananta R. Bhattarai",
            "Xingzhe He",
            "Alla Sheffer",
            "Helge Rhodin"
        ],
        "title": "DreamTexture: Shape from Virtual Texture with Analysis by Augmentation",
        "abstract": "arXiv:2503.16412v1 Announce Type: new  Abstract: DreamFusion established a new paradigm for unsupervised 3D reconstruction from virtual views by combining advances in generative models and differentiable rendering. However, the underlying multi-view rendering, along with supervision from large-scale generative models, is computationally expensive and under-constrained. We propose DreamTexture, a novel Shape-from-Virtual-Texture approach that leverages monocular depth cues to reconstruct 3D objects. Our method textures an input image by aligning a virtual texture with the real depth cues in the input, exploiting the inherent understanding of monocular geometry encoded in modern diffusion models. We then reconstruct depth from the virtual texture deformation with a new conformal map optimization, which alleviates memory-intensive volumetric representations. Our experiments reveal that generative models possess an understanding of monocular shape cues, which can be extracted by augmenting and aligning texture cues -- a novel monocular reconstruction paradigm that we call Analysis by Augmentation.",
        "arxiv_id": "2503.16412",
        "ARXIVID": "2503.16412",
        "COMMENT": "This paper introduces a novel monocular reconstruction paradigm, which is tangentially related to spatial understanding but not directly tied to embodied agents.",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2503.16338": {
        "authors": [
            "Shengjun Zhang",
            "Xin Fei",
            "Fangfu Liu",
            "Haixu Song",
            "Yueqi Duan"
        ],
        "title": "Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images",
        "abstract": "arXiv:2503.16338v1 Announce Type: new  Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis performance. While conventional methods require per-scene optimization, more recently several feed-forward methods have been proposed to generate pixel-aligned Gaussian representations with a learnable network, which are generalizable to different scenes. However, these methods simply combine pixel-aligned Gaussians from multiple views as scene representations, thereby leading to artifacts and extra memory cost without fully capturing the relations of Gaussians from different images. In this paper, we propose Gaussian Graph Network (GGN) to generate efficient and generalizable Gaussian representations. Specifically, we construct Gaussian Graphs to model the relations of Gaussian groups from different views. To support message passing at Gaussian level, we reformulate the basic graph operations over Gaussian representations, enabling each Gaussian to benefit from its connected Gaussian groups with Gaussian feature fusion. Furthermore, we design a Gaussian pooling layer to aggregate various Gaussian groups for efficient representations. We conduct experiments on the large-scale RealEstate10K and ACID datasets to demonstrate the efficiency and generalization of our method. Compared to the state-of-the-art methods, our model uses fewer Gaussians and achieves better image quality with higher rendering speed.",
        "arxiv_id": "2503.16338",
        "ARXIVID": "2503.16338",
        "COMMENT": "This paper introduces a novel Gaussian Graph Network for efficient and generalizable 3D representations, which could be tangentially related to spatial understanding (criterion 1).",
        "RELEVANCE": 4,
        "NOVELTY": 6
    },
    "2503.16218": {
        "authors": [
            "Yu Cao",
            "Zengqun Zhao",
            "Ioannis Patras",
            "Shaogang Gong"
        ],
        "title": "Temporal Score Analysis for Understanding and Correcting Diffusion Artifacts",
        "abstract": "arXiv:2503.16218v1 Announce Type: new  Abstract: Visual artifacts remain a persistent challenge in diffusion models, even with training on massive datasets. Current solutions primarily rely on supervised detectors, yet lack understanding of why these artifacts occur in the first place. In our analysis, we identify three distinct phases in the diffusion generative process: Profiling, Mutation, and Refinement. Artifacts typically emerge during the Mutation phase, where certain regions exhibit anomalous score dynamics over time, causing abrupt disruptions in the normal evolution pattern. This temporal nature explains why existing methods focusing only on spatial uncertainty of the final output fail at effective artifact localization. Based on these insights, we propose ASCED (Abnormal Score Correction for Enhancing Diffusion), that detects artifacts by monitoring abnormal score dynamics during the diffusion process, with a trajectory-aware on-the-fly mitigation strategy that appropriate generation of noise in the detected areas. Unlike most existing methods that apply post hoc corrections, \\eg, by applying a noising-denoising scheme after generation, our mitigation strategy operates seamlessly within the existing diffusion process. Extensive experiments demonstrate that our proposed approach effectively reduces artifacts across diverse domains, matching or surpassing existing supervised methods without additional training.",
        "arxiv_id": "2503.16218",
        "ARXIVID": "2503.16218",
        "COMMENT": "Does not match any specific criteria but discusses diffusion models and artifact correction, which may be of general interest.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.16430": {
        "authors": [
            "Yuqing Wang",
            "Zhijie Lin",
            "Yao Teng",
            "Yuanzhi Zhu",
            "Shuhuai Ren",
            "Jiashi Feng",
            "Xihui Liu"
        ],
        "title": "Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation",
        "abstract": "arXiv:2503.16430v1 Announce Type: new  Abstract: Autoregressive visual generation models typically rely on tokenizers to compress images into tokens that can be predicted sequentially. A fundamental dilemma exists in token representation: discrete tokens enable straightforward modeling with standard cross-entropy loss, but suffer from information loss and tokenizer training instability; continuous tokens better preserve visual details, but require complex distribution modeling, complicating the generation pipeline. In this paper, we propose TokenBridge, which bridges this gap by maintaining the strong representation capacity of continuous tokens while preserving the modeling simplicity of discrete tokens. To achieve this, we decouple discretization from the tokenizer training process through post-training quantization that directly obtains discrete tokens from continuous representations. Specifically, we introduce a dimension-wise quantization strategy that independently discretizes each feature dimension, paired with a lightweight autoregressive prediction mechanism that efficiently model the resulting large token space. Extensive experiments show that our approach achieves reconstruction and generation quality on par with continuous methods while using standard categorical prediction. This work demonstrates that bridging discrete and continuous paradigms can effectively harness the strengths of both approaches, providing a promising direction for high-quality visual generation with simple autoregressive modeling. Project page: https://yuqingwang1029.github.io/TokenBridge.",
        "arxiv_id": "2503.16430",
        "ARXIVID": "2503.16430",
        "COMMENT": "This paper introduces TokenBridge, a novel method for bridging continuous and discrete tokens for autoregressive visual generation. It does not directly match any of the criteria but is relevant to generative modeling in computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.16302": {
        "authors": [
            "Zeqiang Lai",
            "Yunfei Zhao",
            "Zibo Zhao",
            "Haolin Liu",
            "Fuyun Wang",
            "Huiwen Shi",
            "Xianghui Yang",
            "Qinxiang Lin",
            "Jinwei Huang",
            "Yuhong Liu",
            "Jie Jiang",
            "Chunchao Guo",
            "Xiangyu Yue"
        ],
        "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
        "abstract": "arXiv:2503.16302v1 Announce Type: new  Abstract: 3D shape generation has greatly flourished through the development of so-called \"native\" 3D diffusion, particularly through the Vecset Diffusion Model (VDM). While recent advancements have shown promising results in generating high-resolution 3D shapes, VDM still struggles with high-speed generation. Challenges exist because of difficulties not only in accelerating diffusion sampling but also VAE decoding in VDM, areas under-explored in previous works. To address these challenges, we present FlashVDM, a systematic framework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables flexible diffusion sampling with as few as 5 inference steps and comparable quality, which is made possible by stabilizing consistency distillation with our newly introduced Progressive Flow Distillation. For VAE, we introduce a lightning vecset decoder equipped with Adaptive KV Selection, Hierarchical Volume Decoding, and Efficient Network Design. By exploiting the locality of the vecset and the sparsity of shape surface in the volume, our decoder drastically lowers FLOPs, minimizing the overall decoding overhead. We apply FlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic evaluation, we show that our model significantly outperforms existing fast 3D generation methods, achieving comparable performance to the state-of-the-art while reducing inference time by over 45x for reconstruction and 32x for generation. Code and models are available at https://github.com/Tencent/FlashVDM.",
        "arxiv_id": "2503.16302",
        "ARXIVID": "2503.16302",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling in 3D shape generation.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    },
    "2503.16322": {
        "authors": [
            "Ruonan Yu",
            "Songhua Liu",
            "Zhenxiong Tan",
            "Xinchao Wang"
        ],
        "title": "Ultra-Resolution Adaptation with Ease",
        "abstract": "arXiv:2503.16322v1 Announce Type: new  Abstract: Text-to-image diffusion models have achieved remarkable progress in recent years. However, training models for high-resolution image generation remains challenging, particularly when training data and computational resources are limited. In this paper, we explore this practical problem from two key perspectives: data and parameter efficiency, and propose a set of key guidelines for ultra-resolution adaptation termed \\emph{URAE}. For data efficiency, we theoretically and empirically demonstrate that synthetic data generated by some teacher models can significantly promote training convergence. For parameter efficiency, we find that tuning minor components of the weight matrices outperforms widely-used low-rank adapters when synthetic data are unavailable, offering substantial performance gains while maintaining efficiency. Additionally, for models leveraging guidance distillation, such as FLUX, we show that disabling classifier-free guidance, \\textit{i.e.}, setting the guidance scale to 1 during adaptation, is crucial for satisfactory performance. Extensive experiments validate that URAE achieves comparable 2K-generation performance to state-of-the-art closed-source models like FLUX1.1 [Pro] Ultra with only 3K samples and 2K iterations, while setting new benchmarks for 4K-resolution generation. Codes are available \\href{https://github.com/Huage001/URAE}{here}.",
        "arxiv_id": "2503.16322",
        "ARXIVID": "2503.16322",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and high-resolution image generation using diffusion models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.16418": {
        "authors": [
            "Liming Jiang",
            "Qing Yan",
            "Yumin Jia",
            "Zichuan Liu",
            "Hao Kang",
            "Xin Lu"
        ],
        "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity",
        "abstract": "arXiv:2503.16418v1 Announce Type: new  Abstract: Achieving flexible and high-fidelity identity-preserved image generation remains formidable, particularly with advanced Diffusion Transformers (DiTs) like FLUX. We introduce InfiniteYou (InfU), one of the earliest robust frameworks leveraging DiTs for this task. InfU addresses significant issues of existing methods, such as insufficient identity similarity, poor text-image alignment, and low generation quality and aesthetics. Central to InfU is InfuseNet, a component that injects identity features into the DiT base model via residual connections, enhancing identity similarity while maintaining generation capabilities. A multi-stage training strategy, including pretraining and supervised fine-tuning (SFT) with synthetic single-person-multiple-sample (SPMS) data, further improves text-image alignment, ameliorates image quality, and alleviates face copy-pasting. Extensive experiments demonstrate that InfU achieves state-of-the-art performance, surpassing existing baselines. In addition, the plug-and-play design of InfU ensures compatibility with various existing methods, offering a valuable contribution to the broader community.",
        "arxiv_id": "2503.16418",
        "ARXIVID": "2503.16418",
        "COMMENT": "Does not match any specific criterion but is relevant to generative modeling and identity-preserved image generation using diffusion transformers.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.16284": {
        "authors": [
            "Sharon Peled",
            "Yosef E. Maruvka",
            "Moti Freiman"
        ],
        "title": "PSA-MIL: A Probabilistic Spatial Attention-Based Multiple Instance Learning for Whole Slide Image Classification",
        "abstract": "arXiv:2503.16284v1 Announce Type: new  Abstract: Whole Slide Images (WSIs) are high-resolution digital scans widely used in medical diagnostics. WSI classification is typically approached using Multiple Instance Learning (MIL), where the slide is partitioned into tiles treated as interconnected instances. While attention-based MIL methods aim to identify the most informative tiles, they often fail to fully exploit the spatial relationships among them, potentially overlooking intricate tissue structures crucial for accurate diagnosis. To address this limitation, we propose Probabilistic Spatial Attention MIL (PSA-MIL), a novel attention-based MIL framework that integrates spatial context into the attention mechanism through learnable distance-decayed priors, formulated within a probabilistic interpretation of self-attention as a posterior distribution. This formulation enables a dynamic inference of spatial relationships during training, eliminating the need for predefined assumptions often imposed by previous approaches. Additionally, we suggest a spatial pruning strategy for the posterior, effectively reducing self-attention's quadratic complexity. To further enhance spatial modeling, we introduce a diversity loss that encourages variation among attention heads, ensuring each captures distinct spatial representations. Together, PSA-MIL enables a more data-driven and adaptive integration of spatial context, moving beyond predefined constraints. We achieve state-of-the-art performance across both contextual and non-contextual baselines, while significantly reducing computational costs.",
        "arxiv_id": "2503.16284",
        "ARXIVID": "2503.16284",
        "COMMENT": "This paper proposes a novel attention-based MIL framework for WSI classification, which does not match any specific criterion but is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15617": {
        "authors": [
            "Masud Ahmed",
            "Zahid Hasan",
            "Syed Arefinul Haque",
            "Abu Zaher Md Faridee",
            "Sanjay Purushotham",
            "Suya You",
            "Nirmalya Roy"
        ],
        "title": "CAM-Seg: A Continuous-valued Embedding Approach for Semantic Image Generation",
        "abstract": "arXiv:2503.15617v1 Announce Type: new  Abstract: Traditional transformer-based semantic segmentation relies on quantized embeddings. However, our analysis reveals that autoencoder accuracy on segmentation mask using quantized embeddings (e.g. VQ-VAE) is 8% lower than continuous-valued embeddings (e.g. KL-VAE). Motivated by this, we propose a continuous-valued embedding framework for semantic segmentation. By reformulating semantic mask generation as a continuous image-to-embedding diffusion process, our approach eliminates the need for discrete latent representations while preserving fine-grained spatial and semantic details. Our key contribution includes a diffusion-guided autoregressive transformer that learns a continuous semantic embedding space by modeling long-range dependencies in image features. Our framework contains a unified architecture combining a VAE encoder for continuous feature extraction, a diffusion-guided transformer for conditioned embedding generation, and a VAE decoder for semantic mask reconstruction. Our setting facilitates zero-shot domain adaptation capabilities enabled by the continuity of the embedding space. Experiments across diverse datasets (e.g., Cityscapes and domain-shifted variants) demonstrate state-of-the-art robustness to distribution shifts, including adverse weather (e.g., fog, snow) and viewpoint variations. Our model also exhibits strong noise resilience, achieving robust performance ($\\approx$ 95% AP compared to baseline) under gaussian noise, moderate motion blur, and moderate brightness/contrast variations, while experiencing only a moderate impact ($\\approx$ 90% AP compared to baseline) from 50% salt and pepper noise, saturation and hue shifts. Code available: https://github.com/mahmed10/CAMSS.git",
        "arxiv_id": "2503.15617",
        "ARXIVID": "2503.15617",
        "COMMENT": "Does not match any specific criteria but discusses semantic image generation, which is tangentially related to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15934": {
        "authors": [
            "Hongda Liu",
            "Longguang Wang",
            "Ye Zhang",
            "Ziru Yu",
            "Yulan Guo"
        ],
        "title": "SaMam: Style-aware State Space Model for Arbitrary Image Style Transfer",
        "abstract": "arXiv:2503.15934v1 Announce Type: new  Abstract: Global effective receptive field plays a crucial role for image style transfer (ST) to obtain high-quality stylized results. However, existing ST backbones (e.g., CNNs and Transformers) suffer huge computational complexity to achieve global receptive fields. Recently, the State Space Model (SSM), especially the improved variant Mamba, has shown great potential for long-range dependency modeling with linear complexity, which offers a approach to resolve the above dilemma. In this paper, we develop a Mamba-based style transfer framework, termed SaMam. Specifically, a mamba encoder is designed to efficiently extract content and style information. In addition, a style-aware mamba decoder is developed to flexibly adapt to various styles. Moreover, to address the problems of local pixel forgetting, channel redundancy and spatial discontinuity of existing SSMs, we introduce both local enhancement and zigzag scan. Qualitative and quantitative results demonstrate that our SaMam outperforms state-of-the-art methods in terms of both accuracy and efficiency.",
        "arxiv_id": "2503.15934",
        "ARXIVID": "2503.15934",
        "COMMENT": "Does not match any specific criteria but focuses on style transfer, which is tangentially related to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15815": {
        "authors": [
            "Vishnu Asutosh Dasu",
            "Md Rafi ur Rashid",
            "Vipul Gupta",
            "Saeid Tizpaz-Niari",
            "Gang Tan"
        ],
        "title": "Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing",
        "abstract": "arXiv:2503.15815v1 Announce Type: new  Abstract: This paper explores pruning attention heads as a post-processing bias mitigation method for large language models (LLMs). Modern AI systems such as LLMs are expanding into sensitive social contexts where fairness concerns become especially crucial. Since LLMs develop decision-making patterns by training on massive datasets of human-generated content, they naturally encode and perpetuate societal biases. While modifying training datasets and algorithms is expensive and requires significant resources; post-processing techniques-such as selectively deactivating neurons and attention heads in pre-trained LLMs-can provide feasible and effective approaches to improve fairness. However, identifying the optimal subset of parameters to prune presents a combinatorial challenge within LLMs' immense parameter space, requiring solutions that efficiently balance competing objectives across the frontiers of model fairness and utility.   To address the computational challenges, we explore a search-based program repair approach via randomized simulated annealing. Given the prohibitive evaluation costs in billion-parameter LLMs, we develop surrogate deep neural networks that efficiently model the relationship between attention head states (active/inactive) and their corresponding fairness/utility metrics. This allows us to perform optimization over the surrogate models and efficiently identify optimal subsets of attention heads for selective pruning rather than directly searching through the LLM parameter space. This paper introduces Attention Pruning, a fairness-aware surrogate simulated annealing approach to prune attention heads in LLMs that disproportionately contribute to bias while minimally impacting overall model utility. Our experiments show that Attention Pruning achieves up to $40\\%$ reduction in gender bias and outperforms the state-of-the-art bias mitigation strategies.",
        "arxiv_id": "2503.15815",
        "ARXIVID": "2503.15815",
        "COMMENT": "Does not match any specific criteria but discusses fairness in LLMs, which is tangentially related to your friend's interest in language models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15970": {
        "authors": [
            "JunGyu Lee",
            "Kunyoung Lee",
            "Haesol Park",
            "Ig-Jae Kim",
            "Gi Pyo Nam"
        ],
        "title": "V-NAW: Video-based Noise-aware Adaptive Weighting for Facial Expression Recognition",
        "abstract": "arXiv:2503.15970v1 Announce Type: new  Abstract: Facial Expression Recognition (FER) plays a crucial role in human affective analysis and has been widely applied in computer vision tasks such as human-computer interaction and psychological assessment. The 8th Affective Behavior Analysis in-the-Wild (ABAW) Challenge aims to assess human emotions using the video-based Aff-Wild2 dataset. This challenge includes various tasks, including the video-based EXPR recognition track, which is our primary focus. In this paper, we demonstrate that addressing label ambiguity and class imbalance, which are known to cause performance degradation, can lead to meaningful performance improvements. Specifically, we propose Video-based Noise-aware Adaptive Weighting (V-NAW), which adaptively assigns importance to each frame in a clip to address label ambiguity and effectively capture temporal variations in facial expressions. Furthermore, we introduce a simple and effective augmentation strategy to reduce redundancy between consecutive frames, which is a primary cause of overfitting. Through extensive experiments, we validate the effectiveness of our approach, demonstrating significant improvements in video-based FER performance.",
        "arxiv_id": "2503.15970",
        "ARXIVID": "2503.15970",
        "COMMENT": "Does not match any specific criteria but focuses on facial expression recognition, which is tangentially related to vision tasks.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.16203": {
        "authors": [
            "Stefano Fioravanti",
            "Francesco Giannini",
            "Paolo Frazzetto",
            "Fabio Zanasi",
            "Pietro Barbiero"
        ],
        "title": "Logic Explanation of AI Classifiers by Categorical Explaining Functors",
        "abstract": "arXiv:2503.16203v1 Announce Type: new  Abstract: The most common methods in explainable artificial intelligence are post-hoc techniques which identify the most relevant features used by pretrained opaque models. Some of the most advanced post hoc methods can generate explanations that account for the mutual interactions of input features in the form of logic rules. However, these methods frequently fail to guarantee the consistency of the extracted explanations with the model's underlying reasoning. To bridge this gap, we propose a theoretically grounded approach to ensure coherence and fidelity of the extracted explanations, moving beyond the limitations of current heuristic-based approaches. To this end, drawing from category theory, we introduce an explaining functor which structurally preserves logical entailment between the explanation and the opaque model's reasoning. As a proof of concept, we validate the proposed theoretical constructions on a synthetic benchmark verifying how the proposed approach significantly mitigates the generation of contradictory or unfaithful explanations.",
        "arxiv_id": "2503.16203",
        "ARXIVID": "2503.16203",
        "COMMENT": "Does not match any specific criterion but is related to explainable AI, which is tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15831": {
        "authors": [
            "Zihao Zhang",
            "Haoran Chen",
            "Haoyu Zhao",
            "Guansong Lu",
            "Yanwei Fu",
            "Hang Xu",
            "Zuxuan Wu"
        ],
        "title": "EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame Interpolation",
        "abstract": "arXiv:2503.15831v1 Announce Type: new  Abstract: Handling complex or nonlinear motion patterns has long posed challenges for video frame interpolation. Although recent advances in diffusion-based methods offer improvements over traditional optical flow-based approaches, they still struggle to generate sharp, temporally consistent frames in scenarios with large motion. To address this limitation, we introduce EDEN, an Enhanced Diffusion for high-quality large-motion vidEo frame iNterpolation. Our approach first utilizes a transformer-based tokenizer to produce refined latent representations of the intermediate frames for diffusion models. We then enhance the diffusion transformer with temporal attention across the process and incorporate a start-end frame difference embedding to guide the generation of dynamic motion. Extensive experiments demonstrate that EDEN achieves state-of-the-art results across popular benchmarks, including nearly a 10% LPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD.",
        "arxiv_id": "2503.15831",
        "ARXIVID": "2503.15831",
        "COMMENT": "Does not match any specific criterion but is related to generative modeling for video frame interpolation.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.16385": {
        "authors": [
            "Yijia Luo",
            "Yulin Song",
            "Xingyao Zhang",
            "Jiaheng Liu",
            "Weixun Wang",
            "GengRu Chen",
            "Wenbo Su",
            "Bo Zheng"
        ],
        "title": "Deconstructing Long Chain-of-Thought: A Structured Reasoning Optimization Framework for Long CoT Distillation",
        "abstract": "arXiv:2503.16385v1 Announce Type: new  Abstract: Recent advancements in large language models (LLMs) have demonstrated remarkable reasoning capabilities through long chain-of-thought (CoT) reasoning. The R1 distillation scheme has emerged as a promising approach for training cost-effective models with enhanced reasoning abilities. However, the underlying mechanisms driving its effectiveness remain unclear. This study examines the universality of distillation data and identifies key components that enable the efficient transfer of long-chain reasoning capabilities in LLM distillation. Our findings reveal that the effectiveness of long CoT reasoning distillation from teacher models like Qwen-QwQ degrades significantly on nonhomologous models, challenging the assumed universality of current distillation methods. To gain deeper insights into the structure and patterns of long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought), a distillation data enhancement framework. DLCoT consists of three key steps: (1) data segmentation to decompose complex long CoT structures, (2) simplification by eliminating unsolvable and redundant solutions, and (3) optimization of intermediate error states. Our approach significantly improves model performance and token efficiency, facilitating the development of high-performance LLMs.",
        "arxiv_id": "2503.16385",
        "ARXIVID": "2503.16385",
        "COMMENT": "Does not match any specific criterion but is related to reasoning and LLM distillation, which is tangentially relevant.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15937": {
        "authors": [
            "Gaole Dai",
            "Shiqi Jiang",
            "Ting Cao",
            "Yuanchun Li",
            "Yuqing Yang",
            "Rui Tan",
            "Mo Li",
            "Lili Qiu"
        ],
        "title": "Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical Deployment",
        "abstract": "arXiv:2503.15937v1 Announce Type: new  Abstract: We propose V-Droid, a mobile GUI task automation agent. Unlike previous mobile agents that utilize Large Language Models (LLMs) as generators to directly generate actions at each step, V-Droid employs LLMs as verifiers to evaluate candidate actions before making final decisions. To realize this novel paradigm, we introduce a comprehensive framework for constructing verifier-driven mobile agents: the discretized action space construction coupled with the prefilling-only workflow to accelerate the verification process, the pair-wise progress preference training to significantly enhance the verifier's decision-making capabilities, and the scalable human-agent joint annotation scheme to efficiently collect the necessary data at scale. V-Droid sets a new state-of-the-art task success rate across several public mobile task automation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on MobileAgentBench, surpassing existing agents by 9.5%, 2.1%, and 9%, respectively. Furthermore, V-Droid achieves an impressively low latency of 0.7 seconds per step, making it the first mobile agent capable of delivering near-real-time, effective decision-making capabilities.",
        "arxiv_id": "2503.15937",
        "ARXIVID": "2503.15937",
        "COMMENT": "Does not match any specific criterion but is related to general interest in embodied AI and LLMs.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15671": {
        "authors": [
            "Arindam Dutta",
            "Meng Zheng",
            "Zhongpai Gao",
            "Benjamin Planche",
            "Anwesha Choudhuri",
            "Terrence Chen",
            "Amit K. Roy-Chowdhury",
            "Ziyan Wu"
        ],
        "title": "CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-Consistency from a Single Image",
        "abstract": "arXiv:2503.15671v1 Announce Type: new  Abstract: Reconstructing clothed humans from a single image is a fundamental task in computer vision with wide-ranging applications. Although existing monocular clothed human reconstruction solutions have shown promising results, they often rely on the assumption that the human subject is in an occlusion-free environment. Thus, when encountering in-the-wild occluded images, these algorithms produce multiview inconsistent and fragmented reconstructions. Additionally, most algorithms for monocular 3D human reconstruction leverage geometric priors such as SMPL annotations for training and inference, which are extremely challenging to acquire in real-world applications. To address these limitations, we propose CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-ConsistEncy from a Single Image, a novel pipeline designed to reconstruct occlusion-resilient 3D humans with multiview consistency from a single occluded image, without requiring either ground-truth geometric prior annotations or 3D supervision. Specifically, CHROME leverages a multiview diffusion model to first synthesize occlusion-free human images from the occluded input, compatible with off-the-shelf pose control to explicitly enforce cross-view consistency during synthesis. A 3D reconstruction model is then trained to predict a set of 3D Gaussians conditioned on both the occluded input and synthesized views, aligning cross-view details to produce a cohesive and accurate 3D representation. CHROME achieves significant improvements in terms of both novel view synthesis (upto 3 db PSNR) and geometric reconstruction under challenging conditions.",
        "arxiv_id": "2503.15671",
        "ARXIVID": "2503.15671",
        "COMMENT": "This paper focuses on 3D human reconstruction and does not directly match any of the criteria but is related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.16096": {
        "authors": [
            "Lucas Morin",
            "Val\\'ery Weber",
            "Ahmed Nassar",
            "Gerhard Ingmar Meijer",
            "Luc Van Gool",
            "Yawei Li",
            "Peter Staar"
        ],
        "title": "MarkushGrapher: Joint Visual and Textual Recognition of Markush Structures",
        "abstract": "arXiv:2503.16096v1 Announce Type: new  Abstract: The automated analysis of chemical literature holds promise to accelerate discovery in fields such as material science and drug development. In particular, search capabilities for chemical structures and Markush structures (chemical structure templates) within patent documents are valuable, e.g., for prior-art search. Advancements have been made in the automatic extraction of chemical structures from text and images, yet the Markush structures remain largely unexplored due to their complex multi-modal nature. In this work, we present MarkushGrapher, a multi-modal approach for recognizing Markush structures in documents. Our method jointly encodes text, image, and layout information through a Vision-Text-Layout encoder and an Optical Chemical Structure Recognition vision encoder. These representations are merged and used to auto-regressively generate a sequential graph representation of the Markush structure along with a table defining its variable groups. To overcome the lack of real-world training data, we propose a synthetic data generation pipeline that produces a wide range of realistic Markush structures. Additionally, we present M2S, the first annotated benchmark of real-world Markush structures, to advance research on this challenging task. Extensive experiments demonstrate that our approach outperforms state-of-the-art chemistry-specific and general-purpose vision-language models in most evaluation settings. Code, models, and datasets will be available.",
        "arxiv_id": "2503.16096",
        "ARXIVID": "2503.16096",
        "COMMENT": "This paper does not match any of the specific criteria but is related to multi-modal learning in a niche domain (chemical literature).",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.16195": {
        "authors": [
            "Chia-Yi Hsu",
            "Jia-You Chen",
            "Yu-Lin Tsai",
            "Chih-Hsun Lin",
            "Pin-Yu Chen",
            "Chia-Mu Yu",
            "Chun-Ying Huang"
        ],
        "title": "VP-NTK: Exploring the Benefits of Visual Prompting in Differentially Private Data Synthesis",
        "abstract": "arXiv:2503.16195v1 Announce Type: new  Abstract: Differentially private (DP) synthetic data has become the de facto standard for releasing sensitive data. However, many DP generative models suffer from the low utility of synthetic data, especially for high-resolution images. On the other hand, one of the emerging techniques in parameter efficient fine-tuning (PEFT) is visual prompting (VP), which allows well-trained existing models to be reused for the purpose of adapting to subsequent downstream tasks. In this work, we explore such a phenomenon in constructing captivating generative models with DP constraints. We show that VP in conjunction with DP-NTK, a DP generator that exploits the power of the neural tangent kernel (NTK) in training DP generative models, achieves a significant performance boost, particularly for high-resolution image datasets, with accuracy improving from 0.644$\\pm$0.044 to 0.769. Lastly, we perform ablation studies on the effect of different parameters that influence the overall performance of VP-NTK. Our work demonstrates a promising step forward in improving the utility of DP synthetic data, particularly for high-resolution images.",
        "arxiv_id": "2503.16195",
        "ARXIVID": "2503.16195",
        "COMMENT": "Does not match any specific criteria but explores visual prompting in differentially private data synthesis, which is tangentially related to vision foundation models.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15676": {
        "authors": [
            "C\\'edric Vincent",
            "Taehyoung Kim",
            "Henri Mee{\\ss}"
        ],
        "title": "High Temporal Consistency through Semantic Similarity Propagation in Semi-Supervised Video Semantic Segmentation for Autonomous Flight",
        "abstract": "arXiv:2503.15676v1 Announce Type: new  Abstract: Semantic segmentation from RGB cameras is essential to the perception of autonomous flying vehicles. The stability of predictions through the captured videos is paramount to their reliability and, by extension, to the trustworthiness of the agents. In this paper, we propose a lightweight video semantic segmentation approach-suited to onboard real-time inference-achieving high temporal consistency on aerial data through Semantic Similarity Propagation across frames. SSP temporally propagates the predictions of an efficient image segmentation model with global registration alignment to compensate for camera movements. It combines the current estimation and the prior prediction with linear interpolation using weights computed from the features similarities of the two frames. Because data availability is a challenge in this domain, we propose a consistency-aware Knowledge Distillation training procedure for sparsely labeled datasets with few annotations. Using a large image segmentation model as a teacher to train the efficient SSP, we leverage the strong correlations between labeled and unlabeled frames in the same training videos to obtain high-quality supervision on all frames. KD-SSP obtains a significant temporal consistency increase over the base image segmentation model of 12.5% and 6.7% TC on UAVid and RuralScapes respectively, with higher accuracy and comparable inference speed. On these aerial datasets, KD-SSP provides a superior segmentation quality and inference speed trade-off than other video methods proposed for general applications and shows considerably higher consistency. The code will be made publicly available upon acceptance.",
        "arxiv_id": "2503.15676",
        "ARXIVID": "2503.15676",
        "COMMENT": "Does not match any specific criteria but is related to computer vision and machine learning for autonomous systems.",
        "RELEVANCE": 3,
        "NOVELTY": 5
    },
    "2503.15762": {
        "authors": [
            "Elena Malnatsky",
            "Shenghui Wang",
            "Koen V. Hindriks",
            "Mike E. U. Ligthart"
        ],
        "title": "Dialogic Learning in Child-Robot Interaction: A Hybrid Approach to Personalized Educational Content Generation",
        "abstract": "arXiv:2503.15762v1 Announce Type: new  Abstract: Dialogic learning fosters motivation and deeper understanding in education through purposeful and structured dialogues. Foundational models offer a transformative potential for child-robot interactions, enabling the design of personalized, engaging, and scalable interactions. However, their integration into educational contexts presents challenges in terms of ensuring age-appropriate and safe content and alignment with pedagogical goals. We introduce a hybrid approach to designing personalized educational dialogues in child-robot interactions. By combining rule-based systems with LLMs for selective offline content generation and human validation, the framework ensures educational quality and developmental appropriateness. We illustrate this approach through a project aimed at enhancing reading motivation, in which a robot facilitated book-related dialogues.",
        "arxiv_id": "2503.15762",
        "ARXIVID": "2503.15762",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of foundational models and their applications in child-robot interaction.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.16191": {
        "authors": [
            "Yinon Goldshtein",
            "Gal Perelman",
            "Assaf Schuster",
            "Avi Ostfeld"
        ],
        "title": "Large Language Models for Water Distribution Systems Modeling and Decision-Making",
        "abstract": "arXiv:2503.16191v1 Announce Type: new  Abstract: The design, operations, and management of water distribution systems (WDS) involve complex mathematical models. These models are continually improving due to computational advancements, leading to better decision-making and more efficient WDS management. However, the significant time and effort required for modeling, programming, and analyzing results remain substantial challenges. Another issue is the professional burden, which confines the interaction with models, databases, and other sophisticated tools to a small group of experts, thereby causing non-technical stakeholders to depend on these experts or make decisions without modeling support. Furthermore, explaining model results is challenging even for experts, as it is often unclear which conditions cause the model to reach a certain state or recommend a specific policy. The recent advancements in Large Language Models (LLMs) open doors for a new stage in human-model interaction. This study proposes a framework of plain language interactions with hydraulic and water quality models based on LLM-EPANET architecture. This framework is tested with increasing levels of complexity of queries to study the ability of LLMs to interact with WDS models, run complex simulations, and report simulation results. The performance of the proposed framework is evaluated across several categories of queries and hyper-parameter configurations, demonstrating its potential to enhance decision-making processes in WDS management.",
        "arxiv_id": "2503.16191",
        "ARXIVID": "2503.16191",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of large language models and their applications in decision-making.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.16357": {
        "authors": [
            "Tao Feng",
            "Yifan Xie",
            "Xun Guan",
            "Jiyuan Song",
            "Zhou Liu",
            "Fei Ma",
            "Fei Yu"
        ],
        "title": "UniSync: A Unified Framework for Audio-Visual Synchronization",
        "abstract": "arXiv:2503.16357v1 Announce Type: new  Abstract: Precise audio-visual synchronization in speech videos is crucial for content quality and viewer comprehension. Existing methods have made significant strides in addressing this challenge through rule-based approaches and end-to-end learning techniques. However, these methods often rely on limited audio-visual representations and suboptimal learning strategies, potentially constraining their effectiveness in more complex scenarios. To address these limitations, we present UniSync, a novel approach for evaluating audio-visual synchronization using embedding similarities. UniSync offers broad compatibility with various audio representations (e.g., Mel spectrograms, HuBERT) and visual representations (e.g., RGB images, face parsing maps, facial landmarks, 3DMM), effectively handling their significant dimensional differences. We enhance the contrastive learning framework with a margin-based loss component and cross-speaker unsynchronized pairs, improving discriminative capabilities. UniSync outperforms existing methods on standard datasets and demonstrates versatility across diverse audio-visual representations. Its integration into talking face generation frameworks enhances synchronization quality in both natural and AI-generated content.",
        "arxiv_id": "2503.16357",
        "ARXIVID": "2503.16357",
        "COMMENT": "Does not match any specific criterion but is relevant to the general interest area of multimodal learning and synchronization in AI-generated content.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.16165": {
        "authors": [
            "Xiangyu Li",
            "Wanshu Fan",
            "Yue Shen",
            "Cong Wang",
            "Wei Wang",
            "Xin Yang",
            "Qiang Zhang",
            "Dongsheng Zhou"
        ],
        "title": "Iterative Optimal Attention and Local Model for Single Image Rain Streak Removal",
        "abstract": "arXiv:2503.16165v1 Announce Type: new  Abstract: High-fidelity imaging is crucial for the successful safety supervision and intelligent deployment of vision-based measurement systems (VBMS). It ensures high-quality imaging in VBMS, which is fundamental for reliable visual measurement and analysis. However, imaging quality can be significantly impaired by adverse weather conditions, particularly rain, leading to blurred images and reduced contrast. Such impairments increase the risk of inaccurate evaluations and misinterpretations in VBMS. To address these limitations, we propose an Expectation Maximization Reconstruction Transformer (EMResformer) for single image rain streak removal. The EMResformer retains the key self-attention values for feature aggregation, enhancing local features to produce superior image reconstruction. Specifically, we propose an Expectation Maximization Block seamlessly integrated into the single image rain streak removal network, enhancing its ability to eliminate superfluous information and restore a cleaner background image. Additionally, to further enhance local information for improved detail rendition, we introduce a Local Model Residual Block, which integrates two local model blocks along with a sequence of convolutions and activation functions. This integration synergistically facilitates the extraction of more pertinent features for enhanced single image rain streak removal. Extensive experiments validate that our proposed EMResformer surpasses current state-of-the-art single image rain streak removal methods on both synthetic and real-world datasets, achieving an improved balance between model complexity and single image deraining performance. Furthermore, we evaluate the effectiveness of our method in VBMS scenarios, demonstrating that high-quality imaging significantly improves the accuracy and reliability of VBMS tasks.",
        "arxiv_id": "2503.16165",
        "ARXIVID": "2503.16165",
        "COMMENT": "This paper proposes a method for single image rain streak removal, which does not match any specific criterion but is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.16056": {
        "authors": [
            "Wanshu Fan",
            "Yue Wang",
            "Cong Wang",
            "Yunzhe Zhang",
            "Wei Wang",
            "Dongsheng Zhou"
        ],
        "title": "Semantic-Guided Global-Local Collaborative Networks for Lightweight Image Super-Resolution",
        "abstract": "arXiv:2503.16056v1 Announce Type: new  Abstract: Single-Image Super-Resolution (SISR) plays a pivotal role in enhancing the accuracy and reliability of measurement systems, which are integral to various vision-based instrumentation and measurement applications. These systems often require clear and detailed images for precise object detection and recognition. However, images captured by visual measurement tools frequently suffer from degradation, including blurring and loss of detail, which can impede measurement accuracy.As a potential remedy, we in this paper propose a Semantic-Guided Global-Local Collaborative Network (SGGLC-Net) for lightweight SISR. Our SGGLC-Net leverages semantic priors extracted from a pre-trained model to guide the super-resolution process, enhancing image detail quality effectively. Specifically,we propose a Semantic Guidance Module that seamlessly integrates the semantic priors into the super-resolution network, enabling the network to more adeptly capture and utilize semantic priors, thereby enhancing image details. To further explore both local and non-local interactions for improved detail rendition,we propose a Global-Local Collaborative Module, which features three Global and Local Detail Enhancement Modules, as well as a Hybrid Attention Mechanism to work together to efficiently learn more useful features. Our extensive experiments show that SGGLC-Net achieves competitive PSNR and SSIM values across multiple benchmark datasets, demonstrating higher performance with the multi-adds reduction of 12.81G compared to state-of-the-art lightweight super-resolution approaches. These improvements underscore the potential of our approach to enhance the precision and effectiveness of visual measurement systems. Codes are at https://github.com/fanamber831/SGGLC-Net.",
        "arxiv_id": "2503.16056",
        "ARXIVID": "2503.16056",
        "COMMENT": "This paper focuses on lightweight image super-resolution using semantic-guided networks, which does not match any specific criterion but is tangentially related to computer vision.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.16416": {
        "authors": [
            "Asaf Yehudai",
            "Lilach Eden",
            "Alan Li",
            "Guy Uziel",
            "Yilun Zhao",
            "Roy Bar-Haim",
            "Arman Cohan",
            "Michal Shmueli-Scheuer"
        ],
        "title": "Survey on Evaluation of LLM-based Agents",
        "abstract": "arXiv:2503.16416v1 Announce Type: new  Abstract: The emergence of LLM-based agents represents a paradigm shift in AI, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments. This paper provides the first comprehensive survey of evaluation methodologies for these increasingly capable agents. We systematically analyze evaluation benchmarks and frameworks across four critical dimensions: (1) fundamental agent capabilities, including planning, tool use, self-reflection, and memory; (2) application-specific benchmarks for web, software engineering, scientific, and conversational agents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating agents. Our analysis reveals emerging trends, including a shift toward more realistic, challenging evaluations with continuously updated benchmarks. We also identify critical gaps that future research must address-particularly in assessing cost-efficiency, safety, and robustness, and in developing fine-grained, and scalable evaluation methods. This survey maps the rapidly evolving landscape of agent evaluation, reveals the emerging trends in the field, identifies current limitations, and proposes directions for future research.",
        "arxiv_id": "2503.16416",
        "ARXIVID": "2503.16416",
        "COMMENT": "Does not match any specific criteria but provides a survey on evaluation methodologies for LLM-based agents, which is tangentially related to embodied AI.",
        "RELEVANCE": 3,
        "NOVELTY": 4
    },
    "2503.15978": {
        "authors": [
            "Pengyu Liu",
            "Guohua Dong",
            "Dan Guo",
            "Kun Li",
            "Fengling Li",
            "Xun Yang",
            "Meng Wang",
            "Xiaomin Ying"
        ],
        "title": "A Survey on fMRI-based Brain Decoding for Reconstructing Multimodal Stimuli",
        "abstract": "arXiv:2503.15978v1 Announce Type: new  Abstract: In daily life, we encounter diverse external stimuli, such as images, sounds, and videos. As research in multimodal stimuli and neuroscience advances, fMRI-based brain decoding has become a key tool for understanding brain perception and its complex cognitive processes. Decoding brain signals to reconstruct stimuli not only reveals intricate neural mechanisms but also drives progress in AI, disease treatment, and brain-computer interfaces. Recent advancements in neuroimaging and image generation models have significantly improved fMRI-based decoding. While fMRI offers high spatial resolution for precise brain activity mapping, its low temporal resolution and signal noise pose challenges. Meanwhile, techniques like GANs, VAEs, and Diffusion Models have enhanced reconstructed image quality, and multimodal pre-trained models have boosted cross-modal decoding tasks. This survey systematically reviews recent progress in fMRI-based brain decoding, focusing on stimulus reconstruction from passive brain signals. It summarizes datasets, relevant brain regions, and categorizes existing methods by model structure. Additionally, it evaluates model performance and discusses their effectiveness. Finally, it identifies key challenges and proposes future research directions, offering valuable insights for the field. For more information and resources related to this survey, visit https://github.com/LpyNow/BrainDecodingImage.",
        "arxiv_id": "2503.15978",
        "ARXIVID": "2503.15978",
        "COMMENT": "This is a survey on fMRI-based brain decoding and multimodal stimuli reconstruction, which does not match any specific criterion but is tangentially related to multi-modal learning.",
        "RELEVANCE": 3,
        "NOVELTY": 3
    }
}