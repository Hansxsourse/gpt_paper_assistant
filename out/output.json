{
    "2511.05229": {
        "authors": [
            "Mengqi Guo",
            "Bo Xu",
            "Yanyan Li",
            "Gim Hee Lee"
        ],
        "title": "4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos",
        "abstract": "arXiv:2511.05229v1 Announce Type: new  Abstract: Novel view synthesis from monocular videos of dynamic scenes with unknown camera poses remains a fundamental challenge in computer vision and graphics. While recent advances in 3D representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static scenes, they struggle with dynamic content and typically rely on pre-computed camera poses. We present 4D3R, a pose-free dynamic neural rendering framework that decouples static and dynamic components through a two-stage approach. Our method first leverages 3D foundational models for initial pose and geometry estimation, followed by motion-aware refinement. 4D3R introduces two key technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that combines transformer-based learned priors with SAM2 for robust dynamic object segmentation, enabling more accurate camera pose refinement; and (2) an efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses control points with a deformation field MLP and linear blend skinning to model dynamic motion, significantly reducing computational cost while maintaining high-quality reconstruction. Extensive experiments on real-world dynamic datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement over state-of-the-art methods, particularly in challenging scenarios with large dynamic objects, while reducing computational requirements by 5x compared to previous dynamic scene representations.",
        "arxiv_id": "2511.05229",
        "ARXIVID": "2511.05229",
        "COMMENT": "The paper does not match any of the specific criteria. It focuses on novel view synthesis and dynamic scene reconstruction, which are not directly related to joint generation and segmentation, unified diffusion models, or matting.",
        "RELEVANCE": 3,
        "NOVELTY": 6
    }
}