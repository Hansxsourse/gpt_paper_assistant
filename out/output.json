{
    "2508.14327": {
        "authors": [
            "Guile Wu",
            "David Huang",
            "Dongfeng Bai",
            "Bingbing Liu"
        ],
        "title": "MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation",
        "abstract": "arXiv:2508.14327v1 Announce Type: new  Abstract: Video generation has recently shown superiority in urban scene synthesis for autonomous driving. Existing video generation approaches to autonomous driving primarily focus on RGB video generation and lack the ability to support multi-modal video generation. However, multi-modal data, such as depth maps and semantic maps, are crucial for holistic urban scene understanding in autonomous driving. Although it is feasible to use multiple models to generate different modalities, this increases the difficulty of model deployment and does not leverage complementary cues for multi-modal data generation. To address this problem, in this work, we propose a novel multi-modal multi-view video generation approach to autonomous driving. Specifically, we construct a unified diffusion transformer model composed of modal-shared components and modal-specific components. Then, we leverage diverse conditioning inputs to encode controllable scene structure and content cues into the unified diffusion model for multi-modal multi-view video generation. In this way, our approach is capable of generating multi-modal multi-view driving scene videos in a unified framework. Our experiments on the challenging real-world autonomous driving dataset, nuScenes, show that our approach can generate multi-modal multi-view urban scene videos with high fidelity and controllability, surpassing the state-of-the-art methods.",
        "arxiv_id": "2508.14327",
        "ARXIVID": "2508.14327",
        "COMMENT": "Matches criteria 2 closely as it introduces a unified diffusion transformer model for multi-modal video generation, including semantic maps.",
        "RELEVANCE": 10,
        "NOVELTY": 7
    },
    "2508.14437": {
        "authors": [
            "Gabriel Tjio",
            "Jie Zhang",
            "Xulei Yang",
            "Yun Xing",
            "Nhat Chung",
            "Xiaofeng Cao",
            "Ivor W. Tsang",
            "Chee Keong Kwoh",
            "Qing Guo"
        ],
        "title": "FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for mitigating catastrophic forgetting during Test-Time Adaptation",
        "abstract": "arXiv:2508.14437v1 Announce Type: new  Abstract: Test-time adaptation enables models to adapt to evolving domains. However, balancing the tradeoff between preserving knowledge and adapting to domain shifts remains challenging for model adaptation methods, since adapting to domain shifts can induce forgetting of task-relevant knowledge. To address this problem, we propose FOCUS, a novel frequency-based conditioning approach within a diffusion-driven input-adaptation framework. Utilising learned, spatially adaptive frequency priors, our approach conditions the reverse steps during diffusion-driven denoising to preserve task-relevant semantic information for dense prediction.   FOCUS leverages a trained, lightweight, Y-shaped Frequency Prediction Network (Y-FPN) that disentangles high and low frequency information from noisy images. This minimizes the computational costs involved in implementing our approach in a diffusion-driven framework. We train Y-FPN with FrequencyMix, a novel data augmentation method that perturbs the images across diverse frequency bands, which improves the robustness of our approach to diverse corruptions.   We demonstrate the effectiveness of FOCUS for semantic segmentation and monocular depth estimation across 15 corruption types and three datasets, achieving state-of-the-art averaged performance. In addition to improving standalone performance, FOCUS complements existing model adaptation methods since we can derive pseudo labels from FOCUS-denoised images for additional supervision. Even under limited, intermittent supervision with the pseudo labels derived from the FOCUS denoised images, we show that FOCUS mitigates catastrophic forgetting for recent model adaptation methods.",
        "arxiv_id": "2508.14437",
        "ARXIVID": "2508.14437",
        "COMMENT": "Matches criteria 2 closely as it proposes a diffusion-driven framework for multiple tasks including semantic segmentation and depth estimation.",
        "RELEVANCE": 10,
        "NOVELTY": 6
    }
}