# Personalized Daily ArXiv Papers 04/03/2025
Total relevant papers: 49

Paper selection prompt and criteria at the bottom

Table of contents with paper titles:

0. [ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement](#link0)
**Authors:** Runhui Huang, Chunwei Wang, Junwei Yang, Guansong Lu, Yunlong Yuan, Jianhua Han, Lu Hou, Wei Zhang, Lanqing Hong, Hengshuang Zhao, Hang Xu

1. [UniViTAR: Unified Vision Transformer with Native Resolution](#link1)
**Authors:** Limeng Qiao, Yiyang Gan, Bairui Wang, Jie Qin, Shuang Xu, Siqi Yang, Lin Ma

2. [Slow-Fast Architecture for Video Multi-Modal Large Language Models](#link2)
**Authors:** Min Shi, Shihao Wang, Chieh-Yun Chen, Jitesh Jain, Kai Wang, Junjun Xiong, Guilin Liu, Zhiding Yu, Humphrey Shi

3. [Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning](#link3)
**Authors:** Kun Ouyang

4. [v-CLR: View-Consistent Learning for Open-World Instance Segmentation](#link4)
**Authors:** Chang-Bin Zhang, Jinhong Ni, Yujie Zhong, Kai Han

5. [ANNEXE: Unified Analyzing, Answering, and Pixel Grounding for Egocentric Interaction](#link5)
**Authors:** Yuejiao Su, Yi Wang, Qiongyang Hu, Chuang Yang, Lap-Pui Chau

6. [Is Temporal Prompting All We Need For Limited Labeled Action Recognition?](#link6)
**Authors:** Shreyank N Gowda, Boyan Gao, Xiao Gu, Xiaobo Jin

7. [Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness](#link7)
**Authors:** Haochen Wang, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, Zhaoxiang Zhang

8. [COST: Contrastive One-Stage Transformer for Vision-Language Small Object Tracking](#link8)
**Authors:** Chunhui Zhang, Li Liu, Jialin Gao, Xin Sun, Hao Wen, Xi Zhou, Shiming Ge, Yanfeng Wang

9. [FIORD: A Fisheye Indoor-Outdoor Dataset with LIDAR Ground Truth for 3D Scene Reconstruction and Benchmarking](#link9)
**Authors:** Ulas Gunes, Matias Turkulainen, Xuqian Ren, Arno Solin, Juho Kannala, Esa Rahtu

10. [On Data Synthesis and Post-training for Visual Abstract Reasoning](#link10)
**Authors:** Ke Zhu, Yu Wang, Jiangjiang Liu, Qunyi Xie, Shanshan Liu, Gang Zhang

11. [Omnidirectional Depth-Aided Occupancy Prediction based on Cylindrical Voxel for Autonomous Driving](#link11)
**Authors:** Chaofan Wu, Jiaheng Li, Jinghao Cao, Ming Li, Yongkang Feng, Jiayu Wu Shuwen Xu, Zihang Gao, Sidan Du, Yang Li

12. [Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D Reconstruction and Novel View Synthesis](#link12)
**Authors:** Niluthpol Chowdhury Mithun, Tuan Pham, Qiao Wang, Ben Southall, Kshitij Minhas, Bogdan Matei, Stephan Mandt, Supun Samarasekera, Rakesh Kumar

13. [CLIP-SLA: Parameter-Efficient CLIP Adaptation for Continuous Sign Language Recognition](#link13)
**Authors:** Sarah Alyami, Hamzah Luqman

14. [GMAI-VL-R1: Harnessing Reinforcement Learning for Multimodal Medical Reasoning](#link14)
**Authors:** Yanzhou Su, Tianbin Li, Jiyao Liu, Chenglong Ma, Junzhi Ning, Cheng Tang, Sibo Ju, Jin Ye, Pengcheng Chen, Ming Hu, Shixiang Tang, Lihao Liu, Bin Fu, Wenqi Shao, Xiaowei Hu, Xiangwen Liao, Yuanfeng Ji, Junjun He

15. [Bridge 2D-3D: Uncertainty-aware Hierarchical Registration Network with Domain Alignment](#link15)
**Authors:** Zhixin Cheng, Jiacheng Deng, Xinjun Li, Baoqun Yin, Tianzhu Zhang

16. [Proposition of Affordance-Driven Environment Recognition Framework Using Symbol Networks in Large Language Models](#link16)
**Authors:** Kazuma Arii, Satoshi Kurihara

17. [FlowR: Flowing from Sparse to Dense 3D Reconstructions](#link17)
**Authors:** Tobias Fischer, Samuel Rota Bul\`o, Yung-Hsu Yang, Nikhil Varma Keetha, Lorenzo Porzi, Norman M\"uller, Katja Schwarz, Jonathon Luiten, Marc Pollefeys, Peter Kontschieder

18. [VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step](#link18)
**Authors:** Hanyang Wang, Fangfu Liu, Jiawei Chi, Yueqi Duan

19. [Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting Conditions with View-Adaptive Curve Adjustment](#link19)
**Authors:** Ziteng Cui, Xuangeng Chu, Tatsuya Harada

20. [High-fidelity 3D Object Generation from Single Image with RGBN-Volume Gaussian Reconstruction Model](#link20)
**Authors:** Yiyang Shen, Kun Zhou, He Wang, Yin Yang, Tianjia Shao

21. [An Illusion of Progress? Assessing the Current State of Web Agents](#link21)
**Authors:** Tianci Xue, Weijian Qi, Tianneng Shi, Chan Hee Song, Boyu Gou, Dawn Song, Huan Sun, Yu Su

22. [End-to-End Driving with Online Trajectory Evaluation via BEV World Model](#link22)
**Authors:** Yingyan Li, Yuqi Wang, Yang Liu, Jiawei He, Lue Fan, Zhaoxiang Zhang

23. [DALIP: Distribution Alignment-based Language-Image Pre-Training for Domain-Specific Data](#link23)
**Authors:** Junjie Wu, Jiangtao Xie, Zhaolin Zhang, Qilong Wang, Qinghua Hu, Peihua Li, Sen Xu

24. [Mesh Mamba: A Unified State Space Model for Saliency Prediction in Non-Textured and Textured Meshes](#link24)
**Authors:** Kaiwei Zhang, Dandan Zhu, Xiongkuo Min, Guangtao Zhai

25. [Scene-Centric Unsupervised Panoptic Segmentation](#link25)
**Authors:** Oliver Hahn, Christoph Reich, Nikita Araslanov, Daniel Cremers, Christian Rupprecht, Stefan Roth

26. [Gaze-Guided 3D Hand Motion Prediction for Detecting Intent in Egocentric Grasping Tasks](#link26)
**Authors:** Yufei He, Xucong Zhang, Arno H. A. Stienen

27. [Enhanced Cross-modal 3D Retrieval via Tri-modal Reconstruction](#link27)
**Authors:** Junlong Ren, Hao Wang

28. [AI-Newton: A Concept-Driven Physical Law Discovery System without Prior Physical Knowledge](#link28)
**Authors:** You-Le Fang, Dong-Shan Jian, Xiang Li, Yan-Qing Ma

29. [Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks](#link29)
**Authors:** Jiawei Wang, Yushen Zuo, Yuanjun Chai, Zhendong Liu, Yichen Fu, Yichun Feng, Kin-man Lam

30. [A Diffusion-Based Framework for Occluded Object Movement](#link30)
**Authors:** Zheng-Peng Duan, Jiawei Zhang, Siyu Liu, Zheng Lin, Chun-Le Guo, Dongqing Zou, Jimmy Ren, Chongyi Li

31. [Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis](#link31)
**Authors:** Zixuan Wang, Duo Peng, Feng Chen, Yuwei Yang, Yinjie Lei

32. [A$^\text{T}$A: Adaptive Transformation Agent for Text-Guided Subject-Position Variable Background Inpainting](#link32)
**Authors:** Yizhe Tang, Zhimin Sun, Yuzhen Du, Ran Yi, Guangben Lu, Teng Hu, Luying Li, Lizhuang Ma, Fangyuan Zou

33. [InvFussion: Bridging Supervised and Zero-shot Diffusion for Inverse Problems](#link33)
**Authors:** Noam Elata, Hyungjin Chung, Jong Chul Ye, Tomer Michaeli, Michael Elad

34. [Prompt-Guided Attention Head Selection for Focus-Oriented Image Retrieval](#link34)
**Authors:** Yuji Nozawa, Yu-Chieh Lin, Kazumoto Nakamura, Youyang Ng

35. [Understanding Cross-Model Perceptual Invariances Through Ensemble Metamers](#link35)
**Authors:** Lukas Boehm, Jonas Leo Mueller, Christoffer Loeffler, Leo Schwinn, Bjoern Eskofier, Dario Zanca

36. [CFMD: Dynamic Cross-layer Feature Fusion for Salient Object Detection](#link36)
**Authors:** Jin Lian, Zhongyu Wan, Ming Gao, JunFeng Chen

37. [Direction-Aware Hybrid Representation Learning for 3D Hand Pose and Shape Estimation](#link37)
**Authors:** Shiyong Liu, Zhihao Li, Xiao Tang, Jianzhuang Liu

38. [Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation](#link38)
**Authors:** Junjie Chen, Yuecong Xu, Haosheng Li, Kemi Ding

39. [ProtoGuard-guided PROPEL: Class-Aware Prototype Enhancement and Progressive Labeling for Incremental 3D Point Cloud Segmentation](#link39)
**Authors:** Haosheng Li, Yuecong Xu, Junjie Chen, Kemi Ding

40. [Enhancing Interpretability in Generative AI Through Search-Based Data Influence Analysis](#link40)
**Authors:** Theodoros Aivalis, Iraklis A. Klampanos, Antonis Troumpoukis, Joemon M. Jose

41. [DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance](#link41)
**Authors:** Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, Yongming Zhu

42. [LLM-mediated Dynamic Plan Generation with a Multi-Agent Approach](#link42)
**Authors:** Reo Abe, Akifumi Ito, Kanata Takayasu, Satoshi Kurihara

43. [CoMatcher: Multi-View Collaborative Feature Matching](#link43)
**Authors:** Jintao Zhang, Zimin Xia, Mingyue Dong, Shuhan Shen, Linwei Yue, Xianwei Zheng

44. [Robust Unsupervised Domain Adaptation for 3D Point Cloud Segmentation Under Source Adversarial Attacks](#link44)
**Authors:** Haosheng Li, Yuecong Xu, Junjie Chen, Kemi Ding

45. [Knowledge-Base based Semantic Image Transmission Using CLIP](#link45)
**Authors:** Chongyang Li, Yanmei He, Tianqian Zhang, Mingjian He, Shouyin Liu

46. [Deep LG-Track: An Enhanced Localization-Confidence-Guided Multi-Object Tracker](#link46)
**Authors:** Ting Meng, Chunyun Fu, Xiangyan Yan, Zheng Liang, Pan Ji, Jianwen Wang, Tao Huang

47. [All Patches Matter, More Patches Better: Enhance AI-Generated Image Detection via Panoptic Patch Learning](#link47)
**Authors:** Zheng Yang, Ruoxin Chen, Zhiyuan Yan, Ke-Yue Zhang, Xinghe Fu, Shuang Wu, Xiujun Shu, Taiping Yao, Junchi Yan, Shouhong Ding, Xi Li

48. [BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting Binary Segmentation in Segment Anything Models](#link48)
**Authors:** Encheng Su, Hu Cao, Alois Knoll

---
## 0. [ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement](https://arxiv.org/abs/2504.01934) <a id="link0"></a>
**ArXiv ID:** 2504.01934
**Authors:** Runhui Huang, Chunwei Wang, Junwei Yang, Guansong Lu, Yunlong Yuan, Jianhua Han, Lu Hou, Wei Zhang, Lanqing Hong, Hengshuang Zhao, Hang Xu

**Abstract:**  We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: https://illume-unified-mllm.github.io/.

**Comment:** Matches criterion 2 and 4 as it introduces a unified MLLM with dual visual tokenization and diffusion refinement, addressing multimodal understanding, generation, and editing.
**Relevance:** 10
**Novelty:** 8

---

## 1. [UniViTAR: Unified Vision Transformer with Native Resolution](https://arxiv.org/abs/2504.01792) <a id="link1"></a>
**ArXiv ID:** 2504.01792
**Authors:** Limeng Qiao, Yiyang Gan, Bairui Wang, Jie Qin, Shuang Xu, Siqi Yang, Lin Ma

**Abstract:**  Conventional Vision Transformer simplifies visual modeling by standardizing input resolutions, often disregarding the variability of natural visual data and compromising spatial-contextual fidelity. While preliminary explorations have superficially investigated native resolution modeling, existing approaches still lack systematic analysis from a visual representation perspective. To bridge this gap, we introduce UniViTAR, a family of homogeneous vision foundation models tailored for unified visual modality and native resolution scenario in the era of multimodal. Our framework first conducts architectural upgrades to the vanilla paradigm by integrating multiple advanced components. Building upon these improvements, a progressive training paradigm is introduced, which strategically combines two core mechanisms: (1) resolution curriculum learning, transitioning from fixed-resolution pretraining to native resolution tuning, thereby leveraging ViT's inherent adaptability to variable-length sequences, and (2) visual modality adaptation via inter-batch image-video switching, which balances computational efficiency with enhanced temporal reasoning. In parallel, a hybrid training framework further synergizes sigmoid-based contrastive loss with feature distillation from a frozen teacher model, thereby accelerating early-stage convergence. Finally, trained exclusively on public datasets, externsive experiments across multiple model scales from 0.3B to 1B demonstrate its effectiveness.

**Comment:** Matches criterion 4 as it introduces a vision foundation model (UniViTAR) with architectural and training improvements for multimodal and native resolution scenarios.
**Relevance:** 9
**Novelty:** 8

---

## 2. [Slow-Fast Architecture for Video Multi-Modal Large Language Models](https://arxiv.org/abs/2504.01328) <a id="link2"></a>
**ArXiv ID:** 2504.01328
**Authors:** Min Shi, Shihao Wang, Chieh-Yun Chen, Jitesh Jain, Kai Wang, Junjun Xiong, Guilin Liu, Zhiding Yu, Humphrey Shi

**Abstract:**  Balancing temporal resolution and spatial detail under limited compute budget remains a key challenge for video-based multi-modal large language models (MLLMs). Existing methods typically compress video representations using predefined rules before feeding them into the LLM, resulting in irreversible information loss and often ignoring input instructions. To address this, we propose a novel slow-fast architecture that naturally circumvents this trade-off, enabling the use of more input frames while preserving spatial details. Inspired by how humans first skim a video before focusing on relevant parts, our slow-fast design employs a dual-token strategy: 1) "fast" visual tokens -- a compact set of compressed video features -- are fed into the LLM alongside text embeddings to provide a quick overview; 2) "slow" visual tokens -- uncompressed video features -- are cross-attended by text embeddings through specially designed hybrid decoder layers, enabling instruction-aware extraction of relevant visual details with linear complexity. We conduct systematic exploration to optimize both the overall architecture and key components. Experiments show that our model significantly outperforms self-attention-only baselines, extending the input capacity from 16 to 128 frames with just a 3% increase in computation, and achieving a 16% average performance improvement across five video understanding benchmarks. Our 7B model achieves state-of-the-art performance among models of similar size. Furthermore, our slow-fast architecture is a plug-and-play design that can be integrated into other video MLLMs to improve efficiency and scalability.

**Comment:** Matches criterion 2 as it introduces a novel slow-fast architecture for video-based multi-modal large language models.
**Relevance:** 10
**Novelty:** 7

---

## 3. [Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning](https://arxiv.org/abs/2504.01805) <a id="link3"></a>
**ArXiv ID:** 2504.01805
**Authors:** Kun Ouyang

**Abstract:**  Enhancing the spatial reasoning capabilities of Multi-modal Large Language Models (MLLMs) for video understanding is crucial yet challenging. We present Spatial-R1, a targeted approach involving two key contributions: the curation of SR, a new video spatial reasoning dataset from ScanNet with automatically generated QA pairs across seven task types, and the application of Task-Specific Group Relative Policy Optimization (GRPO) for fine-tuning. By training the Qwen2.5-VL-7B-Instruct model on SR using GRPO, Spatial-R1 significantly advances performance on the VSI-Bench benchmark, achieving a 7.4\% gain over the baseline and outperforming strong contemporary models. This work validates the effectiveness of specialized data curation and optimization techniques for improving complex spatial reasoning in video MLLMs.

**Comment:** Matches criterion 1 and 2 as it focuses on improving spatial reasoning in MLLMs for video understanding, with a new dataset and optimization techniques.
**Relevance:** 9
**Novelty:** 7

---

## 4. [v-CLR: View-Consistent Learning for Open-World Instance Segmentation](https://arxiv.org/abs/2504.01383) <a id="link4"></a>
**ArXiv ID:** 2504.01383
**Authors:** Chang-Bin Zhang, Jinhong Ni, Yujie Zhong, Kai Han

**Abstract:**  In this paper, we address the challenging problem of open-world instance segmentation. Existing works have shown that vanilla visual networks are biased toward learning appearance information, \eg texture, to recognize objects. This implicit bias causes the model to fail in detecting novel objects with unseen textures in the open-world setting. To address this challenge, we propose a learning framework, called view-Consistent LeaRning (v-CLR), which aims to enforce the model to learn appearance-invariant representations for robust instance segmentation. In v-CLR, we first introduce additional views for each image, where the texture undergoes significant alterations while preserving the image's underlying structure. We then encourage the model to learn the appearance-invariant representation by enforcing the consistency between object features across different views, for which we obtain class-agnostic object proposals using off-the-shelf unsupervised models that possess strong object-awareness. These proposals enable cross-view object feature matching, greatly reducing the appearance dependency while enhancing the object-awareness. We thoroughly evaluate our method on public benchmarks under both cross-class and cross-dataset settings, achieving state-of-the-art performance. Project page: https://visual-ai.github.io/vclr

**Comment:** Matches criterion 1 as it proposes a new methodological improvement for spatial understanding in open-world instance segmentation by enforcing appearance-invariant representations.
**Relevance:** 8
**Novelty:** 7

---

## 5. [ANNEXE: Unified Analyzing, Answering, and Pixel Grounding for Egocentric Interaction](https://arxiv.org/abs/2504.01472) <a id="link5"></a>
**ArXiv ID:** 2504.01472
**Authors:** Yuejiao Su, Yi Wang, Qiongyang Hu, Chuang Yang, Lap-Pui Chau

**Abstract:**  Egocentric interaction perception is one of the essential branches in investigating human-environment interaction, which lays the basis for developing next-generation intelligent systems. However, existing egocentric interaction understanding methods cannot yield coherent textual and pixel-level responses simultaneously according to user queries, which lacks flexibility for varying downstream application requirements. To comprehend egocentric interactions exhaustively, this paper presents a novel task named Egocentric Interaction Reasoning and pixel Grounding (Ego-IRG). Taking an egocentric image with the query as input, Ego-IRG is the first task that aims to resolve the interactions through three crucial steps: analyzing, answering, and pixel grounding, which results in fluent textual and fine-grained pixel-level responses. Another challenge is that existing datasets cannot meet the conditions for the Ego-IRG task. To address this limitation, this paper creates the Ego-IRGBench dataset based on extensive manual efforts, which includes over 20k egocentric images with 1.6 million queries and corresponding multimodal responses about interactions. Moreover, we design a unified ANNEXE model to generate text- and pixel-level outputs utilizing multimodal large language models, which enables a comprehensive interpretation of egocentric interactions. The experiments on the Ego-IRGBench exhibit the effectiveness of our ANNEXE model compared with other works.

**Comment:** Matches criterion 3 as it introduces a new benchmark and method for egocentric interaction reasoning and pixel grounding, focusing on a novel angle in embodied AI.
**Relevance:** 8
**Novelty:** 7

---

## 6. [Is Temporal Prompting All We Need For Limited Labeled Action Recognition?](https://arxiv.org/abs/2504.01890) <a id="link6"></a>
**ArXiv ID:** 2504.01890
**Authors:** Shreyank N Gowda, Boyan Gao, Xiao Gu, Xiaobo Jin

**Abstract:**  Video understanding has shown remarkable improvements in recent years, largely dependent on the availability of large scaled labeled datasets. Recent advancements in visual-language models, especially based on contrastive pretraining, have shown remarkable generalization in zero-shot tasks, helping to overcome this dependence on labeled datasets. Adaptations of such models for videos, typically involve modifying the architecture of vision-language models to cater to video data. However, this is not trivial, since such adaptations are mostly computationally intensive and struggle with temporal modeling. We present TP-CLIP, an adaptation of CLIP that leverages temporal visual prompting for temporal adaptation without modifying the core CLIP architecture. This preserves its generalization abilities. TP-CLIP efficiently integrates into the CLIP architecture, leveraging its pre-trained capabilities for video data. Extensive experiments across various datasets demonstrate its efficacy in zero-shot and few-shot learning, outperforming existing approaches with fewer parameters and computational efficiency. In particular, we use just 1/3 the GFLOPs and 1/28 the number of tuneable parameters in comparison to recent state-of-the-art and still outperform it by up to 15.8% depending on the task and dataset.

**Comment:** This paper matches criterion 2 as it adapts CLIP for video understanding using temporal visual prompting, achieving strong results in zero-shot and few-shot learning.
**Relevance:** 8
**Novelty:** 7

---

## 7. [Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness](https://arxiv.org/abs/2504.01901) <a id="link7"></a>
**ArXiv ID:** 2504.01901
**Authors:** Haochen Wang, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, Zhaoxiang Zhang

**Abstract:**  The rapid development of Large Multimodal Models (LMMs) for 2D images and videos has spurred efforts to adapt these models for interpreting 3D scenes. However, the absence of large-scale 3D vision-language datasets has posed a significant obstacle. To address this issue, typical approaches focus on injecting 3D awareness into 2D LMMs by designing 3D input-level scene representations. This work provides a new perspective. We introduce reconstructive visual instruction tuning with 3D-awareness (Ross3D), which integrates 3D-aware visual supervision into the training procedure. Specifically, it incorporates cross-view and global-view reconstruction. The former requires reconstructing masked views by aggregating overlapping information from other views. The latter aims to aggregate information from all available views to recover Bird's-Eye-View images, contributing to a comprehensive overview of the entire scene. Empirically, Ross3D achieves state-of-the-art performance across various 3D scene understanding benchmarks. More importantly, our semi-supervised experiments demonstrate significant potential in leveraging large amounts of unlabeled 3D vision-only data.

**Comment:** This paper matches criterion 2 as it introduces a new method for integrating 3D-awareness into large multimodal models (LMMs) and achieves state-of-the-art results.
**Relevance:** 8
**Novelty:** 7

---

## 8. [COST: Contrastive One-Stage Transformer for Vision-Language Small Object Tracking](https://arxiv.org/abs/2504.01321) <a id="link8"></a>
**ArXiv ID:** 2504.01321
**Authors:** Chunhui Zhang, Li Liu, Jialin Gao, Xin Sun, Hao Wen, Xi Zhou, Shiming Ge, Yanfeng Wang

**Abstract:**  Transformer has recently demonstrated great potential in improving vision-language (VL) tracking algorithms. However, most of the existing VL trackers rely on carefully designed mechanisms to perform the multi-stage multi-modal fusion. Additionally, direct multi-modal fusion without alignment ignores distribution discrepancy between modalities in feature space, potentially leading to suboptimal representations. In this work, we propose COST, a contrastive one-stage transformer fusion framework for VL tracking, aiming to learn semantically consistent and unified VL representations. Specifically, we introduce a contrastive alignment strategy that maximizes mutual information (MI) between a video and its corresponding language description. This enables effective cross-modal alignment, yielding semantically consistent features in the representation space. By leveraging a visual-linguistic transformer, we establish an efficient multi-modal fusion and reasoning mechanism, empirically demonstrating that a simple stack of transformer encoders effectively enables unified VL representations. Moreover, we contribute a newly collected VL tracking benchmark dataset for small object tracking, named VL-SOT500, with bounding boxes and language descriptions. Our dataset comprises two challenging subsets, VL-SOT230 and VL-SOT270, dedicated to evaluating generic and high-speed small object tracking, respectively. Small object tracking is notoriously challenging due to weak appearance and limited features, and this dataset is, to the best of our knowledge, the first to explore the usage of language cues to enhance visual representation for small object tracking. Extensive experiments demonstrate that COST achieves state-of-the-art performance on five existing VL tracking datasets, as well as on our proposed VL-SOT500 dataset. Source codes and dataset will be made publicly available.

**Comment:** Matches criterion 2 as it proposes a new vision-language transformer for small object tracking, and introduces a new benchmark dataset.
**Relevance:** 8
**Novelty:** 7

---

## 9. [FIORD: A Fisheye Indoor-Outdoor Dataset with LIDAR Ground Truth for 3D Scene Reconstruction and Benchmarking](https://arxiv.org/abs/2504.01732) <a id="link9"></a>
**ArXiv ID:** 2504.01732
**Authors:** Ulas Gunes, Matias Turkulainen, Xuqian Ren, Arno Solin, Juho Kannala, Esa Rahtu

**Abstract:**  The development of large-scale 3D scene reconstruction and novel view synthesis methods mostly rely on datasets comprising perspective images with narrow fields of view (FoV). While effective for small-scale scenes, these datasets require large image sets and extensive structure-from-motion (SfM) processing, limiting scalability. To address this, we introduce a fisheye image dataset tailored for scene reconstruction tasks. Using dual 200-degree fisheye lenses, our dataset provides full 360-degree coverage of 5 indoor and 5 outdoor scenes. Each scene has sparse SfM point clouds and precise LIDAR-derived dense point clouds that can be used as geometric ground-truth, enabling robust benchmarking under challenging conditions such as occlusions and reflections. While the baseline experiments focus on vanilla Gaussian Splatting and NeRF based Nerfacto methods, the dataset supports diverse approaches for scene reconstruction, novel view synthesis, and image-based rendering.

**Comment:** Matches criterion 3 as it introduces a new benchmark dataset for 3D scene reconstruction with fisheye images and LIDAR ground truth.
**Relevance:** 8
**Novelty:** 7

---

## 10. [On Data Synthesis and Post-training for Visual Abstract Reasoning](https://arxiv.org/abs/2504.01324) <a id="link10"></a>
**ArXiv ID:** 2504.01324
**Authors:** Ke Zhu, Yu Wang, Jiangjiang Liu, Qunyi Xie, Shanshan Liu, Gang Zhang

**Abstract:**  This paper is a pioneering work attempting to address abstract visual reasoning (AVR) problems for large vision-language models (VLMs). We make a common LLaVA-NeXT 7B model capable of perceiving and reasoning about specific AVR problems, surpassing both open-sourced (e.g., Qwen-2-VL-72B) and closed-sourced powerful VLMs (e.g., GPT-4o) with significant margin. This is a great breakthrough since almost all previous VLMs fail or show nearly random performance on representative AVR benchmarks. Our key success is our innovative data synthesis and post-training process, aiming to fully relieve the task difficulty and elicit the model to learn, step by step. Our 7B model is also shown to be behave well on AVR without sacrificing common multimodal comprehension abilities. We hope our paper could serve as an early effort in this area and would inspire further research in abstract visual reasoning.

**Comment:** Matches criterion 2 as it discusses a new vision-language model (VLM) with significant improvements in abstract visual reasoning.
**Relevance:** 8
**Novelty:** 7

---

## 11. [Omnidirectional Depth-Aided Occupancy Prediction based on Cylindrical Voxel for Autonomous Driving](https://arxiv.org/abs/2504.01023) <a id="link11"></a>
**ArXiv ID:** 2504.01023
**Authors:** Chaofan Wu, Jiaheng Li, Jinghao Cao, Ming Li, Yongkang Feng, Jiayu Wu Shuwen Xu, Zihang Gao, Sidan Du, Yang Li

**Abstract:**  Accurate 3D perception is essential for autonomous driving. Traditional methods often struggle with geometric ambiguity due to a lack of geometric prior. To address these challenges, we use omnidirectional depth estimation to introduce geometric prior. Based on the depth information, we propose a Sketch-Coloring framework OmniDepth-Occ. Additionally, our approach introduces a cylindrical voxel representation based on polar coordinate to better align with the radial nature of panoramic camera views. To address the lack of fisheye camera dataset in autonomous driving tasks, we also build a virtual scene dataset with six fisheye cameras, and the data volume has reached twice that of SemanticKITTI. Experimental results demonstrate that our Sketch-Coloring network significantly enhances 3D perception performance.

**Comment:** Matches criterion 3 as it introduces a new benchmark dataset and a novel cylindrical voxel representation for autonomous driving, focusing on panoramic camera views.
**Relevance:** 7
**Novelty:** 7

---

## 12. [Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D Reconstruction and Novel View Synthesis](https://arxiv.org/abs/2504.01960) <a id="link12"></a>
**ArXiv ID:** 2504.01960
**Authors:** Niluthpol Chowdhury Mithun, Tuan Pham, Qiao Wang, Ben Southall, Kshitij Minhas, Bogdan Matei, Stephan Mandt, Supun Samarasekera, Rakesh Kumar

**Abstract:**  Recent advancements in 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have achieved impressive results in real-time 3D reconstruction and novel view synthesis. However, these methods struggle in large-scale, unconstrained environments where sparse and uneven input coverage, transient occlusions, appearance variability, and inconsistent camera settings lead to degraded quality. We propose GS-Diff, a novel 3DGS framework guided by a multi-view diffusion model to address these limitations. By generating pseudo-observations conditioned on multi-view inputs, our method transforms under-constrained 3D reconstruction problems into well-posed ones, enabling robust optimization even with sparse data. GS-Diff further integrates several enhancements, including appearance embedding, monocular depth priors, dynamic object modeling, anisotropy regularization, and advanced rasterization techniques, to tackle geometric and photometric challenges in real-world settings. Experiments on four benchmarks demonstrate that GS-Diff consistently outperforms state-of-the-art baselines by significant margins.

**Comment:** This paper aligns with criterion 4 as it proposes a novel 3D Gaussian Splatting framework guided by a multi-view diffusion model for large-scale 3D reconstruction and novel view synthesis.
**Relevance:** 7
**Novelty:** 7

---

## 13. [CLIP-SLA: Parameter-Efficient CLIP Adaptation for Continuous Sign Language Recognition](https://arxiv.org/abs/2504.01666) <a id="link13"></a>
**ArXiv ID:** 2504.01666
**Authors:** Sarah Alyami, Hamzah Luqman

**Abstract:**  Continuous sign language recognition (CSLR) focuses on interpreting and transcribing sequences of sign language gestures in videos. In this work, we propose CLIP sign language adaptation (CLIP-SLA), a novel CSLR framework that leverages the powerful pre-trained visual encoder from the CLIP model to sign language tasks through parameter-efficient fine-tuning (PEFT). We introduce two variants, SLA-Adapter and SLA-LoRA, which integrate PEFT modules into the CLIP visual encoder, enabling fine-tuning with minimal trainable parameters. The effectiveness of the proposed frameworks is validated on four datasets: Phoenix2014, Phoenix2014-T, CSL-Daily, and Isharah-500, where both CLIP-SLA variants outperformed several SOTA models with fewer trainable parameters. Extensive ablation studies emphasize the effectiveness and flexibility of the proposed methods with different vision-language models for CSLR. These findings showcase the potential of adapting large-scale pre-trained models for scalable and efficient CSLR, which pave the way for future advancements in sign language understanding.

**Comment:** This paper adapts CLIP for continuous sign language recognition, which aligns with criterion 4 as it involves adapting vision foundation models for a specific application.
**Relevance:** 7
**Novelty:** 6

---

## 14. [GMAI-VL-R1: Harnessing Reinforcement Learning for Multimodal Medical Reasoning](https://arxiv.org/abs/2504.01886) <a id="link14"></a>
**ArXiv ID:** 2504.01886
**Authors:** Yanzhou Su, Tianbin Li, Jiyao Liu, Chenglong Ma, Junzhi Ning, Cheng Tang, Sibo Ju, Jin Ye, Pengcheng Chen, Ming Hu, Shixiang Tang, Lihao Liu, Bin Fu, Wenqi Shao, Xiaowei Hu, Xiangwen Liao, Yuanfeng Ji, Junjun He

**Abstract:**  Recent advances in general medical AI have made significant strides, but existing models often lack the reasoning capabilities needed for complex medical decision-making. This paper presents GMAI-VL-R1, a multimodal medical reasoning model enhanced by reinforcement learning (RL) to improve its reasoning abilities. Through iterative training, GMAI-VL-R1 optimizes decision-making, significantly boosting diagnostic accuracy and clinical support. We also develop a reasoning data synthesis method, generating step-by-step reasoning data via rejection sampling, which further enhances the model's generalization. Experimental results show that after RL training, GMAI-VL-R1 excels in tasks such as medical image diagnosis and visual question answering. While the model demonstrates basic memorization with supervised fine-tuning, RL is crucial for true generalization. Our work establishes new evaluation benchmarks and paves the way for future advancements in medical reasoning models. Code, data, and model will be released at \href{https://github.com/uni-medical/GMAI-VL-R1}{this link}.

**Comment:** This paper matches criterion 2 as it introduces a multimodal medical reasoning model enhanced by reinforcement learning, which is a novel application of vision-language models.
**Relevance:** 7
**Novelty:** 6

---

## 15. [Bridge 2D-3D: Uncertainty-aware Hierarchical Registration Network with Domain Alignment](https://arxiv.org/abs/2504.01641) <a id="link15"></a>
**ArXiv ID:** 2504.01641
**Authors:** Zhixin Cheng, Jiacheng Deng, Xinjun Li, Baoqun Yin, Tianzhu Zhang

**Abstract:**  The method for image-to-point cloud registration typically determines the rigid transformation using a coarse-to-fine pipeline. However, directly and uniformly matching image patches with point cloud patches may lead to focusing on incorrect noise patches during matching while ignoring key ones. Moreover, due to the significant differences between image and point cloud modalities, it may be challenging to bridge the domain gap without specific improvements in design. To address the above issues, we innovatively propose the Uncertainty-aware Hierarchical Matching Module (UHMM) and the Adversarial Modal Alignment Module (AMAM). Within the UHMM, we model the uncertainty of critical information in image patches and facilitate multi-level fusion interactions between image and point cloud features. In the AMAM, we design an adversarial approach to reduce the domain gap between image and point cloud. Extensive experiments and ablation studies on RGB-D Scene V2 and 7-Scenes benchmarks demonstrate the superiority of our method, making it a state-of-the-art approach for image-to-point cloud registration tasks.

**Comment:** Matches criterion 1 as it introduces a novel hierarchical registration method for image-to-point cloud alignment, improving spatial understanding.
**Relevance:** 7
**Novelty:** 6

---

## 16. [Proposition of Affordance-Driven Environment Recognition Framework Using Symbol Networks in Large Language Models](https://arxiv.org/abs/2504.01644) <a id="link16"></a>
**ArXiv ID:** 2504.01644
**Authors:** Kazuma Arii, Satoshi Kurihara

**Abstract:**  In the quest to enable robots to coexist with humans, understanding dynamic situations and selecting appropriate actions based on common sense and affordances are essential. Conventional AI systems face challenges in applying affordance, as it represents implicit knowledge derived from common sense. However, large language models (LLMs) offer new opportunities due to their ability to process extensive human knowledge. This study proposes a method for automatic affordance acquisition by leveraging LLM outputs. The process involves generating text using LLMs, reconstructing the output into a symbol network using morphological and dependency analysis, and calculating affordances based on network distances. Experiments using ``apple'' as an example demonstrated the method's ability to extract context-dependent affordances with high explainability. The results suggest that the proposed symbol network, reconstructed from LLM outputs, enables robots to interpret affordances effectively, bridging the gap between symbolized data and human-like situational understanding.

**Comment:** Matches criterion 1 as it proposes a framework for spatial understanding using affordance-driven recognition in embodied agents.
**Relevance:** 7
**Novelty:** 6

---

## 17. [FlowR: Flowing from Sparse to Dense 3D Reconstructions](https://arxiv.org/abs/2504.01647) <a id="link17"></a>
**ArXiv ID:** 2504.01647
**Authors:** Tobias Fischer, Samuel Rota Bul\`o, Yung-Hsu Yang, Nikhil Varma Keetha, Lorenzo Porzi, Norman M\"uller, Katja Schwarz, Jonathon Luiten, Marc Pollefeys, Peter Kontschieder

**Abstract:**  3D Gaussian splatting enables high-quality novel view synthesis (NVS) at real-time frame rates. However, its quality drops sharply as we depart from the training views. Thus, dense captures are needed to match the high-quality expectations of some applications, e.g. Virtual Reality (VR). However, such dense captures are very laborious and expensive to obtain. Existing works have explored using 2D generative models to alleviate this requirement by distillation or generating additional training views. These methods are often conditioned only on a handful of reference input views and thus do not fully exploit the available 3D information, leading to inconsistent generation results and reconstruction artifacts. To tackle this problem, we propose a multi-view, flow matching model that learns a flow to connect novel view renderings from possibly sparse reconstructions to renderings that we expect from dense reconstructions. This enables augmenting scene captures with novel, generated views to improve reconstruction quality. Our model is trained on a novel dataset of 3.6M image pairs and can process up to 45 views at 540x960 resolution (91K tokens) on one H100 GPU in a single forward pass. Our pipeline consistently improves NVS in sparse- and dense-view scenarios, leading to higher-quality reconstructions than prior works across multiple, widely-used NVS benchmarks.

**Comment:** Matches criterion 3 as it proposes a novel method for improving 3D reconstructions and addresses limitations in sparse-to-dense view synthesis.
**Relevance:** 7
**Novelty:** 6

---

## 18. [VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step](https://arxiv.org/abs/2504.01956) <a id="link18"></a>
**ArXiv ID:** 2504.01956
**Authors:** Hanyang Wang, Fangfu Liu, Jiawei Chi, Yueqi Duan

**Abstract:**  Recovering 3D scenes from sparse views is a challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degradation by minimal overlap across input views with insufficient visual information. Fortunately, recent video generative models show promise in addressing this challenge as they are capable of generating video clips with plausible 3D structures. Powered by large pretrained video diffusion models, some pioneering research start to explore the potential of video generative prior and create 3D scenes from sparse views. Despite impressive improvements, they are limited by slow inference time and the lack of 3D constraint, leading to inefficiencies and reconstruction artifacts that do not align with real-world geometry structure. In this paper, we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step, aiming to build an efficient and effective tool to bridge the gap from video to 3D. Specifically, we design a 3D-aware leap flow distillation strategy to leap over time-consuming redundant information and train a dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate that our VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, highlighting its potential as an efficient tool for future video to 3D applications. Project Page: https://hanyang-21.github.io/VideoScene

**Comment:** Matches criterion 4 as it focuses on video diffusion models and their application to 3D scene generation, which is relevant to vision foundation models and their applications.
**Relevance:** 5
**Novelty:** 7

---

## 19. [Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting Conditions with View-Adaptive Curve Adjustment](https://arxiv.org/abs/2504.01503) <a id="link19"></a>
**ArXiv ID:** 2504.01503
**Authors:** Ziteng Cui, Xuangeng Chu, Tatsuya Harada

**Abstract:**  Capturing high-quality photographs under diverse real-world lighting conditions is challenging, as both natural lighting (e.g., low-light) and camera exposure settings (e.g., exposure time) significantly impact image quality. This challenge becomes more pronounced in multi-view scenarios, where variations in lighting and image signal processor (ISP) settings across viewpoints introduce photometric inconsistencies. Such lighting degradations and view-dependent variations pose substantial challenges to novel view synthesis (NVS) frameworks based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). To address this, we introduce Luminance-GS, a novel approach to achieving high-quality novel view synthesis results under diverse challenging lighting conditions using 3DGS. By adopting per-view color matrix mapping and view-adaptive curve adjustments, Luminance-GS achieves state-of-the-art (SOTA) results across various lighting conditions -- including low-light, overexposure, and varying exposure -- while not altering the original 3DGS explicit representation. Compared to previous NeRF- and 3DGS-based baselines, Luminance-GS provides real-time rendering speed with improved reconstruction quality.

**Comment:** This paper matches criterion 4 as it adapts 3D Gaussian Splatting for challenging lighting conditions, improving novel view synthesis under diverse scenarios.
**Relevance:** 6
**Novelty:** 6

---

## 20. [High-fidelity 3D Object Generation from Single Image with RGBN-Volume Gaussian Reconstruction Model](https://arxiv.org/abs/2504.01512) <a id="link20"></a>
**ArXiv ID:** 2504.01512
**Authors:** Yiyang Shen, Kun Zhou, He Wang, Yin Yang, Tianjia Shao

**Abstract:**  Recently single-view 3D generation via Gaussian splatting has emerged and developed quickly. They learn 3D Gaussians from 2D RGB images generated from pre-trained multi-view diffusion (MVD) models, and have shown a promising avenue for 3D generation through a single image. Despite the current progress, these methods still suffer from the inconsistency jointly caused by the geometric ambiguity in the 2D images, and the lack of structure of 3D Gaussians, leading to distorted and blurry 3D object generation. In this paper, we propose to fix these issues by GS-RGBN, a new RGBN-volume Gaussian Reconstruction Model designed to generate high-fidelity 3D objects from single-view images. Our key insight is a structured 3D representation can simultaneously mitigate the afore-mentioned two issues. To this end, we propose a novel hybrid Voxel-Gaussian representation, where a 3D voxel representation contains explicit 3D geometric information, eliminating the geometric ambiguity from 2D images. It also structures Gaussians during learning so that the optimization tends to find better local optima. Our 3D voxel representation is obtained by a fusion module that aligns RGB features and surface normal features, both of which can be estimated from 2D images. Extensive experiments demonstrate the superiority of our methods over prior works in terms of high-quality reconstruction results, robust generalization, and good efficiency.

**Comment:** Matches criterion 4 as it proposes a novel method for high-fidelity 3D object generation from single images, which is relevant to vision foundation models and their applications.
**Relevance:** 5
**Novelty:** 6

---

## 21. [An Illusion of Progress? Assessing the Current State of Web Agents](https://arxiv.org/abs/2504.01382) <a id="link21"></a>
**ArXiv ID:** 2504.01382
**Authors:** Tianci Xue, Weijian Qi, Tianneng Shi, Chan Hee Song, Boyu Gou, Dawn Song, Huan Sun, Yu Su

**Abstract:**  As digitalization and cloud technologies evolve, the web is becoming increasingly important in the modern society. Autonomous web agents based on large language models (LLMs) hold a great potential in work automation. It is therefore important to accurately measure and monitor the progression of their capabilities. In this work, we conduct a comprehensive and rigorous assessment of the current state of web agents. Our results depict a very different picture of the competency of current agents, suggesting over-optimism in previously reported results. This gap can be attributed to shortcomings in existing benchmarks. We introduce Online-Mind2Web, an online evaluation benchmark consisting of 300 diverse and realistic tasks spanning 136 websites. It enables us to evaluate web agents under a setting that approximates how real users use these agents. To facilitate more scalable evaluation and development, we also develop a novel LLM-as-a-Judge automatic evaluation method and show that it can achieve around 85% agreement with human judgment, substantially higher than existing methods. Finally, we present the first comprehensive comparative analysis of current web agents, highlighting both their strengths and limitations to inspire future research.

**Comment:** Matches criterion 3 as it introduces a new benchmark (Online-Mind2Web) for evaluating web agents, which is relevant to embodied AI benchmarks.
**Relevance:** 5
**Novelty:** 6

---

## 22. [End-to-End Driving with Online Trajectory Evaluation via BEV World Model](https://arxiv.org/abs/2504.01941) <a id="link22"></a>
**ArXiv ID:** 2504.01941
**Authors:** Yingyan Li, Yuqi Wang, Yang Liu, Jiawei He, Lue Fan, Zhaoxiang Zhang

**Abstract:**  End-to-end autonomous driving has achieved remarkable progress by integrating perception, prediction, and planning into a fully differentiable framework. Yet, to fully realize its potential, an effective online trajectory evaluation is indispensable to ensure safety. By forecasting the future outcomes of a given trajectory, trajectory evaluation becomes much more effective. This goal can be achieved by employing a world model to capture environmental dynamics and predict future states. Therefore, we propose an end-to-end driving framework WoTE, which leverages a BEV World model to predict future BEV states for Trajectory Evaluation. The proposed BEV world model is latency-efficient compared to image-level world models and can be seamlessly supervised using off-the-shelf BEV-space traffic simulators. We validate our framework on both the NAVSIM benchmark and the closed-loop Bench2Drive benchmark based on the CARLA simulator, achieving state-of-the-art performance. Code is released at https://github.com/liyingyanUCAS/WoTE.

**Comment:** Matches criterion 3 as it proposes a new end-to-end driving framework with a novel BEV world model for trajectory evaluation, which is relevant to embodied AI benchmarks and methods.
**Relevance:** 5
**Novelty:** 6

---

## 23. [DALIP: Distribution Alignment-based Language-Image Pre-Training for Domain-Specific Data](https://arxiv.org/abs/2504.01386) <a id="link23"></a>
**ArXiv ID:** 2504.01386
**Authors:** Junjie Wu, Jiangtao Xie, Zhaolin Zhang, Qilong Wang, Qinghua Hu, Peihua Li, Sen Xu

**Abstract:**  Recently, Contrastive Language-Image Pre-training (CLIP) has shown promising performance in domain-specific data (e.g., biology), and has attracted increasing research attention. Existing works generally focus on collecting extensive domain-specific data and directly tuning the original CLIP models. Intuitively, such a paradigm takes no full consideration of the characteristics lying in domain-specific data (e.g., fine-grained nature of biological data) and so limits model capability, while mostly losing the original ability of CLIP in the general domain. In this paper, we propose a Distribution Alignment-based Language-Image Pre-Training (DALIP) method for biological data. Specifically, DALIP optimizes CLIP models by matching the similarity between feature distribution of image-text pairs instead of the original [cls] token, which can capture rich yet effective information inherent in image-text pairs as powerful representations, and so better cope with fine-grained nature of biological data. Particularly, our DALIP efficiently approximates feature distribution via its first- and second-order statistics, while presenting a Multi-head Brownian Distance Covariance (MBDC) module to acquire second-order statistics of token features efficiently. Furthermore, we collect a new dataset for plant domain (e.g., specific data in biological domain) comprising 10M plant data with 3M general-domain data (namely PlantMix-13M) according to data mixing laws. Extensive experiments show that DALIP clearly outperforms existing CLIP counterparts in biological domain, while well generalizing to remote sensing and medical imaging domains. Besides, our PlantMix-13M dataset further boosts performance of DALIP in plant domain, while preserving model ability in general domain.

**Comment:** Matches criterion 2 as it proposes a new method for domain-specific language-image pre-training, which is relevant to visual large language models (VLLMs).
**Relevance:** 5
**Novelty:** 6

---

## 24. [Mesh Mamba: A Unified State Space Model for Saliency Prediction in Non-Textured and Textured Meshes](https://arxiv.org/abs/2504.01466) <a id="link24"></a>
**ArXiv ID:** 2504.01466
**Authors:** Kaiwei Zhang, Dandan Zhu, Xiongkuo Min, Guangtao Zhai

**Abstract:**  Mesh saliency enhances the adaptability of 3D vision by identifying and emphasizing regions that naturally attract visual attention. To investigate the interaction between geometric structure and texture in shaping visual attention, we establish a comprehensive mesh saliency dataset, which is the first to systematically capture the differences in saliency distribution under both textured and non-textured visual conditions. Furthermore, we introduce mesh Mamba, a unified saliency prediction model based on a state space model (SSM), designed to adapt across various mesh types. Mesh Mamba effectively analyzes the geometric structure of the mesh while seamlessly incorporating texture features into the topological framework, ensuring coherence throughout appearance-enhanced modeling. More importantly, by subgraph embedding and a bidirectional SSM, the model enables global context modeling for both local geometry and texture, preserving the topological structure and improving the understanding of visual details and structural complexity. Through extensive theoretical and empirical validation, our model not only improves performance across various mesh types but also demonstrates high scalability and versatility, particularly through cross validations of various visual features.

**Comment:** Matches criterion 4 as it focuses on mesh saliency prediction and introduces a unified state space model, which is related to vision foundation models.
**Relevance:** 5
**Novelty:** 6

---

## 25. [Scene-Centric Unsupervised Panoptic Segmentation](https://arxiv.org/abs/2504.01955) <a id="link25"></a>
**ArXiv ID:** 2504.01955
**Authors:** Oliver Hahn, Christoph Reich, Nikita Araslanov, Daniel Cremers, Christian Rupprecht, Stefan Roth

**Abstract:**  Unsupervised panoptic segmentation aims to partition an image into semantically meaningful regions and distinct object instances without training on manually annotated data. In contrast to prior work on unsupervised panoptic scene understanding, we eliminate the need for object-centric training data, enabling the unsupervised understanding of complex scenes. To that end, we present the first unsupervised panoptic method that directly trains on scene-centric imagery. In particular, we propose an approach to obtain high-resolution panoptic pseudo labels on complex scene-centric data, combining visual representations, depth, and motion cues. Utilizing both pseudo-label training and a panoptic self-training strategy yields a novel approach that accurately predicts panoptic segmentation of complex scenes without requiring any human annotations. Our approach significantly improves panoptic quality, e.g., surpassing the recent state of the art in unsupervised panoptic segmentation on Cityscapes by 9.4% points in PQ.

**Comment:** Matches criterion 4 as it focuses on unsupervised panoptic segmentation, which is related to vision foundation models and their applications.
**Relevance:** 5
**Novelty:** 6

---

## 26. [Gaze-Guided 3D Hand Motion Prediction for Detecting Intent in Egocentric Grasping Tasks](https://arxiv.org/abs/2504.01024) <a id="link26"></a>
**ArXiv ID:** 2504.01024
**Authors:** Yufei He, Xucong Zhang, Arno H. A. Stienen

**Abstract:**  Human intention detection with hand motion prediction is critical to drive the upper-extremity assistive robots in neurorehabilitation applications. However, the traditional methods relying on physiological signal measurement are restrictive and often lack environmental context. We propose a novel approach that predicts future sequences of both hand poses and joint positions. This method integrates gaze information, historical hand motion sequences, and environmental object data, adapting dynamically to the assistive needs of the patient without prior knowledge of the intended object for grasping. Specifically, we use a vector-quantized variational autoencoder for robust hand pose encoding with an autoregressive generative transformer for effective hand motion sequence prediction. We demonstrate the usability of these novel techniques in a pilot study with healthy subjects. To train and evaluate the proposed method, we collect a dataset consisting of various types of grasp actions on different objects from multiple subjects. Through extensive experiments, we demonstrate that the proposed method can successfully predict sequential hand movement. Especially, the gaze information shows significant enhancements in prediction capabilities, particularly with fewer input frames, highlighting the potential of the proposed method for real-world applications.

**Comment:** Matches criterion 3 as it focuses on embodied AI with a novel method for hand motion prediction using gaze and environmental context.
**Relevance:** 5
**Novelty:** 6

---

## 27. [Enhanced Cross-modal 3D Retrieval via Tri-modal Reconstruction](https://arxiv.org/abs/2504.01476) <a id="link27"></a>
**ArXiv ID:** 2504.01476
**Authors:** Junlong Ren, Hao Wang

**Abstract:**  Cross-modal 3D retrieval is a critical yet challenging task, aiming to achieve bi-directional retrieval between 3D and text modalities. Current methods predominantly rely on a certain 3D representation (e.g., point cloud), with few exploiting the 2D-3D consistency and complementary relationships, which constrains their performance. To bridge this gap, we propose to adopt multi-view images and point clouds to jointly represent 3D shapes, facilitating tri-modal alignment (i.e., image, point, text) for enhanced cross-modal 3D retrieval. Notably, we introduce tri-modal reconstruction to improve the generalization ability of encoders. Given point features, we reconstruct image features under the guidance of text features, and vice versa. With well-aligned point cloud and multi-view image features, we aggregate them as multimodal embeddings through fine-grained 2D-3D fusion to enhance geometric and semantic understanding. Recognizing the significant noise in current datasets where many 3D shapes and texts share similar semantics, we employ hard negative contrastive training to emphasize harder negatives with greater significance, leading to robust discriminative embeddings. Extensive experiments on the Text2Shape dataset demonstrate that our method significantly outperforms previous state-of-the-art methods in both shape-to-text and text-to-shape retrieval tasks by a substantial margin.

**Comment:** Matches criterion 2 as it focuses on cross-modal 3D retrieval and tri-modal alignment, which involves multi-modal learning.
**Relevance:** 5
**Novelty:** 6

---

## 28. [AI-Newton: A Concept-Driven Physical Law Discovery System without Prior Physical Knowledge](https://arxiv.org/abs/2504.01538) <a id="link28"></a>
**ArXiv ID:** 2504.01538
**Authors:** You-Le Fang, Dong-Shan Jian, Xiang Li, Yan-Qing Ma

**Abstract:**  Current limitations in human scientific discovery necessitate a new research paradigm. While advances in artificial intelligence (AI) offer a highly promising solution, enabling AI to emulate human-like scientific discovery remains an open challenge. To address this, we propose AI-Newton, a concept-driven discovery system capable of autonomously deriving physical laws from raw data -- without supervision or prior physical knowledge. The system integrates a knowledge base and knowledge representation centered on physical concepts, along with an autonomous discovery workflow. As a proof of concept, we apply AI-Newton to a large set of Newtonian mechanics problems. Given experimental data with noise, the system successfully rediscovers fundamental laws, including Newton's second law, energy conservation and law of gravitation, using autonomously defined concepts. This achievement marks a significant step toward AI-driven autonomous scientific discovery.

**Comment:** Does not match any specific criterion but is an interesting AI-driven system for physical law discovery, which may appeal to general interest.
**Relevance:** 3
**Novelty:** 8

---

## 29. [Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks](https://arxiv.org/abs/2504.01308) <a id="link29"></a>
**ArXiv ID:** 2504.01308
**Authors:** Jiawei Wang, Yushen Zuo, Yuanjun Chai, Zhendong Liu, Yichen Fu, Yichun Feng, Kin-man Lam

**Abstract:**  Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.

**Comment:** Matches criterion 2 as it focuses on improving the robustness of vision-language models (VLMs) against adversarial attacks.
**Relevance:** 6
**Novelty:** 5

---

## 30. [A Diffusion-Based Framework for Occluded Object Movement](https://arxiv.org/abs/2504.01873) <a id="link30"></a>
**ArXiv ID:** 2504.01873
**Authors:** Zheng-Peng Duan, Jiawei Zhang, Siyu Liu, Zheng Lin, Chun-Le Guo, Dongqing Zou, Jimmy Ren, Chongyi Li

**Abstract:**  Seamlessly moving objects within a scene is a common requirement for image editing, but it is still a challenge for existing editing methods. Especially for real-world images, the occlusion situation further increases the difficulty. The main difficulty is that the occluded portion needs to be completed before movement can proceed. To leverage the real-world knowledge embedded in the pre-trained diffusion models, we propose a Diffusion-based framework specifically designed for Occluded Object Movement, named DiffOOM. The proposed DiffOOM consists of two parallel branches that perform object de-occlusion and movement simultaneously. The de-occlusion branch utilizes a background color-fill strategy and a continuously updated object mask to focus the diffusion process on completing the obscured portion of the target object. Concurrently, the movement branch employs latent optimization to place the completed object in the target location and adopts local text-conditioned guidance to integrate the object into new surroundings appropriately. Extensive evaluations demonstrate the superior performance of our method, which is further validated by a comprehensive user study.

**Comment:** Does not directly match any specific criterion but is relevant to your friend's general interest in generative modeling and image editing.
**Relevance:** 3
**Novelty:** 6

---

## 31. [Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis](https://arxiv.org/abs/2504.01515) <a id="link31"></a>
**ArXiv ID:** 2504.01515
**Authors:** Zixuan Wang, Duo Peng, Feng Chen, Yuwei Yang, Yinjie Lei

**Abstract:**  Conditional image synthesis is a crucial task with broad applications, such as artistic creation and virtual reality. However, current generative methods are often task-oriented with a narrow scope, handling a restricted condition with constrained applicability. In this paper, we propose a novel approach that treats conditional image synthesis as the modular combination of diverse fundamental condition units. Specifically, we divide conditions into three primary units: text, layout, and drag. To enable effective control over these conditions, we design a dedicated alignment module for each. For the text condition, we introduce a Dense Concept Alignment (DCA) module, which achieves dense visual-text alignment by drawing on diverse textual concepts. For the layout condition, we propose a Dense Geometry Alignment (DGA) module to enforce comprehensive geometric constraints that preserve the spatial configuration. For the drag condition, we introduce a Dense Motion Alignment (DMA) module to apply multi-level motion regularization, ensuring that each pixel follows its desired trajectory without visual artifacts. By flexibly inserting and combining these alignment modules, our framework enhances the model's adaptability to diverse conditional generation tasks and greatly expands its application range. Extensive experiments demonstrate the superior performance of our framework across a variety of conditions, including textual description, segmentation mask (bounding box), drag manipulation, and their combinations. Code is available at https://github.com/ZixuanWang0525/DADG.

**Comment:** Does not directly match any specific criterion but is relevant to your friend's general interest in generative modeling and multi-modal learning.
**Relevance:** 3
**Novelty:** 6

---

## 32. [A$^\text{T}$A: Adaptive Transformation Agent for Text-Guided Subject-Position Variable Background Inpainting](https://arxiv.org/abs/2504.01603) <a id="link32"></a>
**ArXiv ID:** 2504.01603
**Authors:** Yizhe Tang, Zhimin Sun, Yuzhen Du, Ran Yi, Guangben Lu, Teng Hu, Luying Li, Lizhuang Ma, Fangyuan Zou

**Abstract:**  Image inpainting aims to fill the missing region of an image. Recently, there has been a surge of interest in foreground-conditioned background inpainting, a sub-task that fills the background of an image while the foreground subject and associated text prompt are provided. Existing background inpainting methods typically strictly preserve the subject's original position from the source image, resulting in inconsistencies between the subject and the generated background. To address this challenge, we propose a new task, the "Text-Guided Subject-Position Variable Background Inpainting", which aims to dynamically adjust the subject position to achieve a harmonious relationship between the subject and the inpainted background, and propose the Adaptive Transformation Agent (A$^\text{T}$A) for this task. Firstly, we design a PosAgent Block that adaptively predicts an appropriate displacement based on given features to achieve variable subject-position. Secondly, we design the Reverse Displacement Transform (RDT) module, which arranges multiple PosAgent blocks in a reverse structure, to transform hierarchical feature maps from deep to shallow based on semantic information. Thirdly, we equip A$^\text{T}$A with a Position Switch Embedding to control whether the subject's position in the generated image is adaptively predicted or fixed. Extensive comparative experiments validate the effectiveness of our A$^\text{T}$A approach, which not only demonstrates superior inpainting capabilities in subject-position variable inpainting, but also ensures good performance on subject-position fixed inpainting.

**Comment:** This paper introduces a novel method for text-guided image inpainting, which does not directly match any of the criteria but may be tangentially relevant to generative modeling in vision tasks.
**Relevance:** 3
**Novelty:** 6

---

## 33. [InvFussion: Bridging Supervised and Zero-shot Diffusion for Inverse Problems](https://arxiv.org/abs/2504.01689) <a id="link33"></a>
**ArXiv ID:** 2504.01689
**Authors:** Noam Elata, Hyungjin Chung, Jong Chul Ye, Tomer Michaeli, Michael Elad

**Abstract:**  Diffusion Models have demonstrated remarkable capabilities in handling inverse problems, offering high-quality posterior-sampling-based solutions. Despite significant advances, a fundamental trade-off persists, regarding the way the conditioned synthesis is employed: Training-based methods achieve high quality results, while zero-shot approaches trade this with flexibility. This work introduces a framework that combines the best of both worlds -- the strong performance of supervised approaches and the flexibility of zero-shot methods. This is achieved through a novel architectural design that seamlessly integrates the degradation operator directly into the denoiser. In each block, our proposed architecture applies the degradation operator on the network activations and conditions the output using the attention mechanism, enabling adaptation to diverse degradation scenarios while maintaining high performance. Our work demonstrates the versatility of the proposed architecture, operating as a general MMSE estimator, a posterior sampler, or a Neural Posterior Principal Component estimator. This flexibility enables a wide range of downstream tasks, highlighting the broad applicability of our framework. The proposed modification of the denoiser network offers a versatile, accurate, and computationally efficient solution, demonstrating the advantages of dedicated network architectures for complex inverse problems. Experimental results on the FFHQ and ImageNet datasets demonstrate state-of-the-art posterior-sampling performance, surpassing both training-based and zero-shot alternatives.

**Comment:** This paper does not match any specific criteria but introduces a novel architectural design for diffusion models in inverse problems, which might be of general interest.
**Relevance:** 3
**Novelty:** 6

---

## 34. [Prompt-Guided Attention Head Selection for Focus-Oriented Image Retrieval](https://arxiv.org/abs/2504.01348) <a id="link34"></a>
**ArXiv ID:** 2504.01348
**Authors:** Yuji Nozawa, Yu-Chieh Lin, Kazumoto Nakamura, Youyang Ng

**Abstract:**  The goal of this paper is to enhance pretrained Vision Transformer (ViT) models for focus-oriented image retrieval with visual prompting. In real-world image retrieval scenarios, both query and database images often exhibit complexity, with multiple objects and intricate backgrounds. Users often want to retrieve images with specific object, which we define as the Focus-Oriented Image Retrieval (FOIR) task. While a standard image encoder can be employed to extract image features for similarity matching, it may not perform optimally in the multi-object-based FOIR task. This is because each image is represented by a single global feature vector. To overcome this, a prompt-based image retrieval solution is required. We propose an approach called Prompt-guided attention Head Selection (PHS) to leverage the head-wise potential of the multi-head attention mechanism in ViT in a promptable manner. PHS selects specific attention heads by matching their attention maps with user's visual prompts, such as a point, box, or segmentation. This empowers the model to focus on specific object of interest while preserving the surrounding visual context. Notably, PHS does not necessitate model re-training and avoids any image alteration. Experimental results show that PHS substantially improves performance on multiple datasets, offering a practical and training-free solution to enhance model performance in the FOIR task.

**Comment:** This paper proposes a novel method for focus-oriented image retrieval using Vision Transformers, which does not directly match any of the criteria but is tangentially related to vision foundation models.
**Relevance:** 3
**Novelty:** 6

---

## 35. [Understanding Cross-Model Perceptual Invariances Through Ensemble Metamers](https://arxiv.org/abs/2504.01739) <a id="link35"></a>
**ArXiv ID:** 2504.01739
**Authors:** Lukas Boehm, Jonas Leo Mueller, Christoffer Loeffler, Leo Schwinn, Bjoern Eskofier, Dario Zanca

**Abstract:**  Understanding the perceptual invariances of artificial neural networks is essential for improving explainability and aligning models with human vision. Metamers - stimuli that are physically distinct yet produce identical neural activations - serve as a valuable tool for investigating these invariances. We introduce a novel approach to metamer generation by leveraging ensembles of artificial neural networks, capturing shared representational subspaces across diverse architectures, including convolutional neural networks and vision transformers. To characterize the properties of the generated metamers, we employ a suite of image-based metrics that assess factors such as semantic fidelity and naturalness. Our findings show that convolutional neural networks generate more recognizable and human-like metamers, while vision transformers produce realistic but less transferable metamers, highlighting the impact of architectural biases on representational invariances.

**Comment:** Does not directly match any specific criterion but is relevant to your friend's general interest in understanding vision models and their invariances.
**Relevance:** 3
**Novelty:** 5

---

## 36. [CFMD: Dynamic Cross-layer Feature Fusion for Salient Object Detection](https://arxiv.org/abs/2504.01326) <a id="link36"></a>
**ArXiv ID:** 2504.01326
**Authors:** Jin Lian, Zhongyu Wan, Ming Gao, JunFeng Chen

**Abstract:**  Cross-layer feature pyramid networks (CFPNs) have achieved notable progress in multi-scale feature fusion and boundary detail preservation for salient object detection. However, traditional CFPNs still suffer from two core limitations: (1) a computational bottleneck caused by complex feature weighting operations, and (2) degraded boundary accuracy due to feature blurring in the upsampling process. To address these challenges, we propose CFMD, a novel cross-layer feature pyramid network that introduces two key innovations. First, we design a context-aware feature aggregation module (CFLMA), which incorporates the state-of-the-art Mamba architecture to construct a dynamic weight distribution mechanism. This module adaptively adjusts feature importance based on image context, significantly improving both representation efficiency and generalization. Second, we introduce an adaptive dynamic upsampling unit (CFLMD) that preserves spatial details during resolution recovery. By adjusting the upsampling range dynamically and initializing with a bilinear strategy, the module effectively reduces feature overlap and maintains fine-grained boundary structures. Extensive experiments on three standard benchmarks using three mainstream backbone networks demonstrate that CFMD achieves substantial improvements in pixel-level accuracy and boundary segmentation quality, especially in complex scenes. The results validate the effectiveness of CFMD in jointly enhancing computational efficiency and segmentation performance, highlighting its strong potential in salient object detection tasks.

**Comment:** This paper focuses on salient object detection with cross-layer feature fusion, which does not directly match any of the criteria but may be tangentially relevant to computer vision tasks.
**Relevance:** 3
**Novelty:** 5

---

## 37. [Direction-Aware Hybrid Representation Learning for 3D Hand Pose and Shape Estimation](https://arxiv.org/abs/2504.01298) <a id="link37"></a>
**ArXiv ID:** 2504.01298
**Authors:** Shiyong Liu, Zhihao Li, Xiao Tang, Jianzhuang Liu

**Abstract:**  Most model-based 3D hand pose and shape estimation methods directly regress the parametric model parameters from an image to obtain 3D joints under weak supervision. However, these methods involve solving a complex optimization problem with many local minima, making training difficult. To address this challenge, we propose learning direction-aware hybrid features (DaHyF) that fuse implicit image features and explicit 2D joint coordinate features. This fusion is enhanced by the pixel direction information in the camera coordinate system to estimate pose, shape, and camera viewpoint. Our method directly predicts 3D hand poses with DaHyF representation and reduces jittering during motion capture using prediction confidence based on contrastive learning. We evaluate our method on the FreiHAND dataset and show that it outperforms existing state-of-the-art methods by more than 33% in accuracy. DaHyF also achieves the top ranking on both the HO3Dv2 and HO3Dv3 leaderboards for the metric of Mean Joint Error (after scale and translation alignment). Compared to the second-best results, the largest improvement observed is 10%. We also demonstrate its effectiveness in real-time motion capture scenarios with hand position variability, occlusion, and motion blur.

**Comment:** Does not match any specific criterion but is related to 3D hand pose and shape estimation, which is tangentially relevant to spatial intelligence.
**Relevance:** 3
**Novelty:** 5

---

## 38. [Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation](https://arxiv.org/abs/2504.01668) <a id="link38"></a>
**ArXiv ID:** 2504.01668
**Authors:** Junjie Chen, Yuecong Xu, Haosheng Li, Kemi Ding

**Abstract:**  3D point cloud semantic segmentation (PCSS) is a cornerstone for environmental perception in robotic systems and autonomous driving, enabling precise scene understanding through point-wise classification. While unsupervised domain adaptation (UDA) mitigates label scarcity in PCSS, existing methods critically overlook the inherent vulnerability to real-world perturbations (e.g., snow, fog, rain) and adversarial distortions. This work first identifies two intrinsic limitations that undermine current PCSS-UDA robustness: (a) unsupervised features overlap from unaligned boundaries in shared-class regions and (b) feature structure erosion caused by domain-invariant learning that suppresses target-specific patterns. To address the proposed problems, we propose a tripartite framework consisting of: 1) a robustness evaluation model quantifying resilience against adversarial attack/corruption types through robustness metrics; 2) an invertible attention alignment module (IAAM) enabling bidirectional domain mapping while preserving discriminative structure via attention-guided overlap suppression; and 3) a contrastive memory bank with quality-aware contrastive learning that progressively refines pseudo-labels with feature quality for more discriminative representations. Extensive experiments on SynLiDAR-to-SemanticPOSS adaptation demonstrate a maximum mIoU improvement of 14.3\% under adversarial attack.

**Comment:** Does not match any specific criterion but is related to 3D semantic segmentation and domain adaptation, which is tangentially relevant to spatial intelligence.
**Relevance:** 3
**Novelty:** 5

---

## 39. [ProtoGuard-guided PROPEL: Class-Aware Prototype Enhancement and Progressive Labeling for Incremental 3D Point Cloud Segmentation](https://arxiv.org/abs/2504.01648) <a id="link39"></a>
**ArXiv ID:** 2504.01648
**Authors:** Haosheng Li, Yuecong Xu, Junjie Chen, Kemi Ding

**Abstract:**  3D point cloud semantic segmentation technology has been widely used. However, in real-world scenarios, the environment is evolving. Thus, offline-trained segmentation models may lead to catastrophic forgetting of previously seen classes. Class-incremental learning (CIL) is designed to address the problem of catastrophic forgetting. While point clouds are common, we observe high similarity and unclear boundaries between different classes. Meanwhile, they are known to be imbalanced in class distribution. These lead to issues including misclassification between similar classes and the long-tail problem, which have not been adequately addressed in previous CIL methods. We thus propose ProtoGuard and PROPEL (Progressive Refinement Of PsEudo-Labels). In the base-class training phase, ProtoGuard maintains geometric and semantic prototypes for each class, which are combined into prototype features using an attention mechanism. In the novel-class training phase, PROPEL inherits the base feature extractor and classifier, guiding pseudo-label propagation and updates based on density distribution and semantic similarity. Extensive experiments show that our approach achieves remarkable results on both the S3DIS and ScanNet datasets, improving the mIoU of 3D point cloud segmentation by a maximum of 20.39% under the 5-step CIL scenario on S3DIS.

**Comment:** Does not match any specific criterion but is related to 3D point cloud segmentation, which is tangentially relevant to spatial intelligence.
**Relevance:** 3
**Novelty:** 5

---

## 40. [Enhancing Interpretability in Generative AI Through Search-Based Data Influence Analysis](https://arxiv.org/abs/2504.01771) <a id="link40"></a>
**ArXiv ID:** 2504.01771
**Authors:** Theodoros Aivalis, Iraklis A. Klampanos, Antonis Troumpoukis, Joemon M. Jose

**Abstract:**  Generative AI models offer powerful capabilities but often lack transparency, making it difficult to interpret their output. This is critical in cases involving artistic or copyrighted content. This work introduces a search-inspired approach to improve the interpretability of these models by analysing the influence of training data on their outputs. Our method provides observational interpretability by focusing on a model's output rather than on its internal state. We consider both raw data and latent-space embeddings when searching for the influence of data items in generated content. We evaluate our method by retraining models locally and by demonstrating the method's ability to uncover influential subsets in the training data. This work lays the groundwork for future extensions, including user-based evaluations with domain experts, which is expected to improve observational interpretability further.

**Comment:** Does not match any specific criteria but is related to interpretability in generative AI, which may be of general interest.
**Relevance:** 3
**Novelty:** 5

---

## 41. [DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance](https://arxiv.org/abs/2504.01724) <a id="link41"></a>
**ArXiv ID:** 2504.01724
**Authors:** Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, Yongming Zhu

**Abstract:**  While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/.

**Comment:** Does not match any specific criteria but is related to generative modeling in multi-modal learning.
**Relevance:** 3
**Novelty:** 5

---

## 42. [LLM-mediated Dynamic Plan Generation with a Multi-Agent Approach](https://arxiv.org/abs/2504.01637) <a id="link42"></a>
**ArXiv ID:** 2504.01637
**Authors:** Reo Abe, Akifumi Ito, Kanata Takayasu, Satoshi Kurihara

**Abstract:**  Planning methods with high adaptability to dynamic environments are crucial for the development of autonomous and versatile robots. We propose a method for leveraging a large language model (GPT-4o) to automatically generate networks capable of adapting to dynamic environments. The proposed method collects environmental "status," representing conditions and goals, and uses them to generate agents. These agents are interconnected on the basis of specific conditions, resulting in networks that combine flexibility and generality. We conducted evaluation experiments to compare the networks automatically generated with the proposed method with manually constructed ones, confirming the comprehensiveness of the proposed method's networks and their higher generality. This research marks a significant advancement toward the development of versatile planning methods applicable to robotics, autonomous vehicles, smart systems, and other complex environments.

**Comment:** This paper does not match any specific criteria but discusses dynamic plan generation using large language models, which might be of general interest for embodied AI.
**Relevance:** 3
**Novelty:** 5

---

## 43. [CoMatcher: Multi-View Collaborative Feature Matching](https://arxiv.org/abs/2504.01872) <a id="link43"></a>
**ArXiv ID:** 2504.01872
**Authors:** Jintao Zhang, Zimin Xia, Mingyue Dong, Shuhan Shen, Linwei Yue, Xianwei Zheng

**Abstract:**  This paper proposes a multi-view collaborative matching strategy for reliable track construction in complex scenarios. We observe that the pairwise matching paradigms applied to image set matching often result in ambiguous estimation when the selected independent pairs exhibit significant occlusions or extreme viewpoint changes. This challenge primarily stems from the inherent uncertainty in interpreting intricate 3D structures based on limited two-view observations, as the 3D-to-2D projection leads to significant information loss. To address this, we introduce CoMatcher, a deep multi-view matcher to (i) leverage complementary context cues from different views to form a holistic 3D scene understanding and (ii) utilize cross-view projection consistency to infer a reliable global solution. Building on CoMatcher, we develop a groupwise framework that fully exploits cross-view relationships for large-scale matching tasks. Extensive experiments on various complex scenarios demonstrate the superiority of our method over the mainstream two-view matching paradigm.

**Comment:** This paper does not directly match any criteria but is related to multi-view collaborative feature matching, which might be of general interest for 3D scene understanding.
**Relevance:** 3
**Novelty:** 5

---

## 44. [Robust Unsupervised Domain Adaptation for 3D Point Cloud Segmentation Under Source Adversarial Attacks](https://arxiv.org/abs/2504.01659) <a id="link44"></a>
**ArXiv ID:** 2504.01659
**Authors:** Haosheng Li, Yuecong Xu, Junjie Chen, Kemi Ding

**Abstract:**  Unsupervised domain adaptation (UDA) frameworks have shown good generalization capabilities for 3D point cloud semantic segmentation models on clean data. However, existing works overlook adversarial robustness when the source domain itself is compromised. To comprehensively explore the robustness of the UDA frameworks, we first design a stealthy adversarial point cloud generation attack that can significantly contaminate datasets with only minor perturbations to the point cloud surface. Based on that, we propose a novel dataset, AdvSynLiDAR, comprising synthesized contaminated LiDAR point clouds. With the generated corrupted data, we further develop the Adversarial Adaptation Framework (AAF) as the countermeasure. Specifically, by extending the key point sensitive (KPS) loss towards the Robust Long-Tail loss (RLT loss) and utilizing a decoder branch, our approach enables the model to focus on long-tail classes during the pre-training phase and leverages high-confidence decoded point cloud information to restore point cloud structures during the adaptation phase. We evaluated our AAF method on the AdvSynLiDAR dataset, where the results demonstrate that our AAF method can mitigate performance degradation under source adversarial perturbations for UDA in the 3D point cloud segmentation application.

**Comment:** This paper does not match any of the specific criteria but is related to 3D point cloud segmentation and domain adaptation, which might be of general interest.
**Relevance:** 3
**Novelty:** 5

---

## 45. [Knowledge-Base based Semantic Image Transmission Using CLIP](https://arxiv.org/abs/2504.01053) <a id="link45"></a>
**ArXiv ID:** 2504.01053
**Authors:** Chongyang Li, Yanmei He, Tianqian Zhang, Mingjian He, Shouyin Liu

**Abstract:**  This paper proposes a novel knowledge-Base (KB) assisted semantic communication framework for image transmission. At the receiver, a Facebook AI Similarity Search (FAISS) based vector database is constructed by extracting semantic embeddings from images using the Contrastive Language-Image Pre-Training (CLIP) model. During transmission, the transmitter first extracts a 512-dimensional semantic feature using the CLIP model, then compresses it with a lightweight neural network for transmission. After receiving the signal, the receiver reconstructs the feature back to 512 dimensions and performs similarity matching from the KB to retrieve the most semantically similar image. Semantic transmission success is determined by category consistency between the transmitted and retrieved images, rather than traditional metrics like Peak Signal-to-Noise Ratio (PSNR). The proposed system prioritizes semantic accuracy, offering a new evaluation paradigm for semantic-aware communication systems. Experimental validation on CIFAR100 demonstrates the effectiveness of the framework in achieving semantic image transmission.

**Comment:** Does not match any specific criterion but is related to semantic image transmission using CLIP, which may appeal to general interest.
**Relevance:** 3
**Novelty:** 5

---

## 46. [Deep LG-Track: An Enhanced Localization-Confidence-Guided Multi-Object Tracker](https://arxiv.org/abs/2504.01457) <a id="link46"></a>
**ArXiv ID:** 2504.01457
**Authors:** Ting Meng, Chunyun Fu, Xiangyan Yan, Zheng Liang, Pan Ji, Jianwen Wang, Tao Huang

**Abstract:**  Multi-object tracking plays a crucial role in various applications, such as autonomous driving and security surveillance. This study introduces Deep LG-Track, a novel multi-object tracker that incorporates three key enhancements to improve the tracking accuracy and robustness. First, an adaptive Kalman filter is developed to dynamically update the covariance of measurement noise based on detection confidence and trajectory disappearance. Second, a novel cost matrix is formulated to adaptively fuse motion and appearance information, leveraging localization confidence and detection confidence as weighting factors. Third, a dynamic appearance feature updating strategy is introduced, adjusting the relative weighting of historical and current appearance features based on appearance clarity and localization accuracy. Comprehensive evaluations on the MOT17 and MOT20 datasets demonstrate that the proposed Deep LG-Track consistently outperforms state-of-the-art trackers across multiple performance metrics, highlighting its effectiveness in multi-object tracking tasks.

**Comment:** Does not match any specific criterion but is related to multi-object tracking, which is tangentially relevant to computer vision.
**Relevance:** 3
**Novelty:** 4

---

## 47. [All Patches Matter, More Patches Better: Enhance AI-Generated Image Detection via Panoptic Patch Learning](https://arxiv.org/abs/2504.01396) <a id="link47"></a>
**ArXiv ID:** 2504.01396
**Authors:** Zheng Yang, Ruoxin Chen, Zhiyuan Yan, Ke-Yue Zhang, Xinghe Fu, Shuang Wu, Xiujun Shu, Taiping Yao, Junchi Yan, Shouhong Ding, Xi Li

**Abstract:**  The exponential growth of AI-generated images (AIGIs) underscores the urgent need for robust and generalizable detection methods. In this paper, we establish two key principles for AIGI detection through systematic analysis: \textbf{(1) All Patches Matter:} Unlike conventional image classification where discriminative features concentrate on object-centric regions, each patch in AIGIs inherently contains synthetic artifacts due to the uniform generation process, suggesting that every patch serves as an important artifact source for detection. \textbf{(2) More Patches Better}: Leveraging distributed artifacts across more patches improves detection robustness by capturing complementary forensic evidence and reducing over-reliance on specific patches, thereby enhancing robustness and generalization. However, our counterfactual analysis reveals an undesirable phenomenon: naively trained detectors often exhibit a \textbf{Few-Patch Bias}, discriminating between real and synthetic images based on minority patches. We identify \textbf{Lazy Learner} as the root cause: detectors preferentially learn conspicuous artifacts in limited patches while neglecting broader artifact distributions. To address this bias, we propose the \textbf{P}anoptic \textbf{P}atch \textbf{L}earning (PPL) framework, involving: (1) Random Patch Replacement that randomly substitutes synthetic patches with real counterparts to compel models to identify artifacts in underutilized regions, encouraging the broader use of more patches; (2) Patch-wise Contrastive Learning that enforces consistent discriminative capability across all patches, ensuring uniform utilization of all patches. Extensive experiments across two different settings on several benchmarks verify the effectiveness of our approach.

**Comment:** Does not match any specific criterion but focuses on AI-generated image detection, which is tangentially relevant to computer vision.
**Relevance:** 3
**Novelty:** 4

---

## 48. [BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting Binary Segmentation in Segment Anything Models](https://arxiv.org/abs/2504.01452) <a id="link48"></a>
**ArXiv ID:** 2504.01452
**Authors:** Encheng Su, Hu Cao, Alois Knoll

**Abstract:**  Accurate segmentation of polyps and skin lesions is essential for diagnosing colorectal and skin cancers. While various segmentation methods for polyps and skin lesions using fully supervised deep learning techniques have been developed, the pixel-level annotation of medical images by doctors is both time-consuming and costly. Foundational vision models like the Segment Anything Model (SAM) have demonstrated superior performance; however, directly applying SAM to medical segmentation may not yield satisfactory results due to the lack of domain-specific medical knowledge. In this paper, we propose BiSeg-SAM, a SAM-guided weakly supervised prompting and boundary refinement network for the segmentation of polyps and skin lesions. Specifically, we fine-tune SAM combined with a CNN module to learn local features. We introduce a WeakBox with two functions: automatically generating box prompts for the SAM model and using our proposed Multi-choice Mask-to-Box (MM2B) transformation for rough mask-to-box conversion, addressing the mismatch between coarse labels and precise predictions. Additionally, we apply scale consistency (SC) loss for prediction scale alignment. Our DetailRefine module enhances boundary precision and segmentation accuracy by refining coarse predictions using a limited amount of ground truth labels. This comprehensive approach enables BiSeg-SAM to achieve excellent multi-task segmentation performance. Our method demonstrates significant superiority over state-of-the-art (SOTA) methods when tested on five polyp datasets and one skin cancer dataset.

**Comment:** Does not match any specific criterion but is related to vision foundation models and segmentation tasks.
**Relevance:** 3
**Novelty:** 4

---


---

## Paper selection prompt
 1. New methodological improvements to spatial understanding, spatial intelligence on embodied agents;
 2. Shows new VLLMs (visual large language models) or MLLMs (multi-modal large language models)
 3. Embodied AI papers on buliding new benchmark (simulator related) or new methods. These papers should focus on novel angles that previous work ignored.
 4. Vision foundation models related and its applications.

 In suggesting papers to your friend, remember that he enjoys papers on computer vision and machine learning, and generative modeling in multi-modal learning.
 Your friend also likes learning about surprising empirical or insightful results in vision-language models or embodied AI, as well as clever statistical tricks.