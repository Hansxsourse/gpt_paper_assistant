# Personalized Daily ArXiv Papers 03/10/2025
Total relevant papers: 37

Paper selection prompt and criteria at the bottom

Table of contents with paper titles:

0. [SHAPE : Self-Improved Visual Preference Alignment by Iteratively Generating Holistic Winner](#link0)
**Authors:** Kejia Chen, Jiawen Zhang, Jiacong Hu, Jiazhen Yang, Jian Lou, Zunlei Feng, Mingli Song

1. [CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data](#link1)
**Authors:** Disheng Liu, Yiran Qiao, Wuche Liu, Yiren Lu, Yunlai Zhou, Tuo Liang, Yu Yin, Jing Ma

2. [Taming Video Diffusion Prior with Scene-Grounding Guidance for 3D Gaussian Splatting from Sparse Inputs](#link2)
**Authors:** Yingji Zhong, Zhihao Li, Dave Zhenyu Chen, Lanqing Hong, Dan Xu

3. [LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning](#link3)
**Authors:** Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, Jinsong Su

4. [Combined Physics and Event Camera Simulator for Slip Detection](#link4)
**Authors:** Thilo Reinold, Suman Ghosh, Guillermo Gallego

5. [FedMABench: Benchmarking Mobile Agents on Decentralized Heterogeneous User Data](#link5)
**Authors:** Wenhao Wang, Zijie Yu, Rui Ye, Jianqing Zhang, Siheng Chen, Yanfeng Wang

6. [Manboformer: Learning Gaussian Representations via Spatial-temporal Attention Mechanism](#link6)
**Authors:** Ziyue Zhao, Qining Qi, Jianfa Ma

7. [High-Precision Transformer-Based Visual Servoing for Humanoid Robots in Aligning Tiny Objects](#link7)
**Authors:** Jialong Xue, Wei Gao, Yu Wang, Chao Ji, Dongdong Zhao, Shi Yan, Shiwu Zhang

8. [DA-STGCN: 4D Trajectory Prediction Based on Spatiotemporal Feature Extraction](#link8)
**Authors:** Yuheng Kuang, Zhengning Wang, Jianping Zhang, Zhenyu Shi, Yuding Zhang

9. [FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D Object Placement](#link9)
**Authors:** Ian Huang, Yanan Bao, Karen Truong, Howard Zhou, Cordelia Schmid, Leonidas Guibas, Alireza Fathi

10. [StickMotion: Generating 3D Human Motions by Drawing a Stickman](#link10)
**Authors:** Tao Wang, Zhihua Wu, Qiaozhi He, Jiaming Chu, Ling Qian, Yu Cheng, Junliang Xing, Jian Zhao, Lei Jin

11. [Novel Object 6D Pose Estimation with a Single Reference View](#link11)
**Authors:** Jian Liu, Wei Sun, Kai Zeng, Jin Zheng, Hui Yang, Lin Wang, Hossein Rahmani, Ajmal Mian

12. [S4M: Segment Anything with 4 Extreme Points](#link12)
**Authors:** Adrien Meyer, Lorenzo Arboit, Giuseppe Massimiani, Francesco Brucchi, Luca Emanuele Amodio, Didier Mutter, Nicolas Padoy

13. [HexPlane Representation for 3D Semantic Scene Understanding](#link13)
**Authors:** Zeren Chen, Yuenan Hou, Yulin Chen, Li Liu, Xiao Sun, Lu Sheng

14. [Spectral Informed Mamba for Robust Point Cloud Processing](#link14)
**Authors:** Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, Sahar Dastani, Milad Cheraghalikhani, David Osowiechi, Gustavo Adolfo Vargas Hakim, Farzad Beizaee, Ismail Ben Ayed, Christian Desrosiers

15. [Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent Spaces](#link15)
**Authors:** Souhail Hadgi, Luca Moschella, Andrea Santilli, Diego Gomez, Qixing Huang, Emanuele Rodol\`a, Simone Melzi, Maks Ovsjanikov

16. [Unified Reward Model for Multimodal Understanding and Generation](#link16)
**Authors:** Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, Jiaqi Wang

17. [CMMCoT: Enhancing Complex Multi-Image Comprehension via Multi-Modal Chain-of-Thought and Memory Augmentation](#link17)
**Authors:** Guanghao Zhang, Tao Zhong, Yan Xia, Zhelun Yu, Haoyuan Li, Wanggui He, Fangxun Shu, Mushui Liu, Dong She, Yi Wang, Hao Jiang

18. [Frequency Autoregressive Image Generation with Continuous Tokens](#link18)
**Authors:** Hu Yu, Hao Luo, Hangjie Yuan, Yu Rong, Feng Zhao

19. [D2GV: Deformable 2D Gaussian Splatting for Video Representation in 400FPS](#link19)
**Authors:** Mufan Liu, Qi Yang, Miaoran Zhao, He Huang, Le Yang, Zhu Li, Yiling Xu

20. [Distilling Dataset into Neural Field](#link20)
**Authors:** Donghyeok Shin, HeeSun Bae, Gyuwon Sim, Wanmo Kang, Il-Chul Moon

21. [PhysicsGen: Can Generative Models Learn from Images to Predict Complex Physical Relations?](#link21)
**Authors:** Martin Spitznagel, Jan Vaillant, Janis Keuper

22. [RecipeGen: A Benchmark for Real-World Recipe Image Generation](#link22)
**Authors:** Ruoxuan Zhang, Hongxia Xie, Yi Yao, Jian-Yu Jiang-Lin, Bin Wen, Ling Lo, Hong-Han Shuai, Yung-Hui Li, Wen-Huang Cheng

23. [Post-Hoc Concept Disentanglement: From Correlated to Isolated Concept Representations](#link23)
**Authors:** Eren Erogullari, Sebastian Lapuschkin, Wojciech Samek, Frederik Pahde

24. [R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning](#link24)
**Authors:** Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen

25. [Data-Efficient Generalization for Zero-shot Composed Image Retrieval](#link25)
**Authors:** Zining Chen, Zhicheng Zhao, Fei Su, Xiaoqin Zhang, Shijian Lu

26. [Ontology Generation using Large Language Models](#link26)
**Authors:** Anna Sofia Lippolis, Mohammad Javad Saeedizade, Robin Keskis\"arkk\"a, Sara Zuppiroli, Miguel Ceriani, Aldo Gangemi, Eva Blomqvist, Andrea Giovanni Nuzzolese

27. [Stereo Any Video: Temporally Consistent Stereo Matching](#link27)
**Authors:** Junpeng Jing, Weixun Luo, Ye Mao, Krystian Mikolajczyk

28. [Invisible Strings: Revealing Latent Dancer-to-Dancer Interactions with Graph Neural Networks](#link28)
**Authors:** Luis Vitor Zerkowski, Zixuan Wang, Ilya Vidrin, Mariel Pettee

29. [Semantic Shift Estimation via Dual-Projection and Classifier Reconstruction for Exemplar-Free Class-Incremental Learning](#link29)
**Authors:** Run He, Di Fang, Yicheng Xu, Yawen Cui, Ming Li, Cen Chen, Ziqian Zeng, Huiping Zhuang

30. [EvolvingGS: High-Fidelity Streamable Volumetric Video via Evolving 3D Gaussian Representation](#link30)
**Authors:** Chao Zhang, Yifeng Zhou, Shuheng Wang, Wenfa Li, Degang Wang, Yi Xu, Shaohui Jiao

31. [Toward an Evaluation Science for Generative AI Systems](#link31)
**Authors:** Laura Weidinger, Deb Raji, Hanna Wallach, Margaret Mitchell, Angelina Wang, Olawale Salaudeen, Rishi Bommasani, Sayash Kapoor, Deep Ganguli, Sanmi Koyejo, William Isaac

32. [Disconnect to Connect: A Data Augmentation Method for Improving Topology Accuracy in Image Segmentation](#link32)
**Authors:** Juan Miguel Valverde, Maja {\O}stergaard, Adrian Rodriguez-Palomo, Peter Alling Strange Vibe, Nina K{\o}lln Wittig, Henrik Birkedal, Anders Bjorholm Dahl

33. [EDM: Efficient Deep Feature Matching](#link33)
**Authors:** Xi Li, Tong Rao, Cihui Pan

34. [WritingBench: A Comprehensive Benchmark for Generative Writing](#link34)
**Authors:** Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, SHaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, Fei Huang

35. [HyDA: Hypernetworks for Test Time Domain Adaptation in Medical Imaging Analysis](#link35)
**Authors:** Doron Serebro, Tammy Riklin-Raviv

36. [Path Pooling: Train-Free Structure Enhancement for Efficient Knowledge Graph Retrieval-Augmented Generation](#link36)
**Authors:** Hairu Wang, Yuan Feng, Xike Xie, S Kevin Zhou

---
## 0. [SHAPE : Self-Improved Visual Preference Alignment by Iteratively Generating Holistic Winner](https://arxiv.org/abs/2503.04858) <a id="link0"></a>
**ArXiv ID:** 2503.04858
**Authors:** Kejia Chen, Jiawen Zhang, Jiacong Hu, Jiazhen Yang, Jian Lou, Zunlei Feng, Mingli Song

**Abstract:**  Large Visual Language Models (LVLMs) increasingly rely on preference alignment to ensure reliability, which steers the model behavior via preference fine-tuning on preference data structured as ``image - winner text - loser text'' triplets. However, existing approaches often suffer from limited diversity and high costs associated with human-annotated preference data, hindering LVLMs from fully achieving their intended alignment capabilities. We present \projectname, a self-supervised framework capable of transforming the already abundant supervised text-image pairs into holistic preference triplets for more effective and cheaper LVLM alignment, eliminating the need for human preference annotations. Our approach facilitates LVLMs in progressively enhancing alignment capabilities through iterative self-improvement. The key design rationale is to devise preference triplets where the winner text consistently improves in holisticness and outperforms the loser response in quality, thereby pushing the model to ``strive to the utmost'' of alignment performance through preference fine-tuning. For each given text-image pair, SHAPE introduces multiple visual augmentations and pairs them with a summarized text to serve as the winner response, while designating the original text as the loser response. Experiments across \textbf{12} benchmarks on various model architectures and sizes, including LLaVA and DeepSeek-VL, show that SHAPE achieves significant gains, for example, achieving +11.3\% on MMVet (comprehensive evaluation), +1.4\% on MMBench (general VQA), and +8.0\% on POPE (hallucination robustness) over baselines in 7B models. Notably, qualitative analyses confirm enhanced attention to visual details and better alignment with human preferences for holistic descriptions.

**Comment:** Matches criterion 2 as it proposes a self-supervised framework for improving visual large language models (LVLMs).
**Relevance:** 9
**Novelty:** 7

---

## 1. [CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data](https://arxiv.org/abs/2503.04852) <a id="link1"></a>
**ArXiv ID:** 2503.04852
**Authors:** Disheng Liu, Yiran Qiao, Wuche Liu, Yiren Lu, Yunlai Zhou, Tuo Liang, Yu Yin, Jing Ma

**Abstract:**  True intelligence hinges on the ability to uncover and leverage hidden causal relations. Despite significant progress in AI and computer vision (CV), there remains a lack of benchmarks for assessing models' abilities to infer latent causality from complex visual data. In this paper, we introduce \textsc{\textbf{Causal3D}}, a novel and comprehensive benchmark that integrates structured data (tables) with corresponding visual representations (images) to evaluate causal reasoning. Designed within a systematic framework, Causal3D comprises 19 3D-scene datasets capturing diverse causal relations, views, and backgrounds, enabling evaluations across scenes of varying complexity. We assess multiple state-of-the-art methods, including classical causal discovery, causal representation learning, and large/vision-language models (LLMs/VLMs). Our experiments show that as causal structures grow more complex without prior knowledge, performance declines significantly, highlighting the challenges even advanced methods face in complex causal scenarios. Causal3D serves as a vital resource for advancing causal reasoning in CV and fostering trustworthy AI in critical domains.

**Comment:** Matches criterion 3 as it introduces a new benchmark (CAUSAL3D) for causal learning from visual data, focusing on complex causal reasoning.
**Relevance:** 7
**Novelty:** 8

---

## 2. [Taming Video Diffusion Prior with Scene-Grounding Guidance for 3D Gaussian Splatting from Sparse Inputs](https://arxiv.org/abs/2503.05082) <a id="link2"></a>
**ArXiv ID:** 2503.05082
**Authors:** Yingji Zhong, Zhihao Li, Dave Zhenyu Chen, Lanqing Hong, Dan Xu

**Abstract:**  Despite recent successes in novel view synthesis using 3D Gaussian Splatting (3DGS), modeling scenes with sparse inputs remains a challenge. In this work, we address two critical yet overlooked issues in real-world sparse-input modeling: extrapolation and occlusion. To tackle these issues, we propose to use a reconstruction by generation pipeline that leverages learned priors from video diffusion models to provide plausible interpretations for regions outside the field of view or occluded. However, the generated sequences exhibit inconsistencies that do not fully benefit subsequent 3DGS modeling. To address the challenge of inconsistencies, we introduce a novel scene-grounding guidance based on rendered sequences from an optimized 3DGS, which tames the diffusion model to generate consistent sequences. This guidance is training-free and does not require any fine-tuning of the diffusion model. To facilitate holistic scene modeling, we also propose a trajectory initialization method. It effectively identifies regions that are outside the field of view and occluded. We further design a scheme tailored for 3DGS optimization with generated sequences. Experiments demonstrate that our method significantly improves upon the baseline and achieves state-of-the-art performance on challenging benchmarks.

**Comment:** Matches criterion 3 as it proposes a novel method for 3D Gaussian Splatting with scene-grounding guidance, addressing overlooked issues in sparse-input modeling.
**Relevance:** 7
**Novelty:** 8

---

## 3. [LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning](https://arxiv.org/abs/2503.04812) <a id="link3"></a>
**ArXiv ID:** 2503.04812
**Authors:** Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, Jinsong Su

**Abstract:**  Universal multimodal embedding models play a critical role in tasks such as interleaved image-text retrieval, multimodal RAG, and multimodal clustering. However, our empirical results indicate that existing LMM-based embedding models trained with the standard InfoNCE loss exhibit a high degree of overlap in similarity distribution between positive and negative pairs, making it challenging to distinguish hard negative pairs effectively. To deal with this issue, we propose a simple yet effective framework that dynamically improves the embedding model's representation learning for negative pairs based on their discriminative difficulty. Within this framework, we train a series of models, named LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks and 36 datasets. Experimental results show that LLaVE establishes stronger baselines that achieve state-of-the-art (SOTA) performance while demonstrating strong scalability and efficiency. Specifically, LLaVE-2B surpasses the previous SOTA 7B models, while LLaVE-7B achieves a further performance improvement of 6.2 points. Although LLaVE is trained on image-text data, it can generalize to text-video retrieval tasks in a zero-shot manner and achieve strong performance, demonstrating its remarkable potential for transfer to other embedding tasks.

**Comment:** Matches criterion 2 as it introduces a new visual large language model (LLaVE) with methodological improvements in embedding learning.
**Relevance:** 8
**Novelty:** 7

---

## 4. [Combined Physics and Event Camera Simulator for Slip Detection](https://arxiv.org/abs/2503.04838) <a id="link4"></a>
**ArXiv ID:** 2503.04838
**Authors:** Thilo Reinold, Suman Ghosh, Guillermo Gallego

**Abstract:**  Robot manipulation is a common task in fields like industrial manufacturing. Detecting when objects slip from a robot's grasp is crucial for safe and reliable operation. Event cameras, which register pixel-level brightness changes at high temporal resolution (called ``events''), offer an elegant feature when mounted on a robot's end effector: since they only detect motion relative to their viewpoint, a properly grasped object produces no events, while a slipping object immediately triggers them. To research this feature, representative datasets are essential, both for analytic approaches and for training machine learning models. The majority of current research on slip detection with event-based data is done on real-world scenarios and manual data collection, as well as additional setups for data labeling. This can result in a significant increase in the time required for data collection, a lack of flexibility in scene setups, and a high level of complexity in the repetition of experiments. This paper presents a simulation pipeline for generating slip data using the described camera-gripper configuration in a robot arm, and demonstrates its effectiveness through initial data-driven experiments. The use of a simulator, once it is set up, has the potential to reduce the time spent on data collection, provide the ability to alter the setup at any time, simplify the process of repetition and the generation of arbitrarily large data sets. Two distinct datasets were created and validated through visual inspection and artificial neural networks (ANNs). Visual inspection confirmed photorealistic frame generation and accurate slip modeling, while three ANNs trained on this data achieved high validation accuracy and demonstrated good generalization capabilities on a separate test set, along with initial applicability to real-world data. Project page: https://github.com/tub-rip/event_slip

**Comment:** Matches criterion 3 as it introduces a new simulator for slip detection in embodied AI, focusing on novel data generation and flexibility.
**Relevance:** 8
**Novelty:** 7

---

## 5. [FedMABench: Benchmarking Mobile Agents on Decentralized Heterogeneous User Data](https://arxiv.org/abs/2503.05143) <a id="link5"></a>
**ArXiv ID:** 2503.05143
**Authors:** Wenhao Wang, Zijie Yu, Rui Ye, Jianqing Zhang, Siheng Chen, Yanfeng Wang

**Abstract:**  Mobile agents have attracted tremendous research participation recently. Traditional approaches to mobile agent training rely on centralized data collection, leading to high cost and limited scalability. Distributed training utilizing federated learning offers an alternative by harnessing real-world user data, providing scalability and reducing costs. However, pivotal challenges, including the absence of standardized benchmarks, hinder progress in this field.   To tackle the challenges, we introduce FedMABench, the first benchmark for federated training and evaluation of mobile agents, specifically designed for heterogeneous scenarios. FedMABench features 6 datasets with 30+ subsets, 8 federated algorithms, 10+ base models, and over 800 apps across 5 categories, providing a comprehensive framework for evaluating mobile agents across diverse environments. Through extensive experiments, we uncover several key insights: federated algorithms consistently outperform local training; the distribution of specific apps plays a crucial role in heterogeneity; and, even apps from distinct categories can exhibit correlations during training. FedMABench is publicly available at: https://github.com/wwh0411/FedMABench with the datasets at: https://huggingface.co/datasets/wwh0411/FedMABench.

**Comment:** Matches criterion 3 as it introduces a new benchmark (FedMABench) for federated training of mobile agents, focusing on heterogeneous scenarios.
**Relevance:** 6
**Novelty:** 7

---

## 6. [Manboformer: Learning Gaussian Representations via Spatial-temporal Attention Mechanism](https://arxiv.org/abs/2503.04863) <a id="link6"></a>
**ArXiv ID:** 2503.04863
**Authors:** Ziyue Zhao, Qining Qi, Jianfa Ma

**Abstract:**  Compared with voxel-based grid prediction, in the field of 3D semantic occupation prediction for autonomous driving, GaussianFormer proposed using 3D Gaussian to describe scenes with sparse 3D semantic Gaussian based on objects is another scheme with lower memory requirements. Each 3D Gaussian function represents a flexible region of interest and its semantic features, which are iteratively refined by the attention mechanism. In the experiment, it is found that the Gaussian function required by this method is larger than the query resolution of the original dense grid network, resulting in impaired performance. Therefore, we consider optimizing GaussianFormer by using unused temporal information. We learn the Spatial-Temporal Self-attention Mechanism from the previous grid-given occupation network and improve it to GaussianFormer. The experiment was conducted with the NuScenes dataset, and the experiment is currently underway.

**Comment:** Matches criterion 1 as it improves spatial-temporal attention mechanisms for 3D semantic occupation prediction.
**Relevance:** 7
**Novelty:** 6

---

## 7. [High-Precision Transformer-Based Visual Servoing for Humanoid Robots in Aligning Tiny Objects](https://arxiv.org/abs/2503.04862) <a id="link7"></a>
**ArXiv ID:** 2503.04862
**Authors:** Jialong Xue, Wei Gao, Yu Wang, Chao Ji, Dongdong Zhao, Shi Yan, Shiwu Zhang

**Abstract:**  High-precision tiny object alignment remains a common and critical challenge for humanoid robots in real-world. To address this problem, this paper proposes a vision-based framework for precisely estimating and controlling the relative position between a handheld tool and a target object for humanoid robots, e.g., a screwdriver tip and a screw head slot. By fusing images from the head and torso cameras on a robot with its head joint angles, the proposed Transformer-based visual servoing method can correct the handheld tool's positional errors effectively, especially at a close distance. Experiments on M4-M8 screws demonstrate an average convergence error of 0.8-1.3 mm and a success rate of 93\%-100\%. Through comparative analysis, the results validate that this capability of high-precision tiny object alignment is enabled by the Distance Estimation Transformer architecture and the Multi-Perception-Head mechanism proposed in this paper.

**Comment:** Matches criterion 1 as it proposes a new Transformer-based method for spatial understanding in humanoid robots.
**Relevance:** 7
**Novelty:** 6

---

## 8. [DA-STGCN: 4D Trajectory Prediction Based on Spatiotemporal Feature Extraction](https://arxiv.org/abs/2503.04823) <a id="link8"></a>
**ArXiv ID:** 2503.04823
**Authors:** Yuheng Kuang, Zhengning Wang, Jianping Zhang, Zhenyu Shi, Yuding Zhang

**Abstract:**  The importance of four-dimensional (4D) trajectory prediction within air traffic management systems is on the rise. Key operations such as conflict detection and resolution, aircraft anomaly monitoring, and the management of congested flight paths are increasingly reliant on this foundational technology, underscoring the urgent demand for intelligent solutions. The dynamics in airport terminal zones and crowded airspaces are intricate and ever-changing; however, current methodologies do not sufficiently account for the interactions among aircraft. To tackle these challenges, we propose DA-STGCN, an innovative spatiotemporal graph convolutional network that integrates a dual attention mechanism. Our model reconstructs the adjacency matrix through a self-attention approach, enhancing the capture of node correlations, and employs graph attention to distill spatiotemporal characteristics, thereby generating a probabilistic distribution of predicted trajectories. This novel adjacency matrix, reconstructed with the self-attention mechanism, is dynamically optimized throughout the network's training process, offering a more nuanced reflection of the inter-node relationships compared to traditional algorithms. The performance of the model is validated on two ADS-B datasets, one near the airport terminal area and the other in dense airspace. Experimental results demonstrate a notable improvement over current 4D trajectory prediction methods, achieving a 20% and 30% reduction in the Average Displacement Error (ADE) and Final Displacement Error (FDE), respectively. The incorporation of a Dual-Attention module has been shown to significantly enhance the extraction of node correlations, as verified by ablation experiments.

**Comment:** Matches criterion 3 as it proposes a novel spatiotemporal graph convolutional network for trajectory prediction, focusing on previously ignored aspects like aircraft interactions.
**Relevance:** 6
**Novelty:** 7

---

## 9. [FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D Object Placement](https://arxiv.org/abs/2503.04919) <a id="link9"></a>
**ArXiv ID:** 2503.04919
**Authors:** Ian Huang, Yanan Bao, Karen Truong, Howard Zhou, Cordelia Schmid, Leonidas Guibas, Alireza Fathi

**Abstract:**  Scene generation with 3D assets presents a complex challenge, requiring both high-level semantic understanding and low-level geometric reasoning. While Multimodal Large Language Models (MLLMs) excel at semantic tasks, their application to 3D scene generation is hindered by their limited grounding on 3D geometry. In this paper, we investigate how to best work with MLLMs in an object placement task. Towards this goal, we introduce a novel framework, FirePlace, that applies existing MLLMs in (1) 3D geometric reasoning and the extraction of relevant geometric details from the 3D scene, (2) constructing and solving geometric constraints on the extracted low-level geometry, and (3) pruning for final placements that conform to common sense. By combining geometric reasoning with real-world understanding of MLLMs, our method can propose object placements that satisfy both geometric constraints as well as high-level semantic common-sense considerations. Our experiments show that these capabilities allow our method to place objects more effectively in complex scenes with intricate geometry, surpassing the quality of prior work.

**Comment:** Matches criterion 1 as it focuses on spatial intelligence and reasoning for 3D object placement using MLLMs.
**Relevance:** 6
**Novelty:** 7

---

## 10. [StickMotion: Generating 3D Human Motions by Drawing a Stickman](https://arxiv.org/abs/2503.04829) <a id="link10"></a>
**ArXiv ID:** 2503.04829
**Authors:** Tao Wang, Zhihua Wu, Qiaozhi He, Jiaming Chu, Ling Qian, Yu Cheng, Junliang Xing, Jian Zhao, Lei Jin

**Abstract:**  Text-to-motion generation, which translates textual descriptions into human motions, has been challenging in accurately capturing detailed user-imagined motions from simple text inputs. This paper introduces StickMotion, an efficient diffusion-based network designed for multi-condition scenarios, which generates desired motions based on traditional text and our proposed stickman conditions for global and local control of these motions, respectively. We address the challenges introduced by the user-friendly stickman from three perspectives: 1) Data generation. We develop an algorithm to generate hand-drawn stickmen automatically across different dataset formats. 2) Multi-condition fusion. We propose a multi-condition module that integrates into the diffusion process and obtains outputs of all possible condition combinations, reducing computational complexity and enhancing StickMotion's performance compared to conventional approaches with the self-attention module. 3) Dynamic supervision. We empower StickMotion to make minor adjustments to the stickman's position within the output sequences, generating more natural movements through our proposed dynamic supervision strategy. Through quantitative experiments and user studies, sketching stickmen saves users about 51.5% of their time generating motions consistent with their imagination. Our codes, demos, and relevant data will be released to facilitate further research and validation within the scientific community.

**Comment:** Matches criterion 4 as it introduces a novel application of vision foundation models for generating 3D human motions from stickman drawings.
**Relevance:** 5
**Novelty:** 7

---

## 11. [Novel Object 6D Pose Estimation with a Single Reference View](https://arxiv.org/abs/2503.05578) <a id="link11"></a>
**ArXiv ID:** 2503.05578
**Authors:** Jian Liu, Wei Sun, Kai Zeng, Jin Zheng, Hui Yang, Lin Wang, Hossein Rahmani, Ajmal Mian

**Abstract:**  Existing novel object 6D pose estimation methods typically rely on CAD models or dense reference views, which are both difficult to acquire. Using only a single reference view is more scalable, but challenging due to large pose discrepancies and limited geometric and spatial information. To address these issues, we propose a Single-Reference-based novel object 6D (SinRef-6D) pose estimation method. Our key idea is to iteratively establish point-wise alignment in the camera coordinate system based on state space models (SSMs). Specifically, iterative camera-space point-wise alignment can effectively handle large pose discrepancies, while our proposed RGB and Points SSMs can capture long-range dependencies and spatial information from a single view, offering linear complexity and superior spatial modeling capability. Once pre-trained on synthetic data, SinRef-6D can estimate the 6D pose of a novel object using only a single reference view, without requiring retraining or a CAD model. Extensive experiments on six popular datasets and real-world robotic scenes demonstrate that we achieve on-par performance with CAD-based and dense reference view-based methods, despite operating in the more challenging single reference setting. Code will be released at https://github.com/CNJianLiu/SinRef-6D.

**Comment:** Matches criterion 1 as it introduces a novel method for spatial understanding in 6D pose estimation using a single reference view.
**Relevance:** 5
**Novelty:** 7

---

## 12. [S4M: Segment Anything with 4 Extreme Points](https://arxiv.org/abs/2503.05534) <a id="link12"></a>
**ArXiv ID:** 2503.05534
**Authors:** Adrien Meyer, Lorenzo Arboit, Giuseppe Massimiani, Francesco Brucchi, Luca Emanuele Amodio, Didier Mutter, Nicolas Padoy

**Abstract:**  The Segment Anything Model (SAM) has revolutionized open-set interactive image segmentation, inspiring numerous adapters for the medical domain. However, SAM primarily relies on sparse prompts such as point or bounding box, which may be suboptimal for fine-grained instance segmentation, particularly in endoscopic imagery, where precise localization is critical and existing prompts struggle to capture object boundaries effectively. To address this, we introduce S4M (Segment Anything with 4 Extreme Points), which augments SAM by leveraging extreme points -- the top-, bottom-, left-, and right-most points of an instance -- prompts. These points are intuitive to identify and provide a faster, structured alternative to box prompts. However, a na\"ive use of extreme points degrades performance, due to SAM's inability to interpret their semantic roles. To resolve this, we introduce dedicated learnable embeddings, enabling the model to distinguish extreme points from generic free-form points and better reason about their spatial relationships. We further propose an auxiliary training task through the Canvas module, which operates solely on prompts -- without vision input -- to predict a coarse instance mask. This encourages the model to internalize the relationship between extreme points and mask distributions, leading to more robust segmentation. S4M outperforms other SAM-based approaches on three endoscopic surgical datasets, demonstrating its effectiveness in complex scenarios. Finally, we validate our approach through a human annotation study on surgical endoscopic videos, confirming that extreme points are faster to acquire than bounding boxes.

**Comment:** Matches criterion 4 as it extends the Segment Anything Model (SAM) for fine-grained segmentation in medical imagery.
**Relevance:** 6
**Novelty:** 5

---

## 13. [HexPlane Representation for 3D Semantic Scene Understanding](https://arxiv.org/abs/2503.05127) <a id="link13"></a>
**ArXiv ID:** 2503.05127
**Authors:** Zeren Chen, Yuenan Hou, Yulin Chen, Li Liu, Xiao Sun, Lu Sheng

**Abstract:**  In this paper, we introduce the HexPlane representation for 3D semantic scene understanding. Specifically, we first design the View Projection Module (VPM) to project the 3D point cloud into six planes to maximally retain the original spatial information. Features of six planes are extracted by the 2D encoder and sent to the HexPlane Association Module (HAM) to adaptively fuse the most informative information for each point. The fused point features are further fed to the task head to yield the ultimate predictions. Compared to the popular point and voxel representation, the HexPlane representation is efficient and can utilize highly optimized 2D operations to process sparse and unordered 3D point clouds. It can also leverage off-the-shelf 2D models, network weights, and training recipes to achieve accurate scene understanding in 3D space. On ScanNet and SemanticKITTI benchmarks, our algorithm, dubbed HexNet3D, achieves competitive performance with previous algorithms. In particular, on the ScanNet 3D segmentation task, our method obtains 77.0 mIoU on the validation set, surpassing Point Transformer V2 by 1.6 mIoU. We also observe encouraging results in indoor 3D detection tasks. Note that our method can be seamlessly integrated into existing voxel-based, point-based, and range-based approaches and brings considerable gains without bells and whistles. The codes will be available upon publication.

**Comment:** Matches criterion 4 as it introduces a new representation (HexPlane) for 3D semantic scene understanding, which is relevant to vision foundation models.
**Relevance:** 5
**Novelty:** 6

---

## 14. [Spectral Informed Mamba for Robust Point Cloud Processing](https://arxiv.org/abs/2503.04953) <a id="link14"></a>
**ArXiv ID:** 2503.04953
**Authors:** Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, Sahar Dastani, Milad Cheraghalikhani, David Osowiechi, Gustavo Adolfo Vargas Hakim, Farzad Beizaee, Ismail Ben Ayed, Christian Desrosiers

**Abstract:**  State space models have shown significant promise in Natural Language Processing (NLP) and, more recently, computer vision. This paper introduces a new methodology leveraging Mamba and Masked Autoencoder networks for point cloud data in both supervised and self-supervised learning. We propose three key contributions to enhance Mamba's capability in processing complex point cloud structures. First, we exploit the spectrum of a graph Laplacian to capture patch connectivity, defining an isometry-invariant traversal order that is robust to viewpoints and better captures shape manifolds than traditional 3D grid-based traversals. Second, we adapt segmentation via a recursive patch partitioning strategy informed by Laplacian spectral components, allowing finer integration and segment analysis. Third, we address token placement in Masked Autoencoder for Mamba by restoring tokens to their original positions, which preserves essential order and improves learning. Extensive experiments demonstrate the improvements of our approach in classification, segmentation, and few-shot tasks over state-of-the-art baselines.

**Comment:** Matches criterion 1 as it introduces new methods for spatial understanding in point cloud processing.
**Relevance:** 5
**Novelty:** 6

---

## 15. [Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent Spaces](https://arxiv.org/abs/2503.05283) <a id="link15"></a>
**ArXiv ID:** 2503.05283
**Authors:** Souhail Hadgi, Luca Moschella, Andrea Santilli, Diego Gomez, Qixing Huang, Emanuele Rodol\`a, Simone Melzi, Maks Ovsjanikov

**Abstract:**  Recent works have shown that, when trained at scale, uni-modal 2D vision and text encoders converge to learned features that share remarkable structural properties, despite arising from different representations. However, the role of 3D encoders with respect to other modalities remains unexplored. Furthermore, existing 3D foundation models that leverage large datasets are typically trained with explicit alignment objectives with respect to frozen encoders from other representations. In this work, we investigate the possibility of a posteriori alignment of representations obtained from uni-modal 3D encoders compared to text-based feature spaces. We show that naive post-training feature alignment of uni-modal text and 3D encoders results in limited performance. We then focus on extracting subspaces of the corresponding feature spaces and discover that by projecting learned representations onto well-chosen lower-dimensional subspaces the quality of alignment becomes significantly higher, leading to improved accuracy on matching and retrieval tasks. Our analysis further sheds light on the nature of these shared subspaces, which roughly separate between semantic and geometric data representations. Overall, ours is the first work that helps to establish a baseline for post-training alignment of 3D uni-modal and text feature spaces, and helps to highlight both the shared and unique properties of 3D data compared to other representations.

**Comment:** Matches criterion 4 as it explores alignment of 3D and text latent spaces, which is relevant to vision foundation models and their applications.
**Relevance:** 5
**Novelty:** 6

---

## 16. [Unified Reward Model for Multimodal Understanding and Generation](https://arxiv.org/abs/2503.05236) <a id="link16"></a>
**ArXiv ID:** 2503.05236
**Authors:** Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, Jiaqi Wang

**Abstract:**  Recent advances in human preference alignment have significantly enhanced multimodal generation and understanding. A key approach is training reward models to guide preference optimization. However, existing models are often task-specific, limiting their adaptability across diverse visual applications. We also argue that jointly learning to assess multiple tasks may foster a synergistic effect, where improved image understanding enhances image generation assessment, and refined image evaluation benefits video assessment through better frame analysis. To this end, this paper proposes UnifiedReward, the first unified reward model for multimodal understanding and generation assessment, enabling both pairwise ranking and pointwise scoring, which can be employed for vision model preference alignment. Specifically, (1) we first develop UnifiedReward on our constructed large-scale human preference dataset, including both image and video generation/understanding tasks. (2) Then, it is utilized to automatically construct high-quality preference pair data based on the vision models, fine-gradually filtering their outputs through pair ranking and point sifting. (3) Finally, these data are used for their preference alignment through Direct Preference Optimization (DPO). Experimental results demonstrate that joint learning to assess diverse visual tasks can lead to substantial mutual benefits and we apply our pipeline to both image and video understanding/generation tasks, significantly improving the performance in each domain.

**Comment:** Matches criterion 2 as it proposes a unified reward model for multimodal understanding and generation, which aligns with advancements in multi-modal large language models.
**Relevance:** 5
**Novelty:** 6

---

## 17. [CMMCoT: Enhancing Complex Multi-Image Comprehension via Multi-Modal Chain-of-Thought and Memory Augmentation](https://arxiv.org/abs/2503.05255) <a id="link17"></a>
**ArXiv ID:** 2503.05255
**Authors:** Guanghao Zhang, Tao Zhong, Yan Xia, Zhelun Yu, Haoyuan Li, Wanggui He, Fangxun Shu, Mushui Liu, Dong She, Yi Wang, Hao Jiang

**Abstract:**  While previous multimodal slow-thinking methods have demonstrated remarkable success in single-image understanding scenarios, their effectiveness becomes fundamentally constrained when extended to more complex multi-image comprehension tasks. This limitation stems from their predominant reliance on text-based intermediate reasoning processes. While for human, when engaging in sophisticated multi-image analysis, they typically perform two complementary cognitive operations: (1) continuous cross-image visual comparison through region-of-interest matching, and (2) dynamic memorization of critical visual concepts throughout the reasoning chain. Motivated by these observations, we propose the Complex Multi-Modal Chain-of-Thought (CMMCoT) framework, a multi-step reasoning framework that mimics human-like "slow thinking" for multi-image understanding. Our approach incorporates two key innovations: 1. The construction of interleaved multimodal multi-step reasoning chains, which utilize critical visual region tokens, extracted from intermediate reasoning steps, as supervisory signals. This mechanism not only facilitates comprehensive cross-modal understanding but also enhances model interpretability. 2. The introduction of a test-time memory augmentation module that expands the model reasoning capacity during inference while preserving parameter efficiency. Furthermore, to facilitate research in this direction, we have curated a novel multi-image slow-thinking dataset. Extensive experiments demonstrate the effectiveness of our model.

**Comment:** Matches criterion 2 as it proposes a multi-modal chain-of-thought framework for multi-image comprehension, which aligns with advancements in multi-modal large language models.
**Relevance:** 5
**Novelty:** 6

---

## 18. [Frequency Autoregressive Image Generation with Continuous Tokens](https://arxiv.org/abs/2503.05305) <a id="link18"></a>
**ArXiv ID:** 2503.05305
**Authors:** Hu Yu, Hao Luo, Hangjie Yuan, Yu Rong, Feng Zhao

**Abstract:**  Autoregressive (AR) models for image generation typically adopt a two-stage paradigm of vector quantization and raster-scan ``next-token prediction", inspired by its great success in language modeling. However, due to the huge modality gap, image autoregressive models may require a systematic reevaluation from two perspectives: tokenizer format and regression direction. In this paper, we introduce the frequency progressive autoregressive (\textbf{FAR}) paradigm and instantiate FAR with the continuous tokenizer. Specifically, we identify spectral dependency as the desirable regression direction for FAR, wherein higher-frequency components build upon the lower one to progressively construct a complete image. This design seamlessly fits the causality requirement for autoregressive models and preserves the unique spatial locality of image data. Besides, we delve into the integration of FAR and the continuous tokenizer, introducing a series of techniques to address optimization challenges and improve the efficiency of training and inference processes. We demonstrate the efficacy of FAR through comprehensive experiments on the ImageNet dataset and verify its potential on text-to-image generation.

**Comment:** Does not match any specific criterion but is related to generative modeling in image generation.
**Relevance:** 3
**Novelty:** 6

---

## 19. [D2GV: Deformable 2D Gaussian Splatting for Video Representation in 400FPS](https://arxiv.org/abs/2503.05600) <a id="link19"></a>
**ArXiv ID:** 2503.05600
**Authors:** Mufan Liu, Qi Yang, Miaoran Zhao, He Huang, Le Yang, Zhu Li, Yiling Xu

**Abstract:**  Implicit Neural Representations (INRs) have emerged as a powerful approach for video representation, offering versatility across tasks such as compression and inpainting. However, their implicit formulation limits both interpretability and efficacy, undermining their practicality as a comprehensive solution. We propose a novel video representation based on deformable 2D Gaussian splatting, dubbed D2GV, which aims to achieve three key objectives: 1) improved efficiency while delivering superior quality; 2) enhanced scalability and interpretability; and 3) increased friendliness for downstream tasks. Specifically, we initially divide the video sequence into fixed-length Groups of Pictures (GoP) to allow parallel training and linear scalability with video length. For each GoP, D2GV represents video frames by applying differentiable rasterization to 2D Gaussians, which are deformed from a canonical space into their corresponding timestamps. Notably, leveraging efficient CUDA-based rasterization, D2GV converges fast and decodes at speeds exceeding 400 FPS, while delivering quality that matches or surpasses state-of-the-art INRs. Moreover, we incorporate a learnable pruning and quantization strategy to streamline D2GV into a more compact representation. We demonstrate D2GV's versatility in tasks including video interpolation, inpainting and denoising, underscoring its potential as a promising solution for video representation. Code is available at: \href{https://github.com/Evan-sudo/D2GV}{https://github.com/Evan-sudo/D2GV}.

**Comment:** Does not match any specific criteria but introduces a novel video representation method, which is tangentially relevant to computer vision.
**Relevance:** 3
**Novelty:** 6

---

## 20. [Distilling Dataset into Neural Field](https://arxiv.org/abs/2503.04835) <a id="link20"></a>
**ArXiv ID:** 2503.04835
**Authors:** Donghyeok Shin, HeeSun Bae, Gyuwon Sim, Wanmo Kang, Il-Chul Moon

**Abstract:**  Utilizing a large-scale dataset is essential for training high-performance deep learning models, but it also comes with substantial computation and storage costs. To overcome these challenges, dataset distillation has emerged as a promising solution by compressing the large-scale dataset into a smaller synthetic dataset that retains the essential information needed for training. This paper proposes a novel parameterization framework for dataset distillation, coined Distilling Dataset into Neural Field (DDiF), which leverages the neural field to store the necessary information of the large-scale dataset. Due to the unique nature of the neural field, which takes coordinates as input and output quantity, DDiF effectively preserves the information and easily generates various shapes of data. We theoretically confirm that DDiF exhibits greater expressiveness than some previous literature when the utilized budget for a single synthetic instance is the same. Through extensive experiments, we demonstrate that DDiF achieves superior performance on several benchmark datasets, extending beyond the image domain to include video, audio, and 3D voxel. We release the code at https://github.com/aailab-kaist/DDiF.

**Comment:** Does not match any specific criteria. Focuses on dataset distillation using neural fields, which is not directly related to spatial understanding, VLLMs, embodied AI, or vision foundation models.
**Relevance:** 3
**Novelty:** 6

---

## 21. [PhysicsGen: Can Generative Models Learn from Images to Predict Complex Physical Relations?](https://arxiv.org/abs/2503.05333) <a id="link21"></a>
**ArXiv ID:** 2503.05333
**Authors:** Martin Spitznagel, Jan Vaillant, Janis Keuper

**Abstract:**  The image-to-image translation abilities of generative learning models have recently made significant progress in the estimation of complex (steered) mappings between image distributions. While appearance based tasks like image in-painting or style transfer have been studied at length, we propose to investigate the potential of generative models in the context of physical simulations. Providing a dataset of 300k image-pairs and baseline evaluations for three different physical simulation tasks, we propose a benchmark to investigate the following research questions: i) are generative models able to learn complex physical relations from input-output image pairs? ii) what speedups can be achieved by replacing differential equation based simulations? While baseline evaluations of different current models show the potential for high speedups (ii), these results also show strong limitations toward the physical correctness (i). This underlines the need for new methods to enforce physical correctness. Data, baseline models and evaluation code http://www.physics-gen.org.

**Comment:** Does not match any specific criterion but is tangentially related to generative modeling and physical simulations.
**Relevance:** 3
**Novelty:** 5

---

## 22. [RecipeGen: A Benchmark for Real-World Recipe Image Generation](https://arxiv.org/abs/2503.05228) <a id="link22"></a>
**ArXiv ID:** 2503.05228
**Authors:** Ruoxuan Zhang, Hongxia Xie, Yi Yao, Jian-Yu Jiang-Lin, Bin Wen, Ling Lo, Hong-Han Shuai, Yung-Hui Li, Wen-Huang Cheng

**Abstract:**  Recipe image generation is an important challenge in food computing, with applications from culinary education to interactive recipe platforms. However, there is currently no real-world dataset that comprehensively connects recipe goals, sequential steps, and corresponding images. To address this, we introduce RecipeGen, the first real-world goal-step-image benchmark for recipe generation, featuring diverse ingredients, varied recipe steps, multiple cooking styles, and a broad collection of food categories. Data is in https://github.com/zhangdaxia22/RecipeGen.

**Comment:** Does not match any specific criterion but is tangentially related to generative modeling in multi-modal learning.
**Relevance:** 3
**Novelty:** 5

---

## 23. [Post-Hoc Concept Disentanglement: From Correlated to Isolated Concept Representations](https://arxiv.org/abs/2503.05522) <a id="link23"></a>
**ArXiv ID:** 2503.05522
**Authors:** Eren Erogullari, Sebastian Lapuschkin, Wojciech Samek, Frederik Pahde

**Abstract:**  Concept Activation Vectors (CAVs) are widely used to model human-understandable concepts as directions within the latent space of neural networks. They are trained by identifying directions from the activations of concept samples to those of non-concept samples. However, this method often produces similar, non-orthogonal directions for correlated concepts, such as "beard" and "necktie" within the CelebA dataset, which frequently co-occur in images of men. This entanglement complicates the interpretation of concepts in isolation and can lead to undesired effects in CAV applications, such as activation steering. To address this issue, we introduce a post-hoc concept disentanglement method that employs a non-orthogonality loss, facilitating the identification of orthogonal concept directions while preserving directional correctness. We evaluate our approach with real-world and controlled correlated concepts in CelebA and a synthetic FunnyBirds dataset with VGG16 and ResNet18 architectures. We further demonstrate the superiority of orthogonalized concept representations in activation steering tasks, allowing (1) the insertion of isolated concepts into input images through generative models and (2) the removal of concepts for effective shortcut suppression with reduced impact on correlated concepts in comparison to baseline CAVs.

**Comment:** Does not match any specific criteria but introduces a method for disentangling concept representations, which is tangentially relevant to computer vision.
**Relevance:** 3
**Novelty:** 5

---

## 24. [R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2503.05592) <a id="link24"></a>
**ArXiv ID:** 2503.05592
**Authors:** Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen

**Abstract:**  Existing Large Reasoning Models (LRMs) have shown the potential of reinforcement learning (RL) to enhance the complex reasoning capabilities of Large Language Models~(LLMs). While they achieve remarkable performance on challenging tasks such as mathematics and coding, they often rely on their internal knowledge to solve problems, which can be inadequate for time-sensitive or knowledge-intensive questions, leading to inaccuracies and hallucinations. To address this, we propose \textbf{R1-Searcher}, a novel two-stage outcome-based RL approach designed to enhance the search capabilities of LLMs. This method allows LLMs to autonomously invoke external search systems to access additional knowledge during the reasoning process. Our framework relies exclusively on RL, without requiring process rewards or distillation for a cold start. % effectively generalizing to out-of-domain datasets and supporting both Base and Instruct models. Our experiments demonstrate that our method significantly outperforms previous strong RAG methods, even when compared to the closed-source GPT-4o-mini.

**Comment:** Does not match any specific criteria but introduces reinforcement learning for search capabilities in LLMs, which is tangentially relevant to generative modeling.
**Relevance:** 3
**Novelty:** 5

---

## 25. [Data-Efficient Generalization for Zero-shot Composed Image Retrieval](https://arxiv.org/abs/2503.05204) <a id="link25"></a>
**ArXiv ID:** 2503.05204
**Authors:** Zining Chen, Zhicheng Zhao, Fei Su, Xiaoqin Zhang, Shijian Lu

**Abstract:**  Zero-shot Composed Image Retrieval (ZS-CIR) aims to retrieve the target image based on a reference image and a text description without requiring in-distribution triplets for training. One prevalent approach follows the vision-language pretraining paradigm that employs a mapping network to transfer the image embedding to a pseudo-word token in the text embedding space. However, this approach tends to impede network generalization due to modality discrepancy and distribution shift between training and inference. To this end, we propose a Data-efficient Generalization (DeG) framework, including two novel designs, namely, Textual Supplement (TS) module and Semantic-Set (S-Set). The TS module exploits compositional textual semantics during training, enhancing the pseudo-word token with more linguistic semantics and thus mitigating the modality discrepancy effectively. The S-Set exploits the zero-shot capability of pretrained Vision-Language Models (VLMs), alleviating the distribution shift and mitigating the overfitting issue from the redundancy of the large-scale image-text data. Extensive experiments over four ZS-CIR benchmarks show that DeG outperforms the state-of-the-art (SOTA) methods with much less training data, and saves substantial training and inference time for practical usage.

**Comment:** Does not match any specific criteria but is related to zero-shot image retrieval, which is tangentially relevant to vision-language models.
**Relevance:** 3
**Novelty:** 5

---

## 26. [Ontology Generation using Large Language Models](https://arxiv.org/abs/2503.05388) <a id="link26"></a>
**ArXiv ID:** 2503.05388
**Authors:** Anna Sofia Lippolis, Mohammad Javad Saeedizade, Robin Keskis\"arkk\"a, Sara Zuppiroli, Miguel Ceriani, Aldo Gangemi, Eva Blomqvist, Andrea Giovanni Nuzzolese

**Abstract:**  The ontology engineering process is complex, time-consuming, and error-prone, even for experienced ontology engineers. In this work, we investigate the potential of Large Language Models (LLMs) to provide effective OWL ontology drafts directly from ontological requirements described using user stories and competency questions. Our main contribution is the presentation and evaluation of two new prompting techniques for automated ontology development: Memoryless CQbyCQ and Ontogenia. We also emphasize the importance of three structural criteria for ontology assessment, alongside expert qualitative evaluation, highlighting the need for a multi-dimensional evaluation in order to capture the quality and usability of the generated ontologies. Our experiments, conducted on a benchmark dataset of ten ontologies with 100 distinct CQs and 29 different user stories, compare the performance of three LLMs using the two prompting techniques. The results demonstrate improvements over the current state-of-the-art in LLM-supported ontology engineering. More specifically, the model OpenAI o1-preview with Ontogenia produces ontologies of sufficient quality to meet the requirements of ontology engineers, significantly outperforming novice ontology engineers in modelling ability. However, we still note some common mistakes and variability of result quality, which is important to take into account when using LLMs for ontology authoring support. We discuss these limitations and propose directions for future research.

**Comment:** Does not match any specific criteria but is related to ontology generation using LLMs, which is tangentially relevant to your friend's interest in generative modeling.
**Relevance:** 3
**Novelty:** 5

---

## 27. [Stereo Any Video: Temporally Consistent Stereo Matching](https://arxiv.org/abs/2503.05549) <a id="link27"></a>
**ArXiv ID:** 2503.05549
**Authors:** Junpeng Jing, Weixun Luo, Ye Mao, Krystian Mikolajczyk

**Abstract:**  This paper introduces Stereo Any Video, a powerful framework for video stereo matching. It can estimate spatially accurate and temporally consistent disparities without relying on auxiliary information such as camera poses or optical flow. The strong capability is driven by rich priors from monocular video depth models, which are integrated with convolutional features to produce stable representations. To further enhance performance, key architectural innovations are introduced: all-to-all-pairs correlation, which constructs smooth and robust matching cost volumes, and temporal convex upsampling, which improves temporal coherence. These components collectively ensure robustness, accuracy, and temporal consistency, setting a new standard in video stereo matching. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple datasets both qualitatively and quantitatively in zero-shot settings, as well as strong generalization to real-world indoor and outdoor scenarios.

**Comment:** Does not match any specific criterion but is related to stereo video processing, which is tangentially relevant to vision tasks.
**Relevance:** 3
**Novelty:** 5

---

## 28. [Invisible Strings: Revealing Latent Dancer-to-Dancer Interactions with Graph Neural Networks](https://arxiv.org/abs/2503.04816) <a id="link28"></a>
**ArXiv ID:** 2503.04816
**Authors:** Luis Vitor Zerkowski, Zixuan Wang, Ilya Vidrin, Mariel Pettee

**Abstract:**  Dancing in a duet often requires a heightened attunement to one's partner: their orientation in space, their momentum, and the forces they exert on you. Dance artists who work in partnered settings might have a strong embodied understanding in the moment of how their movements relate to their partner's, but typical documentation of dance fails to capture these varied and subtle relationships. Working closely with dance artists interested in deepening their understanding of partnering, we leverage Graph Neural Networks (GNNs) to highlight and interpret the intricate connections shared by two dancers. Using a video-to-3D-pose extraction pipeline, we extract 3D movements from curated videos of contemporary dance duets, apply a dedicated pre-processing to improve the reconstruction, and train a GNN to predict weighted connections between the dancers. By visualizing and interpreting the predicted relationships between the two movers, we demonstrate the potential for graph-based methods to construct alternate models of the collaborative dynamics of duets. Finally, we offer some example strategies for how to use these insights to inform a generative and co-creative studio practice.

**Comment:** Does not match any specific criterion but is an interesting application of graph neural networks to dance, which may be of tangential interest.
**Relevance:** 3
**Novelty:** 5

---

## 29. [Semantic Shift Estimation via Dual-Projection and Classifier Reconstruction for Exemplar-Free Class-Incremental Learning](https://arxiv.org/abs/2503.05423) <a id="link29"></a>
**ArXiv ID:** 2503.05423
**Authors:** Run He, Di Fang, Yicheng Xu, Yawen Cui, Ming Li, Cen Chen, Ziqian Zeng, Huiping Zhuang

**Abstract:**  Exemplar-Free Class-Incremental Learning (EFCIL) aims to sequentially learn from distinct categories without retaining exemplars but easily suffers from catastrophic forgetting of learned knowledge. While existing EFCIL methods leverage knowledge distillation to alleviate forgetting, they still face two critical challenges: semantic shift and decision bias. Specifically, the embeddings of old tasks shift in the embedding space after learning new tasks, and the classifier becomes biased towards new tasks due to training solely with new data, thereby hindering the balance between old and new knowledge. To address these issues, we propose the Dual-Projection Shift Estimation and Classifier Reconstruction (DPCR) approach for EFCIL. DPCR effectively estimates semantic shift through a dual-projection, which combines a learnable transformation with a row-space projection to capture both task-wise and category-wise shifts. Furthermore, to mitigate decision bias, DPCR employs ridge regression to reformulate classifier training as a reconstruction process. This reconstruction exploits previous information encoded in covariance and prototype of each class after calibration with estimated shift, thereby reducing decision bias. Extensive experiments demonstrate that, across various datasets, DPCR effectively balances old and new tasks, outperforming state-of-the-art EFCIL methods.

**Comment:** Does not match any specific criterion but is related to machine learning and incremental learning, which is a general interest area.
**Relevance:** 3
**Novelty:** 5

---

## 30. [EvolvingGS: High-Fidelity Streamable Volumetric Video via Evolving 3D Gaussian Representation](https://arxiv.org/abs/2503.05162) <a id="link30"></a>
**ArXiv ID:** 2503.05162
**Authors:** Chao Zhang, Yifeng Zhou, Shuheng Wang, Wenfa Li, Degang Wang, Yi Xu, Shaohui Jiao

**Abstract:**  We have recently seen great progress in 3D scene reconstruction through explicit point-based 3D Gaussian Splatting (3DGS), notable for its high quality and fast rendering speed. However, reconstructing dynamic scenes such as complex human performances with long durations remains challenging. Prior efforts fall short of modeling a long-term sequence with drastic motions, frequent topology changes or interactions with props, and resort to segmenting the whole sequence into groups of frames that are processed independently, which undermines temporal stability and thereby leads to an unpleasant viewing experience and inefficient storage footprint. In view of this, we introduce EvolvingGS, a two-stage strategy that first deforms the Gaussian model to coarsely align with the target frame, and then refines it with minimal point addition/subtraction, particularly in fast-changing areas. Owing to the flexibility of the incrementally evolving representation, our method outperforms existing approaches in terms of both per-frame and temporal quality metrics while maintaining fast rendering through its purely explicit representation. Moreover, by exploiting temporal coherence between successive frames, we propose a simple yet effective compression algorithm that achieves over 50x compression rate. Extensive experiments on both public benchmarks and challenging custom datasets demonstrate that our method significantly advances the state-of-the-art in dynamic scene reconstruction, particularly for extended sequences with complex human performances.

**Comment:** Does not directly match any specific criterion but is relevant to dynamic 3D scene reconstruction, which is tangentially related to embodied AI.
**Relevance:** 3
**Novelty:** 5

---

## 31. [Toward an Evaluation Science for Generative AI Systems](https://arxiv.org/abs/2503.05336) <a id="link31"></a>
**ArXiv ID:** 2503.05336
**Authors:** Laura Weidinger, Deb Raji, Hanna Wallach, Margaret Mitchell, Angelina Wang, Olawale Salaudeen, Rishi Bommasani, Sayash Kapoor, Deep Ganguli, Sanmi Koyejo, William Isaac

**Abstract:**  There is an increasing imperative to anticipate and understand the performance and safety of generative AI systems in real-world deployment contexts. However, the current evaluation ecosystem is insufficient: Commonly used static benchmarks face validity challenges, and ad hoc case-by-case audits rarely scale. In this piece, we advocate for maturing an evaluation science for generative AI systems. While generative AI creates unique challenges for system safety engineering and measurement science, the field can draw valuable insights from the development of safety evaluation practices in other fields, including transportation, aerospace, and pharmaceutical engineering. In particular, we present three key lessons: Evaluation metrics must be applicable to real-world performance, metrics must be iteratively refined, and evaluation institutions and norms must be established. Applying these insights, we outline a concrete path toward a more rigorous approach for evaluating generative AI systems.

**Comment:** Does not match any specific criteria. Focuses on evaluation science for generative AI systems, which is not directly related to spatial understanding, VLLMs, embodied AI, or vision foundation models.
**Relevance:** 3
**Novelty:** 5

---

## 32. [Disconnect to Connect: A Data Augmentation Method for Improving Topology Accuracy in Image Segmentation](https://arxiv.org/abs/2503.05541) <a id="link32"></a>
**ArXiv ID:** 2503.05541
**Authors:** Juan Miguel Valverde, Maja {\O}stergaard, Adrian Rodriguez-Palomo, Peter Alling Strange Vibe, Nina K{\o}lln Wittig, Henrik Birkedal, Anders Bjorholm Dahl

**Abstract:**  Accurate segmentation of thin, tubular structures (e.g., blood vessels) is challenging for deep neural networks. These networks classify individual pixels, and even minor misclassifications can break the thin connections within these structures. Existing methods for improving topology accuracy, such as topology loss functions, rely on very precise, topologically-accurate training labels, which are difficult to obtain. This is because annotating images, especially 3D images, is extremely laborious and time-consuming. Low image resolution and contrast further complicates the annotation by causing tubular structures to appear disconnected. We present CoLeTra, a data augmentation strategy that integrates to the models the prior knowledge that structures that appear broken are actually connected. This is achieved by creating images with the appearance of disconnected structures while maintaining the original labels. Our extensive experiments, involving different architectures, loss functions, and datasets, demonstrate that CoLeTra leads to segmentations topologically more accurate while often improving the Dice coefficient and Hausdorff distance. CoLeTra's hyper-parameters are intuitive to tune, and our sensitivity analysis shows that CoLeTra is robust to changes in these hyper-parameters. We also release a dataset specifically suited for image segmentation methods with a focus on topology accuracy. CoLetra's code can be found at https://github.com/jmlipman/CoLeTra.

**Comment:** Does not match any specific criteria. Focuses on improving topology accuracy in image segmentation, which is not directly related to spatial understanding, VLLMs, embodied AI, or vision foundation models.
**Relevance:** 3
**Novelty:** 5

---

## 33. [EDM: Efficient Deep Feature Matching](https://arxiv.org/abs/2503.05122) <a id="link33"></a>
**ArXiv ID:** 2503.05122
**Authors:** Xi Li, Tong Rao, Cihui Pan

**Abstract:**  Recent feature matching methods have achieved remarkable performance but lack efficiency consideration. In this paper, we revisit the mainstream detector-free matching pipeline and improve all its stages considering both accuracy and efficiency. We propose an Efficient Deep feature Matching network, EDM. We first adopt a deeper CNN with fewer dimensions to extract multi-level features. Then we present a Correlation Injection Module that conducts feature transformation on high-level deep features, and progressively injects feature correlations from global to local for efficient multi-scale feature aggregation, improving both speed and performance. In the refinement stage, a novel lightweight bidirectional axis-based regression head is designed to directly predict subpixel-level correspondences from latent features, avoiding the significant computational cost of explicitly locating keypoints on high-resolution local feature heatmaps. Moreover, effective selection strategies are introduced to enhance matching accuracy. Extensive experiments show that our EDM achieves competitive matching accuracy on various benchmarks and exhibits excellent efficiency, offering valuable best practices for real-world applications. The code is available at https://github.com/chicleee/EDM.

**Comment:** Does not match any specific criteria. Focuses on efficient feature matching, which is not directly related to spatial understanding, VLLMs, embodied AI, or vision foundation models.
**Relevance:** 3
**Novelty:** 5

---

## 34. [WritingBench: A Comprehensive Benchmark for Generative Writing](https://arxiv.org/abs/2503.05244) <a id="link34"></a>
**ArXiv ID:** 2503.05244
**Authors:** Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, SHaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, Fei Huang

**Abstract:**  Recent advancements in large language models (LLMs) have significantly enhanced text generation capabilities, yet evaluating their performance in generative writing remains a challenge. Existing benchmarks primarily focus on generic text generation or limited in writing tasks, failing to capture the diverse requirements of high-quality written contents across various domains. To bridge this gap, we present WritingBench, a comprehensive benchmark designed to evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing creative, persuasive, informative, and technical writing. We further propose a query-dependent evaluation framework that empowers LLMs to dynamically generate instance-specific assessment criteria. This framework is complemented by a fine-tuned critic model for criteria-aware scoring, enabling evaluations in style, format and length. The framework's validity is further demonstrated by its data curation capability, which enables 7B-parameter models to approach state-of-the-art (SOTA) performance. We open-source the benchmark, along with evaluation tools and modular framework components, to advance the development of LLMs in writing.

**Comment:** Does not match any specific criterion but is related to generative modeling in text, which is tangentially relevant to multi-modal learning.
**Relevance:** 3
**Novelty:** 4

---

## 35. [HyDA: Hypernetworks for Test Time Domain Adaptation in Medical Imaging Analysis](https://arxiv.org/abs/2503.04979) <a id="link35"></a>
**ArXiv ID:** 2503.04979
**Authors:** Doron Serebro, Tammy Riklin-Raviv

**Abstract:**  Medical imaging datasets often vary due to differences in acquisition protocols, patient demographics, and imaging devices. These variations in data distribution, known as domain shift, present a significant challenge in adapting imaging analysis models for practical healthcare applications.   Most current domain adaptation (DA) approaches aim either to align the distributions between the source and target domains or to learn an invariant feature space that generalizes well across all domains. However, both strategies require access to a sufficient number of examples, though not necessarily annotated, from the test domain during training. This limitation hinders the widespread deployment of models in clinical settings, where target domain data may only be accessible in real time.   In this work, we introduce HyDA, a novel hypernetwork framework that leverages domain characteristics rather than suppressing them, enabling dynamic adaptation at inference time. Specifically, HyDA learns implicit domain representations and uses them to adjust model parameters on-the-fly, effectively interpolating to unseen domains. We validate HyDA on two clinically relevant applications - MRI brain age prediction and chest X-ray pathology classification - demonstrating its ability to generalize across tasks and modalities. Our code is available at TBD.

**Comment:** Does not match any specific criteria. Focuses on domain adaptation in medical imaging, which is outside the scope of the specified interests.
**Relevance:** 3
**Novelty:** 4

---

## 36. [Path Pooling: Train-Free Structure Enhancement for Efficient Knowledge Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2503.05203) <a id="link36"></a>
**ArXiv ID:** 2503.05203
**Authors:** Hairu Wang, Yuan Feng, Xike Xie, S Kevin Zhou

**Abstract:**  Although Large Language Models achieve strong success in many tasks, they still suffer from hallucinations and knowledge deficiencies in real-world applications. Many knowledge graph-based retrieval-augmented generation (KG-RAG) methods enhance the quality and credibility of LLMs by leveraging structure and semantic information in KGs as external knowledge bases. However, these methods struggle to effectively incorporate structure information, either incurring high computational costs or underutilizing available knowledge. Inspired by smoothing operations in graph representation learning, we propose path pooling, a simple, train-free strategy that introduces structure information through a novel path-centric pooling operation. It seamlessly integrates into existing KG-RAG methods in a plug-and-play manner, enabling richer structure information utilization. Extensive experiments demonstrate that incorporating the path pooling into the state-of-the-art KG-RAG method consistently improves performance across various settings while introducing negligible additional cost. Code is coming soon at https://github.com/hrwang00/path-pooling.

**Comment:** Does not match any specific criteria. Focuses on knowledge graph-based retrieval-augmented generation, which is not directly related to spatial understanding, VLLMs, embodied AI, or vision foundation models.
**Relevance:** 3
**Novelty:** 4

---


---

## Paper selection prompt
 1. New methodological improvements to spatial understanding, spatial intelligence on embodied agents;
 2. Shows new VLLMs (visual large language models) or MLLMs (multi-modal large language models)
 3. Embodied AI papers on buliding new benchmark (simulator related) or new methods. These papers should focus on novel angles that previous work ignored.
 4. Vision foundation models related and its applications.

 In suggesting papers to your friend, remember that he enjoys papers on computer vision and machine learning, and generative modeling in multi-modal learning.
 Your friend also likes learning about surprising empirical or insightful results in vision-language models or embodied AI, as well as clever statistical tricks.