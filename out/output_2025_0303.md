# Personalized Daily ArXiv Papers 03/03/2025
Total relevant papers: 69

Paper selection prompt and criteria at the bottom

Table of contents with paper titles:

0. [CoTMR: Chain-of-Thought Multi-Scale Reasoning for Training-Free Zero-Shot Composed Image Retrieval](#link0)
**Authors:** Zelong Sun, Dong Jing, Zhiwu Lu

1. [MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts](#link1)
**Authors:** Peijie Wang, Zhongzhi Li, Fei Yin, Dekang Ran, Chenglin Liu

2. [Spatial Reasoning with Denoising Models](#link2)
**Authors:** Christopher Wewer, Bart Pogodzinski, Bernt Schiele, Jan Eric Lenssen

3. [WorldModelBench: Judging Video Generation Models As World Models](#link3)
**Authors:** Dacheng Li, Yunhao Fang, Yukang Chen, Shuo Yang, Shiyi Cao, Justin Wong, Michael Luo, Xiaolong Wang, Hongxu Yin, Joseph E. Gonzalez, Ion Stoica, Song Han, Yao Lu

4. [Adaptive Keyframe Sampling for Long Video Understanding](#link4)
**Authors:** Xi Tang, Jihao Qiu, Lingxi Xie, Yunjie Tian, Jianbin Jiao, Qixiang Ye

5. [Structured Preference Optimization for Vision-Language Long-Horizon Task Planning](#link5)
**Authors:** Xiwen Liang, Min Lin, Weiqi Ruan, Rongtao Xu, Yuecheng Liu, Jiaqi Chen, Bingqian Lin, Yuzheng Zhuang, Xiaodan Liang

6. [EgoNormia: Benchmarking Physical Social Norm Understanding](#link6)
**Authors:** MohammadHossein Rezaei, Yicheng Fu, Phil Cuvin, Caleb Ziems, Yanzhe Zhang, Hao Zhu, Diyi Yang

7. [Interpreting CLIP with Hierarchical Sparse Autoencoders](#link7)
**Authors:** Vladimir Zaigrajew, Hubert Baniecki, Przemyslaw Biecek

8. [ARIES: Autonomous Reasoning with LLMs on Interactive Thought Graph Environments](#link8)
**Authors:** Pedro Gimenes, Zeyu Cao, Jeffrey Wong, Yiren Zhao

9. [STPro: Spatial and Temporal Progressive Learning for Weakly Supervised Spatio-Temporal Grounding](#link9)
**Authors:** Aaryan Garg, Akash Kumar, Yogesh S Rawat

10. [DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking](#link10)
**Authors:** Zhuoqun Li, Haiyang Yu, Xuanang Chen, Hongyu Lin, Yaojie Lu, Fei Huang, Xianpei Han, Yongbin Li, Le Sun

11. [egoPPG: Heart Rate Estimation from Eye-Tracking Cameras in Egocentric Systems to Benefit Downstream Vision Tasks](#link11)
**Authors:** Bj\"orn Braun, Rayan Armani, Manuel Meier, Max Moebus, Christian Holz

12. [DiffBrush:Just Painting the Art by Your Hands](#link12)
**Authors:** Jiaming Chu, Lei Jin, Tao Wang, Junliang Xing, Jian Zhao

13. [Can We Simplify Slide-level Fine-tuning of Pathology Foundation Models?](#link13)
**Authors:** Jiawen Li, Jiali Hu, Qiehe Sun, Renao Yan, Minxi Ouyang, Tian Guan, Anjia Han, Chao He, Yonghong He

14. [MedHallTune: An Instruction-Tuning Benchmark for Mitigating Medical Hallucination in Vision-Language Models](#link14)
**Authors:** Qiao Yan, Yuchen Yuan, Xiaowei Hu, Yihan Wang, Jiaqi Xu, Jinpeng Li, Chi-Wing Fu, Pheng-Ann Heng

15. [HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models](#link15)
**Authors:** Xiao Wang, Jingyun Hua, Weihong Lin, Yuanxing Zhang, Fuzheng Zhang, Jianlong Wu, Di Zhang, Liqiang Nie

16. [Towards General Visual-Linguistic Face Forgery Detection(V2)](#link16)
**Authors:** Ke Sun, Shen Chen, Taiping Yao, Ziyin Zhou, Jiayi Ji, Xiaoshuai Sun, Chia-Wen Lin, Rongrong Ji

17. [VideoA11y: Method and Dataset for Accessible Video Description](#link17)
**Authors:** Chaoyu Li, Sid Padmanabhuni, Maryam Cheema, Hasti Seifi, Pooyan Fazli

18. [Raccoon: Multi-stage Diffusion Training with Coarse-to-Fine Curating Videos](#link18)
**Authors:** Zhiyu Tan, Junyan Wang, Hao Yang, Luozheng Qin, Hesen Chen, Qiang Zhou, Hao Li

19. [The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition](#link19)
**Authors:** Otto Brookes, Maksim Kukushkin, Majid Mirmehdi, Colleen Stephens, Paula Dieguez, Thurston C. Hicks, Sorrel Jones, Kevin Lee, Maureen S. McCarthy, Amelia Meier, Emmanuelle Normand, Erin G. Wessling, Roman M. Wittig, Kevin Langergraber, Klaus Zuberb\"uhler, Lukas Boesch, Thomas Schmid, Mimi Arandjelovic, Hjalmar K\"uhl, Tilo Burghardt

20. [FC-Attack: Jailbreaking Large Vision-Language Models via Auto-Generated Flowcharts](#link20)
**Authors:** Ziyi Zhang, Zhen Sun, Zongmin Zhang, Jihui Guo, Xinlei He

21. [Back to the Future Cyclopean Stereo: a human perception approach unifying deep and geometric constraints](#link21)
**Authors:** Sherlon Almeida da Silva, Davi Geiger, Luiz Velho, Moacir Antonelli Ponti

22. [On Benchmarking Human-Like Intelligence in Machines](#link22)
**Authors:** Lance Ying, Katherine M. Collins, Lionel Wong, Ilia Sucholutsky, Ryan Liu, Adrian Weller, Tianmin Shu, Thomas L. Griffiths, Joshua B. Tenenbaum

23. [VLEER: Vision and Language Embeddings for Explainable Whole Slide Image Representation](#link23)
**Authors:** Anh Tien Nguyen, Keunho Byeon, Kyungeun Kim, Jin Tae Kwak

24. [PathVG: A New Benchmark and Dataset for Pathology Visual Grounding](#link24)
**Authors:** Chunlin Zhong, Shuang Hao, Junhua Wu, Xiaona Chang, Jiwei Jiang, Xiu Nie, He Tang, Xiang Bai

25. [T2ICount: Enhancing Cross-modal Understanding for Zero-Shot Counting](#link25)
**Authors:** Yifei Qian, Zhongliang Guo, Bowen Deng, Chun Tong Lei, Shuai Zhao, Chun Pong Lau, Xiaopeng Hong, Michael P. Pound

26. [How far can we go with ImageNet for Text-to-Image generation?](#link26)
**Authors:** L. Degeorge, A. Ghosh, N. Dufour, D. Picard, V. Kalogeiton

27. [Fine-Grained Retrieval-Augmented Generation for Visual Question Answering](#link27)
**Authors:** Zhengxuan Zhang, Yin Wu, Yuyu Luo, Nan Tang

28. [OpenEarthSensing: Large-Scale Fine-Grained Benchmark for Open-World Remote Sensing](#link28)
**Authors:** Xiang Xiang, Zhuo Xu, Yao Deng, Qinhao Zhou, Yifan Liang, Ke Chen, Qingfang Zheng, Yaowei Wang, Xilin Chen, Wen Gao

29. [CADDreamer: CAD object Generation from Single-view Images](#link29)
**Authors:** Yuan Li, Cheng Lin, Yuan Liu, Xiaoxiao Long, Chenxu Zhang, Ningna Wang, Xin Li, Wenping Wang, Xiaohu Guo

30. [MESC-3D:Mining Effective Semantic Cues for 3D Reconstruction from a Single Image](#link30)
**Authors:** Shaoming Li, Qing Cai, Songqi Kong, Runqing Tan, Heng Tong, Shiji Qiu, Yongguo Jiang, Zhi Liu

31. [CoCa-CXR: Contrastive Captioners Learn Strong Temporal Structures for Chest X-Ray Vision-Language Understanding](#link31)
**Authors:** Yixiong Chen, Shawn Xu, Andrew Sellergren, Yossi Matias, Avinatan Hassidim, Shravya Shetty, Daniel Golden, Alan Yuille, Lin Yang

32. [FlexDrive: Toward Trajectory Flexibility in Driving Scene Reconstruction and Rendering](#link32)
**Authors:** Jingqiu Zhou, Lue Fan, Linjiang Huang, Xiaoyu Shi, Si Liu, Zhaoxiang Zhang, Hongsheng Li

33. [Training-free and Adaptive Sparse Attention for Efficient Long Video Generation](#link33)
**Authors:** Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, Bin Cui

34. [PersonaBench: Evaluating AI Models on Understanding Personal Information through Accessing (Synthetic) Private User Data](#link34)
**Authors:** Juntao Tan, Liangwei Yang, Zuxin Liu, Zhiwei Liu, Rithesh Murthy, Tulika Manoj Awalgaonkar, Jianguo Zhang, Weiran Yao, Ming Zhu, Shirley Kokane, Silvio Savarese, Huan Wang, Caiming Xiong, Shelby Heinecke

35. [LISArD: Learning Image Similarity to Defend Against Gray-box Adversarial Attacks](#link35)
**Authors:** Joana C. Costa, Tiago Roxo, Hugo Proen\c{c}a, Pedro R. M. In\'acio

36. [Towards High-performance Spiking Transformers from ANN to SNN Conversion](#link36)
**Authors:** Zihan Huang, Xinyu Shi, Zecheng Hao, Tong Bu, Jianhao Ding, Zhaofei Yu, Tiejun Huang

37. [Enhancing deep neural networks through complex-valued representations and Kuramoto synchronization dynamics](#link37)
**Authors:** Sabine Muzellec, Andrea Alamia, Thomas Serre, Rufin VanRullen

38. [A Review on Generative AI For Text-To-Image and Image-To-Image Generation and Implications To Scientific Images](#link38)
**Authors:** Zineb Sordo, Eric Chagnon, Daniela Ushizima

39. [Decoder Gradient Shield: Provable and High-Fidelity Prevention of Gradient-Based Box-Free Watermark Removal](#link39)
**Authors:** Haonan An, Guang Hua, Zhengru Fang, Guowen Xu, Susanto Rahardja, Yuguang Fang

40. [Optimizing Large Language Models for ESG Activity Detection in Financial Texts](#link40)
**Authors:** Mattia Birti, Francesco Osborne, Andrea Maurino

41. [HoloMine: A Synthetic Dataset for Buried Landmines Recognition using Microwave Holographic Imaging](#link41)
**Authors:** Emanuele Vivoli, Lorenzo Capineri, Marco Bertini

42. [BST: Badminton Stroke-type Transformer for Skeleton-based Action Recognition in Racket Sports](#link42)
**Authors:** Jing-Yuan Chang

43. [LesionLocator: Zero-Shot Universal Tumor Segmentation and Tracking in 3D Whole-Body Imaging](#link43)
**Authors:** Maximilian Rokuss, Yannick Kirchhoff, Seval Akbal, Balint Kovacs, Saikat Roy, Constantin Ulrich, Tassilo Wald, Lukas T. Rotkopf, Heinz-Peter Schlemmer, Klaus Maier-Hein

44. [An LLM-based Delphi Study to Predict GenAI Evolution](#link44)
**Authors:** Francesco Bertolotti, Luca Mari

45. [SEE: See Everything Every Time -- Adaptive Brightness Adjustment for Broad Light Range Images via Events](#link45)
**Authors:** Yunfan Lu, Xiaogang Xu, Hao Lu, Yanlin Qian, Pengteng Li, Huizai Yao, Bin Yang, Junyi Li, Qianyi Cai, Weiyu Guo, Hui Xiong

46. [Anatomically-guided masked autoencoder pre-training for aneurysm detection](#link46)
**Authors:** Alberto Mario Ceballos-Arroyo, Jisoo Kim, Chu-Hsuan Lin, Lei Qin, Geoffrey S. Young, Huaizu Jiang

47. [BadRefSR: Backdoor Attacks Against Reference-based Image Super Resolution](#link47)
**Authors:** Xue Yang, Tao Chen, Lei Guo, Wenbo Jiang, Ji Guo, Yongming Li, Jiaming He

48. [EDM: Equirectangular Projection-Oriented Dense Kernelized Feature Matching](#link48)
**Authors:** Dongki Jung, Jaehoon Choi, Yonghan Lee, Somi Jeong, Taejae Lee, Dinesh Manocha, Suyong Yeon

49. [Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on Diffusion Models](#link49)
**Authors:** Yu Pan, Bingrong Dai, Jiahao Chen, Lin Wang, Yi Du, Jiao Liu

50. [InstaFace: Identity-Preserving Facial Editing with Single Image Inference](#link50)
**Authors:** MD Wahiduzzaman Khan, Mingshan Jia, Shaolin Zhang, En Yu, Kaska Musial-Gabrys

51. [ProAI: Proactive Multi-Agent Conversational AI with Structured Knowledge Base for Psychiatric Diagnosis](#link51)
**Authors:** Yuqi Wu, Guangya Wan, Jingjing Li, Shengming Zhao, Lingfeng Ma, Tianyi Ye, Ion Pop, Yanbo Zhang, Jie Chen

52. [Contextualizing biological perturbation experiments through language](#link52)
**Authors:** Menghua Wu, Russell Littman, Jacob Levine, Lin Qiu, Tommaso Biancalani, David Richmond, Jan-Christian Huetter

53. [Large Language Model Strategic Reasoning Evaluation through Behavioral Game Theory](#link53)
**Authors:** Jingru Jia, Zehua Yuan, Junhao Pan, Paul E. McNamara, Deming Chen

54. [Unsupervised Parameter Efficient Source-free Post-pretraining](#link54)
**Authors:** Abhishek Jha, Tinne Tuytelaars, Yuki M. Asano

55. [Two-Stream Spatial-Temporal Transformer Framework for Person Identification via Natural Conversational Keypoints](#link55)
**Authors:** Masoumeh Chapariniya, Hossein Ranjbar, Teodora Vukovic, Sarah Ebling, Volker Dellwo

56. [Towards Lossless Implicit Neural Representation via Bit Plane Decomposition](#link56)
**Authors:** Woo Kyoung Han, Byeonghun Lee, Hyunmin Cho, Sunghoon Im, Kyong Hwan Jin

57. [Re-evaluating Theory of Mind evaluation in large language models](#link57)
**Authors:** Jennifer Hu, Felix Sosa, Tomer Ullman

58. [Are foundation models useful feature extractors for electroencephalography analysis?](#link58)
**Authors:** \"Ozg\"un Turgut, Felix S. Bott, Markus Ploner, Daniel Rueckert

59. [Fast and Accurate Gigapixel Pathological Image Classification with Hierarchical Distillation Multi-Instance Learning](#link59)
**Authors:** Jiuyang Dong, Junjun Jiang, Kui Jiang, Jiahan Li, Yongbing Zhang

60. [Foundation Models -- A Panacea for Artificial Intelligence in Pathology?](#link60)
**Authors:** Nita Mulliqi (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden), Anders Blilie (Department of Pathology, Stavanger University Hospital, Stavanger, Norway, Faculty of Health Sciences, University of Stavanger, Stavanger, Norway), Xiaoyi Ji (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden), Kelvin Szolnoky (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden), Henrik Olsson (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden), Sol Erika Boman (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden, Department of Molecular Medicine, Surgery, Karolinska Institutet, Stockholm, Sweden), Matteo Titus (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden), Geraldine Martinez Gonzalez (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden), Julia Anna Mielcarz (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden), Masi Valkonen (Institute of Biomedicine, University of Turku, Turku, Finland), Einar Gudlaugsson (Department of Pathology, Stavanger University Hospital, Stavanger, Norway), Svein R. Kjosavik (The General Practice, Care Coordination Research Group, Stavanger University Hospital, Norway, Department of Global Public Health, Primary Care, Faculty of Medicine, University of Bergen, Norway), Jos\'e Asenjo (Department of Pathology, Synlab, Madrid, Spain), Marcello Gambacorta (Department of Pathology, Synlab, Brescia, Italy), Paolo Libretti (Department of Pathology, Synlab, Brescia, Italy), Marcin Braun (Department of Pathology, Chair of Oncology, Medical University of Lodz, Lodz, Poland), Radzislaw Kordek (Department of Pathology, Chair of Oncology, Medical University of Lodz, Lodz, Poland), Roman {\L}owicki (1st Department of Urology, Medical University of Lodz, Lodz, Poland), Kristina Hotakainen (Department of Clinical Chemistry, Hematology, University of Helsinki, Helsinki, Finland, Laboratory Services, Mehil\"ainen Oy, Helsinki, Finland), P\"aivi V\"are (Department of Pathology, Mehil\"ainen L\"ansi-Pohja Hospital, Kemi, Finland), Bodil Ginnerup Pedersen (Department of Radiology, Aarhus University Hospital, Aarhus, Denmark, Department of Clinical Medicine, Aarhus University, Aarhus, Denmark), Karina Dalsgaard S{\o}rensen (Department of Clinical Medicine, Aarhus University, Aarhus, Denmark, Department of Molecular Medicine, Aarhus University Hospital, Aarhus, Denmark), Benedicte Parm Ulh{\o}i (Department of Pathology, Aarhus University Hospital, Aarhus, Denmark), Pekka Ruusuvuori (Institute of Biomedicine, University of Turku, Turku, Finland, InFLAMES Research Flagship, University of Turku, Turku, Finland, Faculty of Medicine, Health Technology, Tampere University, Tampere, Finland), Brett Delahunt (Malaghan Institute of Medical Research, Wellington, New Zealand, Department of Oncology, Pathology, Karolinska Institutet, Stockholm, Sweden), Hemamali Samaratunga (Aquesta Uropathology, University of Queensland, QLD, Brisbane, Australia), Toyonori Tsuzuki (Department of Surgical Pathology, School of Medicine, Aichi Medical University, Nagoya, Japan), Emilius A. M. Janssen (Department of Pathology, Stavanger University Hospital, Stavanger, Norway, Department of Chemistry, Bioscience, Environmental Engineering, University of Stavanger, Stavanger, Norway, Institute for Biomedicine, Glycomics, Griffith University, Queensland, Australia), Lars Egevad (Department of Oncology, Pathology, Karolinska Institutet, Stockholm, Sweden), Martin Eklund (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden), Kimmo Kartasalo (Department of Medical Epidemiology, Biostatistics, SciLifeLab, Karolinska Institutet, Stockholm, Sweden)

61. [Adaptive Illumination-Invariant Synergistic Feature Integration in a Stratified Granular Framework for Visible-Infrared Re-Identification](#link61)
**Authors:** Yuheng Jia, Wesley Armour

62. [Real-Time Aerial Fire Detection on Resource-Constrained Devices Using Knowledge Distillation](#link62)
**Authors:** Sabina Jangirova, Branislava Jankovic, Waseem Ullah, Latif U. Khan, Mohsen Guizani

63. [Diffusion Restoration Adapter for Real-World Image Restoration](#link63)
**Authors:** Hanbang Liang, Zhen Wang, Weihui Deng

64. [Less is More? Revisiting the Importance of Frame Rate in Real-Time Zero-Shot Surgical Video Segmentation](#link64)
**Authors:** Utku Ozbulak, Seyed Amir Mousavi, Francesca Tozzi, Nikdokht Rashidian, Wouter Willaert, Wesley De Neve, Joris Vankerschaver

65. [Distribution Prototype Diffusion Learning for Open-set Supervised Anomaly Detection](#link65)
**Authors:** Fuyun Wang, Tong Zhang, Yuanzhi Wang, Yide Qiu, Xin Liu, Xu Guo, Zhen Cui

66. [VRM: Knowledge Distillation via Virtual Relation Matching](#link66)
**Authors:** Weijia Zhang, Fei Xie, Weidong Cai, Chao Ma

67. [The Common Objects Underwater (COU) Dataset for Robust Underwater Object Detection](#link67)
**Authors:** Rishi Mukherjee, Sakshi Singh, Jack McWilliams, Junaed Sattar

68. [Towards long-term player tracking with graph hierarchies and domain-specific features](#link68)
**Authors:** Maria Koshkina, James H. Elder

---
## 0. [CoTMR: Chain-of-Thought Multi-Scale Reasoning for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2502.20826) <a id="link0"></a>
**ArXiv ID:** 2502.20826
**Authors:** Zelong Sun, Dong Jing, Zhiwu Lu

**Abstract:**  Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images by integrating information from a composed query (reference image and modification text) without training samples. Existing methods primarily combine caption models and large language models (LLMs) to generate target captions based on composed queries but face various issues such as incompatibility, visual information loss, and insufficient reasoning. In this work, we propose CoTMR, a training-free framework crafted for ZS-CIR with novel Chain-of-thought (CoT) and Multi-scale Reasoning. Instead of relying on caption models for modality transformation, CoTMR employs the Large Vision-Language Model (LVLM) to achieve unified understanding and reasoning for composed queries. To enhance the reasoning reliability, we devise CIRCoT, which guides the LVLM through a step-by-step inference process using predefined subtasks. Considering that existing approaches focus solely on global-level reasoning, our CoTMR incorporates multi-scale reasoning to achieve more comprehensive inference via fine-grained predictions about the presence or absence of key elements at the object scale. Further, we design a Multi-Grained Scoring (MGS) mechanism, which integrates CLIP similarity scores of the above reasoning outputs with candidate images to realize precise retrieval. Extensive experiments demonstrate that our CoTMR not only drastically outperforms previous methods across four prominent benchmarks but also offers appealing interpretability.

**Comment:** This paper proposes a novel framework for zero-shot composed image retrieval using large vision-language models, which matches criterion 2 on new VLLMs/MLLMs and criterion 4 on vision foundation models.
**Relevance:** 9
**Novelty:** 8

---

## 1. [MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts](https://arxiv.org/abs/2502.20808) <a id="link1"></a>
**ArXiv ID:** 2502.20808
**Authors:** Peijie Wang, Zhongzhi Li, Fei Yin, Dekang Ran, Chenglin Liu

**Abstract:**  Multimodal Large Language Models (MLLMs) have shown promising capabilities in mathematical reasoning within visual contexts across various datasets. However, most existing multimodal math benchmarks are limited to single-visual contexts, which diverges from the multi-visual scenarios commonly encountered in real-world mathematical applications. To address this gap, we introduce MV-MATH: a meticulously curated dataset of 2,009 high-quality mathematical problems. Each problem integrates multiple images interleaved with text, derived from authentic K-12 scenarios, and enriched with detailed annotations. MV-MATH includes multiple-choice, free-form, and multi-step questions, covering 11 subject areas across 3 difficulty levels, and serves as a comprehensive and rigorous benchmark for assessing MLLMs' mathematical reasoning in multi-visual contexts. Through extensive experimentation, we observe that MLLMs encounter substantial challenges in multi-visual math tasks, with a considerable performance gap relative to human capabilities on MV-MATH. Furthermore, we analyze the performance and error patterns of various models, providing insights into MLLMs' mathematical reasoning capabilities within multi-visual settings.

**Comment:** This paper introduces a new benchmark for multimodal math reasoning in multi-visual contexts, which matches criterion 3 on building new benchmarks for embodied AI or multimodal tasks.
**Relevance:** 7
**Novelty:** 7

---

## 2. [Spatial Reasoning with Denoising Models](https://arxiv.org/abs/2502.21075) <a id="link2"></a>
**ArXiv ID:** 2502.21075
**Authors:** Christopher Wewer, Bart Pogodzinski, Bernt Schiele, Jan Eric Lenssen

**Abstract:**  We introduce Spatial Reasoning Models (SRMs), a framework to perform reasoning over sets of continuous variables via denoising generative models. SRMs infer continuous representations on a set of unobserved variables, given observations on observed variables. Current generative models on spatial domains, such as diffusion and flow matching models, often collapse to hallucination in case of complex distributions. To measure this, we introduce a set of benchmark tasks that test the quality of complex reasoning in generative models and can quantify hallucination. The SRM framework allows to report key findings about importance of sequentialization in generation, the associated order, as well as the sampling strategies during training. It demonstrates, for the first time, that order of generation can successfully be predicted by the denoising network itself. Using these findings, we can increase the accuracy of specific reasoning tasks from 50%.

**Comment:** Matches criterion 1. Proposes a framework for spatial reasoning using denoising generative models, which aligns with spatial understanding improvements.
**Relevance:** 8
**Novelty:** 6

---

## 3. [WorldModelBench: Judging Video Generation Models As World Models](https://arxiv.org/abs/2502.20694) <a id="link3"></a>
**ArXiv ID:** 2502.20694
**Authors:** Dacheng Li, Yunhao Fang, Yukang Chen, Shuo Yang, Shiyi Cao, Justin Wong, Michael Luo, Xiaolong Wang, Hongxu Yin, Joseph E. Gonzalez, Ion Stoica, Song Han, Yao Lu

**Abstract:**  Video generation models have rapidly progressed, positioning themselves as video world models capable of supporting decision-making applications like robotics and autonomous driving. However, current benchmarks fail to rigorously evaluate these claims, focusing only on general video quality, ignoring important factors to world models such as physics adherence. To bridge this gap, we propose WorldModelBench, a benchmark designed to evaluate the world modeling capabilities of video generation models in application-driven domains. WorldModelBench offers two key advantages: (1) Against to nuanced world modeling violations: By incorporating instruction-following and physics-adherence dimensions, WorldModelBench detects subtle violations, such as irregular changes in object size that breach the mass conservation law - issues overlooked by prior benchmarks. (2) Aligned with large-scale human preferences: We crowd-source 67K human labels to accurately measure 14 frontier models. Using our high-quality human labels, we further fine-tune an accurate judger to automate the evaluation procedure, achieving 8.6% higher average accuracy in predicting world modeling violations than GPT-4o with 2B parameters. In addition, we demonstrate that training to align human annotations by maximizing the rewards from the judger noticeably improve the world modeling capability. The website is available at https://worldmodelbench-team.github.io.

**Comment:** Matches criterion 3. Introduces a new benchmark for evaluating video generation models as world models, focusing on physics adherence and nuanced violations.
**Relevance:** 7
**Novelty:** 7

---

## 4. [Adaptive Keyframe Sampling for Long Video Understanding](https://arxiv.org/abs/2502.21271) <a id="link4"></a>
**ArXiv ID:** 2502.21271
**Authors:** Xi Tang, Jihao Qiu, Lingxi Xie, Yunjie Tian, Jianbin Jiao, Qixiang Ye

**Abstract:**  Multimodal large language models (MLLMs) have enabled open-world visual understanding by injecting visual input as extra tokens into large language models (LLMs) as contexts. However, when the visual input changes from a single image to a long video, the above paradigm encounters difficulty because the vast amount of video tokens has significantly exceeded the maximal capacity of MLLMs. Therefore, existing video-based MLLMs are mostly established upon sampling a small portion of tokens from input data, which can cause key information to be lost and thus produce incorrect answers. This paper presents a simple yet effective algorithm named Adaptive Keyframe Sampling (AKS). It inserts a plug-and-play module known as keyframe selection, which aims to maximize the useful information with a fixed number of video tokens. We formulate keyframe selection as an optimization involving (1) the relevance between the keyframes and the prompt, and (2) the coverage of the keyframes over the video, and present an adaptive algorithm to approximate the best solution. Experiments on two long video understanding benchmarks validate that Adaptive Keyframe Sampling improves video QA accuracy (beyond strong baselines) upon selecting informative keyframes. Our study reveals the importance of information pre-filtering in video-based MLLMs. Code is available at https://github.com/ncTimTang/AKS.

**Comment:** Matches criterion 2. Proposes a method for adaptive keyframe sampling in multimodal large language models for long video understanding.
**Relevance:** 7
**Novelty:** 6

---

## 5. [Structured Preference Optimization for Vision-Language Long-Horizon Task Planning](https://arxiv.org/abs/2502.20742) <a id="link5"></a>
**ArXiv ID:** 2502.20742
**Authors:** Xiwen Liang, Min Lin, Weiqi Ruan, Rongtao Xu, Yuecheng Liu, Jiaqi Chen, Bingqian Lin, Yuzheng Zhuang, Xiaodan Liang

**Abstract:**  Existing methods for vision-language task planning excel in short-horizon tasks but often fall short in complex, long-horizon planning within dynamic environments. These challenges primarily arise from the difficulty of effectively training models to produce high-quality reasoning processes for long-horizon tasks. To address this, we propose Structured Preference Optimization (SPO), which aims to enhance reasoning and action selection in long-horizon task planning through structured preference evaluation and optimized training strategies. Specifically, SPO introduces: 1) Preference-Based Scoring and Optimization, which systematically evaluates reasoning chains based on task relevance, visual grounding, and historical consistency; and 2) Curriculum-Guided Training, where the model progressively adapts from simple to complex tasks, improving its generalization ability in long-horizon scenarios and enhancing reasoning robustness. To advance research in vision-language long-horizon task planning, we introduce ExtendaBench, a comprehensive benchmark covering 1,509 tasks across VirtualHome and Habitat 2.0, categorized into ultra-short, short, medium, and long tasks. Experimental results demonstrate that SPO significantly improves reasoning quality and final decision accuracy, outperforming prior methods on long-horizon tasks and underscoring the effectiveness of preference-driven optimization in vision-language task planning. Specifically, SPO achieves a +5.98% GCR and +4.68% SR improvement in VirtualHome and a +3.30% GCR and +2.11% SR improvement in Habitat over the best-performing baselines.

**Comment:** Matches criterion 3 as it introduces a new benchmark (ExtendaBench) and method (SPO) for long-horizon task planning in vision-language models.
**Relevance:** 5
**Novelty:** 7

---

## 6. [EgoNormia: Benchmarking Physical Social Norm Understanding](https://arxiv.org/abs/2502.20490) <a id="link6"></a>
**ArXiv ID:** 2502.20490
**Authors:** MohammadHossein Rezaei, Yicheng Fu, Phil Cuvin, Caleb Ziems, Yanzhe Zhang, Hao Zhu, Diyi Yang

**Abstract:**  Human activity is moderated by norms. When performing actions in the real world, humans not only follow norms, but also consider the trade-off between different norms However, machines are often trained without explicit supervision on norm understanding and reasoning, especially when the norms are grounded in a physical and social context. To improve and evaluate the normative reasoning capability of vision-language models (VLMs), we present EgoNormia $\|\epsilon\|$, consisting of 1,853 ego-centric videos of human interactions, each of which has two related questions evaluating both the prediction and justification of normative actions. The normative actions encompass seven categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. To compile this dataset at scale, we propose a novel pipeline leveraging video sampling, automatic answer generation, filtering, and human validation. Our work demonstrates that current state-of-the-art vision-language models lack robust norm understanding, scoring a maximum of 45% on EgoNormia (versus a human bench of 92%). Our analysis of performance in each dimension highlights the significant risks of safety, privacy, and the lack of collaboration and communication capability when applied to real-world agents. We additionally show that through a retrieval-based generation method, it is possible to use EgoNomia to enhance normative reasoning in VLMs.

**Comment:** Matches criterion 3 as it introduces a new benchmark (EgoNormia) for evaluating normative reasoning in vision-language models.
**Relevance:** 5
**Novelty:** 7

---

## 7. [Interpreting CLIP with Hierarchical Sparse Autoencoders](https://arxiv.org/abs/2502.20578) <a id="link7"></a>
**ArXiv ID:** 2502.20578
**Authors:** Vladimir Zaigrajew, Hubert Baniecki, Przemyslaw Biecek

**Abstract:**  Sparse autoencoders (SAEs) are useful for detecting and steering interpretable features in neural networks, with particular potential for understanding complex multimodal representations. Given their ability to uncover interpretable features, SAEs are particularly valuable for analyzing large-scale vision-language models (e.g., CLIP and SigLIP), which are fundamental building blocks in modern systems yet remain challenging to interpret and control. However, current SAE methods are limited by optimizing both reconstruction quality and sparsity simultaneously, as they rely on either activation suppression or rigid sparsity constraints. To this end, we introduce Matryoshka SAE (MSAE), a new architecture that learns hierarchical representations at multiple granularities simultaneously, enabling a direct optimization of both metrics without compromise. MSAE establishes a new state-of-the-art Pareto frontier between reconstruction quality and sparsity for CLIP, achieving 0.99 cosine similarity and less than 0.1 fraction of variance unexplained while maintaining ~80% sparsity. Finally, we demonstrate the utility of MSAE as a tool for interpreting and controlling CLIP by extracting over 120 semantic concepts from its representation to perform concept-based similarity search and bias analysis in downstream tasks like CelebA.

**Comment:** This paper introduces Matryoshka SAE for interpreting and controlling CLIP, which aligns with criterion 4 as it focuses on vision-language foundation models and their interpretability.
**Relevance:** 5
**Novelty:** 7

---

## 8. [ARIES: Autonomous Reasoning with LLMs on Interactive Thought Graph Environments](https://arxiv.org/abs/2502.21208) <a id="link8"></a>
**ArXiv ID:** 2502.21208
**Authors:** Pedro Gimenes, Zeyu Cao, Jeffrey Wong, Yiren Zhao

**Abstract:**  Recent research has shown that LLM performance on reasoning tasks can be enhanced by scaling test-time compute. One promising approach, particularly with decomposable problems, involves arranging intermediate solutions as a graph on which transformations are performed to explore the solution space. However, prior works rely on pre-determined, task-specific transformation schedules which are subject to a set of searched hyperparameters. In this work, we view thought graph transformations as actions in a Markov decision process, and implement policy agents to drive effective action policies for the underlying reasoning LLM agent. In particular, we investigate the ability for another LLM to act as a policy agent on thought graph environments and introduce ARIES, a multi-agent architecture for reasoning with LLMs. In ARIES, reasoning LLM agents solve decomposed subproblems, while policy LLM agents maintain visibility of the thought graph states, and dynamically adapt the problem-solving strategy. Through extensive experiments, we observe that using off-the-shelf LLMs as policy agents with no supervised fine-tuning (SFT) can yield up to $29\%$ higher accuracy on HumanEval relative to static transformation schedules, as well as reducing inference costs by $35\%$ and avoid any search requirements. We also conduct a thorough analysis of observed failure modes, highlighting that limitations on LLM sizes and the depth of problem decomposition can be seen as challenges to scaling LLM-guided reasoning.

**Comment:** This paper introduces ARIES, a multi-agent architecture for reasoning with LLMs, which aligns with criterion 2 as it explores novel reasoning methods for LLMs.
**Relevance:** 5
**Novelty:** 7

---

## 9. [STPro: Spatial and Temporal Progressive Learning for Weakly Supervised Spatio-Temporal Grounding](https://arxiv.org/abs/2502.20678) <a id="link9"></a>
**ArXiv ID:** 2502.20678
**Authors:** Aaryan Garg, Akash Kumar, Yogesh S Rawat

**Abstract:**  In this work we study Weakly Supervised Spatio-Temporal Video Grounding (WSTVG), a challenging task of localizing subjects spatio-temporally in videos using only textual queries and no bounding box supervision. Inspired by recent advances in vision-language foundation models, we investigate their utility for WSTVG, leveraging their zero-shot grounding capabilities. However, we find that a simple adaptation lacks essential spatio-temporal grounding abilities. To bridge this gap, we introduce Tubelet Referral Grounding (TRG), which connects textual queries to tubelets to enable spatio-temporal predictions. Despite its promise, TRG struggles with compositional action understanding and dense scene scenarios. To address these limitations, we propose STPro, a novel progressive learning framework with two key modules: (1) Sub-Action Temporal Curriculum Learning (SA-TCL), which incrementally builds compositional action understanding, and (2) Congestion-Guided Spatial Curriculum Learning (CG-SCL), which adapts the model to complex scenes by spatially increasing task difficulty. STPro achieves state-of-the-art results on three benchmark datasets, with improvements of 1.0% on VidSTG-Declarative and 3.0% on HCSTVG-v1.

**Comment:** Matches criterion 3 as it proposes a novel method (STPro) for weakly supervised spatio-temporal grounding, addressing limitations in compositional action understanding and dense scenes.
**Relevance:** 5
**Novelty:** 7

---

## 10. [DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking](https://arxiv.org/abs/2502.20730) <a id="link10"></a>
**ArXiv ID:** 2502.20730
**Authors:** Zhuoqun Li, Haiyang Yu, Xuanang Chen, Hongyu Lin, Yaojie Lu, Fei Huang, Xianpei Han, Yongbin Li, Le Sun

**Abstract:**  Designing solutions for complex engineering challenges is crucial in human production activities. However, previous research in the retrieval-augmented generation (RAG) field has not sufficiently addressed tasks related to the design of complex engineering solutions. To fill this gap, we introduce a new benchmark, SolutionBench, to evaluate a system's ability to generate complete and feasible solutions for engineering problems with multiple complex constraints. To further advance the design of complex engineering solutions, we propose a novel system, SolutionRAG, that leverages the tree-based exploration and bi-point thinking mechanism to generate reliable solutions. Extensive experimental results demonstrate that SolutionRAG achieves state-of-the-art (SOTA) performance on the SolutionBench, highlighting its potential to enhance the automation and reliability of complex engineering solution design in real-world applications.

**Comment:** Matches criterion 3 as it introduces a new benchmark and method for complex engineering solution design, focusing on novel angles.
**Relevance:** 5
**Novelty:** 7

---

## 11. [egoPPG: Heart Rate Estimation from Eye-Tracking Cameras in Egocentric Systems to Benefit Downstream Vision Tasks](https://arxiv.org/abs/2502.20879) <a id="link11"></a>
**ArXiv ID:** 2502.20879
**Authors:** Bj\"orn Braun, Rayan Armani, Manuel Meier, Max Moebus, Christian Holz

**Abstract:**  Egocentric vision systems aim to understand the spatial surroundings and the wearer's behavior inside it, including motions, activities, and interaction with objects. Since a person's attention and situational responses are influenced by their physiological state, egocentric systems must also detect this state for better context awareness. In this paper, we propose egoPPG, a novel task for egocentric vision systems to extract a person's heart rate (HR) as a key indicator of the wearer's physiological state from the system's built-in sensors (e.g., eye tracking videos). We then propose EgoPulseFormer, a method that solely takes eye-tracking video as input to estimate a person's photoplethysmogram (PPG) from areas around the eyes to track HR values-without requiring additional or dedicated hardware. We demonstrate the downstream benefit of EgoPulseFormer on EgoExo4D, where we find that augmenting existing models with tracked HR values improves proficiency estimation by 14%. To train and validate EgoPulseFormer, we collected a dataset of 13+ hours of eye-tracking videos from Project Aria and contact-based blood volume pulse signals as well as an electrocardiogram (ECG) for ground-truth HR values. 25 participants performed diverse everyday activities such as office work, cooking, dancing, and exercising, which induced significant natural motion and HR variation (44-164 bpm). Our model robustly estimates HR (MAE=8.82 bpm) and captures patterns (r=0.81). Our results show how egocentric systems may unify environmental and physiological tracking to better understand user actions and internal states.

**Comment:** Matches criterion 1 as it proposes a novel method for spatial understanding in egocentric systems by integrating physiological state tracking.
**Relevance:** 5
**Novelty:** 7

---

## 12. [DiffBrush:Just Painting the Art by Your Hands](https://arxiv.org/abs/2502.20904) <a id="link12"></a>
**ArXiv ID:** 2502.20904
**Authors:** Jiaming Chu, Lei Jin, Tao Wang, Junliang Xing, Jian Zhao

**Abstract:**  The rapid development of image generation and editing algorithms in recent years has enabled ordinary user to produce realistic images. However, the current AI painting ecosystem predominantly relies on text-driven diffusion models (T2I), which pose challenges in accurately capturing user requirements. Furthermore, achieving compatibility with other modalities incurs substantial training costs. To this end, we introduce DiffBrush, which is compatible with T2I models and allows users to draw and edit images. By manipulating and adapting the internal representation of the diffusion model, DiffBrush guides the model-generated images to converge towards the user's hand-drawn sketches for user's specific needs without additional training. DiffBrush achieves control over the color, semantic, and instance of objects in images by continuously guiding the latent and instance-level attention map during the denoising process of the diffusion model. Besides, we propose a latent regeneration, which refines the randomly sampled noise in the diffusion model, obtaining a better image generation layout. Finally, users only need to roughly draw the mask of the instance (acceptable colors) on the canvas, DiffBrush can naturally generate the corresponding instance at the corresponding location.

**Comment:** Matches criterion 4. Introduces a method for user-guided image generation compatible with text-to-image diffusion models.
**Relevance:** 6
**Novelty:** 6

---

## 13. [Can We Simplify Slide-level Fine-tuning of Pathology Foundation Models?](https://arxiv.org/abs/2502.20823) <a id="link13"></a>
**ArXiv ID:** 2502.20823
**Authors:** Jiawen Li, Jiali Hu, Qiehe Sun, Renao Yan, Minxi Ouyang, Tian Guan, Anjia Han, Chao He, Yonghong He

**Abstract:**  The emergence of foundation models in computational pathology has transformed histopathological image analysis, with whole slide imaging (WSI) diagnosis being a core application. Traditionally, weakly supervised fine-tuning via multiple instance learning (MIL) has been the primary method for adapting foundation models to WSIs. However, in this work we present a key experimental finding: a simple nonlinear mapping strategy combining mean pooling and a multilayer perceptron, called SiMLP, can effectively adapt patch-level foundation models to slide-level tasks without complex MIL-based learning. Through extensive experiments across diverse downstream tasks, we demonstrate the superior performance of SiMLP with state-of-the-art methods. For instance, on a large-scale pan-cancer classification task, SiMLP surpasses popular MIL-based methods by 3.52%. Furthermore, SiMLP shows strong learning ability in few-shot classification and remaining highly competitive with slide-level foundation models pretrained on tens of thousands of slides. Finally, SiMLP exhibits remarkable robustness and transferability in lung cancer subtyping. Overall, our findings challenge the conventional MIL-based fine-tuning paradigm, demonstrating that a task-agnostic representation strategy alone can effectively adapt foundation models to WSI analysis. These insights offer a unique and meaningful perspective for future research in digital pathology, paving the way for more efficient and broadly applicable methodologies.

**Comment:** This paper focuses on simplifying fine-tuning for pathology foundation models, which aligns with criterion 4 on vision foundation models and their applications.
**Relevance:** 5
**Novelty:** 6

---

## 14. [MedHallTune: An Instruction-Tuning Benchmark for Mitigating Medical Hallucination in Vision-Language Models](https://arxiv.org/abs/2502.20780) <a id="link14"></a>
**ArXiv ID:** 2502.20780
**Authors:** Qiao Yan, Yuchen Yuan, Xiaowei Hu, Yihan Wang, Jiaqi Xu, Jinpeng Li, Chi-Wing Fu, Pheng-Ann Heng

**Abstract:**  The increasing use of vision-language models (VLMs) in healthcare applications presents great challenges related to hallucinations, in which the models may generate seemingly plausible results that are in fact incorrect. Such hallucinations can jeopardize clinical decision making, potentially harming the diagnosis and treatments. In this work, we propose MedHallTune, a large-scale benchmark designed specifically to evaluate and mitigate hallucinations in medical VLMs. Comprising over 100,000 images and 1,000,000 instruction pairs, MedHallTune includes both hallucination and non-hallucination samples, each with ground-truth annotations. We conduct a comprehensive evaluation of current medical and general VLMs using MedHallTune, assessing their performance across key metrics, including clinical accuracy, relevance, detail level, and risk level. The experimental results show that fine-tuning with MedHallTune successfully improves the ability of several existing models to manage hallucinations and boost their zero-shot performance on downstream visual-question-answering (VQA) tasks, making them more reliable for practical medical applications. Our work contributes to the development of more trustworthy VLMs. Codes and dataset will be available at \href{https://github.com/russellyq/MedHallTune}{MedHallTune}.

**Comment:** Matches criterion 2 as it focuses on mitigating hallucinations in medical vision-language models (VLMs).
**Relevance:** 5
**Novelty:** 6

---

## 15. [HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models](https://arxiv.org/abs/2502.20811) <a id="link15"></a>
**ArXiv ID:** 2502.20811
**Authors:** Xiao Wang, Jingyun Hua, Weihong Lin, Yuanxing Zhang, Fuzheng Zhang, Jianlong Wu, Di Zhang, Liqiang Nie

**Abstract:**  Recent Multi-modal Large Language Models (MLLMs) have made great progress in video understanding. However, their performance on videos involving human actions is still limited by the lack of high-quality data. To address this, we introduce a two-stage data annotation pipeline. First, we design strategies to accumulate videos featuring clear human actions from the Internet. Second, videos are annotated in a standardized caption format that uses human attributes to distinguish individuals and chronologically details their actions and interactions. Through this pipeline, we curate two datasets, namely HAICTrain and HAICBench. \textbf{HAICTrain} comprises 126K video-caption pairs generated by Gemini-Pro and verified for training purposes. Meanwhile, \textbf{HAICBench} includes 500 manually annotated video-caption pairs and 1,400 QA pairs, for a comprehensive evaluation of human action understanding. Experimental results demonstrate that training with HAICTrain not only significantly enhances human understanding abilities across 4 benchmarks, but can also improve text-to-video generation results. Both the HAICTrain and HAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.

**Comment:** Matches criterion 2 as it discusses improving MLLMs for human action understanding and generation.
**Relevance:** 5
**Novelty:** 6

---

## 16. [Towards General Visual-Linguistic Face Forgery Detection(V2)](https://arxiv.org/abs/2502.20698) <a id="link16"></a>
**ArXiv ID:** 2502.20698
**Authors:** Ke Sun, Shen Chen, Taiping Yao, Ziyin Zhou, Jiayi Ji, Xiaoshuai Sun, Chia-Wen Lin, Rongrong Ji

**Abstract:**  Face manipulation techniques have achieved significant advances, presenting serious challenges to security and social trust. Recent works demonstrate that leveraging multimodal models can enhance the generalization and interpretability of face forgery detection. However, existing annotation approaches, whether through human labeling or direct Multimodal Large Language Model (MLLM) generation, often suffer from hallucination issues, leading to inaccurate text descriptions, especially for high-quality forgeries. To address this, we propose Face Forgery Text Generator (FFTG), a novel annotation pipeline that generates accurate text descriptions by leveraging forgery masks for initial region and type identification, followed by a comprehensive prompting strategy to guide MLLMs in reducing hallucination. We validate our approach through fine-tuning both CLIP with a three-branch training framework combining unimodal and multimodal objectives, and MLLMs with our structured annotations. Experimental results demonstrate that our method not only achieves more accurate annotations with higher region identification accuracy, but also leads to improvements in model performance across various forgery detection benchmarks. Our Codes are available in https://github.com/skJack/VLFFD.git.

**Comment:** Matches criterion 2 as it focuses on improving multimodal large language models (MLLMs) for face forgery detection.
**Relevance:** 5
**Novelty:** 6

---

## 17. [VideoA11y: Method and Dataset for Accessible Video Description](https://arxiv.org/abs/2502.20480) <a id="link17"></a>
**ArXiv ID:** 2502.20480
**Authors:** Chaoyu Li, Sid Padmanabhuni, Maryam Cheema, Hasti Seifi, Pooyan Fazli

**Abstract:**  Video descriptions are crucial for blind and low vision (BLV) users to access visual content. However, current artificial intelligence models for generating descriptions often fall short due to limitations in the quality of human annotations within training datasets, resulting in descriptions that do not fully meet BLV users' needs. To address this gap, we introduce VideoA11y, an approach that leverages multimodal large language models (MLLMs) and video accessibility guidelines to generate descriptions tailored for BLV individuals. Using this method, we have curated VideoA11y-40K, the largest and most comprehensive dataset of 40,000 videos described for BLV users. Rigorous experiments across 15 video categories, involving 347 sighted participants, 40 BLV participants, and seven professional describers, showed that VideoA11y descriptions outperform novice human annotations and are comparable to trained human annotations in clarity, accuracy, objectivity, descriptiveness, and user satisfaction. We evaluated models on VideoA11y-40K using both standard and custom metrics, demonstrating that MLLMs fine-tuned on this dataset produce high-quality accessible descriptions. Code and dataset are available at https://people-robots.github.io/VideoA11y.

**Comment:** Matches criterion 2 as it leverages multimodal large language models (MLLMs) for accessible video description.
**Relevance:** 5
**Novelty:** 6

---

## 18. [Raccoon: Multi-stage Diffusion Training with Coarse-to-Fine Curating Videos](https://arxiv.org/abs/2502.21314) <a id="link18"></a>
**ArXiv ID:** 2502.21314
**Authors:** Zhiyu Tan, Junyan Wang, Hao Yang, Luozheng Qin, Hesen Chen, Qiang Zhou, Hao Li

**Abstract:**  Text-to-video generation has demonstrated promising progress with the advent of diffusion models, yet existing approaches are limited by dataset quality and computational resources. To address these limitations, this paper presents a comprehensive approach that advances both data curation and model design. We introduce CFC-VIDS-1M, a high-quality video dataset constructed through a systematic coarse-to-fine curation pipeline. The pipeline first evaluates video quality across multiple dimensions, followed by a fine-grained stage that leverages vision-language models to enhance text-video alignment and semantic richness. Building upon the curated dataset's emphasis on visual quality and temporal coherence, we develop RACCOON, a transformer-based architecture with decoupled spatial-temporal attention mechanisms. The model is trained through a progressive four-stage strategy designed to efficiently handle the complexities of video generation. Extensive experiments demonstrate that our integrated approach of high-quality data curation and efficient training strategy generates visually appealing and temporally coherent videos while maintaining computational efficiency. We will release our dataset, code, and models.

**Comment:** Matches criterion 4 as it discusses a vision foundation model (RACCOON) and its application in text-to-video generation.
**Relevance:** 5
**Novelty:** 6

---

## 19. [The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition](https://arxiv.org/abs/2502.21201) <a id="link19"></a>
**ArXiv ID:** 2502.21201
**Authors:** Otto Brookes, Maksim Kukushkin, Majid Mirmehdi, Colleen Stephens, Paula Dieguez, Thurston C. Hicks, Sorrel Jones, Kevin Lee, Maureen S. McCarthy, Amelia Meier, Emmanuelle Normand, Erin G. Wessling, Roman M. Wittig, Kevin Langergraber, Klaus Zuberb\"uhler, Lukas Boesch, Thomas Schmid, Mimi Arandjelovic, Hjalmar K\"uhl, Tilo Burghardt

**Abstract:**  Computer vision analysis of camera trap video footage is essential for wildlife conservation, as captured behaviours offer some of the earliest indicators of changes in population health. Recently, several high-impact animal behaviour datasets and methods have been introduced to encourage their use; however, the role of behaviour-correlated background information and its significant effect on out-of-distribution generalisation remain unexplored. In response, we present the PanAf-FGBG dataset, featuring 20 hours of wild chimpanzee behaviours, recorded at over 350 individual camera locations. Uniquely, it pairs every video with a chimpanzee (referred to as a foreground video) with a corresponding background video (with no chimpanzee) from the same camera location. We present two views of the dataset: one with overlapping camera locations and one with disjoint locations. This setup enables, for the first time, direct evaluation of in-distribution and out-of-distribution conditions, and for the impact of backgrounds on behaviour recognition models to be quantified. All clips come with rich behavioural annotations and metadata including unique camera IDs and detailed textual scene descriptions. Additionally, we establish several baselines and present a highly effective latent-space normalisation technique that boosts out-of-distribution performance by +5.42% mAP for convolutional and +3.75% mAP for transformer-based models. Finally, we provide an in-depth analysis on the role of backgrounds in out-of-distribution behaviour recognition, including the so far unexplored impact of background durations (i.e., the count of background frames within foreground videos).

**Comment:** This paper introduces a dataset for wildlife behavior recognition and explores the role of backgrounds, which aligns with criterion 3 as it provides a novel benchmark for behavior recognition.
**Relevance:** 5
**Novelty:** 6

---

## 20. [FC-Attack: Jailbreaking Large Vision-Language Models via Auto-Generated Flowcharts](https://arxiv.org/abs/2502.21059) <a id="link20"></a>
**ArXiv ID:** 2502.21059
**Authors:** Ziyi Zhang, Zhen Sun, Zongmin Zhang, Jihui Guo, Xinlei He

**Abstract:**  Large Vision-Language Models (LVLMs) have become powerful and widely adopted in some practical applications. However, recent research has revealed their vulnerability to multimodal jailbreak attacks, whereby the model can be induced to generate harmful content, leading to safety risks. Although most LVLMs have undergone safety alignment, recent research shows that the visual modality is still vulnerable to jailbreak attacks. In our work, we discover that by using flowcharts with partially harmful information, LVLMs can be induced to provide additional harmful details. Based on this, we propose a jailbreak attack method based on auto-generated flowcharts, FC-Attack. Specifically, FC-Attack first fine-tunes a pre-trained LLM to create a step-description generator based on benign datasets. The generator is then used to produce step descriptions corresponding to a harmful query, which are transformed into flowcharts in 3 different shapes (vertical, horizontal, and S-shaped) as visual prompts. These flowcharts are then combined with a benign textual prompt to execute a jailbreak attack on LVLMs. Our evaluations using the Advbench dataset show that FC-Attack achieves over 90% attack success rates on Gemini-1.5, Llaval-Next, Qwen2-VL, and InternVL-2.5 models, outperforming existing LVLM jailbreak methods. Additionally, we investigate factors affecting the attack performance, including the number of steps and the font styles in the flowcharts. Our evaluation shows that FC-Attack can improve the jailbreak performance from 4% to 28% in Claude-3.5 by changing the font style. To mitigate the attack, we explore several defenses and find that AdaShield can largely reduce the jailbreak performance but with the cost of utility drop.

**Comment:** This paper proposes a jailbreak attack on large vision-language models, which aligns with Criterion 2 on VLLMs and MLLMs.
**Relevance:** 5
**Novelty:** 6

---

## 21. [Back to the Future Cyclopean Stereo: a human perception approach unifying deep and geometric constraints](https://arxiv.org/abs/2502.21280) <a id="link21"></a>
**ArXiv ID:** 2502.21280
**Authors:** Sherlon Almeida da Silva, Davi Geiger, Luiz Velho, Moacir Antonelli Ponti

**Abstract:**  We innovate in stereo vision by explicitly providing analytical 3D surface models as viewed by a cyclopean eye model that incorporate depth discontinuities and occlusions. This geometrical foundation combined with learned stereo features allows our system to benefit from the strengths of both approaches. We also invoke a prior monocular model of surfaces to fill in occlusion regions or texture-less regions where data matching is not sufficient. Our results already are on par with the state-of-the-art purely data-driven methods and are of much better visual quality, emphasizing the importance of the 3D geometrical model to capture critical visual information. Such qualitative improvements may find applicability in virtual reality, for a better human experience, as well as in robotics, for reducing critical errors. Our approach aims to demonstrate that understanding and modeling geometrical properties of 3D surfaces is beneficial to computer vision research.

**Comment:** This paper combines geometric and learned stereo features for 3D surface modeling, which aligns with Criterion 1 on spatial understanding and intelligence.
**Relevance:** 5
**Novelty:** 6

---

## 22. [On Benchmarking Human-Like Intelligence in Machines](https://arxiv.org/abs/2502.20502) <a id="link22"></a>
**ArXiv ID:** 2502.20502
**Authors:** Lance Ying, Katherine M. Collins, Lionel Wong, Ilia Sucholutsky, Ryan Liu, Adrian Weller, Tianmin Shu, Thomas L. Griffiths, Joshua B. Tenenbaum

**Abstract:**  Recent benchmark studies have claimed that AI has approached or even surpassed human-level performances on various cognitive tasks. However, this position paper argues that current AI evaluation paradigms are insufficient for assessing human-like cognitive capabilities. We identify a set of key shortcomings: a lack of human-validated labels, inadequate representation of human response variability and uncertainty, and reliance on simplified and ecologically-invalid tasks. We support our claims by conducting a human evaluation study on ten existing AI benchmarks, suggesting significant biases and flaws in task and label designs. To address these limitations, we propose five concrete recommendations for developing future benchmarks that will enable more rigorous and meaningful evaluations of human-like cognitive capacities in AI with various implications for such AI applications.

**Comment:** This paper critiques current AI benchmarks and proposes recommendations for better evaluation, which aligns with Criterion 3 on new benchmarks for embodied AI.
**Relevance:** 5
**Novelty:** 6

---

## 23. [VLEER: Vision and Language Embeddings for Explainable Whole Slide Image Representation](https://arxiv.org/abs/2502.20850) <a id="link23"></a>
**ArXiv ID:** 2502.20850
**Authors:** Anh Tien Nguyen, Keunho Byeon, Kyungeun Kim, Jin Tae Kwak

**Abstract:**  Recent advances in vision-language models (VLMs) have shown remarkable potential in bridging visual and textual modalities. In computational pathology, domain-specific VLMs, which are pre-trained on extensive histopathology image-text datasets, have succeeded in various downstream tasks. However, existing research has primarily focused on the pre-training process and direct applications of VLMs on the patch level, leaving their great potential for whole slide image (WSI) applications unexplored. In this study, we hypothesize that pre-trained VLMs inherently capture informative and interpretable WSI representations through quantitative feature extraction. To validate this hypothesis, we introduce Vision and Language Embeddings for Explainable WSI Representation (VLEER), a novel method designed to leverage VLMs for WSI representation. We systematically evaluate VLEER on three pathological WSI datasets, proving its better performance in WSI analysis compared to conventional vision features. More importantly, VLEER offers the unique advantage of interpretability, enabling direct human-readable insights into the results by leveraging the textual modality for detailed pathology annotations, providing clear reasoning for WSI-level pathology downstream tasks.

**Comment:** Matches criterion 2 as it explores vision-language models (VLMs) for whole slide image representation, leveraging their interpretability and multi-modal capabilities.
**Relevance:** 5
**Novelty:** 6

---

## 24. [PathVG: A New Benchmark and Dataset for Pathology Visual Grounding](https://arxiv.org/abs/2502.20869) <a id="link24"></a>
**ArXiv ID:** 2502.20869
**Authors:** Chunlin Zhong, Shuang Hao, Junhua Wu, Xiaona Chang, Jiwei Jiang, Xiu Nie, He Tang, Xiang Bai

**Abstract:**  With the rapid development of computational pathology, many AI-assisted diagnostic tasks have emerged. Cellular nuclei segmentation can segment various types of cells for downstream analysis, but it relies on predefined categories and lacks flexibility. Moreover, pathology visual question answering can perform image-level understanding but lacks region-level detection capability. To address this, we propose a new benchmark called Pathology Visual Grounding (PathVG), which aims to detect regions based on expressions with different attributes. To evaluate PathVG, we create a new dataset named RefPath which contains 27,610 images with 33,500 language-grounded boxes. Compared to visual grounding in other domains, PathVG presents pathological images at multi-scale and contains expressions with pathological knowledge. In the experimental study, we found that the biggest challenge was the implicit information underlying the pathological expressions. Based on this, we proposed Pathology Knowledge-enhanced Network (PKNet) as the baseline model for PathVG. PKNet leverages the knowledge-enhancement capabilities of Large Language Models (LLMs) to convert pathological terms with implicit information into explicit visual features, and fuses knowledge features with expression features through the designed Knowledge Fusion Module (KFM). The proposed method achieves state-of-the-art performance on the PathVG benchmark.

**Comment:** Matches criterion 3 as it introduces a new benchmark (PathVG) and dataset (RefPath) for pathology visual grounding, which is a novel angle in the domain of visual grounding.
**Relevance:** 5
**Novelty:** 6

---

## 25. [T2ICount: Enhancing Cross-modal Understanding for Zero-Shot Counting](https://arxiv.org/abs/2502.20625) <a id="link25"></a>
**ArXiv ID:** 2502.20625
**Authors:** Yifei Qian, Zhongliang Guo, Bowen Deng, Chun Tong Lei, Shuai Zhao, Chun Pong Lau, Xiaopeng Hong, Michael P. Pound

**Abstract:**  Zero-shot object counting aims to count instances of arbitrary object categories specified by text descriptions. Existing methods typically rely on vision-language models like CLIP, but often exhibit limited sensitivity to text prompts. We present T2ICount, a diffusion-based framework that leverages rich prior knowledge and fine-grained visual understanding from pretrained diffusion models. While one-step denoising ensures efficiency, it leads to weakened text sensitivity. To address this challenge, we propose a Hierarchical Semantic Correction Module that progressively refines text-image feature alignment, and a Representational Regional Coherence Loss that provides reliable supervision signals by leveraging the cross-attention maps extracted from the denosing U-Net. Furthermore, we observe that current benchmarks mainly focus on majority objects in images, potentially masking models' text sensitivity. To address this, we contribute a challenging re-annotated subset of FSC147 for better evaluation of text-guided counting ability. Extensive experiments demonstrate that our method achieves superior performance across different benchmarks. Code is available at https://github.com/cha15yq/T2ICount.

**Comment:** Matches criterion 2 as it introduces a novel diffusion-based framework for zero-shot object counting, leveraging vision-language models.
**Relevance:** 5
**Novelty:** 6

---

## 26. [How far can we go with ImageNet for Text-to-Image generation?](https://arxiv.org/abs/2502.21318) <a id="link26"></a>
**ArXiv ID:** 2502.21318
**Authors:** L. Degeorge, A. Ghosh, N. Dufour, D. Picard, V. Kalogeiton

**Abstract:**  Recent text-to-image (T2I) generation models have achieved remarkable results by training on billion-scale datasets, following a `bigger is better' paradigm that prioritizes data quantity over quality. We challenge this established paradigm by demonstrating that strategic data augmentation of small, well-curated datasets can match or outperform models trained on massive web-scraped collections. Using only ImageNet enhanced with well-designed text and image augmentations, we achieve a +2 overall score over SD-XL on GenEval and +5 on DPGBench while using just 1/10th the parameters and 1/1000th the training images. Our results suggest that strategic data augmentation, rather than massive datasets, could offer a more sustainable path forward for T2I generation.

**Comment:** Matches criterion 4 as it challenges the 'bigger is better' paradigm in text-to-image generation, focusing on strategic data augmentation.
**Relevance:** 5
**Novelty:** 6

---

## 27. [Fine-Grained Retrieval-Augmented Generation for Visual Question Answering](https://arxiv.org/abs/2502.20964) <a id="link27"></a>
**ArXiv ID:** 2502.20964
**Authors:** Zhengxuan Zhang, Yin Wu, Yuyu Luo, Nan Tang

**Abstract:**  Visual Question Answering (VQA) focuses on providing answers to natural language questions by utilizing information from images. Although cutting-edge multimodal large language models (MLLMs) such as GPT-4o achieve strong performance on VQA tasks, they frequently fall short in accessing domain-specific or the latest knowledge. To mitigate this issue, retrieval-augmented generation (RAG) leveraging external knowledge bases (KBs), referred to as KB-VQA, emerges as a promising approach. Nevertheless, conventional unimodal retrieval techniques, which translate images into textual descriptions, often result in the loss of critical visual details. This study presents fine-grained knowledge units, which merge textual snippets with entity images stored in vector databases. Furthermore, we introduce a knowledge unit retrieval-augmented generation framework (KU-RAG) that integrates fine-grained retrieval with MLLMs. The proposed KU-RAG framework ensures precise retrieval of relevant knowledge and enhances reasoning capabilities through a knowledge correction chain. Experimental findings demonstrate that our approach significantly boosts the performance of leading KB-VQA methods, achieving improvements of up to 10%.

**Comment:** Matches criterion 2 as it introduces a novel retrieval-augmented generation framework for MLLMs in visual question answering.
**Relevance:** 5
**Novelty:** 6

---

## 28. [OpenEarthSensing: Large-Scale Fine-Grained Benchmark for Open-World Remote Sensing](https://arxiv.org/abs/2502.20668) <a id="link28"></a>
**ArXiv ID:** 2502.20668
**Authors:** Xiang Xiang, Zhuo Xu, Yao Deng, Qinhao Zhou, Yifan Liang, Ke Chen, Qingfang Zheng, Yaowei Wang, Xilin Chen, Wen Gao

**Abstract:**  In open-world remote sensing, deployed models must continuously adapt to a steady influx of new data, which often exhibits various shifts compared to what the model encountered during the training phase. To effectively handle the new data, models are required to detect semantic shifts, adapt to covariate shifts, and continuously update themselves. These challenges give rise to a variety of open-world tasks. However, existing open-world remote sensing studies typically train and test within a single dataset to simulate open-world conditions. Currently, there is a lack of large-scale benchmarks capable of evaluating multiple open-world tasks. In this paper, we introduce OpenEarthSensing, a large-scale fine-grained benchmark for open-world remote sensing. OpenEarthSensing includes 189 scene and objects categories, covering the vast majority of potential semantic shifts that may occur in the real world. Additionally, OpenEarthSensing encompasses five data domains with significant covariate shifts, including two RGB satellite domians, one RGB aerial domian, one MS RGB domian, and one infrared domian. The various domains provide a more comprehensive testbed for evaluating the generalization performance of open-world models. We conduct the baseline evaluation of current mainstream open-world tasks and methods on OpenEarthSensing, demonstrating that it serves as a challenging benchmark for open-world remote sensing.

**Comment:** This paper introduces a large-scale benchmark for open-world remote sensing, which aligns with criterion 3 on new benchmarks.
**Relevance:** 5
**Novelty:** 6

---

## 29. [CADDreamer: CAD object Generation from Single-view Images](https://arxiv.org/abs/2502.20732) <a id="link29"></a>
**ArXiv ID:** 2502.20732
**Authors:** Yuan Li, Cheng Lin, Yuan Liu, Xiaoxiao Long, Chenxu Zhang, Ningna Wang, Xin Li, Wenping Wang, Xiaohu Guo

**Abstract:**  Diffusion-based 3D generation has made remarkable progress in recent years. However, existing 3D generative models often produce overly dense and unstructured meshes, which stand in stark contrast to the compact, structured, and sharply-edged Computer-Aided Design (CAD) models crafted by human designers. To address this gap, we introduce CADDreamer, a novel approach for generating boundary representations (B-rep) of CAD objects from a single image. CADDreamer employs a primitive-aware multi-view diffusion model that captures both local geometric details and high-level structural semantics during the generation process. By encoding primitive semantics into the color domain, the method leverages the strong priors of pre-trained diffusion models to align with well-defined primitives. This enables the inference of multi-view normal maps and semantic maps from a single image, facilitating the reconstruction of a mesh with primitive labels. Furthermore, we introduce geometric optimization techniques and topology-preserving extraction methods to mitigate noise and distortion in the generated primitives. These enhancements result in a complete and seamless B-rep of the CAD model. Experimental results demonstrate that our method effectively recovers high-quality CAD objects from single-view images. Compared to existing 3D generation techniques, the B-rep models produced by CADDreamer are compact in representation, clear in structure, sharp in edges, and watertight in topology.

**Comment:** This paper introduces a diffusion-based method for CAD object generation, which aligns with criterion 4 on vision foundation models and applications.
**Relevance:** 5
**Novelty:** 6

---

## 30. [MESC-3D:Mining Effective Semantic Cues for 3D Reconstruction from a Single Image](https://arxiv.org/abs/2502.20861) <a id="link30"></a>
**ArXiv ID:** 2502.20861
**Authors:** Shaoming Li, Qing Cai, Songqi Kong, Runqing Tan, Heng Tong, Shiji Qiu, Yongguo Jiang, Zhi Liu

**Abstract:**  Reconstructing 3D shapes from a single image plays an important role in computer vision. Many methods have been proposed and achieve impressive performance. However, existing methods mainly focus on extracting semantic information from images and then simply concatenating it with 3D point clouds without further exploring the concatenated semantics. As a result, these entangled semantic features significantly hinder the reconstruction performance. In this paper, we propose a novel single-image 3D reconstruction method called Mining Effective Semantic Cues for 3D Reconstruction from a Single Image (MESC-3D), which can actively mine effective semantic cues from entangled features. Specifically, we design an Effective Semantic Mining Module to establish connections between point clouds and image semantic attributes, enabling the point clouds to autonomously select the necessary information. Furthermore, to address the potential insufficiencies in semantic information from a single image, such as occlusions, inspired by the human ability to represent 3D objects using prior knowledge drawn from daily experiences, we introduce a 3D Semantic Prior Learning Module. This module incorporates semantic understanding of spatial structures, enabling the model to interpret and reconstruct 3D objects with greater accuracy and realism, closely mirroring human perception of complex 3D environments. Extensive evaluations show that our method achieves significant improvements in reconstruction quality and robustness compared to prior works. Additionally, further experiments validate the strong generalization capabilities and excels in zero-shot preformance on unseen classes. Code is available at https://github.com/QINGQINGLE/MESC-3D.

**Comment:** This paper proposes a novel method for single-image 3D reconstruction, which aligns with criterion 1 on spatial understanding improvements.
**Relevance:** 5
**Novelty:** 6

---

## 31. [CoCa-CXR: Contrastive Captioners Learn Strong Temporal Structures for Chest X-Ray Vision-Language Understanding](https://arxiv.org/abs/2502.20509) <a id="link31"></a>
**ArXiv ID:** 2502.20509
**Authors:** Yixiong Chen, Shawn Xu, Andrew Sellergren, Yossi Matias, Avinatan Hassidim, Shravya Shetty, Daniel Golden, Alan Yuille, Lin Yang

**Abstract:**  Vision-language models have proven to be of great benefit for medical image analysis since they learn rich semantics from both images and reports. Prior efforts have focused on better alignment of image and text representations to enhance image understanding. However, though explicit reference to a prior image is common in Chest X-Ray (CXR) reports, aligning progression descriptions with the semantics differences in image pairs remains under-explored. In this work, we propose two components to address this issue. (1) A CXR report processing pipeline to extract temporal structure. It processes reports with a large language model (LLM) to separate the description and comparison contexts, and extracts fine-grained annotations from reports. (2) A contrastive captioner model for CXR, namely CoCa-CXR, to learn how to both describe images and their temporal progressions. CoCa-CXR incorporates a novel regional cross-attention module to identify local differences between paired CXR images. Extensive experiments show the superiority of CoCa-CXR on both progression analysis and report generation compared to previous methods. Notably, on MS-CXR-T progression classification, CoCa-CXR obtains 65.0% average testing accuracy on five pulmonary conditions, outperforming the previous state-of-the-art (SOTA) model BioViL-T by 4.8%. It also achieves a RadGraph F1 of 24.2% on MIMIC-CXR, which is comparable to the Med-Gemini foundation model.

**Comment:** This paper introduces a vision-language model for medical imaging with temporal structure learning, which aligns with criterion 2 on VLLMs.
**Relevance:** 5
**Novelty:** 6

---

## 32. [FlexDrive: Toward Trajectory Flexibility in Driving Scene Reconstruction and Rendering](https://arxiv.org/abs/2502.21093) <a id="link32"></a>
**ArXiv ID:** 2502.21093
**Authors:** Jingqiu Zhou, Lue Fan, Linjiang Huang, Xiaoyu Shi, Si Liu, Zhaoxiang Zhang, Hongsheng Li

**Abstract:**  Driving scene reconstruction and rendering have advanced significantly using the 3D Gaussian Splatting. However, most prior research has focused on the rendering quality along a pre-recorded vehicle path and struggles to generalize to out-of-path viewpoints, which is caused by the lack of high-quality supervision in those out-of-path views. To address this issue, we introduce an Inverse View Warping technique to create compact and high-quality images as supervision for the reconstruction of the out-of-path views, enabling high-quality rendering results for those views. For accurate and robust inverse view warping, a depth bootstrap strategy is proposed to obtain on-the-fly dense depth maps during the optimization process, overcoming the sparsity and incompleteness of LiDAR depth data. Our method achieves superior in-path and out-of-path reconstruction and rendering performance on the widely used Waymo Open dataset. In addition, a simulator-based benchmark is proposed to obtain the out-of-path ground truth and quantitatively evaluate the performance of out-of-path rendering, where our method outperforms previous methods by a significant margin.

**Comment:** This paper proposes a simulator-based benchmark for driving scene reconstruction, matching criterion 3 on new benchmarks for embodied AI.
**Relevance:** 5
**Novelty:** 6

---

## 33. [Training-free and Adaptive Sparse Attention for Efficient Long Video Generation](https://arxiv.org/abs/2502.21079) <a id="link33"></a>
**ArXiv ID:** 2502.21079
**Authors:** Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, Bin Cui

**Abstract:**  Generating high-fidelity long videos with Diffusion Transformers (DiTs) is often hindered by significant latency, primarily due to the computational demands of attention mechanisms. For instance, generating an 8-second 720p video (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500 PFLOPs consumed by attention computations. To address this issue, we propose AdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention method. Firstly, to realize the Dynamic Pattern, we introduce a blockified pattern to efficiently capture the hierarchical sparsity inherent in DiTs. This is based on our observation that sparse characteristics of DiTs exhibit hierarchical and blockified structures between and within different modalities. This blockified approach significantly reduces the complexity of attention computation while maintaining high fidelity in the generated videos. Secondly, to enable Online Precise Search, we propose the Fused LSE-Cached Search with Head-adaptive Hierarchical Block Sparse Attention. This method is motivated by our finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and heads, but remain invariant across denoising steps. By leveraging this invariance across denoising steps, it adapts to the dynamic nature of DiTs and allows for precise, real-time identification of sparse indices with minimal overhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can be integrated seamlessly with existing DiTs, requiring neither additional fine-tuning nor a dataset-dependent profiling. Extensive experiments validate that AdaSpa delivers substantial acceleration across various models while preserving video quality, establishing itself as a robust and scalable approach to efficient video generation.

**Comment:** Matches criterion 4. Proposes a novel sparse attention method for efficient video generation, which could be relevant to vision foundation models.
**Relevance:** 5
**Novelty:** 6

---

## 34. [PersonaBench: Evaluating AI Models on Understanding Personal Information through Accessing (Synthetic) Private User Data](https://arxiv.org/abs/2502.20616) <a id="link34"></a>
**ArXiv ID:** 2502.20616
**Authors:** Juntao Tan, Liangwei Yang, Zuxin Liu, Zhiwei Liu, Rithesh Murthy, Tulika Manoj Awalgaonkar, Jianguo Zhang, Weiran Yao, Ming Zhu, Shirley Kokane, Silvio Savarese, Huan Wang, Caiming Xiong, Shelby Heinecke

**Abstract:**  Personalization is critical in AI assistants, particularly in the context of private AI models that work with individual users. A key scenario in this domain involves enabling AI models to access and interpret a user's private data (e.g., conversation history, user-AI interactions, app usage) to understand personal details such as biographical information, preferences, and social connections. However, due to the sensitive nature of such data, there are no publicly available datasets that allow us to assess an AI model's ability to understand users through direct access to personal information.   To address this gap, we introduce a synthetic data generation pipeline that creates diverse, realistic user profiles and private documents simulating human activities. Leveraging this synthetic data, we present PersonaBench, a benchmark designed to evaluate AI models' performance in understanding personal information derived from simulated private user data.   We evaluate Retrieval-Augmented Generation (RAG) pipelines using questions directly related to a user's personal information, supported by the relevant private documents provided to the models. Our results reveal that current retrieval-augmented AI models struggle to answer private questions by extracting personal information from user documents, highlighting the need for improved methodologies to enhance personalization capabilities in AI.

**Comment:** This paper introduces PersonaBench, a benchmark for evaluating AI models on understanding personal information, which does not match any specific criteria but is related to personalization in AI.
**Relevance:** 3
**Novelty:** 6

---

## 35. [LISArD: Learning Image Similarity to Defend Against Gray-box Adversarial Attacks](https://arxiv.org/abs/2502.20562) <a id="link35"></a>
**ArXiv ID:** 2502.20562
**Authors:** Joana C. Costa, Tiago Roxo, Hugo Proen\c{c}a, Pedro R. M. In\'acio

**Abstract:**  State-of-the-art defense mechanisms are typically evaluated in the context of white-box attacks, which is not realistic, as it assumes the attacker can access the gradients of the target network. To protect against this scenario, Adversarial Training (AT) and Adversarial Distillation (AD) include adversarial examples during the training phase, and Adversarial Purification uses a generative model to reconstruct all the images given to the classifier. This paper considers an even more realistic evaluation scenario: gray-box attacks, which assume that the attacker knows the architecture and the dataset used to train the target network, but cannot access its gradients. We provide empirical evidence that models are vulnerable to gray-box attacks and propose LISArD, a defense mechanism that does not increase computational and temporal costs but provides robustness against gray-box and white-box attacks without including AT. Our method approximates a cross-correlation matrix, created with the embeddings of perturbed and clean images, to a diagonal matrix while simultaneously conducting classification learning. Our results show that LISArD can effectively protect against gray-box attacks, can be used in multiple architectures, and carries over its resilience to the white-box scenario. Also, state-of-the-art AD models underperform greatly when removing AT and/or moving to gray-box settings, highlighting the lack of robustness from existing approaches to perform in various conditions (aside from white-box settings). All the source code is available at https://github.com/Joana-Cabral/LISArD.

**Comment:** This paper proposes a defense mechanism against gray-box adversarial attacks, which does not match any specific criteria but is related to robustness in machine learning.
**Relevance:** 3
**Novelty:** 6

---

## 36. [Towards High-performance Spiking Transformers from ANN to SNN Conversion](https://arxiv.org/abs/2502.21193) <a id="link36"></a>
**ArXiv ID:** 2502.21193
**Authors:** Zihan Huang, Xinyu Shi, Zecheng Hao, Tong Bu, Jianhao Ding, Zhaofei Yu, Tiejun Huang

**Abstract:**  Spiking neural networks (SNNs) show great potential due to their energy efficiency, fast processing capabilities, and robustness. There are two main approaches to constructing SNNs. Direct training methods require much memory, while conversion methods offer a simpler and more efficient option. However, current conversion methods mainly focus on converting convolutional neural networks (CNNs) to SNNs. Converting Transformers to SNN is challenging because of the presence of non-linear modules. In this paper, we propose an Expectation Compensation Module to preserve the accuracy of the conversion. The core idea is to use information from the previous T time-steps to calculate the expected output at time-step T. We also propose a Multi-Threshold Neuron and the corresponding Parallel Parameter normalization to address the challenge of large time steps needed for high accuracy, aiming to reduce network latency and power consumption. Our experimental results demonstrate that our approach achieves state-of-the-art performance. For example, we achieve a top-1 accuracy of 88.60\% with only a 1\% loss in accuracy using 4 time steps while consuming only 35\% of the original power of the Transformer. To our knowledge, this is the first successful Artificial Neural Network (ANN) to SNN conversion for Spiking Transformers that achieves high accuracy, low latency, and low power consumption on complex datasets. The source codes of the proposed method are available at https://github.com/h-z-h-cell/Transformer-to-SNN-ECMT.

**Comment:** This paper does not match any of the specific criteria but is related to spiking neural networks and energy-efficient transformers, which are outside the scope of the specified interests.
**Relevance:** 3
**Novelty:** 6

---

## 37. [Enhancing deep neural networks through complex-valued representations and Kuramoto synchronization dynamics](https://arxiv.org/abs/2502.21077) <a id="link37"></a>
**ArXiv ID:** 2502.21077
**Authors:** Sabine Muzellec, Andrea Alamia, Thomas Serre, Rufin VanRullen

**Abstract:**  Neural synchrony is hypothesized to play a crucial role in how the brain organizes visual scenes into structured representations, enabling the robust encoding of multiple objects within a scene. However, current deep learning models often struggle with object binding, limiting their ability to represent multiple objects effectively. Inspired by neuroscience, we investigate whether synchrony-based mechanisms can enhance object encoding in artificial models trained for visual categorization. Specifically, we combine complex-valued representations with Kuramoto dynamics to promote phase alignment, facilitating the grouping of features belonging to the same object. We evaluate two architectures employing synchrony: a feedforward model and a recurrent model with feedback connections to refine phase synchronization using top-down information. Both models outperform their real-valued counterparts and complex-valued models without Kuramoto synchronization on tasks involving multi-object images, such as overlapping handwritten digits, noisy inputs, and out-of-distribution transformations. Our findings highlight the potential of synchrony-driven mechanisms to enhance deep learning models, improving their performance, robustness, and generalization in complex visual categorization tasks.

**Comment:** This paper does not match any specific criteria. It explores synchrony-based mechanisms inspired by neuroscience for visual categorization, which is tangentially related to computer vision but not directly relevant to the specified interests.
**Relevance:** 3
**Novelty:** 6

---

## 38. [A Review on Generative AI For Text-To-Image and Image-To-Image Generation and Implications To Scientific Images](https://arxiv.org/abs/2502.21151) <a id="link38"></a>
**ArXiv ID:** 2502.21151
**Authors:** Zineb Sordo, Eric Chagnon, Daniela Ushizima

**Abstract:**  This review surveys the state-of-the-art in text-to-image and image-to-image generation within the scope of generative AI. We provide a comparative analysis of three prominent architectures: Variational Autoencoders, Generative Adversarial Networks and Diffusion Models. For each, we elucidate core concepts, architectural innovations, and practical strengths and limitations, particularly for scientific image understanding. Finally, we discuss critical open challenges and potential future research directions in this rapidly evolving field.

**Comment:** Matches criterion 4 as it reviews generative AI methods for text-to-image and image-to-image generation, relevant to vision foundation models.
**Relevance:** 5
**Novelty:** 4

---

## 39. [Decoder Gradient Shield: Provable and High-Fidelity Prevention of Gradient-Based Box-Free Watermark Removal](https://arxiv.org/abs/2502.20924) <a id="link39"></a>
**ArXiv ID:** 2502.20924
**Authors:** Haonan An, Guang Hua, Zhengru Fang, Guowen Xu, Susanto Rahardja, Yuguang Fang

**Abstract:**  The intellectual property of deep image-to-image models can be protected by the so-called box-free watermarking. It uses an encoder and a decoder, respectively, to embed into and extract from the model's output images invisible copyright marks. Prior works have improved watermark robustness, focusing on the design of better watermark encoders. In this paper, we reveal an overlooked vulnerability of the unprotected watermark decoder which is jointly trained with the encoder and can be exploited to train a watermark removal network. To defend against such an attack, we propose the decoder gradient shield (DGS) as a protection layer in the decoder API to prevent gradient-based watermark removal with a closed-form solution. The fundamental idea is inspired by the classical adversarial attack, but is utilized for the first time as a defensive mechanism in the box-free model watermarking. We then demonstrate that DGS can reorient and rescale the gradient directions of watermarked queries and stop the watermark remover's training loss from converging to the level without DGS, while retaining decoder output image quality. Experimental results verify the effectiveness of proposed method. Code of paper will be made available upon acceptance.

**Comment:** This paper focuses on watermarking for intellectual property protection in image-to-image models, which does not match any specific criteria but is tangentially related to computer vision.
**Relevance:** 3
**Novelty:** 5

---

## 40. [Optimizing Large Language Models for ESG Activity Detection in Financial Texts](https://arxiv.org/abs/2502.21112) <a id="link40"></a>
**ArXiv ID:** 2502.21112
**Authors:** Mattia Birti, Francesco Osborne, Andrea Maurino

**Abstract:**  The integration of Environmental, Social, and Governance (ESG) factors into corporate decision-making is a fundamental aspect of sustainable finance. However, ensuring that business practices align with evolving regulatory frameworks remains a persistent challenge. AI-driven solutions for automatically assessing the alignment of sustainability reports and non-financial disclosures with specific ESG activities could greatly support this process. Yet, this task remains complex due to the limitations of general-purpose Large Language Models (LLMs) in domain-specific contexts and the scarcity of structured, high-quality datasets. In this paper, we investigate the ability of current-generation LLMs to identify text related to environmental activities. Furthermore, we demonstrate that their performance can be significantly enhanced through fine-tuning on a combination of original and synthetically generated data. To this end, we introduce ESG-Activities, a benchmark dataset containing 1,325 labelled text segments classified according to the EU ESG taxonomy. Our experimental results show that fine-tuning on ESG-Activities significantly enhances classification accuracy, with open models such as Llama 7B and Gemma 7B outperforming large proprietary solutions in specific configurations. These findings have important implications for financial analysts, policymakers, and AI researchers seeking to enhance ESG transparency and compliance through advanced natural language processing techniques.

**Comment:** This paper focuses on fine-tuning LLMs for ESG activity detection in financial texts, which does not match any specific criteria but is related to domain-specific applications of LLMs.
**Relevance:** 3
**Novelty:** 5

---

## 41. [HoloMine: A Synthetic Dataset for Buried Landmines Recognition using Microwave Holographic Imaging](https://arxiv.org/abs/2502.21054) <a id="link41"></a>
**ArXiv ID:** 2502.21054
**Authors:** Emanuele Vivoli, Lorenzo Capineri, Marco Bertini

**Abstract:**  The detection and removal of landmines is a complex and risky task that requires advanced remote sensing techniques to reduce the risk for the professionals involved in this task. In this paper, we propose a novel synthetic dataset for buried landmine detection to provide researchers with a valuable resource to observe, measure, locate, and address issues in landmine detection. The dataset consists of 41,800 microwave holographic images (2D) and their holographic inverted scans (3D) of different types of buried objects, including landmines, clutter, and pottery objects, and is collected by means of a microwave holography sensor.   We evaluate the performance of several state-of-the-art deep learning models trained on our synthetic dataset for various classification tasks. While the results do not yield yet high performances, showing the difficulty of the proposed task, we believe that our dataset has significant potential to drive progress in the field of landmine detection thanks to the accuracy and resolution obtainable using holographic radars.   To the best of our knowledge, our dataset is the first of its kind and will help drive further research on computer vision methods to automatize mine detection, with the overall goal of reducing the risks and the costs of the demining process.

**Comment:** This paper introduces a synthetic dataset for landmine detection, which does not match any specific criteria but is an application of computer vision.
**Relevance:** 3
**Novelty:** 5

---

## 42. [BST: Badminton Stroke-type Transformer for Skeleton-based Action Recognition in Racket Sports](https://arxiv.org/abs/2502.21085) <a id="link42"></a>
**ArXiv ID:** 2502.21085
**Authors:** Jing-Yuan Chang

**Abstract:**  Badminton, known for having the fastest ball speeds among all sports, presents significant challenges to the field of computer vision, including player identification, court line detection, shuttlecock trajectory tracking, and player stroke-type classification. In this paper, we introduce a novel video segmentation strategy to extract frames of each player's racket swing in a badminton broadcast match. These segmented frames are then processed by two existing models: one for Human Pose Estimation to obtain player skeletal joints, and the other for shuttlecock trajectory detection to extract shuttlecock trajectories. Leveraging these joints, trajectories, and player positions as inputs, we propose Badminton Stroke-type Transformer (BST) to classify player stroke-types in singles. To the best of our knowledge, experimental results demonstrate that our method outperforms the previous state-of-the-art on the largest publicly available badminton video dataset, ShuttleSet, which shows that effectively leveraging ball trajectory is likely to be a trend for racket sports action recognition.

**Comment:** This paper focuses on action recognition in badminton using transformers, which does not match any specific criteria but is tangentially related to computer vision.
**Relevance:** 3
**Novelty:** 5

---

## 43. [LesionLocator: Zero-Shot Universal Tumor Segmentation and Tracking in 3D Whole-Body Imaging](https://arxiv.org/abs/2502.20985) <a id="link43"></a>
**ArXiv ID:** 2502.20985
**Authors:** Maximilian Rokuss, Yannick Kirchhoff, Seval Akbal, Balint Kovacs, Saikat Roy, Constantin Ulrich, Tassilo Wald, Lukas T. Rotkopf, Heinz-Peter Schlemmer, Klaus Maier-Hein

**Abstract:**  In this work, we present LesionLocator, a framework for zero-shot longitudinal lesion tracking and segmentation in 3D medical imaging, establishing the first end-to-end model capable of 4D tracking with dense spatial prompts. Our model leverages an extensive dataset of 23,262 annotated medical scans, as well as synthesized longitudinal data across diverse lesion types. The diversity and scale of our dataset significantly enhances model generalizability to real-world medical imaging challenges and addresses key limitations in longitudinal data availability. LesionLocator outperforms all existing promptable models in lesion segmentation by nearly 10 dice points, reaching human-level performance, and achieves state-of-the-art results in lesion tracking, with superior lesion retrieval and segmentation accuracy. LesionLocator not only sets a new benchmark in universal promptable lesion segmentation and automated longitudinal lesion tracking but also provides the first open-access solution of its kind, releasing our synthetic 4D dataset and model to the community, empowering future advancements in medical imaging. Code is available at: www.github.com/MIC-DKFZ/LesionLocator

**Comment:** This paper introduces a framework for zero-shot lesion tracking and segmentation, which is not directly related to the criteria but involves vision applications.
**Relevance:** 3
**Novelty:** 5

---

## 44. [An LLM-based Delphi Study to Predict GenAI Evolution](https://arxiv.org/abs/2502.21092) <a id="link44"></a>
**ArXiv ID:** 2502.21092
**Authors:** Francesco Bertolotti, Luca Mari

**Abstract:**  Predicting the future trajectory of complex and rapidly evolving systems remains a significant challenge, particularly in domains where data is scarce or unreliable. This study introduces a novel approach to qualitative forecasting by leveraging Large Language Models to conduct Delphi studies. The methodology was applied to explore the future evolution of Generative Artificial Intelligence, revealing insights into key factors such as geopolitical tensions, economic disparities, regulatory frameworks, and ethical considerations. The results highlight how LLM-based Delphi studies can facilitate structured scenario analysis, capturing diverse perspectives while mitigating issues such as respondent fatigue. However, limitations emerge in terms of knowledge cutoffs, inherent biases, and sensitivity to initial conditions. While the approach provides an innovative means for structured foresight, this method could be also considered as a novel form of reasoning. further research is needed to refine its ability to manage heterogeneity, improve reliability, and integrate external data sources.

**Comment:** This paper uses LLMs for Delphi studies to predict generative AI evolution, which is not directly related to the criteria but involves generative AI.
**Relevance:** 3
**Novelty:** 5

---

## 45. [SEE: See Everything Every Time -- Adaptive Brightness Adjustment for Broad Light Range Images via Events](https://arxiv.org/abs/2502.21120) <a id="link45"></a>
**ArXiv ID:** 2502.21120
**Authors:** Yunfan Lu, Xiaogang Xu, Hao Lu, Yanlin Qian, Pengteng Li, Huizai Yao, Bin Yang, Junyi Li, Qianyi Cai, Weiyu Guo, Hui Xiong

**Abstract:**  Event cameras, with a high dynamic range exceeding $120dB$, significantly outperform traditional embedded cameras, robustly recording detailed changing information under various lighting conditions, including both low- and high-light situations. However, recent research on utilizing event data has primarily focused on low-light image enhancement, neglecting image enhancement and brightness adjustment across a broader range of lighting conditions, such as normal or high illumination. Based on this, we propose a novel research question: how to employ events to enhance and adaptively adjust the brightness of images captured under broad lighting conditions? To investigate this question, we first collected a new dataset, SEE-600K, consisting of 610,126 images and corresponding events across 202 scenarios, each featuring an average of four lighting conditions with over a 1000-fold variation in illumination. Subsequently, we propose a framework that effectively utilizes events to smoothly adjust image brightness through the use of prompts. Our framework captures color through sensor patterns, uses cross-attention to model events as a brightness dictionary, and adjusts the image's dynamic range to form a broad light-range representation (BLR), which is then decoded at the pixel level based on the brightness prompt. Experimental results demonstrate that our method not only performs well on the low-light enhancement dataset but also shows robust performance on broader light-range image enhancement using the SEE-600K dataset. Additionally, our approach enables pixel-level brightness adjustment, providing flexibility for post-processing and inspiring more imaging applications. The dataset and source code are publicly available at:https://github.com/yunfanLu/SEE.

**Comment:** This paper introduces a dataset and method for adaptive brightness adjustment using event cameras, which is not directly related to the criteria but involves vision applications.
**Relevance:** 3
**Novelty:** 5

---

## 46. [Anatomically-guided masked autoencoder pre-training for aneurysm detection](https://arxiv.org/abs/2502.21244) <a id="link46"></a>
**ArXiv ID:** 2502.21244
**Authors:** Alberto Mario Ceballos-Arroyo, Jisoo Kim, Chu-Hsuan Lin, Lei Qin, Geoffrey S. Young, Huaizu Jiang

**Abstract:**  Intracranial aneurysms are a major cause of morbidity and mortality worldwide, and detecting them manually is a complex, time-consuming task. Albeit automated solutions are desirable, the limited availability of training data makes it difficult to develop such solutions using typical supervised learning frameworks. In this work, we propose a novel pre-training strategy using more widely available unannotated head CT scan data to pre-train a 3D Vision Transformer model prior to fine-tuning for the aneurysm detection task. Specifically, we modify masked auto-encoder (MAE) pre-training in the following ways: we use a factorized self-attention mechanism to make 3D attention computationally viable, we restrict the masked patches to areas near arteries to focus on areas where aneurysms are likely to occur, and we reconstruct not only CT scan intensity values but also artery distance maps, which describe the distance between each voxel and the closest artery, thereby enhancing the backbone's learned representations. Compared with SOTA aneurysm detection models, our approach gains +4-8% absolute Sensitivity at a false positive rate of 0.5. Code and weights will be released.

**Comment:** This paper proposes a novel pre-training strategy for aneurysm detection using 3D Vision Transformers, which is not directly related to the criteria but involves vision models.
**Relevance:** 3
**Novelty:** 5

---

## 47. [BadRefSR: Backdoor Attacks Against Reference-based Image Super Resolution](https://arxiv.org/abs/2502.20943) <a id="link47"></a>
**ArXiv ID:** 2502.20943
**Authors:** Xue Yang, Tao Chen, Lei Guo, Wenbo Jiang, Ji Guo, Yongming Li, Jiaming He

**Abstract:**  Reference-based image super-resolution (RefSR) represents a promising advancement in super-resolution (SR). In contrast to single-image super-resolution (SISR), RefSR leverages an additional reference image to help recover high-frequency details, yet its vulnerability to backdoor attacks has not been explored. To fill this research gap, we propose a novel attack framework called BadRefSR, which embeds backdoors in the RefSR model by adding triggers to the reference images and training with a mixed loss function. Extensive experiments across various backdoor attack settings demonstrate the effectiveness of BadRefSR. The compromised RefSR network performs normally on clean input images, while outputting attacker-specified target images on triggered input images. Our study aims to alert researchers to the potential backdoor risks in RefSR. Codes are available at https://github.com/xuefusiji/BadRefSR.

**Comment:** This paper does not match any specific criteria. It focuses on backdoor attacks in reference-based image super-resolution, which is not directly related to the specified interests.
**Relevance:** 3
**Novelty:** 5

---

## 48. [EDM: Equirectangular Projection-Oriented Dense Kernelized Feature Matching](https://arxiv.org/abs/2502.20685) <a id="link48"></a>
**ArXiv ID:** 2502.20685
**Authors:** Dongki Jung, Jaehoon Choi, Yonghan Lee, Somi Jeong, Taejae Lee, Dinesh Manocha, Suyong Yeon

**Abstract:**  We introduce the first learning-based dense matching algorithm, termed Equirectangular Projection-Oriented Dense Kernelized Feature Matching (EDM), specifically designed for omnidirectional images. Equirectangular projection (ERP) images, with their large fields of view, are particularly suited for dense matching techniques that aim to establish comprehensive correspondences across images. However, ERP images are subject to significant distortions, which we address by leveraging the spherical camera model and geodesic flow refinement in the dense matching method. To further mitigate these distortions, we propose spherical positional embeddings based on 3D Cartesian coordinates of the feature grid. Additionally, our method incorporates bidirectional transformations between spherical and Cartesian coordinate systems during refinement, utilizing a unit sphere to improve matching performance. We demonstrate that our proposed method achieves notable performance enhancements, with improvements of +26.72 and +42.62 in AUC@5{\deg} on the Matterport3D and Stanford2D3D datasets.

**Comment:** This paper does not match any specific criteria. It focuses on dense matching for omnidirectional images, which is not directly related to the specified interests.
**Relevance:** 3
**Novelty:** 5

---

## 49. [Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on Diffusion Models](https://arxiv.org/abs/2502.20650) <a id="link49"></a>
**ArXiv ID:** 2502.20650
**Authors:** Yu Pan, Bingrong Dai, Jiahao Chen, Lin Wang, Yi Du, Jiao Liu

**Abstract:**  In recent years, Diffusion Models (DMs) have demonstrated significant advances in the field of image generation. However, according to current research, DMs are vulnerable to backdoor attacks, which allow attackers to control the model's output by inputting data containing covert triggers, such as a specific patch or phrase. Existing defense strategies are well equipped to thwart such attacks through backdoor detection and trigger inversion because previous attack methods are constrained by limited input spaces and triggers defined by low-dimensional features. To bridge these gaps, we propose Gungnir, a novel method that enables attackers to activate the backdoor in DMs through hidden style triggers within input images. Our approach proposes using stylistic features as triggers for the first time and implements backdoor attacks successfully in image2image tasks by utilizing Reconstructing-Adversarial Noise (RAN) and Short-Term-Timesteps-Retention (STTR) of DMs. Meanwhile, experiments demonstrate that our method can easily bypass existing defense methods. Among existing DM main backdoor defense frameworks, our approach achieves a 0\% backdoor detection rate (BDR). Our codes are available at https://github.com/paoche11/Gungnir.

**Comment:** Does not match any specific criteria. Focuses on backdoor attacks on diffusion models, which is outside the specified areas.
**Relevance:** 3
**Novelty:** 5

---

## 50. [InstaFace: Identity-Preserving Facial Editing with Single Image Inference](https://arxiv.org/abs/2502.20577) <a id="link50"></a>
**ArXiv ID:** 2502.20577
**Authors:** MD Wahiduzzaman Khan, Mingshan Jia, Shaolin Zhang, En Yu, Kaska Musial-Gabrys

**Abstract:**  Facial appearance editing is crucial for digital avatars, AR/VR, and personalized content creation, driving realistic user experiences. However, preserving identity with generative models is challenging, especially in scenarios with limited data availability. Traditional methods often require multiple images and still struggle with unnatural face shifts, inconsistent hair alignment, or excessive smoothing effects. To overcome these challenges, we introduce a novel diffusion-based framework, InstaFace, to generate realistic images while preserving identity using only a single image. Central to InstaFace, we introduce an efficient guidance network that harnesses 3D perspectives by integrating multiple 3DMM-based conditionals without introducing additional trainable parameters. Moreover, to ensure maximum identity retention as well as preservation of background, hair, and other contextual features like accessories, we introduce a novel module that utilizes feature embeddings from a facial recognition model and a pre-trained vision-language model. Quantitative evaluations demonstrate that our method outperforms several state-of-the-art approaches in terms of identity preservation, photorealism, and effective control of pose, expression, and lighting.

**Comment:** Does not match any specific criteria. Focuses on identity-preserving facial editing, which is outside the specified areas.
**Relevance:** 3
**Novelty:** 5

---

## 51. [ProAI: Proactive Multi-Agent Conversational AI with Structured Knowledge Base for Psychiatric Diagnosis](https://arxiv.org/abs/2502.20689) <a id="link51"></a>
**ArXiv ID:** 2502.20689
**Authors:** Yuqi Wu, Guangya Wan, Jingjing Li, Shengming Zhao, Lingfeng Ma, Tianyi Ye, Ion Pop, Yanbo Zhang, Jie Chen

**Abstract:**  Most LLM-driven conversational AI systems operate reactively, responding to user prompts without guiding the interaction. Most LLM-driven conversational AI systems operate reactively, responding to user prompts without guiding the interaction. However, many real-world applications-such as psychiatric diagnosis, consulting, and interviews-require AI to take a proactive role, asking the right questions and steering conversations toward specific objectives. Using mental health differential diagnosis as an application context, we introduce ProAI, a goal-oriented, proactive conversational AI framework. ProAI integrates structured knowledge-guided memory, multi-agent proactive reasoning, and a multi-faceted evaluation strategy, enabling LLMs to engage in clinician-style diagnostic reasoning rather than simple response generation. Through simulated patient interactions, user experience assessment, and professional clinical validation, we demonstrate that ProAI achieves up to 83.3% accuracy in mental disorder differential diagnosis while maintaining professional and empathetic interaction standards. These results highlight the potential for more reliable, adaptive, and goal-driven AI diagnostic assistants, advancing LLMs beyond reactive dialogue systems.

**Comment:** Does not match any specific criteria. Focuses on proactive conversational AI for psychiatric diagnosis, which is outside the specified areas.
**Relevance:** 3
**Novelty:** 5

---

## 52. [Contextualizing biological perturbation experiments through language](https://arxiv.org/abs/2502.21290) <a id="link52"></a>
**ArXiv ID:** 2502.21290
**Authors:** Menghua Wu, Russell Littman, Jacob Levine, Lin Qiu, Tommaso Biancalani, David Richmond, Jan-Christian Huetter

**Abstract:**  High-content perturbation experiments allow scientists to probe biomolecular systems at unprecedented resolution, but experimental and analysis costs pose significant barriers to widespread adoption. Machine learning has the potential to guide efficient exploration of the perturbation space and extract novel insights from these data. However, current approaches neglect the semantic richness of the relevant biology, and their objectives are misaligned with downstream biological analyses. In this paper, we hypothesize that large language models (LLMs) present a natural medium for representing complex biological relationships and rationalizing experimental outcomes. We propose PerturbQA, a benchmark for structured reasoning over perturbation experiments. Unlike current benchmarks that primarily interrogate existing knowledge, PerturbQA is inspired by open problems in perturbation modeling: prediction of differential expression and change of direction for unseen perturbations, and gene set enrichment. We evaluate state-of-the-art machine learning and statistical approaches for modeling perturbations, as well as standard LLM reasoning strategies, and we find that current methods perform poorly on PerturbQA. As a proof of feasibility, we introduce Summer (SUMMarize, retrievE, and answeR, a simple, domain-informed LLM framework that matches or exceeds the current state-of-the-art. Our code and data are publicly available at https://github.com/genentech/PerturbQA.

**Comment:** This paper proposes a benchmark for reasoning over biological perturbation experiments, which does not match any specific criteria but is tangentially relevant to machine learning.
**Relevance:** 3
**Novelty:** 5

---

## 53. [Large Language Model Strategic Reasoning Evaluation through Behavioral Game Theory](https://arxiv.org/abs/2502.20432) <a id="link53"></a>
**ArXiv ID:** 2502.20432
**Authors:** Jingru Jia, Zehua Yuan, Junhao Pan, Paul E. McNamara, Deming Chen

**Abstract:**  Strategic decision-making involves interactive reasoning where agents adapt their choices in response to others, yet existing evaluations of large language models (LLMs) often emphasize Nash Equilibrium (NE) approximation, overlooking the mechanisms driving their strategic choices. To bridge this gap, we introduce an evaluation framework grounded in behavioral game theory, disentangling reasoning capability from contextual effects. Testing 22 state-of-the-art LLMs, we find that GPT-o3-mini, GPT-o1, and DeepSeek-R1 dominate most games yet also demonstrate that the model scale alone does not determine performance. In terms of prompting enhancement, Chain-of-Thought (CoT) prompting is not universally effective, as it increases strategic reasoning only for models at certain levels while providing limited gains elsewhere. Additionally, we investigate the impact of encoded demographic features on the models, observing that certain assignments impact the decision-making pattern. For instance, GPT-4o shows stronger strategic reasoning with female traits than males, while Gemma assigns higher reasoning levels to heterosexual identities compared to other sexual orientations, indicating inherent biases. These findings underscore the need for ethical standards and contextual alignment to balance improved reasoning with fairness.

**Comment:** This paper evaluates LLMs for strategic reasoning, which does not match any specific criteria but is tangentially relevant to multi-modal learning.
**Relevance:** 3
**Novelty:** 5

---

## 54. [Unsupervised Parameter Efficient Source-free Post-pretraining](https://arxiv.org/abs/2502.21313) <a id="link54"></a>
**ArXiv ID:** 2502.21313
**Authors:** Abhishek Jha, Tinne Tuytelaars, Yuki M. Asano

**Abstract:**  Following the success in NLP, the best vision models are now in the billion parameter ranges. Adapting these large models to a target distribution has become computationally and economically prohibitive. Addressing this challenge, we introduce UpStep, an Unsupervised Parameter-efficient Source-free post-pretraining approach, designed to efficiently adapt a base model from a source domain to a target domain: i) we design a self-supervised training scheme to adapt a pretrained model on an unlabeled target domain in a setting where source domain data is unavailable. Such source-free setting comes with the risk of catastrophic forgetting, hence, ii) we propose center vector regularization (CVR), a set of auxiliary operations that minimize catastrophic forgetting and additionally reduces the computational cost by skipping backpropagation in 50\% of the training iterations. Finally iii) we perform this adaptation process in a parameter-efficient way by adapting the pretrained model through low-rank adaptation methods, resulting in a fraction of parameters to optimize. We utilize various general backbone architectures, both supervised and unsupervised, trained on Imagenet as our base model and adapt them to a diverse set of eight target domains demonstrating the adaptability and generalizability of our proposed approach.

**Comment:** Does not match any specific criteria. Focuses on parameter-efficient adaptation of large vision models.
**Relevance:** 3
**Novelty:** 5

---

## 55. [Two-Stream Spatial-Temporal Transformer Framework for Person Identification via Natural Conversational Keypoints](https://arxiv.org/abs/2502.20803) <a id="link55"></a>
**ArXiv ID:** 2502.20803
**Authors:** Masoumeh Chapariniya, Hossein Ranjbar, Teodora Vukovic, Sarah Ebling, Volker Dellwo

**Abstract:**  In the age of AI-driven generative technologies, traditional biometric recognition systems face unprecedented challenges, particularly from sophisticated deepfake and face reenactment techniques. In this study, we propose a Two-Stream Spatial-Temporal Transformer Framework for person identification using upper body keypoints visible during online conversations, which we term conversational keypoints. Our framework processes both spatial relationships between keypoints and their temporal evolution through two specialized branches: a Spatial Transformer (STR) that learns distinctive structural patterns in keypoint configurations, and a Temporal Transformer (TTR) that captures sequential motion patterns. Using the state-of-the-art Sapiens pose estimator, we extract 133 keypoints (based on COCO-WholeBody format) representing facial features, head pose, and hand positions. The framework was evaluated on a dataset of 114 individuals engaged in natural conversations, achieving recognition accuracies of 80.12% for the spatial stream, 63.61% for the temporal stream. We then explored two fusion strategies: a shared loss function approach achieving 82.22% accuracy, and a feature-level fusion method that concatenates feature maps from both streams, significantly improving performance to 94.86%. By jointly modeling both static anatomical relationships and dynamic movement patterns, our approach learns comprehensive identity signatures that are more robust to spoofing than traditional appearance-based methods.

**Comment:** Does not match any specific criteria. Focuses on person identification using spatial-temporal transformers.
**Relevance:** 3
**Novelty:** 5

---

## 56. [Towards Lossless Implicit Neural Representation via Bit Plane Decomposition](https://arxiv.org/abs/2502.21001) <a id="link56"></a>
**ArXiv ID:** 2502.21001
**Authors:** Woo Kyoung Han, Byeonghun Lee, Hyunmin Cho, Sunghoon Im, Kyong Hwan Jin

**Abstract:**  We quantify the upper bound on the size of the implicit neural representation (INR) model from a digital perspective. The upper bound of the model size increases exponentially as the required bit-precision increases. To this end, we present a bit-plane decomposition method that makes INR predict bit-planes, producing the same effect as reducing the upper bound of the model size. We validate our hypothesis that reducing the upper bound leads to faster convergence with constant model size. Our method achieves lossless representation in 2D image and audio fitting, even for high bit-depth signals, such as 16-bit, which was previously unachievable. We pioneered the presence of bit bias, which INR prioritizes as the most significant bit (MSB). We expand the application of the INR task to bit depth expansion, lossless image compression, and extreme network quantization. Our source code is available at https://github.com/WooKyoungHan/LosslessINR

**Comment:** Does not match any specific criteria. Focuses on implicit neural representation and bit-plane decomposition.
**Relevance:** 3
**Novelty:** 5

---

## 57. [Re-evaluating Theory of Mind evaluation in large language models](https://arxiv.org/abs/2502.21098) <a id="link57"></a>
**ArXiv ID:** 2502.21098
**Authors:** Jennifer Hu, Felix Sosa, Tomer Ullman

**Abstract:**  The question of whether large language models (LLMs) possess Theory of Mind (ToM) -- often defined as the ability to reason about others' mental states -- has sparked significant scientific and public interest. However, the evidence as to whether LLMs possess ToM is mixed, and the recent growth in evaluations has not resulted in a convergence. Here, we take inspiration from cognitive science to re-evaluate the state of ToM evaluation in LLMs. We argue that a major reason for the disagreement on whether LLMs have ToM is a lack of clarity on whether models should be expected to match human behaviors, or the computations underlying those behaviors. We also highlight ways in which current evaluations may be deviating from "pure" measurements of ToM abilities, which also contributes to the confusion. We conclude by discussing several directions for future research, including the relationship between ToM and pragmatic communication, which could advance our understanding of artificial systems as well as human cognition.

**Comment:** This paper does not match any of the specific criteria but discusses Theory of Mind evaluation in LLMs, which is tangentially related to your friend's interest in language models.
**Relevance:** 3
**Novelty:** 4

---

## 58. [Are foundation models useful feature extractors for electroencephalography analysis?](https://arxiv.org/abs/2502.21086) <a id="link58"></a>
**ArXiv ID:** 2502.21086
**Authors:** \"Ozg\"un Turgut, Felix S. Bott, Markus Ploner, Daniel Rueckert

**Abstract:**  The success of foundation models in natural language processing and computer vision has motivated similar approaches for general time series analysis. While these models are effective for a variety of tasks, their applicability in medical domains with limited data remains largely unexplored. To address this, we investigate the effectiveness of foundation models in medical time series analysis involving electroencephalography (EEG). Through extensive experiments on tasks such as age prediction, seizure detection, and the classification of clinically relevant EEG events, we compare their diagnostic accuracy with that of specialised EEG models. Our analysis shows that foundation models extract meaningful EEG features, outperform specialised models even without domain adaptation, and localise task-specific biomarkers. Moreover, we demonstrate that diagnostic accuracy is substantially influenced by architectural choices such as context length. Overall, our study reveals that foundation models with general time series understanding eliminate the dependency on large domain-specific datasets, making them valuable tools for clinical practice.

**Comment:** Does not match any specific criterion but is tangentially related to foundation models in medical time series analysis.
**Relevance:** 3
**Novelty:** 4

---

## 59. [Fast and Accurate Gigapixel Pathological Image Classification with Hierarchical Distillation Multi-Instance Learning](https://arxiv.org/abs/2502.21130) <a id="link59"></a>
**ArXiv ID:** 2502.21130
**Authors:** Jiuyang Dong, Junjun Jiang, Kui Jiang, Jiahan Li, Yongbing Zhang

**Abstract:**  Although multi-instance learning (MIL) has succeeded in pathological image classification, it faces the challenge of high inference costs due to processing numerous patches from gigapixel whole slide images (WSIs). To address this, we propose HDMIL, a hierarchical distillation multi-instance learning framework that achieves fast and accurate classification by eliminating irrelevant patches. HDMIL consists of two key components: the dynamic multi-instance network (DMIN) and the lightweight instance pre-screening network (LIPN). DMIN operates on high-resolution WSIs, while LIPN operates on the corresponding low-resolution counterparts. During training, DMIN are trained for WSI classification while generating attention-score-based masks that indicate irrelevant patches. These masks then guide the training of LIPN to predict the relevance of each low-resolution patch. During testing, LIPN first determines the useful regions within low-resolution WSIs, which indirectly enables us to eliminate irrelevant regions in high-resolution WSIs, thereby reducing inference time without causing performance degradation. In addition, we further design the first Chebyshev-polynomials-based Kolmogorov-Arnold classifier in computational pathology, which enhances the performance of HDMIL through learnable activation layers. Extensive experiments on three public datasets demonstrate that HDMIL outperforms previous state-of-the-art methods, e.g., achieving improvements of 3.13% in AUC while reducing inference time by 28.6% on the Camelyon16 dataset.

**Comment:** Does not match any specific criterion but is tangentially related to computer vision and machine learning in pathology.
**Relevance:** 3
**Novelty:** 4

---

## 60. [Foundation Models -- A Panacea for Artificial Intelligence in Pathology?](https://arxiv.org/abs/2502.21264) <a id="link60"></a>
**ArXiv ID:** 2502.21264
**Authors:** Nita Mulliqi (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden), Anders Blilie (Department of Pathology, Stavanger University Hospital, Stavanger, Norway, Faculty of Health Sciences, University of Stavanger, Stavanger, Norway), Xiaoyi Ji (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden), Kelvin Szolnoky (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden), Henrik Olsson (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden), Sol Erika Boman (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden, Department of Molecular Medicine, Surgery, Karolinska Institutet, Stockholm, Sweden), Matteo Titus (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden), Geraldine Martinez Gonzalez (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden), Julia Anna Mielcarz (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden), Masi Valkonen (Institute of Biomedicine, University of Turku, Turku, Finland), Einar Gudlaugsson (Department of Pathology, Stavanger University Hospital, Stavanger, Norway), Svein R. Kjosavik (The General Practice, Care Coordination Research Group, Stavanger University Hospital, Norway, Department of Global Public Health, Primary Care, Faculty of Medicine, University of Bergen, Norway), Jos\'e Asenjo (Department of Pathology, Synlab, Madrid, Spain), Marcello Gambacorta (Department of Pathology, Synlab, Brescia, Italy), Paolo Libretti (Department of Pathology, Synlab, Brescia, Italy), Marcin Braun (Department of Pathology, Chair of Oncology, Medical University of Lodz, Lodz, Poland), Radzislaw Kordek (Department of Pathology, Chair of Oncology, Medical University of Lodz, Lodz, Poland), Roman {\L}owicki (1st Department of Urology, Medical University of Lodz, Lodz, Poland), Kristina Hotakainen (Department of Clinical Chemistry, Hematology, University of Helsinki, Helsinki, Finland, Laboratory Services, Mehil\"ainen Oy, Helsinki, Finland), P\"aivi V\"are (Department of Pathology, Mehil\"ainen L\"ansi-Pohja Hospital, Kemi, Finland), Bodil Ginnerup Pedersen (Department of Radiology, Aarhus University Hospital, Aarhus, Denmark, Department of Clinical Medicine, Aarhus University, Aarhus, Denmark), Karina Dalsgaard S{\o}rensen (Department of Clinical Medicine, Aarhus University, Aarhus, Denmark, Department of Molecular Medicine, Aarhus University Hospital, Aarhus, Denmark), Benedicte Parm Ulh{\o}i (Department of Pathology, Aarhus University Hospital, Aarhus, Denmark), Pekka Ruusuvuori (Institute of Biomedicine, University of Turku, Turku, Finland, InFLAMES Research Flagship, University of Turku, Turku, Finland, Faculty of Medicine, Health Technology, Tampere University, Tampere, Finland), Brett Delahunt (Malaghan Institute of Medical Research, Wellington, New Zealand, Department of Oncology, Pathology, Karolinska Institutet, Stockholm, Sweden), Hemamali Samaratunga (Aquesta Uropathology, University of Queensland, QLD, Brisbane, Australia), Toyonori Tsuzuki (Department of Surgical Pathology, School of Medicine, Aichi Medical University, Nagoya, Japan), Emilius A. M. Janssen (Department of Pathology, Stavanger University Hospital, Stavanger, Norway, Department of Chemistry, Bioscience, Environmental Engineering, University of Stavanger, Stavanger, Norway, Institute for Biomedicine, Glycomics, Griffith University, Queensland, Australia), Lars Egevad (Department of Oncology, Pathology, Karolinska Institutet, Stockholm, Sweden), Martin Eklund (Department of Medical Epidemiology, Biostatistics, Karolinska Institutet, Stockholm, Sweden), Kimmo Kartasalo (Department of Medical Epidemiology, Biostatistics, SciLifeLab, Karolinska Institutet, Stockholm, Sweden)

**Abstract:**  The role of artificial intelligence (AI) in pathology has evolved from aiding diagnostics to uncovering predictive morphological patterns in whole slide images (WSIs). Recently, foundation models (FMs) leveraging self-supervised pre-training have been widely advocated as a universal solution for diverse downstream tasks. However, open questions remain about their clinical applicability and generalization advantages over end-to-end learning using task-specific (TS) models. Here, we focused on AI with clinical-grade performance for prostate cancer diagnosis and Gleason grading. We present the largest validation of AI for this task, using over 100,000 core needle biopsies from 7,342 patients across 15 sites in 11 countries. We compared two FMs with a fully end-to-end TS model in a multiple instance learning framework. Our findings challenge assumptions that FMs universally outperform TS models. While FMs demonstrated utility in data-scarce scenarios, their performance converged with - and was in some cases surpassed by - TS models when sufficient labeled training data were available. Notably, extensive task-specific training markedly reduced clinically significant misgrading, misdiagnosis of challenging morphologies, and variability across different WSI scanners. Additionally, FMs used up to 35 times more energy than the TS model, raising concerns about their sustainability. Our results underscore that while FMs offer clear advantages for rapid prototyping and research, their role as a universal solution for clinically applicable medical AI remains uncertain. For high-stakes clinical applications, rigorous validation and consideration of task-specific training remain critically important. We advocate for integrating the strengths of FMs and end-to-end learning to achieve robust and resource-efficient AI pathology solutions fit for clinical use.

**Comment:** Does not match any specific criterion but is tangentially related to vision foundation models in medical applications.
**Relevance:** 3
**Novelty:** 4

---

## 61. [Adaptive Illumination-Invariant Synergistic Feature Integration in a Stratified Granular Framework for Visible-Infrared Re-Identification](https://arxiv.org/abs/2502.21163) <a id="link61"></a>
**ArXiv ID:** 2502.21163
**Authors:** Yuheng Jia, Wesley Armour

**Abstract:**  Visible-Infrared Person Re-Identification (VI-ReID) plays a crucial role in applications such as search and rescue, infrastructure protection, and nighttime surveillance. However, it faces significant challenges due to modality discrepancies, varying illumination, and frequent occlusions. To overcome these obstacles, we propose \textbf{AMINet}, an Adaptive Modality Interaction Network. AMINet employs multi-granularity feature extraction to capture comprehensive identity attributes from both full-body and upper-body images, improving robustness against occlusions and background clutter. The model integrates an interactive feature fusion strategy for deep intra-modal and cross-modal alignment, enhancing generalization and effectively bridging the RGB-IR modality gap. Furthermore, AMINet utilizes phase congruency for robust, illumination-invariant feature extraction and incorporates an adaptive multi-scale kernel MMD to align feature distributions across varying scales. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach, achieving a Rank-1 accuracy of $74.75\%$ on SYSU-MM01, surpassing the baseline by $7.93\%$ and outperforming the current state-of-the-art by $3.95\%$.

**Comment:** This paper proposes a model for visible-infrared person re-identification, which is not directly related to the criteria but involves vision applications.
**Relevance:** 3
**Novelty:** 4

---

## 62. [Real-Time Aerial Fire Detection on Resource-Constrained Devices Using Knowledge Distillation](https://arxiv.org/abs/2502.20979) <a id="link62"></a>
**ArXiv ID:** 2502.20979
**Authors:** Sabina Jangirova, Branislava Jankovic, Waseem Ullah, Latif U. Khan, Mohsen Guizani

**Abstract:**  Wildfire catastrophes cause significant environmental degradation, human losses, and financial damage. To mitigate these severe impacts, early fire detection and warning systems are crucial. Current systems rely primarily on fixed CCTV cameras with a limited field of view, restricting their effectiveness in large outdoor environments. The fusion of intelligent fire detection with remote sensing improves coverage and mobility, enabling monitoring in remote and challenging areas. Existing approaches predominantly utilize convolutional neural networks and vision transformer models. While these architectures provide high accuracy in fire detection, their computational complexity limits real-time performance on edge devices such as UAVs. In our work, we present a lightweight fire detection model based on MobileViT-S, compressed through the distillation of knowledge from a stronger teacher model. The ablation study highlights the impact of a teacher model and the chosen distillation technique on the model's performance improvement. We generate activation map visualizations using Grad-CAM to confirm the model's ability to focus on relevant fire regions. The high accuracy and efficiency of the proposed model make it well-suited for deployment on satellites, UAVs, and IoT devices for effective fire detection. Experiments on common fire benchmarks demonstrate that our model suppresses the state-of-the-art model by 0.44%, 2.00% while maintaining a compact model size. Our model delivers the highest processing speed among existing works, achieving real-time performance on resource-constrained devices.

**Comment:** This paper focuses on lightweight fire detection models for UAVs, which is not directly related to the criteria but involves embodied AI applications.
**Relevance:** 3
**Novelty:** 4

---

## 63. [Diffusion Restoration Adapter for Real-World Image Restoration](https://arxiv.org/abs/2502.20679) <a id="link63"></a>
**ArXiv ID:** 2502.20679
**Authors:** Hanbang Liang, Zhen Wang, Weihui Deng

**Abstract:**  Diffusion models have demonstrated their powerful image generation capabilities, effectively fitting highly complex image distributions. These models can serve as strong priors for image restoration. Existing methods often utilize techniques like ControlNet to sample high quality images with low quality images from these priors. However, ControlNet typically involves copying a large part of the original network, resulting in a significantly large number of parameters as the prior scales up. In this paper, we propose a relatively lightweight Adapter that leverages the powerful generative capabilities of pretrained priors to achieve photo-realistic image restoration. The Adapters can be adapt to both denoising UNet and DiT, and performs excellent.

**Comment:** This paper focuses on image restoration using diffusion models, which does not match any of the specific criteria but is tangentially related to generative modeling.
**Relevance:** 3
**Novelty:** 4

---

## 64. [Less is More? Revisiting the Importance of Frame Rate in Real-Time Zero-Shot Surgical Video Segmentation](https://arxiv.org/abs/2502.20934) <a id="link64"></a>
**ArXiv ID:** 2502.20934
**Authors:** Utku Ozbulak, Seyed Amir Mousavi, Francesca Tozzi, Nikdokht Rashidian, Wouter Willaert, Wesley De Neve, Joris Vankerschaver

**Abstract:**  Real-time video segmentation is a promising feature for AI-assisted surgery, providing intraoperative guidance by identifying surgical tools and anatomical structures. However, deploying state-of-the-art segmentation models, such as SAM2, in real-time settings is computationally demanding, which makes it essential to balance frame rate and segmentation performance. In this study, we investigate the impact of frame rate on zero-shot surgical video segmentation, evaluating SAM2's effectiveness across multiple frame sampling rates for cholecystectomy procedures. Surprisingly, our findings indicate that in conventional evaluation settings, frame rates as low as a single frame per second can outperform 25 FPS, as fewer frames smooth out segmentation inconsistencies. However, when assessed in a real-time streaming scenario, higher frame rates yield superior temporal coherence and stability, particularly for dynamic objects such as surgical graspers. Finally, we investigate human perception of real-time surgical video segmentation among professionals who work closely with such data and find that respondents consistently prefer high FPS segmentation mask overlays, reinforcing the importance of real-time evaluation in AI-assisted surgery.

**Comment:** Does not match any specific criteria. Focuses on frame rate impact in surgical video segmentation, which is not directly related to the specified areas.
**Relevance:** 3
**Novelty:** 4

---

## 65. [Distribution Prototype Diffusion Learning for Open-set Supervised Anomaly Detection](https://arxiv.org/abs/2502.20981) <a id="link65"></a>
**ArXiv ID:** 2502.20981
**Authors:** Fuyun Wang, Tong Zhang, Yuanzhi Wang, Yide Qiu, Xin Liu, Xu Guo, Zhen Cui

**Abstract:**  In Open-set Supervised Anomaly Detection (OSAD), the existing methods typically generate pseudo anomalies to compensate for the scarcity of observed anomaly samples, while overlooking critical priors of normal samples, leading to less effective discriminative boundaries. To address this issue, we propose a Distribution Prototype Diffusion Learning (DPDL) method aimed at enclosing normal samples within a compact and discriminative distribution space. Specifically, we construct multiple learnable Gaussian prototypes to create a latent representation space for abundant and diverse normal samples and learn a Schr\"odinger bridge to facilitate a diffusive transition toward these prototypes for normal samples while steering anomaly samples away. Moreover, to enhance inter-sample separation, we design a dispersion feature learning way in hyperspherical space, which benefits the identification of out-of-distribution anomalies. Experimental results demonstrate the effectiveness and superiority of our proposed DPDL, achieving state-of-the-art performance on 9 public datasets.

**Comment:** This paper proposes a method for anomaly detection, which does not match any specific criteria but is tangentially relevant to machine learning.
**Relevance:** 3
**Novelty:** 4

---

## 66. [VRM: Knowledge Distillation via Virtual Relation Matching](https://arxiv.org/abs/2502.20760) <a id="link66"></a>
**ArXiv ID:** 2502.20760
**Authors:** Weijia Zhang, Fei Xie, Weidong Cai, Chao Ma

**Abstract:**  Knowledge distillation (KD) aims to transfer the knowledge of a more capable yet cumbersome teacher model to a lightweight student model. In recent years, relation-based KD methods have fallen behind, as their instance-matching counterparts dominate in performance. In this paper, we revive relational KD by identifying and tackling several key issues in relation-based methods, including their susceptibility to overfitting and spurious responses. Specifically, we transfer novelly constructed affinity graphs that compactly encapsulate a wealth of beneficial inter-sample, inter-class, and inter-view correlations by exploiting virtual views and relations as a new kind of knowledge. As a result, the student has access to richer guidance signals and stronger regularisation throughout the distillation process. To further mitigate the adverse impact of spurious responses, we prune the affinity graphs by dynamically detaching redundant and unreliable edges. Extensive experiments on CIFAR-100 and ImageNet datasets demonstrate the superior performance of the proposed virtual relation matching (VRM) method over a range of models, architectures, and set-ups. For instance, VRM for the first time hits 74.0% accuracy for ResNet50-to-MobileNetV2 distillation on ImageNet, and improves DeiT-T by 14.44% on CIFAR-100 with a ResNet56 teacher. Thorough analyses are also conducted to gauge the soundness, properties, and complexity of our designs. Code and models will be released.

**Comment:** This paper focuses on knowledge distillation and does not match any specific criteria but is tangentially relevant to machine learning.
**Relevance:** 3
**Novelty:** 4

---

## 67. [The Common Objects Underwater (COU) Dataset for Robust Underwater Object Detection](https://arxiv.org/abs/2502.20651) <a id="link67"></a>
**ArXiv ID:** 2502.20651
**Authors:** Rishi Mukherjee, Sakshi Singh, Jack McWilliams, Junaed Sattar

**Abstract:**  We introduce COU: Common Objects Underwater, an instance-segmented image dataset of commonly found man-made objects in multiple aquatic and marine environments. COU contains approximately 10K segmented images, annotated from images collected during a number of underwater robot field trials in diverse locations. COU has been created to address the lack of datasets with robust class coverage curated for underwater instance segmentation, which is particularly useful for training light-weight, real-time capable detectors for Autonomous Underwater Vehicles (AUVs). In addition, COU addresses the lack of diversity in object classes since the commonly available underwater image datasets focus only on marine life. Currently, COU contains images from both closed-water (pool) and open-water (lakes and oceans) environments, of 24 different classes of objects including marine debris, dive tools, and AUVs. To assess the efficacy of COU in training underwater object detectors, we use three state-of-the-art models to evaluate its performance and accuracy, using a combination of standard accuracy and efficiency metrics. The improved performance of COU-trained detectors over those solely trained on terrestrial data demonstrates the clear advantage of training with annotated underwater images. We make COU available for broad use under open-source licenses.

**Comment:** This paper introduces a new dataset for underwater object detection, which does not match any of the specific criteria but is relevant to computer vision.
**Relevance:** 3
**Novelty:** 4

---

## 68. [Towards long-term player tracking with graph hierarchies and domain-specific features](https://arxiv.org/abs/2502.21242) <a id="link68"></a>
**ArXiv ID:** 2502.21242
**Authors:** Maria Koshkina, James H. Elder

**Abstract:**  In team sports analytics, long-term player tracking remains a challenging task due to player appearance similarity, occlusion, and dynamic motion patterns. Accurately re-identifying players and reconnecting tracklets after extended absences from the field of view or prolonged occlusions is crucial for robust analysis. We introduce SportsSUSHI, a hierarchical graph-based approach that leverages domain-specific features, including jersey numbers, team IDs, and field coordinates, to enhance tracking accuracy. SportsSUSHI achieves high performance on the SoccerNet dataset and a newly proposed hockey tracking dataset. Our hockey dataset, recorded using a stationary camera capturing the entire playing surface, contains long sequences and annotations for team IDs and jersey numbers, making it well-suited for evaluating long-term tracking capabilities. The inclusion of domain-specific features in our approach significantly improves association accuracy, as demonstrated in our experiments. The dataset and code are available at https://github.com/mkoshkina/sports-SUSHI.

**Comment:** Does not match any specific criteria. Focuses on player tracking in sports analytics.
**Relevance:** 3
**Novelty:** 4

---


---

## Paper selection prompt
 1. New methodological improvements to spatial understanding, spatial intelligence on embodied agents;
 2. Shows new VLLMs (visual large language models) or MLLMs (multi-modal large language models)
 3. Embodied AI papers on buliding new benchmark (simulator related) or new methods. These papers should focus on novel angles that previous work ignored.
 4. Vision foundation models related and its applications.

 In suggesting papers to your friend, remember that he enjoys papers on computer vision and machine learning, and generative modeling in multi-modal learning.
 Your friend also likes learning about surprising empirical or insightful results in vision-language models or embodied AI, as well as clever statistical tricks.